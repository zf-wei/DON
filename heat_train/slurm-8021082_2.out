The dimension of y_tensor is torch.Size([5000, 2]).
The dimension of y_expanded is torch.Size([500, 5000, 2]) after expanding.
The dimensions of the initial conditions are: (500, 50)
The dimensions of the solutions are: (500, 100, 50)
The dimension of u_tensor is torch.Size([500, 50]).
The dimension of u_expanded is torch.Size([500, 5000, 50]) after expanding.
torch.Size([500, 5000, 1])
The loaded solution dataset has dimension (500, 100, 50),
	 while the arranged linearized dataset has dimension (500, 5000).
The dimension of s_tensor is torch.Size([500, 5000]).
The dimension of s_expanded is torch.Size([500, 5000, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.015631350.
A best model at epoch 1 has been saved with training error 0.014140409.
A best model at epoch 1 has been saved with training error 0.010188307.
A best model at epoch 1 has been saved with training error 0.009438002.
Epoch 1, Loss: 0.015454936, Improvement: 0.015454936, Best Loss: 0.009438002 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.008022442.
A best model at epoch 2 has been saved with training error 0.007677448.
A best model at epoch 2 has been saved with training error 0.007423675.
Epoch 2, Loss: 0.010686625, Improvement: -0.004768312, Best Loss: 0.007423675 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.006237859.
Epoch 3, Loss: 0.010061310, Improvement: -0.000625315, Best Loss: 0.006237859 in Epoch 3
Epoch 4
Epoch 4, Loss: 0.009742503, Improvement: -0.000318807, Best Loss: 0.006237859 in Epoch 3
Epoch 5
Epoch 5, Loss: 0.009371292, Improvement: -0.000371211, Best Loss: 0.006237859 in Epoch 3
Epoch 6
A best model at epoch 6 has been saved with training error 0.004988042.
Epoch 6, Loss: 0.008795324, Improvement: -0.000575968, Best Loss: 0.004988042 in Epoch 6
Epoch 7
Epoch 7, Loss: 0.008040886, Improvement: -0.000754438, Best Loss: 0.004988042 in Epoch 6
Epoch 8
A best model at epoch 8 has been saved with training error 0.004844711.
A best model at epoch 8 has been saved with training error 0.004069557.
Epoch 8, Loss: 0.007951561, Improvement: -0.000089325, Best Loss: 0.004069557 in Epoch 8
Epoch 9
Epoch 9, Loss: 0.006923966, Improvement: -0.001027595, Best Loss: 0.004069557 in Epoch 8
Epoch 10
A best model at epoch 10 has been saved with training error 0.003667992.
A best model at epoch 10 has been saved with training error 0.003051501.
Epoch 10, Loss: 0.005661383, Improvement: -0.001262583, Best Loss: 0.003051501 in Epoch 10
Epoch 11
A best model at epoch 11 has been saved with training error 0.002784317.
Epoch 11, Loss: 0.005152913, Improvement: -0.000508470, Best Loss: 0.002784317 in Epoch 11
Epoch 12
Epoch 12, Loss: 0.004876620, Improvement: -0.000276292, Best Loss: 0.002784317 in Epoch 11
Epoch 13
Epoch 13, Loss: 0.005695298, Improvement: 0.000818677, Best Loss: 0.002784317 in Epoch 11
Epoch 14
Epoch 14, Loss: 0.005461694, Improvement: -0.000233604, Best Loss: 0.002784317 in Epoch 11
Epoch 15
Epoch 15, Loss: 0.004794383, Improvement: -0.000667310, Best Loss: 0.002784317 in Epoch 11
Epoch 16
A best model at epoch 16 has been saved with training error 0.002602907.
Epoch 16, Loss: 0.004454778, Improvement: -0.000339605, Best Loss: 0.002602907 in Epoch 16
Epoch 17
Epoch 17, Loss: 0.004322814, Improvement: -0.000131964, Best Loss: 0.002602907 in Epoch 16
Epoch 18
Epoch 18, Loss: 0.004356513, Improvement: 0.000033698, Best Loss: 0.002602907 in Epoch 16
Epoch 19
A best model at epoch 19 has been saved with training error 0.002205890.
Epoch 19, Loss: 0.004281754, Improvement: -0.000074759, Best Loss: 0.002205890 in Epoch 19
Epoch 20
Epoch 20, Loss: 0.004167075, Improvement: -0.000114679, Best Loss: 0.002205890 in Epoch 19
Epoch 21
Epoch 21, Loss: 0.004083252, Improvement: -0.000083822, Best Loss: 0.002205890 in Epoch 19
Epoch 22
Epoch 22, Loss: 0.003878026, Improvement: -0.000205226, Best Loss: 0.002205890 in Epoch 19
Epoch 23
Epoch 23, Loss: 0.003781416, Improvement: -0.000096610, Best Loss: 0.002205890 in Epoch 19
Epoch 24
A best model at epoch 24 has been saved with training error 0.002072692.
Epoch 24, Loss: 0.004245709, Improvement: 0.000464293, Best Loss: 0.002072692 in Epoch 24
Epoch 25
Epoch 25, Loss: 0.003752155, Improvement: -0.000493554, Best Loss: 0.002072692 in Epoch 24
Epoch 26
Epoch 26, Loss: 0.003521161, Improvement: -0.000230994, Best Loss: 0.002072692 in Epoch 24
Epoch 27
Epoch 27, Loss: 0.003521008, Improvement: -0.000000153, Best Loss: 0.002072692 in Epoch 24
Epoch 28
Epoch 28, Loss: 0.004099213, Improvement: 0.000578205, Best Loss: 0.002072692 in Epoch 24
Epoch 29
Epoch 29, Loss: 0.003683046, Improvement: -0.000416167, Best Loss: 0.002072692 in Epoch 24
Epoch 30
Epoch 30, Loss: 0.003224461, Improvement: -0.000458585, Best Loss: 0.002072692 in Epoch 24
Epoch 31
A best model at epoch 31 has been saved with training error 0.002029321.
Epoch 31, Loss: 0.003048027, Improvement: -0.000176434, Best Loss: 0.002029321 in Epoch 31
Epoch 32
A best model at epoch 32 has been saved with training error 0.001986605.
Epoch 32, Loss: 0.002918590, Improvement: -0.000129436, Best Loss: 0.001986605 in Epoch 32
Epoch 33
Epoch 33, Loss: 0.004634294, Improvement: 0.001715704, Best Loss: 0.001986605 in Epoch 32
Epoch 34
Epoch 34, Loss: 0.003500375, Improvement: -0.001133919, Best Loss: 0.001986605 in Epoch 32
Epoch 35
A best model at epoch 35 has been saved with training error 0.001907856.
Epoch 35, Loss: 0.003001842, Improvement: -0.000498533, Best Loss: 0.001907856 in Epoch 35
Epoch 36
A best model at epoch 36 has been saved with training error 0.001609303.
Epoch 36, Loss: 0.002666867, Improvement: -0.000334975, Best Loss: 0.001609303 in Epoch 36
Epoch 37
A best model at epoch 37 has been saved with training error 0.001081555.
Epoch 37, Loss: 0.002437071, Improvement: -0.000229796, Best Loss: 0.001081555 in Epoch 37
Epoch 38
Epoch 38, Loss: 0.002438063, Improvement: 0.000000992, Best Loss: 0.001081555 in Epoch 37
Epoch 39
Epoch 39, Loss: 0.003503834, Improvement: 0.001065771, Best Loss: 0.001081555 in Epoch 37
Epoch 40
Epoch 40, Loss: 0.002692878, Improvement: -0.000810956, Best Loss: 0.001081555 in Epoch 37
Epoch 41
Epoch 41, Loss: 0.002575960, Improvement: -0.000116918, Best Loss: 0.001081555 in Epoch 37
Epoch 42
Epoch 42, Loss: 0.002312687, Improvement: -0.000263273, Best Loss: 0.001081555 in Epoch 37
Epoch 43
Epoch 43, Loss: 0.002116504, Improvement: -0.000196184, Best Loss: 0.001081555 in Epoch 37
Epoch 44
Epoch 44, Loss: 0.002097562, Improvement: -0.000018941, Best Loss: 0.001081555 in Epoch 37
Epoch 45
Epoch 45, Loss: 0.002880835, Improvement: 0.000783273, Best Loss: 0.001081555 in Epoch 37
Epoch 46
Epoch 46, Loss: 0.002582356, Improvement: -0.000298479, Best Loss: 0.001081555 in Epoch 37
Epoch 47
Epoch 47, Loss: 0.002248328, Improvement: -0.000334028, Best Loss: 0.001081555 in Epoch 37
Epoch 48
Epoch 48, Loss: 0.002164503, Improvement: -0.000083825, Best Loss: 0.001081555 in Epoch 37
Epoch 49
Epoch 49, Loss: 0.001868485, Improvement: -0.000296019, Best Loss: 0.001081555 in Epoch 37
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.002329822, Improvement: 0.000461338, Best Loss: 0.001081555 in Epoch 37
Epoch 51
Epoch 51, Loss: 0.002934664, Improvement: 0.000604842, Best Loss: 0.001081555 in Epoch 37
Epoch 52
A best model at epoch 52 has been saved with training error 0.001050266.
Epoch 52, Loss: 0.002171820, Improvement: -0.000762844, Best Loss: 0.001050266 in Epoch 52
Epoch 53
A best model at epoch 53 has been saved with training error 0.001017908.
A best model at epoch 53 has been saved with training error 0.000960489.
Epoch 53, Loss: 0.001947928, Improvement: -0.000223892, Best Loss: 0.000960489 in Epoch 53
Epoch 54
Epoch 54, Loss: 0.001863251, Improvement: -0.000084677, Best Loss: 0.000960489 in Epoch 53
Epoch 55
Epoch 55, Loss: 0.001758894, Improvement: -0.000104356, Best Loss: 0.000960489 in Epoch 53
Epoch 56
Epoch 56, Loss: 0.002004059, Improvement: 0.000245164, Best Loss: 0.000960489 in Epoch 53
Epoch 57
Epoch 57, Loss: 0.004306431, Improvement: 0.002302372, Best Loss: 0.000960489 in Epoch 53
Epoch 58
Epoch 58, Loss: 0.003516693, Improvement: -0.000789738, Best Loss: 0.000960489 in Epoch 53
Epoch 59
Epoch 59, Loss: 0.002675933, Improvement: -0.000840760, Best Loss: 0.000960489 in Epoch 53
Epoch 60
Epoch 60, Loss: 0.002162502, Improvement: -0.000513430, Best Loss: 0.000960489 in Epoch 53
Epoch 61
Epoch 61, Loss: 0.001858784, Improvement: -0.000303719, Best Loss: 0.000960489 in Epoch 53
Epoch 62
Epoch 62, Loss: 0.001687008, Improvement: -0.000171776, Best Loss: 0.000960489 in Epoch 53
Epoch 63
Epoch 63, Loss: 0.001616576, Improvement: -0.000070432, Best Loss: 0.000960489 in Epoch 53
Epoch 64
A best model at epoch 64 has been saved with training error 0.000920015.
Epoch 64, Loss: 0.001591617, Improvement: -0.000024958, Best Loss: 0.000920015 in Epoch 64
Epoch 65
Epoch 65, Loss: 0.001541569, Improvement: -0.000050049, Best Loss: 0.000920015 in Epoch 64
Epoch 66
Epoch 66, Loss: 0.001668279, Improvement: 0.000126710, Best Loss: 0.000920015 in Epoch 64
Epoch 67
Epoch 67, Loss: 0.001877071, Improvement: 0.000208792, Best Loss: 0.000920015 in Epoch 64
Epoch 68
Epoch 68, Loss: 0.001751096, Improvement: -0.000125974, Best Loss: 0.000920015 in Epoch 64
Epoch 69
Epoch 69, Loss: 0.001711482, Improvement: -0.000039615, Best Loss: 0.000920015 in Epoch 64
Epoch 70
Epoch 70, Loss: 0.001712043, Improvement: 0.000000561, Best Loss: 0.000920015 in Epoch 64
Epoch 71
Epoch 71, Loss: 0.001545784, Improvement: -0.000166259, Best Loss: 0.000920015 in Epoch 64
Epoch 72
Epoch 72, Loss: 0.001417875, Improvement: -0.000127909, Best Loss: 0.000920015 in Epoch 64
Epoch 73
A best model at epoch 73 has been saved with training error 0.000887554.
A best model at epoch 73 has been saved with training error 0.000872064.
Epoch 73, Loss: 0.001378930, Improvement: -0.000038945, Best Loss: 0.000872064 in Epoch 73
Epoch 74
Epoch 74, Loss: 0.001402392, Improvement: 0.000023462, Best Loss: 0.000872064 in Epoch 73
Epoch 75
Epoch 75, Loss: 0.001825837, Improvement: 0.000423445, Best Loss: 0.000872064 in Epoch 73
Epoch 76
Epoch 76, Loss: 0.001657012, Improvement: -0.000168825, Best Loss: 0.000872064 in Epoch 73
Epoch 77
Epoch 77, Loss: 0.001563868, Improvement: -0.000093144, Best Loss: 0.000872064 in Epoch 73
Epoch 78
Epoch 78, Loss: 0.001362698, Improvement: -0.000201170, Best Loss: 0.000872064 in Epoch 73
Epoch 79
A best model at epoch 79 has been saved with training error 0.000867717.
A best model at epoch 79 has been saved with training error 0.000791920.
Epoch 79, Loss: 0.001244646, Improvement: -0.000118052, Best Loss: 0.000791920 in Epoch 79
Epoch 80
Epoch 80, Loss: 0.002012052, Improvement: 0.000767406, Best Loss: 0.000791920 in Epoch 79
Epoch 81
Epoch 81, Loss: 0.002919164, Improvement: 0.000907112, Best Loss: 0.000791920 in Epoch 79
Epoch 82
Epoch 82, Loss: 0.001752729, Improvement: -0.001166435, Best Loss: 0.000791920 in Epoch 79
Epoch 83
Epoch 83, Loss: 0.001443259, Improvement: -0.000309471, Best Loss: 0.000791920 in Epoch 79
Epoch 84
A best model at epoch 84 has been saved with training error 0.000747514.
Epoch 84, Loss: 0.001253946, Improvement: -0.000189312, Best Loss: 0.000747514 in Epoch 84
Epoch 85
A best model at epoch 85 has been saved with training error 0.000733572.
Epoch 85, Loss: 0.001200570, Improvement: -0.000053377, Best Loss: 0.000733572 in Epoch 85
Epoch 86
Epoch 86, Loss: 0.001158227, Improvement: -0.000042343, Best Loss: 0.000733572 in Epoch 85
Epoch 87
Epoch 87, Loss: 0.001131886, Improvement: -0.000026341, Best Loss: 0.000733572 in Epoch 85
Epoch 88
Epoch 88, Loss: 0.001859425, Improvement: 0.000727539, Best Loss: 0.000733572 in Epoch 85
Epoch 89
Epoch 89, Loss: 0.002103316, Improvement: 0.000243891, Best Loss: 0.000733572 in Epoch 85
Epoch 90
Epoch 90, Loss: 0.001506165, Improvement: -0.000597152, Best Loss: 0.000733572 in Epoch 85
Epoch 91
A best model at epoch 91 has been saved with training error 0.000661162.
Epoch 91, Loss: 0.001192856, Improvement: -0.000313308, Best Loss: 0.000661162 in Epoch 91
Epoch 92
A best model at epoch 92 has been saved with training error 0.000535193.
Epoch 92, Loss: 0.001071847, Improvement: -0.000121009, Best Loss: 0.000535193 in Epoch 92
Epoch 93
Epoch 93, Loss: 0.001060888, Improvement: -0.000010959, Best Loss: 0.000535193 in Epoch 92
Epoch 94
Epoch 94, Loss: 0.002106346, Improvement: 0.001045458, Best Loss: 0.000535193 in Epoch 92
Epoch 95
Epoch 95, Loss: 0.001239750, Improvement: -0.000866597, Best Loss: 0.000535193 in Epoch 92
Epoch 96
Epoch 96, Loss: 0.001090205, Improvement: -0.000149545, Best Loss: 0.000535193 in Epoch 92
Epoch 97
Epoch 97, Loss: 0.001044857, Improvement: -0.000045348, Best Loss: 0.000535193 in Epoch 92
Epoch 98
Epoch 98, Loss: 0.000967312, Improvement: -0.000077545, Best Loss: 0.000535193 in Epoch 92
Epoch 99
Epoch 99, Loss: 0.000927502, Improvement: -0.000039810, Best Loss: 0.000535193 in Epoch 92
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.000893618, Improvement: -0.000033885, Best Loss: 0.000535193 in Epoch 92
Epoch 101
Epoch 101, Loss: 0.000883537, Improvement: -0.000010081, Best Loss: 0.000535193 in Epoch 92
Epoch 102
A best model at epoch 102 has been saved with training error 0.000527370.
Epoch 102, Loss: 0.000860528, Improvement: -0.000023009, Best Loss: 0.000527370 in Epoch 102
Epoch 103
Epoch 103, Loss: 0.000907325, Improvement: 0.000046797, Best Loss: 0.000527370 in Epoch 102
Epoch 104
Epoch 104, Loss: 0.001430593, Improvement: 0.000523268, Best Loss: 0.000527370 in Epoch 102
Epoch 105
Epoch 105, Loss: 0.001543277, Improvement: 0.000112683, Best Loss: 0.000527370 in Epoch 102
Epoch 106
Epoch 106, Loss: 0.001416261, Improvement: -0.000127016, Best Loss: 0.000527370 in Epoch 102
Epoch 107
Epoch 107, Loss: 0.001026131, Improvement: -0.000390129, Best Loss: 0.000527370 in Epoch 102
Epoch 108
Epoch 108, Loss: 0.000938576, Improvement: -0.000087556, Best Loss: 0.000527370 in Epoch 102
Epoch 109
A best model at epoch 109 has been saved with training error 0.000477685.
Epoch 109, Loss: 0.000914787, Improvement: -0.000023789, Best Loss: 0.000477685 in Epoch 109
Epoch 110
Epoch 110, Loss: 0.000897735, Improvement: -0.000017052, Best Loss: 0.000477685 in Epoch 109
Epoch 111
Epoch 111, Loss: 0.000880374, Improvement: -0.000017361, Best Loss: 0.000477685 in Epoch 109
Epoch 112
Epoch 112, Loss: 0.000825349, Improvement: -0.000055025, Best Loss: 0.000477685 in Epoch 109
Epoch 113
A best model at epoch 113 has been saved with training error 0.000461649.
Epoch 113, Loss: 0.000897107, Improvement: 0.000071758, Best Loss: 0.000461649 in Epoch 113
Epoch 114
Epoch 114, Loss: 0.000789664, Improvement: -0.000107443, Best Loss: 0.000461649 in Epoch 113
Epoch 115
Epoch 115, Loss: 0.000884461, Improvement: 0.000094797, Best Loss: 0.000461649 in Epoch 113
Epoch 116
Epoch 116, Loss: 0.000947013, Improvement: 0.000062552, Best Loss: 0.000461649 in Epoch 113
Epoch 117
Epoch 117, Loss: 0.001403458, Improvement: 0.000456445, Best Loss: 0.000461649 in Epoch 113
Epoch 118
Epoch 118, Loss: 0.001425192, Improvement: 0.000021734, Best Loss: 0.000461649 in Epoch 113
Epoch 119
Epoch 119, Loss: 0.001107604, Improvement: -0.000317588, Best Loss: 0.000461649 in Epoch 113
Epoch 120
Epoch 120, Loss: 0.001093513, Improvement: -0.000014091, Best Loss: 0.000461649 in Epoch 113
Epoch 121
Epoch 121, Loss: 0.000852291, Improvement: -0.000241222, Best Loss: 0.000461649 in Epoch 113
Epoch 122
A best model at epoch 122 has been saved with training error 0.000448064.
Epoch 122, Loss: 0.000769403, Improvement: -0.000082888, Best Loss: 0.000448064 in Epoch 122
Epoch 123
A best model at epoch 123 has been saved with training error 0.000429666.
Epoch 123, Loss: 0.000748211, Improvement: -0.000021192, Best Loss: 0.000429666 in Epoch 123
Epoch 124
A best model at epoch 124 has been saved with training error 0.000423162.
Epoch 124, Loss: 0.000766968, Improvement: 0.000018757, Best Loss: 0.000423162 in Epoch 124
Epoch 125
Epoch 125, Loss: 0.000822506, Improvement: 0.000055538, Best Loss: 0.000423162 in Epoch 124
Epoch 126
Epoch 126, Loss: 0.001207360, Improvement: 0.000384854, Best Loss: 0.000423162 in Epoch 124
Epoch 127
Epoch 127, Loss: 0.001067866, Improvement: -0.000139494, Best Loss: 0.000423162 in Epoch 124
Epoch 128
Epoch 128, Loss: 0.000917987, Improvement: -0.000149880, Best Loss: 0.000423162 in Epoch 124
Epoch 129
Epoch 129, Loss: 0.000828369, Improvement: -0.000089617, Best Loss: 0.000423162 in Epoch 124
Epoch 130
Epoch 130, Loss: 0.000766993, Improvement: -0.000061376, Best Loss: 0.000423162 in Epoch 124
Epoch 131
A best model at epoch 131 has been saved with training error 0.000325147.
Epoch 131, Loss: 0.000679418, Improvement: -0.000087575, Best Loss: 0.000325147 in Epoch 131
Epoch 132
Epoch 132, Loss: 0.000737191, Improvement: 0.000057773, Best Loss: 0.000325147 in Epoch 131
Epoch 133
Epoch 133, Loss: 0.000986263, Improvement: 0.000249072, Best Loss: 0.000325147 in Epoch 131
Epoch 134
Epoch 134, Loss: 0.000984350, Improvement: -0.000001913, Best Loss: 0.000325147 in Epoch 131
Epoch 135
Epoch 135, Loss: 0.000902106, Improvement: -0.000082244, Best Loss: 0.000325147 in Epoch 131
Epoch 136
Epoch 136, Loss: 0.000901187, Improvement: -0.000000919, Best Loss: 0.000325147 in Epoch 131
Epoch 137
Epoch 137, Loss: 0.000757980, Improvement: -0.000143207, Best Loss: 0.000325147 in Epoch 131
Epoch 138
Epoch 138, Loss: 0.000686607, Improvement: -0.000071373, Best Loss: 0.000325147 in Epoch 131
Epoch 139
Epoch 139, Loss: 0.000742546, Improvement: 0.000055939, Best Loss: 0.000325147 in Epoch 131
Epoch 140
Epoch 140, Loss: 0.001653857, Improvement: 0.000911311, Best Loss: 0.000325147 in Epoch 131
Epoch 141
Epoch 141, Loss: 0.001222605, Improvement: -0.000431252, Best Loss: 0.000325147 in Epoch 131
Epoch 142
Epoch 142, Loss: 0.000970783, Improvement: -0.000251822, Best Loss: 0.000325147 in Epoch 131
Epoch 143
Epoch 143, Loss: 0.000737015, Improvement: -0.000233768, Best Loss: 0.000325147 in Epoch 131
Epoch 144
Epoch 144, Loss: 0.000640186, Improvement: -0.000096829, Best Loss: 0.000325147 in Epoch 131
Epoch 145
Epoch 145, Loss: 0.000605978, Improvement: -0.000034209, Best Loss: 0.000325147 in Epoch 131
Epoch 146
A best model at epoch 146 has been saved with training error 0.000301850.
Epoch 146, Loss: 0.000613874, Improvement: 0.000007896, Best Loss: 0.000301850 in Epoch 146
Epoch 147
Epoch 147, Loss: 0.000598754, Improvement: -0.000015120, Best Loss: 0.000301850 in Epoch 146
Epoch 148
Epoch 148, Loss: 0.000562077, Improvement: -0.000036677, Best Loss: 0.000301850 in Epoch 146
Epoch 149
Epoch 149, Loss: 0.000577928, Improvement: 0.000015851, Best Loss: 0.000301850 in Epoch 146
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.001761864, Improvement: 0.001183936, Best Loss: 0.000301850 in Epoch 146
Epoch 151
Epoch 151, Loss: 0.001229462, Improvement: -0.000532402, Best Loss: 0.000301850 in Epoch 146
Epoch 152
Epoch 152, Loss: 0.000828499, Improvement: -0.000400963, Best Loss: 0.000301850 in Epoch 146
Epoch 153
Epoch 153, Loss: 0.000759217, Improvement: -0.000069283, Best Loss: 0.000301850 in Epoch 146
Epoch 154
Epoch 154, Loss: 0.000656456, Improvement: -0.000102760, Best Loss: 0.000301850 in Epoch 146
Epoch 155
Epoch 155, Loss: 0.000625872, Improvement: -0.000030585, Best Loss: 0.000301850 in Epoch 146
Epoch 156
Epoch 156, Loss: 0.000589867, Improvement: -0.000036004, Best Loss: 0.000301850 in Epoch 146
Epoch 157
Epoch 157, Loss: 0.000517650, Improvement: -0.000072217, Best Loss: 0.000301850 in Epoch 146
Epoch 158
Epoch 158, Loss: 0.000491597, Improvement: -0.000026053, Best Loss: 0.000301850 in Epoch 146
Epoch 159
A best model at epoch 159 has been saved with training error 0.000290011.
Epoch 159, Loss: 0.000475216, Improvement: -0.000016381, Best Loss: 0.000290011 in Epoch 159
Epoch 160
Epoch 160, Loss: 0.000476551, Improvement: 0.000001335, Best Loss: 0.000290011 in Epoch 159
Epoch 161
Epoch 161, Loss: 0.000534470, Improvement: 0.000057919, Best Loss: 0.000290011 in Epoch 159
Epoch 162
Epoch 162, Loss: 0.000520845, Improvement: -0.000013624, Best Loss: 0.000290011 in Epoch 159
Epoch 163
Epoch 163, Loss: 0.000534126, Improvement: 0.000013281, Best Loss: 0.000290011 in Epoch 159
Epoch 164
Epoch 164, Loss: 0.000606500, Improvement: 0.000072374, Best Loss: 0.000290011 in Epoch 159
Epoch 165
Epoch 165, Loss: 0.000705609, Improvement: 0.000099109, Best Loss: 0.000290011 in Epoch 159
Epoch 166
Epoch 166, Loss: 0.000587479, Improvement: -0.000118130, Best Loss: 0.000290011 in Epoch 159
Epoch 167
Epoch 167, Loss: 0.000468702, Improvement: -0.000118778, Best Loss: 0.000290011 in Epoch 159
Epoch 168
Epoch 168, Loss: 0.000461119, Improvement: -0.000007583, Best Loss: 0.000290011 in Epoch 159
Epoch 169
Epoch 169, Loss: 0.000455143, Improvement: -0.000005975, Best Loss: 0.000290011 in Epoch 159
Epoch 170
Epoch 170, Loss: 0.000643440, Improvement: 0.000188297, Best Loss: 0.000290011 in Epoch 159
Epoch 171
Epoch 171, Loss: 0.000544008, Improvement: -0.000099433, Best Loss: 0.000290011 in Epoch 159
Epoch 172
A best model at epoch 172 has been saved with training error 0.000239815.
Epoch 172, Loss: 0.000504228, Improvement: -0.000039779, Best Loss: 0.000239815 in Epoch 172
Epoch 173
Epoch 173, Loss: 0.000458748, Improvement: -0.000045481, Best Loss: 0.000239815 in Epoch 172
Epoch 174
Epoch 174, Loss: 0.000518435, Improvement: 0.000059687, Best Loss: 0.000239815 in Epoch 172
Epoch 175
Epoch 175, Loss: 0.000467404, Improvement: -0.000051031, Best Loss: 0.000239815 in Epoch 172
Epoch 176
Epoch 176, Loss: 0.000556647, Improvement: 0.000089243, Best Loss: 0.000239815 in Epoch 172
Epoch 177
Epoch 177, Loss: 0.001016621, Improvement: 0.000459973, Best Loss: 0.000239815 in Epoch 172
Epoch 178
Epoch 178, Loss: 0.000680107, Improvement: -0.000336514, Best Loss: 0.000239815 in Epoch 172
Epoch 179
Epoch 179, Loss: 0.000567374, Improvement: -0.000112733, Best Loss: 0.000239815 in Epoch 172
Epoch 180
Epoch 180, Loss: 0.000454827, Improvement: -0.000112546, Best Loss: 0.000239815 in Epoch 172
Epoch 181
Epoch 181, Loss: 0.000402732, Improvement: -0.000052095, Best Loss: 0.000239815 in Epoch 172
Epoch 182
A best model at epoch 182 has been saved with training error 0.000232853.
Epoch 182, Loss: 0.000388246, Improvement: -0.000014486, Best Loss: 0.000232853 in Epoch 182
Epoch 183
A best model at epoch 183 has been saved with training error 0.000184382.
Epoch 183, Loss: 0.000396108, Improvement: 0.000007861, Best Loss: 0.000184382 in Epoch 183
Epoch 184
Epoch 184, Loss: 0.000497155, Improvement: 0.000101047, Best Loss: 0.000184382 in Epoch 183
Epoch 185
Epoch 185, Loss: 0.000750316, Improvement: 0.000253161, Best Loss: 0.000184382 in Epoch 183
Epoch 186
Epoch 186, Loss: 0.000764656, Improvement: 0.000014341, Best Loss: 0.000184382 in Epoch 183
Epoch 187
Epoch 187, Loss: 0.000835713, Improvement: 0.000071057, Best Loss: 0.000184382 in Epoch 183
Epoch 188
Epoch 188, Loss: 0.000591290, Improvement: -0.000244423, Best Loss: 0.000184382 in Epoch 183
Epoch 189
Epoch 189, Loss: 0.000422470, Improvement: -0.000168820, Best Loss: 0.000184382 in Epoch 183
Epoch 190
Epoch 190, Loss: 0.000386266, Improvement: -0.000036204, Best Loss: 0.000184382 in Epoch 183
Epoch 191
Epoch 191, Loss: 0.000411200, Improvement: 0.000024934, Best Loss: 0.000184382 in Epoch 183
Epoch 192
Epoch 192, Loss: 0.000663648, Improvement: 0.000252448, Best Loss: 0.000184382 in Epoch 183
Epoch 193
Epoch 193, Loss: 0.000831651, Improvement: 0.000168003, Best Loss: 0.000184382 in Epoch 183
Epoch 194
Epoch 194, Loss: 0.000703066, Improvement: -0.000128585, Best Loss: 0.000184382 in Epoch 183
Epoch 195
Epoch 195, Loss: 0.000574296, Improvement: -0.000128770, Best Loss: 0.000184382 in Epoch 183
Epoch 196
Epoch 196, Loss: 0.000471413, Improvement: -0.000102883, Best Loss: 0.000184382 in Epoch 183
Epoch 197
Epoch 197, Loss: 0.000409695, Improvement: -0.000061718, Best Loss: 0.000184382 in Epoch 183
Epoch 198
Epoch 198, Loss: 0.000398492, Improvement: -0.000011202, Best Loss: 0.000184382 in Epoch 183
Epoch 199
Epoch 199, Loss: 0.000350131, Improvement: -0.000048361, Best Loss: 0.000184382 in Epoch 183
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.000332442, Improvement: -0.000017690, Best Loss: 0.000184382 in Epoch 183
Epoch 201
Epoch 201, Loss: 0.000296254, Improvement: -0.000036187, Best Loss: 0.000184382 in Epoch 183
Epoch 202
Epoch 202, Loss: 0.000295751, Improvement: -0.000000503, Best Loss: 0.000184382 in Epoch 183
Epoch 203
Epoch 203, Loss: 0.000441820, Improvement: 0.000146068, Best Loss: 0.000184382 in Epoch 183
Epoch 204
Epoch 204, Loss: 0.000712602, Improvement: 0.000270783, Best Loss: 0.000184382 in Epoch 183
Epoch 205
Epoch 205, Loss: 0.000534979, Improvement: -0.000177623, Best Loss: 0.000184382 in Epoch 183
Epoch 206
Epoch 206, Loss: 0.000350990, Improvement: -0.000183990, Best Loss: 0.000184382 in Epoch 183
Epoch 207
Epoch 207, Loss: 0.000313692, Improvement: -0.000037297, Best Loss: 0.000184382 in Epoch 183
Epoch 208
Epoch 208, Loss: 0.000294990, Improvement: -0.000018703, Best Loss: 0.000184382 in Epoch 183
Epoch 209
A best model at epoch 209 has been saved with training error 0.000183010.
A best model at epoch 209 has been saved with training error 0.000182040.
Epoch 209, Loss: 0.000339489, Improvement: 0.000044500, Best Loss: 0.000182040 in Epoch 209
Epoch 210
Epoch 210, Loss: 0.000459776, Improvement: 0.000120287, Best Loss: 0.000182040 in Epoch 209
Epoch 211
Epoch 211, Loss: 0.000529926, Improvement: 0.000070150, Best Loss: 0.000182040 in Epoch 209
Epoch 212
Epoch 212, Loss: 0.000349803, Improvement: -0.000180123, Best Loss: 0.000182040 in Epoch 209
Epoch 213
Epoch 213, Loss: 0.000475816, Improvement: 0.000126013, Best Loss: 0.000182040 in Epoch 209
Epoch 214
Epoch 214, Loss: 0.000506234, Improvement: 0.000030418, Best Loss: 0.000182040 in Epoch 209
Epoch 215
Epoch 215, Loss: 0.000925832, Improvement: 0.000419599, Best Loss: 0.000182040 in Epoch 209
Epoch 216
Epoch 216, Loss: 0.000817677, Improvement: -0.000108155, Best Loss: 0.000182040 in Epoch 209
Epoch 217
Epoch 217, Loss: 0.000521161, Improvement: -0.000296516, Best Loss: 0.000182040 in Epoch 209
Epoch 218
Epoch 218, Loss: 0.000354557, Improvement: -0.000166604, Best Loss: 0.000182040 in Epoch 209
Epoch 219
Epoch 219, Loss: 0.000277806, Improvement: -0.000076751, Best Loss: 0.000182040 in Epoch 209
Epoch 220
A best model at epoch 220 has been saved with training error 0.000171647.
Epoch 220, Loss: 0.000247146, Improvement: -0.000030660, Best Loss: 0.000171647 in Epoch 220
Epoch 221
A best model at epoch 221 has been saved with training error 0.000137741.
Epoch 221, Loss: 0.000230039, Improvement: -0.000017107, Best Loss: 0.000137741 in Epoch 221
Epoch 222
A best model at epoch 222 has been saved with training error 0.000118191.
Epoch 222, Loss: 0.000212851, Improvement: -0.000017188, Best Loss: 0.000118191 in Epoch 222
Epoch 223
A best model at epoch 223 has been saved with training error 0.000105639.
Epoch 223, Loss: 0.000217713, Improvement: 0.000004862, Best Loss: 0.000105639 in Epoch 223
Epoch 224
Epoch 224, Loss: 0.000398016, Improvement: 0.000180303, Best Loss: 0.000105639 in Epoch 223
Epoch 225
Epoch 225, Loss: 0.001016887, Improvement: 0.000618871, Best Loss: 0.000105639 in Epoch 223
Epoch 226
Epoch 226, Loss: 0.001067888, Improvement: 0.000051001, Best Loss: 0.000105639 in Epoch 223
Epoch 227
Epoch 227, Loss: 0.000662850, Improvement: -0.000405038, Best Loss: 0.000105639 in Epoch 223
Epoch 228
Epoch 228, Loss: 0.000391136, Improvement: -0.000271715, Best Loss: 0.000105639 in Epoch 223
Epoch 229
Epoch 229, Loss: 0.000291221, Improvement: -0.000099915, Best Loss: 0.000105639 in Epoch 223
Epoch 230
Epoch 230, Loss: 0.000255732, Improvement: -0.000035488, Best Loss: 0.000105639 in Epoch 223
Epoch 231
Epoch 231, Loss: 0.000227141, Improvement: -0.000028591, Best Loss: 0.000105639 in Epoch 223
Epoch 232
Epoch 232, Loss: 0.000211348, Improvement: -0.000015793, Best Loss: 0.000105639 in Epoch 223
Epoch 233
Epoch 233, Loss: 0.000205563, Improvement: -0.000005785, Best Loss: 0.000105639 in Epoch 223
Epoch 234
Epoch 234, Loss: 0.000193336, Improvement: -0.000012227, Best Loss: 0.000105639 in Epoch 223
Epoch 235
Epoch 235, Loss: 0.000188283, Improvement: -0.000005053, Best Loss: 0.000105639 in Epoch 223
Epoch 236
Epoch 236, Loss: 0.000181057, Improvement: -0.000007226, Best Loss: 0.000105639 in Epoch 223
Epoch 237
Epoch 237, Loss: 0.000193794, Improvement: 0.000012737, Best Loss: 0.000105639 in Epoch 223
Epoch 238
Epoch 238, Loss: 0.000195661, Improvement: 0.000001868, Best Loss: 0.000105639 in Epoch 223
Epoch 239
Epoch 239, Loss: 0.000175595, Improvement: -0.000020067, Best Loss: 0.000105639 in Epoch 223
Epoch 240
Epoch 240, Loss: 0.000169512, Improvement: -0.000006083, Best Loss: 0.000105639 in Epoch 223
Epoch 241
Epoch 241, Loss: 0.000155696, Improvement: -0.000013816, Best Loss: 0.000105639 in Epoch 223
Epoch 242
Epoch 242, Loss: 0.000170818, Improvement: 0.000015121, Best Loss: 0.000105639 in Epoch 223
Epoch 243
Epoch 243, Loss: 0.000463218, Improvement: 0.000292401, Best Loss: 0.000105639 in Epoch 223
Epoch 244
Epoch 244, Loss: 0.000563618, Improvement: 0.000100400, Best Loss: 0.000105639 in Epoch 223
Epoch 245
Epoch 245, Loss: 0.000297456, Improvement: -0.000266162, Best Loss: 0.000105639 in Epoch 223
Epoch 246
Epoch 246, Loss: 0.000221476, Improvement: -0.000075979, Best Loss: 0.000105639 in Epoch 223
Epoch 247
Epoch 247, Loss: 0.000210510, Improvement: -0.000010966, Best Loss: 0.000105639 in Epoch 223
Epoch 248
Epoch 248, Loss: 0.000206259, Improvement: -0.000004252, Best Loss: 0.000105639 in Epoch 223
Epoch 249
Epoch 249, Loss: 0.000202996, Improvement: -0.000003263, Best Loss: 0.000105639 in Epoch 223
Epoch 250
A best model at epoch 250 has been saved with training error 0.000104318.
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.000184058, Improvement: -0.000018938, Best Loss: 0.000104318 in Epoch 250
Epoch 251
Epoch 251, Loss: 0.000155180, Improvement: -0.000028878, Best Loss: 0.000104318 in Epoch 250
Epoch 252
Epoch 252, Loss: 0.000182625, Improvement: 0.000027445, Best Loss: 0.000104318 in Epoch 250
Epoch 253
Epoch 253, Loss: 0.000209526, Improvement: 0.000026902, Best Loss: 0.000104318 in Epoch 250
Epoch 254
Epoch 254, Loss: 0.000177760, Improvement: -0.000031766, Best Loss: 0.000104318 in Epoch 250
Epoch 255
Epoch 255, Loss: 0.000149629, Improvement: -0.000028132, Best Loss: 0.000104318 in Epoch 250
Epoch 256
A best model at epoch 256 has been saved with training error 0.000088633.
Epoch 256, Loss: 0.000144301, Improvement: -0.000005328, Best Loss: 0.000088633 in Epoch 256
Epoch 257
A best model at epoch 257 has been saved with training error 0.000084530.
Epoch 257, Loss: 0.000141624, Improvement: -0.000002677, Best Loss: 0.000084530 in Epoch 257
Epoch 258
Epoch 258, Loss: 0.000137566, Improvement: -0.000004058, Best Loss: 0.000084530 in Epoch 257
Epoch 259
A best model at epoch 259 has been saved with training error 0.000081465.
Epoch 259, Loss: 0.000150589, Improvement: 0.000013024, Best Loss: 0.000081465 in Epoch 259
Epoch 260
Epoch 260, Loss: 0.000223169, Improvement: 0.000072579, Best Loss: 0.000081465 in Epoch 259
Epoch 261
Epoch 261, Loss: 0.000396393, Improvement: 0.000173224, Best Loss: 0.000081465 in Epoch 259
Epoch 262
Epoch 262, Loss: 0.000350496, Improvement: -0.000045897, Best Loss: 0.000081465 in Epoch 259
Epoch 263
Epoch 263, Loss: 0.000298398, Improvement: -0.000052098, Best Loss: 0.000081465 in Epoch 259
Epoch 264
Epoch 264, Loss: 0.000283616, Improvement: -0.000014782, Best Loss: 0.000081465 in Epoch 259
Epoch 265
Epoch 265, Loss: 0.000296979, Improvement: 0.000013362, Best Loss: 0.000081465 in Epoch 259
Epoch 266
Epoch 266, Loss: 0.000220375, Improvement: -0.000076603, Best Loss: 0.000081465 in Epoch 259
Epoch 267
Epoch 267, Loss: 0.000169297, Improvement: -0.000051078, Best Loss: 0.000081465 in Epoch 259
Epoch 268
Epoch 268, Loss: 0.000168877, Improvement: -0.000000420, Best Loss: 0.000081465 in Epoch 259
Epoch 269
Epoch 269, Loss: 0.000162344, Improvement: -0.000006532, Best Loss: 0.000081465 in Epoch 259
Epoch 270
Epoch 270, Loss: 0.000201461, Improvement: 0.000039116, Best Loss: 0.000081465 in Epoch 259
Epoch 271
Epoch 271, Loss: 0.000429219, Improvement: 0.000227758, Best Loss: 0.000081465 in Epoch 259
Epoch 272
Epoch 272, Loss: 0.000368326, Improvement: -0.000060893, Best Loss: 0.000081465 in Epoch 259
Epoch 273
Epoch 273, Loss: 0.000259747, Improvement: -0.000108579, Best Loss: 0.000081465 in Epoch 259
Epoch 274
Epoch 274, Loss: 0.000192444, Improvement: -0.000067303, Best Loss: 0.000081465 in Epoch 259
Epoch 275
Epoch 275, Loss: 0.000181492, Improvement: -0.000010952, Best Loss: 0.000081465 in Epoch 259
Epoch 276
Epoch 276, Loss: 0.000146904, Improvement: -0.000034588, Best Loss: 0.000081465 in Epoch 259
Epoch 277
Epoch 277, Loss: 0.000136649, Improvement: -0.000010254, Best Loss: 0.000081465 in Epoch 259
Epoch 278
Epoch 278, Loss: 0.000257343, Improvement: 0.000120694, Best Loss: 0.000081465 in Epoch 259
Epoch 279
Epoch 279, Loss: 0.000585100, Improvement: 0.000327757, Best Loss: 0.000081465 in Epoch 259
Epoch 280
Epoch 280, Loss: 0.000416594, Improvement: -0.000168506, Best Loss: 0.000081465 in Epoch 259
Epoch 281
Epoch 281, Loss: 0.000241344, Improvement: -0.000175250, Best Loss: 0.000081465 in Epoch 259
Epoch 282
Epoch 282, Loss: 0.000191111, Improvement: -0.000050233, Best Loss: 0.000081465 in Epoch 259
Epoch 283
Epoch 283, Loss: 0.000165874, Improvement: -0.000025237, Best Loss: 0.000081465 in Epoch 259
Epoch 284
Epoch 284, Loss: 0.000139341, Improvement: -0.000026533, Best Loss: 0.000081465 in Epoch 259
Epoch 285
Epoch 285, Loss: 0.000131944, Improvement: -0.000007398, Best Loss: 0.000081465 in Epoch 259
Epoch 286
Epoch 286, Loss: 0.000131938, Improvement: -0.000000006, Best Loss: 0.000081465 in Epoch 259
Epoch 287
Epoch 287, Loss: 0.000125466, Improvement: -0.000006472, Best Loss: 0.000081465 in Epoch 259
Epoch 288
Epoch 288, Loss: 0.000114369, Improvement: -0.000011097, Best Loss: 0.000081465 in Epoch 259
Epoch 289
A best model at epoch 289 has been saved with training error 0.000077126.
Epoch 289, Loss: 0.000108910, Improvement: -0.000005459, Best Loss: 0.000077126 in Epoch 289
Epoch 290
Epoch 290, Loss: 0.000123481, Improvement: 0.000014571, Best Loss: 0.000077126 in Epoch 289
Epoch 291
Epoch 291, Loss: 0.000133848, Improvement: 0.000010366, Best Loss: 0.000077126 in Epoch 289
Epoch 292
Epoch 292, Loss: 0.000132763, Improvement: -0.000001085, Best Loss: 0.000077126 in Epoch 289
Epoch 293
Epoch 293, Loss: 0.000117841, Improvement: -0.000014922, Best Loss: 0.000077126 in Epoch 289
Epoch 294
Epoch 294, Loss: 0.000119131, Improvement: 0.000001290, Best Loss: 0.000077126 in Epoch 289
Epoch 295
A best model at epoch 295 has been saved with training error 0.000073096.
Epoch 295, Loss: 0.000105579, Improvement: -0.000013552, Best Loss: 0.000073096 in Epoch 295
Epoch 296
Epoch 296, Loss: 0.000186028, Improvement: 0.000080449, Best Loss: 0.000073096 in Epoch 295
Epoch 297
Epoch 297, Loss: 0.000349891, Improvement: 0.000163862, Best Loss: 0.000073096 in Epoch 295
Epoch 298
Epoch 298, Loss: 0.000238875, Improvement: -0.000111016, Best Loss: 0.000073096 in Epoch 295
Epoch 299
Epoch 299, Loss: 0.000158279, Improvement: -0.000080596, Best Loss: 0.000073096 in Epoch 295
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.000140159, Improvement: -0.000018120, Best Loss: 0.000073096 in Epoch 295
Epoch 301
Epoch 301, Loss: 0.000139004, Improvement: -0.000001155, Best Loss: 0.000073096 in Epoch 295
Epoch 302
Epoch 302, Loss: 0.000129578, Improvement: -0.000009426, Best Loss: 0.000073096 in Epoch 295
Epoch 303
A best model at epoch 303 has been saved with training error 0.000068445.
Epoch 303, Loss: 0.000105253, Improvement: -0.000024325, Best Loss: 0.000068445 in Epoch 303
Epoch 304
Epoch 304, Loss: 0.000098440, Improvement: -0.000006812, Best Loss: 0.000068445 in Epoch 303
Epoch 305
Epoch 305, Loss: 0.000131693, Improvement: 0.000033253, Best Loss: 0.000068445 in Epoch 303
Epoch 306
Epoch 306, Loss: 0.000545872, Improvement: 0.000414179, Best Loss: 0.000068445 in Epoch 303
Epoch 307
Epoch 307, Loss: 0.000759389, Improvement: 0.000213517, Best Loss: 0.000068445 in Epoch 303
Epoch 308
Epoch 308, Loss: 0.000332868, Improvement: -0.000426521, Best Loss: 0.000068445 in Epoch 303
Epoch 309
Epoch 309, Loss: 0.000260506, Improvement: -0.000072362, Best Loss: 0.000068445 in Epoch 303
Epoch 310
Epoch 310, Loss: 0.000159806, Improvement: -0.000100701, Best Loss: 0.000068445 in Epoch 303
Epoch 311
Epoch 311, Loss: 0.000126276, Improvement: -0.000033530, Best Loss: 0.000068445 in Epoch 303
Epoch 312
Epoch 312, Loss: 0.000114534, Improvement: -0.000011741, Best Loss: 0.000068445 in Epoch 303
Epoch 313
A best model at epoch 313 has been saved with training error 0.000067670.
Epoch 313, Loss: 0.000108867, Improvement: -0.000005667, Best Loss: 0.000067670 in Epoch 313
Epoch 314
Epoch 314, Loss: 0.000106629, Improvement: -0.000002238, Best Loss: 0.000067670 in Epoch 313
Epoch 315
A best model at epoch 315 has been saved with training error 0.000067613.
Epoch 315, Loss: 0.000102701, Improvement: -0.000003928, Best Loss: 0.000067613 in Epoch 315
Epoch 316
Epoch 316, Loss: 0.000102024, Improvement: -0.000000678, Best Loss: 0.000067613 in Epoch 315
Epoch 317
Epoch 317, Loss: 0.000117147, Improvement: 0.000015124, Best Loss: 0.000067613 in Epoch 315
Epoch 318
Epoch 318, Loss: 0.000098968, Improvement: -0.000018179, Best Loss: 0.000067613 in Epoch 315
Epoch 319
Epoch 319, Loss: 0.000094006, Improvement: -0.000004961, Best Loss: 0.000067613 in Epoch 315
Epoch 320
A best model at epoch 320 has been saved with training error 0.000063321.
Epoch 320, Loss: 0.000101499, Improvement: 0.000007493, Best Loss: 0.000063321 in Epoch 320
Epoch 321
Epoch 321, Loss: 0.000093192, Improvement: -0.000008308, Best Loss: 0.000063321 in Epoch 320
Epoch 322
Epoch 322, Loss: 0.000130405, Improvement: 0.000037213, Best Loss: 0.000063321 in Epoch 320
Epoch 323
Epoch 323, Loss: 0.000171623, Improvement: 0.000041218, Best Loss: 0.000063321 in Epoch 320
Epoch 324
Epoch 324, Loss: 0.000337566, Improvement: 0.000165944, Best Loss: 0.000063321 in Epoch 320
Epoch 325
Epoch 325, Loss: 0.000332259, Improvement: -0.000005308, Best Loss: 0.000063321 in Epoch 320
Epoch 326
Epoch 326, Loss: 0.000209467, Improvement: -0.000122792, Best Loss: 0.000063321 in Epoch 320
Epoch 327
Epoch 327, Loss: 0.000177525, Improvement: -0.000031942, Best Loss: 0.000063321 in Epoch 320
Epoch 328
Epoch 328, Loss: 0.000148240, Improvement: -0.000029285, Best Loss: 0.000063321 in Epoch 320
Epoch 329
Epoch 329, Loss: 0.000111636, Improvement: -0.000036603, Best Loss: 0.000063321 in Epoch 320
Epoch 330
Epoch 330, Loss: 0.000129483, Improvement: 0.000017847, Best Loss: 0.000063321 in Epoch 320
Epoch 331
Epoch 331, Loss: 0.000115663, Improvement: -0.000013820, Best Loss: 0.000063321 in Epoch 320
Epoch 332
Epoch 332, Loss: 0.000141100, Improvement: 0.000025436, Best Loss: 0.000063321 in Epoch 320
Epoch 333
Epoch 333, Loss: 0.000155028, Improvement: 0.000013928, Best Loss: 0.000063321 in Epoch 320
Epoch 334
Epoch 334, Loss: 0.000132504, Improvement: -0.000022524, Best Loss: 0.000063321 in Epoch 320
Epoch 335
Epoch 335, Loss: 0.000127466, Improvement: -0.000005038, Best Loss: 0.000063321 in Epoch 320
Epoch 336
Epoch 336, Loss: 0.000124485, Improvement: -0.000002980, Best Loss: 0.000063321 in Epoch 320
Epoch 337
Epoch 337, Loss: 0.000157552, Improvement: 0.000033067, Best Loss: 0.000063321 in Epoch 320
Epoch 338
Epoch 338, Loss: 0.000245895, Improvement: 0.000088343, Best Loss: 0.000063321 in Epoch 320
Epoch 339
Epoch 339, Loss: 0.000289533, Improvement: 0.000043638, Best Loss: 0.000063321 in Epoch 320
Epoch 340
Epoch 340, Loss: 0.000175417, Improvement: -0.000114116, Best Loss: 0.000063321 in Epoch 320
Epoch 341
A best model at epoch 341 has been saved with training error 0.000060787.
Epoch 341, Loss: 0.000105096, Improvement: -0.000070321, Best Loss: 0.000060787 in Epoch 341
Epoch 342
A best model at epoch 342 has been saved with training error 0.000051635.
Epoch 342, Loss: 0.000087238, Improvement: -0.000017858, Best Loss: 0.000051635 in Epoch 342
Epoch 343
Epoch 343, Loss: 0.000082529, Improvement: -0.000004708, Best Loss: 0.000051635 in Epoch 342
Epoch 344
Epoch 344, Loss: 0.000078307, Improvement: -0.000004222, Best Loss: 0.000051635 in Epoch 342
Epoch 345
Epoch 345, Loss: 0.000081847, Improvement: 0.000003540, Best Loss: 0.000051635 in Epoch 342
Epoch 346
Epoch 346, Loss: 0.000090775, Improvement: 0.000008928, Best Loss: 0.000051635 in Epoch 342
Epoch 347
Epoch 347, Loss: 0.000114171, Improvement: 0.000023396, Best Loss: 0.000051635 in Epoch 342
Epoch 348
Epoch 348, Loss: 0.000154528, Improvement: 0.000040357, Best Loss: 0.000051635 in Epoch 342
Epoch 349
Epoch 349, Loss: 0.000432818, Improvement: 0.000278290, Best Loss: 0.000051635 in Epoch 342
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.000489145, Improvement: 0.000056327, Best Loss: 0.000051635 in Epoch 342
Epoch 351
Epoch 351, Loss: 0.000367656, Improvement: -0.000121489, Best Loss: 0.000051635 in Epoch 342
Epoch 352
Epoch 352, Loss: 0.000373474, Improvement: 0.000005818, Best Loss: 0.000051635 in Epoch 342
Epoch 353
Epoch 353, Loss: 0.000218727, Improvement: -0.000154747, Best Loss: 0.000051635 in Epoch 342
Epoch 354
Epoch 354, Loss: 0.000152259, Improvement: -0.000066467, Best Loss: 0.000051635 in Epoch 342
Epoch 355
Epoch 355, Loss: 0.000109206, Improvement: -0.000043053, Best Loss: 0.000051635 in Epoch 342
Epoch 356
Epoch 356, Loss: 0.000090281, Improvement: -0.000018925, Best Loss: 0.000051635 in Epoch 342
Epoch 357
Epoch 357, Loss: 0.000081155, Improvement: -0.000009126, Best Loss: 0.000051635 in Epoch 342
Epoch 358
A best model at epoch 358 has been saved with training error 0.000046100.
Epoch 358, Loss: 0.000077972, Improvement: -0.000003182, Best Loss: 0.000046100 in Epoch 358
Epoch 359
Epoch 359, Loss: 0.000074973, Improvement: -0.000002999, Best Loss: 0.000046100 in Epoch 358
Epoch 360
Epoch 360, Loss: 0.000074112, Improvement: -0.000000861, Best Loss: 0.000046100 in Epoch 358
Epoch 361
Epoch 361, Loss: 0.000071683, Improvement: -0.000002429, Best Loss: 0.000046100 in Epoch 358
Epoch 362
Epoch 362, Loss: 0.000071058, Improvement: -0.000000625, Best Loss: 0.000046100 in Epoch 358
Epoch 363
Epoch 363, Loss: 0.000065350, Improvement: -0.000005709, Best Loss: 0.000046100 in Epoch 358
Epoch 364
Epoch 364, Loss: 0.000076957, Improvement: 0.000011608, Best Loss: 0.000046100 in Epoch 358
Epoch 365
Epoch 365, Loss: 0.000076781, Improvement: -0.000000176, Best Loss: 0.000046100 in Epoch 358
Epoch 366
Epoch 366, Loss: 0.000122110, Improvement: 0.000045329, Best Loss: 0.000046100 in Epoch 358
Epoch 367
Epoch 367, Loss: 0.000156370, Improvement: 0.000034260, Best Loss: 0.000046100 in Epoch 358
Epoch 368
Epoch 368, Loss: 0.000155866, Improvement: -0.000000504, Best Loss: 0.000046100 in Epoch 358
Epoch 369
Epoch 369, Loss: 0.000105080, Improvement: -0.000050786, Best Loss: 0.000046100 in Epoch 358
Epoch 370
A best model at epoch 370 has been saved with training error 0.000043624.
Epoch 370, Loss: 0.000087765, Improvement: -0.000017315, Best Loss: 0.000043624 in Epoch 370
Epoch 371
Epoch 371, Loss: 0.000071062, Improvement: -0.000016703, Best Loss: 0.000043624 in Epoch 370
Epoch 372
Epoch 372, Loss: 0.000070987, Improvement: -0.000000075, Best Loss: 0.000043624 in Epoch 370
Epoch 373
Epoch 373, Loss: 0.000065836, Improvement: -0.000005152, Best Loss: 0.000043624 in Epoch 370
Epoch 374
Epoch 374, Loss: 0.000067887, Improvement: 0.000002051, Best Loss: 0.000043624 in Epoch 370
Epoch 375
Epoch 375, Loss: 0.000084054, Improvement: 0.000016167, Best Loss: 0.000043624 in Epoch 370
Epoch 376
Epoch 376, Loss: 0.000255565, Improvement: 0.000171511, Best Loss: 0.000043624 in Epoch 370
Epoch 377
Epoch 377, Loss: 0.000155509, Improvement: -0.000100056, Best Loss: 0.000043624 in Epoch 370
Epoch 378
Epoch 378, Loss: 0.000098875, Improvement: -0.000056634, Best Loss: 0.000043624 in Epoch 370
Epoch 379
Epoch 379, Loss: 0.000084522, Improvement: -0.000014353, Best Loss: 0.000043624 in Epoch 370
Epoch 380
Epoch 380, Loss: 0.000089409, Improvement: 0.000004887, Best Loss: 0.000043624 in Epoch 370
Epoch 381
Epoch 381, Loss: 0.000076410, Improvement: -0.000012999, Best Loss: 0.000043624 in Epoch 370
Epoch 382
A best model at epoch 382 has been saved with training error 0.000043594.
Epoch 382, Loss: 0.000070648, Improvement: -0.000005762, Best Loss: 0.000043594 in Epoch 382
Epoch 383
Epoch 383, Loss: 0.000069963, Improvement: -0.000000685, Best Loss: 0.000043594 in Epoch 382
Epoch 384
A best model at epoch 384 has been saved with training error 0.000039495.
Epoch 384, Loss: 0.000055610, Improvement: -0.000014353, Best Loss: 0.000039495 in Epoch 384
Epoch 385
Epoch 385, Loss: 0.000052366, Improvement: -0.000003244, Best Loss: 0.000039495 in Epoch 384
Epoch 386
A best model at epoch 386 has been saved with training error 0.000039238.
A best model at epoch 386 has been saved with training error 0.000039144.
Epoch 386, Loss: 0.000050607, Improvement: -0.000001759, Best Loss: 0.000039144 in Epoch 386
Epoch 387
A best model at epoch 387 has been saved with training error 0.000037278.
Epoch 387, Loss: 0.000049617, Improvement: -0.000000990, Best Loss: 0.000037278 in Epoch 387
Epoch 388
A best model at epoch 388 has been saved with training error 0.000035472.
A best model at epoch 388 has been saved with training error 0.000033100.
Epoch 388, Loss: 0.000047299, Improvement: -0.000002318, Best Loss: 0.000033100 in Epoch 388
Epoch 389
Epoch 389, Loss: 0.000049766, Improvement: 0.000002466, Best Loss: 0.000033100 in Epoch 388
Epoch 390
Epoch 390, Loss: 0.000055572, Improvement: 0.000005806, Best Loss: 0.000033100 in Epoch 388
Epoch 391
Epoch 391, Loss: 0.000073411, Improvement: 0.000017839, Best Loss: 0.000033100 in Epoch 388
Epoch 392
Epoch 392, Loss: 0.000058304, Improvement: -0.000015107, Best Loss: 0.000033100 in Epoch 388
Epoch 393
Epoch 393, Loss: 0.000060275, Improvement: 0.000001972, Best Loss: 0.000033100 in Epoch 388
Epoch 394
Epoch 394, Loss: 0.000080621, Improvement: 0.000020346, Best Loss: 0.000033100 in Epoch 388
Epoch 395
Epoch 395, Loss: 0.000156169, Improvement: 0.000075548, Best Loss: 0.000033100 in Epoch 388
Epoch 396
Epoch 396, Loss: 0.000173867, Improvement: 0.000017698, Best Loss: 0.000033100 in Epoch 388
Epoch 397
Epoch 397, Loss: 0.000156018, Improvement: -0.000017849, Best Loss: 0.000033100 in Epoch 388
Epoch 398
Epoch 398, Loss: 0.000095524, Improvement: -0.000060495, Best Loss: 0.000033100 in Epoch 388
Epoch 399
Epoch 399, Loss: 0.000078415, Improvement: -0.000017109, Best Loss: 0.000033100 in Epoch 388
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.000063690, Improvement: -0.000014725, Best Loss: 0.000033100 in Epoch 388
Epoch 401
Epoch 401, Loss: 0.000066374, Improvement: 0.000002684, Best Loss: 0.000033100 in Epoch 388
Epoch 402
Epoch 402, Loss: 0.000060158, Improvement: -0.000006216, Best Loss: 0.000033100 in Epoch 388
Epoch 403
Epoch 403, Loss: 0.000056043, Improvement: -0.000004115, Best Loss: 0.000033100 in Epoch 388
Epoch 404
Epoch 404, Loss: 0.000066012, Improvement: 0.000009969, Best Loss: 0.000033100 in Epoch 388
Epoch 405
Epoch 405, Loss: 0.000113404, Improvement: 0.000047392, Best Loss: 0.000033100 in Epoch 388
Epoch 406
Epoch 406, Loss: 0.000162638, Improvement: 0.000049234, Best Loss: 0.000033100 in Epoch 388
Epoch 407
Epoch 407, Loss: 0.000165290, Improvement: 0.000002652, Best Loss: 0.000033100 in Epoch 388
Epoch 408
Epoch 408, Loss: 0.000178084, Improvement: 0.000012794, Best Loss: 0.000033100 in Epoch 388
Epoch 409
Epoch 409, Loss: 0.000131733, Improvement: -0.000046351, Best Loss: 0.000033100 in Epoch 388
Epoch 410
Epoch 410, Loss: 0.000159494, Improvement: 0.000027761, Best Loss: 0.000033100 in Epoch 388
Epoch 411
Epoch 411, Loss: 0.000107785, Improvement: -0.000051709, Best Loss: 0.000033100 in Epoch 388
Epoch 412
Epoch 412, Loss: 0.000114171, Improvement: 0.000006385, Best Loss: 0.000033100 in Epoch 388
Epoch 413
A best model at epoch 413 has been saved with training error 0.000032443.
Epoch 413, Loss: 0.000064704, Improvement: -0.000049466, Best Loss: 0.000032443 in Epoch 413
Epoch 414
Epoch 414, Loss: 0.000053085, Improvement: -0.000011620, Best Loss: 0.000032443 in Epoch 413
Epoch 415
A best model at epoch 415 has been saved with training error 0.000032018.
A best model at epoch 415 has been saved with training error 0.000031485.
Epoch 415, Loss: 0.000046163, Improvement: -0.000006922, Best Loss: 0.000031485 in Epoch 415
Epoch 416
A best model at epoch 416 has been saved with training error 0.000029624.
A best model at epoch 416 has been saved with training error 0.000029128.
Epoch 416, Loss: 0.000045506, Improvement: -0.000000656, Best Loss: 0.000029128 in Epoch 416
Epoch 417
Epoch 417, Loss: 0.000053334, Improvement: 0.000007828, Best Loss: 0.000029128 in Epoch 416
Epoch 418
Epoch 418, Loss: 0.000050014, Improvement: -0.000003320, Best Loss: 0.000029128 in Epoch 416
Epoch 419
Epoch 419, Loss: 0.000066310, Improvement: 0.000016296, Best Loss: 0.000029128 in Epoch 416
Epoch 420
Epoch 420, Loss: 0.000133389, Improvement: 0.000067079, Best Loss: 0.000029128 in Epoch 416
Epoch 421
Epoch 421, Loss: 0.000129928, Improvement: -0.000003462, Best Loss: 0.000029128 in Epoch 416
Epoch 422
Epoch 422, Loss: 0.000167588, Improvement: 0.000037660, Best Loss: 0.000029128 in Epoch 416
Epoch 423
Epoch 423, Loss: 0.000103508, Improvement: -0.000064081, Best Loss: 0.000029128 in Epoch 416
Epoch 424
Epoch 424, Loss: 0.000087843, Improvement: -0.000015665, Best Loss: 0.000029128 in Epoch 416
Epoch 425
Epoch 425, Loss: 0.000083806, Improvement: -0.000004037, Best Loss: 0.000029128 in Epoch 416
Epoch 426
Epoch 426, Loss: 0.000097239, Improvement: 0.000013433, Best Loss: 0.000029128 in Epoch 416
Epoch 427
Epoch 427, Loss: 0.000160022, Improvement: 0.000062783, Best Loss: 0.000029128 in Epoch 416
Epoch 428
Epoch 428, Loss: 0.000160685, Improvement: 0.000000663, Best Loss: 0.000029128 in Epoch 416
Epoch 429
Epoch 429, Loss: 0.000091007, Improvement: -0.000069678, Best Loss: 0.000029128 in Epoch 416
Epoch 430
Epoch 430, Loss: 0.000112763, Improvement: 0.000021756, Best Loss: 0.000029128 in Epoch 416
Epoch 431
Epoch 431, Loss: 0.000106574, Improvement: -0.000006190, Best Loss: 0.000029128 in Epoch 416
Epoch 432
Epoch 432, Loss: 0.000191506, Improvement: 0.000084932, Best Loss: 0.000029128 in Epoch 416
Epoch 433
Epoch 433, Loss: 0.000123752, Improvement: -0.000067753, Best Loss: 0.000029128 in Epoch 416
Epoch 434
Epoch 434, Loss: 0.000108926, Improvement: -0.000014827, Best Loss: 0.000029128 in Epoch 416
Epoch 435
Epoch 435, Loss: 0.000113002, Improvement: 0.000004076, Best Loss: 0.000029128 in Epoch 416
Epoch 436
Epoch 436, Loss: 0.000076644, Improvement: -0.000036358, Best Loss: 0.000029128 in Epoch 416
Epoch 437
Epoch 437, Loss: 0.000071895, Improvement: -0.000004750, Best Loss: 0.000029128 in Epoch 416
Epoch 438
Epoch 438, Loss: 0.000068668, Improvement: -0.000003227, Best Loss: 0.000029128 in Epoch 416
Epoch 439
Epoch 439, Loss: 0.000066457, Improvement: -0.000002211, Best Loss: 0.000029128 in Epoch 416
Epoch 440
Epoch 440, Loss: 0.000057920, Improvement: -0.000008537, Best Loss: 0.000029128 in Epoch 416
Epoch 441
Epoch 441, Loss: 0.000052385, Improvement: -0.000005535, Best Loss: 0.000029128 in Epoch 416
Epoch 442
Epoch 442, Loss: 0.000054783, Improvement: 0.000002398, Best Loss: 0.000029128 in Epoch 416
Epoch 443
Epoch 443, Loss: 0.000102589, Improvement: 0.000047805, Best Loss: 0.000029128 in Epoch 416
Epoch 444
Epoch 444, Loss: 0.000155181, Improvement: 0.000052593, Best Loss: 0.000029128 in Epoch 416
Epoch 445
Epoch 445, Loss: 0.000088619, Improvement: -0.000066562, Best Loss: 0.000029128 in Epoch 416
Epoch 446
Epoch 446, Loss: 0.000065129, Improvement: -0.000023490, Best Loss: 0.000029128 in Epoch 416
Epoch 447
Epoch 447, Loss: 0.000065447, Improvement: 0.000000318, Best Loss: 0.000029128 in Epoch 416
Epoch 448
Epoch 448, Loss: 0.000070542, Improvement: 0.000005095, Best Loss: 0.000029128 in Epoch 416
Epoch 449
Epoch 449, Loss: 0.000082083, Improvement: 0.000011541, Best Loss: 0.000029128 in Epoch 416
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.000121941, Improvement: 0.000039859, Best Loss: 0.000029128 in Epoch 416
Epoch 451
Epoch 451, Loss: 0.000100694, Improvement: -0.000021247, Best Loss: 0.000029128 in Epoch 416
Epoch 452
Epoch 452, Loss: 0.000132112, Improvement: 0.000031417, Best Loss: 0.000029128 in Epoch 416
Epoch 453
Epoch 453, Loss: 0.000098668, Improvement: -0.000033444, Best Loss: 0.000029128 in Epoch 416
Epoch 454
Epoch 454, Loss: 0.000141688, Improvement: 0.000043021, Best Loss: 0.000029128 in Epoch 416
Epoch 455
Epoch 455, Loss: 0.000108917, Improvement: -0.000032771, Best Loss: 0.000029128 in Epoch 416
Epoch 456
Epoch 456, Loss: 0.000073548, Improvement: -0.000035369, Best Loss: 0.000029128 in Epoch 416
Epoch 457
Epoch 457, Loss: 0.000081349, Improvement: 0.000007801, Best Loss: 0.000029128 in Epoch 416
Epoch 458
Epoch 458, Loss: 0.000052203, Improvement: -0.000029145, Best Loss: 0.000029128 in Epoch 416
Epoch 459
Epoch 459, Loss: 0.000056640, Improvement: 0.000004437, Best Loss: 0.000029128 in Epoch 416
Epoch 460
Epoch 460, Loss: 0.000053389, Improvement: -0.000003251, Best Loss: 0.000029128 in Epoch 416
Epoch 461
Epoch 461, Loss: 0.000057902, Improvement: 0.000004513, Best Loss: 0.000029128 in Epoch 416
Epoch 462
A best model at epoch 462 has been saved with training error 0.000023767.
Epoch 462, Loss: 0.000037602, Improvement: -0.000020300, Best Loss: 0.000023767 in Epoch 462
Epoch 463
A best model at epoch 463 has been saved with training error 0.000023009.
Epoch 463, Loss: 0.000032383, Improvement: -0.000005219, Best Loss: 0.000023009 in Epoch 463
Epoch 464
A best model at epoch 464 has been saved with training error 0.000022988.
Epoch 464, Loss: 0.000030672, Improvement: -0.000001711, Best Loss: 0.000022988 in Epoch 464
Epoch 465
A best model at epoch 465 has been saved with training error 0.000017457.
Epoch 465, Loss: 0.000031676, Improvement: 0.000001004, Best Loss: 0.000017457 in Epoch 465
Epoch 466
Epoch 466, Loss: 0.000049616, Improvement: 0.000017940, Best Loss: 0.000017457 in Epoch 465
Epoch 467
Epoch 467, Loss: 0.000081020, Improvement: 0.000031404, Best Loss: 0.000017457 in Epoch 465
Epoch 468
Epoch 468, Loss: 0.000057910, Improvement: -0.000023110, Best Loss: 0.000017457 in Epoch 465
Epoch 469
Epoch 469, Loss: 0.000068021, Improvement: 0.000010110, Best Loss: 0.000017457 in Epoch 465
Epoch 470
Epoch 470, Loss: 0.000168351, Improvement: 0.000100330, Best Loss: 0.000017457 in Epoch 465
Epoch 471
Epoch 471, Loss: 0.000059723, Improvement: -0.000108627, Best Loss: 0.000017457 in Epoch 465
Epoch 472
Epoch 472, Loss: 0.000048014, Improvement: -0.000011709, Best Loss: 0.000017457 in Epoch 465
Epoch 473
Epoch 473, Loss: 0.000042395, Improvement: -0.000005619, Best Loss: 0.000017457 in Epoch 465
Epoch 474
Epoch 474, Loss: 0.000046924, Improvement: 0.000004530, Best Loss: 0.000017457 in Epoch 465
Epoch 475
Epoch 475, Loss: 0.000046581, Improvement: -0.000000343, Best Loss: 0.000017457 in Epoch 465
Epoch 476
Epoch 476, Loss: 0.000039371, Improvement: -0.000007210, Best Loss: 0.000017457 in Epoch 465
Epoch 477
Epoch 477, Loss: 0.000033844, Improvement: -0.000005527, Best Loss: 0.000017457 in Epoch 465
Epoch 478
Epoch 478, Loss: 0.000036572, Improvement: 0.000002728, Best Loss: 0.000017457 in Epoch 465
Epoch 479
Epoch 479, Loss: 0.000065354, Improvement: 0.000028782, Best Loss: 0.000017457 in Epoch 465
Epoch 480
Epoch 480, Loss: 0.000210290, Improvement: 0.000144937, Best Loss: 0.000017457 in Epoch 465
Epoch 481
Epoch 481, Loss: 0.000077088, Improvement: -0.000133202, Best Loss: 0.000017457 in Epoch 465
Epoch 482
Epoch 482, Loss: 0.000046111, Improvement: -0.000030977, Best Loss: 0.000017457 in Epoch 465
Epoch 483
Epoch 483, Loss: 0.000046824, Improvement: 0.000000713, Best Loss: 0.000017457 in Epoch 465
Epoch 484
Epoch 484, Loss: 0.000035522, Improvement: -0.000011302, Best Loss: 0.000017457 in Epoch 465
Epoch 485
Epoch 485, Loss: 0.000029965, Improvement: -0.000005556, Best Loss: 0.000017457 in Epoch 465
Epoch 486
Epoch 486, Loss: 0.000030214, Improvement: 0.000000248, Best Loss: 0.000017457 in Epoch 465
Epoch 487
Epoch 487, Loss: 0.000031345, Improvement: 0.000001132, Best Loss: 0.000017457 in Epoch 465
Epoch 488
Epoch 488, Loss: 0.000035968, Improvement: 0.000004623, Best Loss: 0.000017457 in Epoch 465
Epoch 489
Epoch 489, Loss: 0.000033310, Improvement: -0.000002658, Best Loss: 0.000017457 in Epoch 465
Epoch 490
Epoch 490, Loss: 0.000033717, Improvement: 0.000000407, Best Loss: 0.000017457 in Epoch 465
Epoch 491
A best model at epoch 491 has been saved with training error 0.000017379.
Epoch 491, Loss: 0.000031964, Improvement: -0.000001754, Best Loss: 0.000017379 in Epoch 491
Epoch 492
A best model at epoch 492 has been saved with training error 0.000016807.
Epoch 492, Loss: 0.000029650, Improvement: -0.000002313, Best Loss: 0.000016807 in Epoch 492
Epoch 493
Epoch 493, Loss: 0.000052978, Improvement: 0.000023328, Best Loss: 0.000016807 in Epoch 492
Epoch 494
Epoch 494, Loss: 0.000106944, Improvement: 0.000053966, Best Loss: 0.000016807 in Epoch 492
Epoch 495
Epoch 495, Loss: 0.000067036, Improvement: -0.000039908, Best Loss: 0.000016807 in Epoch 492
Epoch 496
Epoch 496, Loss: 0.000046189, Improvement: -0.000020847, Best Loss: 0.000016807 in Epoch 492
Epoch 497
Epoch 497, Loss: 0.000050392, Improvement: 0.000004202, Best Loss: 0.000016807 in Epoch 492
Epoch 498
Epoch 498, Loss: 0.000077692, Improvement: 0.000027300, Best Loss: 0.000016807 in Epoch 492
Epoch 499
Epoch 499, Loss: 0.000222865, Improvement: 0.000145173, Best Loss: 0.000016807 in Epoch 492
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.000147483, Improvement: -0.000075381, Best Loss: 0.000016807 in Epoch 492
Epoch 501
Epoch 501, Loss: 0.000083611, Improvement: -0.000063873, Best Loss: 0.000016807 in Epoch 492
Epoch 502
Epoch 502, Loss: 0.000091021, Improvement: 0.000007411, Best Loss: 0.000016807 in Epoch 492
Epoch 503
Epoch 503, Loss: 0.000050375, Improvement: -0.000040646, Best Loss: 0.000016807 in Epoch 492
Epoch 504
Epoch 504, Loss: 0.000034767, Improvement: -0.000015608, Best Loss: 0.000016807 in Epoch 492
Epoch 505
Epoch 505, Loss: 0.000034409, Improvement: -0.000000358, Best Loss: 0.000016807 in Epoch 492
Epoch 506
Epoch 506, Loss: 0.000034200, Improvement: -0.000000209, Best Loss: 0.000016807 in Epoch 492
Epoch 507
Epoch 507, Loss: 0.000037111, Improvement: 0.000002911, Best Loss: 0.000016807 in Epoch 492
Epoch 508
Epoch 508, Loss: 0.000031206, Improvement: -0.000005906, Best Loss: 0.000016807 in Epoch 492
Epoch 509
Epoch 509, Loss: 0.000030843, Improvement: -0.000000362, Best Loss: 0.000016807 in Epoch 492
Epoch 510
Epoch 510, Loss: 0.000028712, Improvement: -0.000002132, Best Loss: 0.000016807 in Epoch 492
Epoch 511
Epoch 511, Loss: 0.000026144, Improvement: -0.000002568, Best Loss: 0.000016807 in Epoch 492
Epoch 512
A best model at epoch 512 has been saved with training error 0.000012437.
Epoch 512, Loss: 0.000024767, Improvement: -0.000001377, Best Loss: 0.000012437 in Epoch 512
Epoch 513
Epoch 513, Loss: 0.000024702, Improvement: -0.000000065, Best Loss: 0.000012437 in Epoch 512
Epoch 514
Epoch 514, Loss: 0.000031821, Improvement: 0.000007119, Best Loss: 0.000012437 in Epoch 512
Epoch 515
Epoch 515, Loss: 0.000036579, Improvement: 0.000004758, Best Loss: 0.000012437 in Epoch 512
Epoch 516
Epoch 516, Loss: 0.000048099, Improvement: 0.000011520, Best Loss: 0.000012437 in Epoch 512
Epoch 517
Epoch 517, Loss: 0.000060857, Improvement: 0.000012758, Best Loss: 0.000012437 in Epoch 512
Epoch 518
Epoch 518, Loss: 0.000134567, Improvement: 0.000073710, Best Loss: 0.000012437 in Epoch 512
Epoch 519
Epoch 519, Loss: 0.000129500, Improvement: -0.000005067, Best Loss: 0.000012437 in Epoch 512
Epoch 520
Epoch 520, Loss: 0.000091747, Improvement: -0.000037754, Best Loss: 0.000012437 in Epoch 512
Epoch 521
Epoch 521, Loss: 0.000143924, Improvement: 0.000052178, Best Loss: 0.000012437 in Epoch 512
Epoch 522
Epoch 522, Loss: 0.000062538, Improvement: -0.000081386, Best Loss: 0.000012437 in Epoch 512
Epoch 523
Epoch 523, Loss: 0.000043814, Improvement: -0.000018724, Best Loss: 0.000012437 in Epoch 512
Epoch 524
Epoch 524, Loss: 0.000032930, Improvement: -0.000010884, Best Loss: 0.000012437 in Epoch 512
Epoch 525
Epoch 525, Loss: 0.000034318, Improvement: 0.000001388, Best Loss: 0.000012437 in Epoch 512
Epoch 526
Epoch 526, Loss: 0.000033531, Improvement: -0.000000788, Best Loss: 0.000012437 in Epoch 512
Epoch 527
Epoch 527, Loss: 0.000029817, Improvement: -0.000003713, Best Loss: 0.000012437 in Epoch 512
Epoch 528
Epoch 528, Loss: 0.000026842, Improvement: -0.000002975, Best Loss: 0.000012437 in Epoch 512
Epoch 529
Epoch 529, Loss: 0.000024547, Improvement: -0.000002295, Best Loss: 0.000012437 in Epoch 512
Epoch 530
Epoch 530, Loss: 0.000025002, Improvement: 0.000000456, Best Loss: 0.000012437 in Epoch 512
Epoch 531
Epoch 531, Loss: 0.000022702, Improvement: -0.000002300, Best Loss: 0.000012437 in Epoch 512
Epoch 532
Epoch 532, Loss: 0.000020676, Improvement: -0.000002026, Best Loss: 0.000012437 in Epoch 512
Epoch 533
Epoch 533, Loss: 0.000023394, Improvement: 0.000002718, Best Loss: 0.000012437 in Epoch 512
Epoch 534
Epoch 534, Loss: 0.000030577, Improvement: 0.000007183, Best Loss: 0.000012437 in Epoch 512
Epoch 535
Epoch 535, Loss: 0.000033276, Improvement: 0.000002699, Best Loss: 0.000012437 in Epoch 512
Epoch 536
Epoch 536, Loss: 0.000025879, Improvement: -0.000007397, Best Loss: 0.000012437 in Epoch 512
Epoch 537
Epoch 537, Loss: 0.000047144, Improvement: 0.000021265, Best Loss: 0.000012437 in Epoch 512
Epoch 538
Epoch 538, Loss: 0.000096773, Improvement: 0.000049629, Best Loss: 0.000012437 in Epoch 512
Epoch 539
Epoch 539, Loss: 0.000199124, Improvement: 0.000102351, Best Loss: 0.000012437 in Epoch 512
Epoch 540
Epoch 540, Loss: 0.000109559, Improvement: -0.000089565, Best Loss: 0.000012437 in Epoch 512
Epoch 541
Epoch 541, Loss: 0.000092507, Improvement: -0.000017053, Best Loss: 0.000012437 in Epoch 512
Epoch 542
Epoch 542, Loss: 0.000124814, Improvement: 0.000032307, Best Loss: 0.000012437 in Epoch 512
Epoch 543
Epoch 543, Loss: 0.000076464, Improvement: -0.000048350, Best Loss: 0.000012437 in Epoch 512
Epoch 544
Epoch 544, Loss: 0.000047961, Improvement: -0.000028503, Best Loss: 0.000012437 in Epoch 512
Epoch 545
Epoch 545, Loss: 0.000041150, Improvement: -0.000006811, Best Loss: 0.000012437 in Epoch 512
Epoch 546
Epoch 546, Loss: 0.000028928, Improvement: -0.000012221, Best Loss: 0.000012437 in Epoch 512
Epoch 547
Epoch 547, Loss: 0.000027759, Improvement: -0.000001169, Best Loss: 0.000012437 in Epoch 512
Epoch 548
Epoch 548, Loss: 0.000029471, Improvement: 0.000001712, Best Loss: 0.000012437 in Epoch 512
Epoch 549
Epoch 549, Loss: 0.000041358, Improvement: 0.000011886, Best Loss: 0.000012437 in Epoch 512
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.000038818, Improvement: -0.000002539, Best Loss: 0.000012437 in Epoch 512
Epoch 551
Epoch 551, Loss: 0.000033562, Improvement: -0.000005257, Best Loss: 0.000012437 in Epoch 512
Epoch 552
Epoch 552, Loss: 0.000048241, Improvement: 0.000014680, Best Loss: 0.000012437 in Epoch 512
Epoch 553
Epoch 553, Loss: 0.000049316, Improvement: 0.000001075, Best Loss: 0.000012437 in Epoch 512
Epoch 554
Epoch 554, Loss: 0.000073376, Improvement: 0.000024060, Best Loss: 0.000012437 in Epoch 512
Epoch 555
Epoch 555, Loss: 0.000071263, Improvement: -0.000002112, Best Loss: 0.000012437 in Epoch 512
Epoch 556
Epoch 556, Loss: 0.000064324, Improvement: -0.000006939, Best Loss: 0.000012437 in Epoch 512
Epoch 557
Epoch 557, Loss: 0.000036128, Improvement: -0.000028196, Best Loss: 0.000012437 in Epoch 512
Epoch 558
Epoch 558, Loss: 0.000027714, Improvement: -0.000008414, Best Loss: 0.000012437 in Epoch 512
Epoch 559
Epoch 559, Loss: 0.000030135, Improvement: 0.000002421, Best Loss: 0.000012437 in Epoch 512
Epoch 560
Epoch 560, Loss: 0.000051475, Improvement: 0.000021340, Best Loss: 0.000012437 in Epoch 512
Epoch 561
Epoch 561, Loss: 0.000042605, Improvement: -0.000008870, Best Loss: 0.000012437 in Epoch 512
Epoch 562
Epoch 562, Loss: 0.000035067, Improvement: -0.000007538, Best Loss: 0.000012437 in Epoch 512
Epoch 563
Epoch 563, Loss: 0.000055758, Improvement: 0.000020692, Best Loss: 0.000012437 in Epoch 512
Epoch 564
Epoch 564, Loss: 0.000061747, Improvement: 0.000005989, Best Loss: 0.000012437 in Epoch 512
Epoch 565
Epoch 565, Loss: 0.000050140, Improvement: -0.000011607, Best Loss: 0.000012437 in Epoch 512
Epoch 566
Epoch 566, Loss: 0.000039221, Improvement: -0.000010919, Best Loss: 0.000012437 in Epoch 512
Epoch 567
Epoch 567, Loss: 0.000035138, Improvement: -0.000004082, Best Loss: 0.000012437 in Epoch 512
Epoch 568
Epoch 568, Loss: 0.000049183, Improvement: 0.000014044, Best Loss: 0.000012437 in Epoch 512
Epoch 569
Epoch 569, Loss: 0.000036303, Improvement: -0.000012879, Best Loss: 0.000012437 in Epoch 512
Epoch 570
Epoch 570, Loss: 0.000035719, Improvement: -0.000000584, Best Loss: 0.000012437 in Epoch 512
Epoch 571
Epoch 571, Loss: 0.000062513, Improvement: 0.000026793, Best Loss: 0.000012437 in Epoch 512
Epoch 572
Epoch 572, Loss: 0.000080245, Improvement: 0.000017732, Best Loss: 0.000012437 in Epoch 512
Epoch 573
Epoch 573, Loss: 0.000086269, Improvement: 0.000006025, Best Loss: 0.000012437 in Epoch 512
Epoch 574
Epoch 574, Loss: 0.000039871, Improvement: -0.000046399, Best Loss: 0.000012437 in Epoch 512
Epoch 575
Epoch 575, Loss: 0.000030512, Improvement: -0.000009359, Best Loss: 0.000012437 in Epoch 512
Epoch 576
Epoch 576, Loss: 0.000071656, Improvement: 0.000041143, Best Loss: 0.000012437 in Epoch 512
Epoch 577
Epoch 577, Loss: 0.000104997, Improvement: 0.000033342, Best Loss: 0.000012437 in Epoch 512
Epoch 578
Epoch 578, Loss: 0.000096897, Improvement: -0.000008101, Best Loss: 0.000012437 in Epoch 512
Epoch 579
Epoch 579, Loss: 0.000139496, Improvement: 0.000042600, Best Loss: 0.000012437 in Epoch 512
Epoch 580
Epoch 580, Loss: 0.000123360, Improvement: -0.000016136, Best Loss: 0.000012437 in Epoch 512
Epoch 581
Epoch 581, Loss: 0.000060727, Improvement: -0.000062633, Best Loss: 0.000012437 in Epoch 512
Epoch 582
Epoch 582, Loss: 0.000047973, Improvement: -0.000012754, Best Loss: 0.000012437 in Epoch 512
Epoch 583
Epoch 583, Loss: 0.000037307, Improvement: -0.000010666, Best Loss: 0.000012437 in Epoch 512
Epoch 584
Epoch 584, Loss: 0.000025149, Improvement: -0.000012158, Best Loss: 0.000012437 in Epoch 512
Epoch 585
Epoch 585, Loss: 0.000021930, Improvement: -0.000003218, Best Loss: 0.000012437 in Epoch 512
Epoch 586
Epoch 586, Loss: 0.000027024, Improvement: 0.000005094, Best Loss: 0.000012437 in Epoch 512
Epoch 587
Epoch 587, Loss: 0.000028036, Improvement: 0.000001012, Best Loss: 0.000012437 in Epoch 512
Epoch 588
Epoch 588, Loss: 0.000026470, Improvement: -0.000001566, Best Loss: 0.000012437 in Epoch 512
Epoch 589
A best model at epoch 589 has been saved with training error 0.000011899.
A best model at epoch 589 has been saved with training error 0.000011614.
Epoch 589, Loss: 0.000017908, Improvement: -0.000008562, Best Loss: 0.000011614 in Epoch 589
Epoch 590
Epoch 590, Loss: 0.000018912, Improvement: 0.000001003, Best Loss: 0.000011614 in Epoch 589
Epoch 591
Epoch 591, Loss: 0.000034049, Improvement: 0.000015137, Best Loss: 0.000011614 in Epoch 589
Epoch 592
Epoch 592, Loss: 0.000032631, Improvement: -0.000001417, Best Loss: 0.000011614 in Epoch 589
Epoch 593
Epoch 593, Loss: 0.000022768, Improvement: -0.000009863, Best Loss: 0.000011614 in Epoch 589
Epoch 594
Epoch 594, Loss: 0.000018332, Improvement: -0.000004436, Best Loss: 0.000011614 in Epoch 589
Epoch 595
Epoch 595, Loss: 0.000023636, Improvement: 0.000005304, Best Loss: 0.000011614 in Epoch 589
Epoch 596
Epoch 596, Loss: 0.000046610, Improvement: 0.000022974, Best Loss: 0.000011614 in Epoch 589
Epoch 597
Epoch 597, Loss: 0.000053157, Improvement: 0.000006547, Best Loss: 0.000011614 in Epoch 589
Epoch 598
Epoch 598, Loss: 0.000052657, Improvement: -0.000000500, Best Loss: 0.000011614 in Epoch 589
Epoch 599
Epoch 599, Loss: 0.000029424, Improvement: -0.000023233, Best Loss: 0.000011614 in Epoch 589
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.000022627, Improvement: -0.000006797, Best Loss: 0.000011614 in Epoch 589
Epoch 601
Epoch 601, Loss: 0.000018859, Improvement: -0.000003768, Best Loss: 0.000011614 in Epoch 589
Epoch 602
Epoch 602, Loss: 0.000020863, Improvement: 0.000002004, Best Loss: 0.000011614 in Epoch 589
Epoch 603
A best model at epoch 603 has been saved with training error 0.000011464.
Epoch 603, Loss: 0.000021640, Improvement: 0.000000778, Best Loss: 0.000011464 in Epoch 603
Epoch 604
Epoch 604, Loss: 0.000048983, Improvement: 0.000027343, Best Loss: 0.000011464 in Epoch 603
Epoch 605
Epoch 605, Loss: 0.000055052, Improvement: 0.000006069, Best Loss: 0.000011464 in Epoch 603
Epoch 606
Epoch 606, Loss: 0.000053448, Improvement: -0.000001604, Best Loss: 0.000011464 in Epoch 603
Epoch 607
Epoch 607, Loss: 0.000168973, Improvement: 0.000115525, Best Loss: 0.000011464 in Epoch 603
Epoch 608
Epoch 608, Loss: 0.000070057, Improvement: -0.000098916, Best Loss: 0.000011464 in Epoch 603
Epoch 609
Epoch 609, Loss: 0.000054210, Improvement: -0.000015847, Best Loss: 0.000011464 in Epoch 603
Epoch 610
Epoch 610, Loss: 0.000033361, Improvement: -0.000020850, Best Loss: 0.000011464 in Epoch 603
Epoch 611
Epoch 611, Loss: 0.000024941, Improvement: -0.000008420, Best Loss: 0.000011464 in Epoch 603
Epoch 612
Epoch 612, Loss: 0.000025544, Improvement: 0.000000603, Best Loss: 0.000011464 in Epoch 603
Epoch 613
Epoch 613, Loss: 0.000026350, Improvement: 0.000000806, Best Loss: 0.000011464 in Epoch 603
Epoch 614
Epoch 614, Loss: 0.000023077, Improvement: -0.000003273, Best Loss: 0.000011464 in Epoch 603
Epoch 615
Epoch 615, Loss: 0.000027527, Improvement: 0.000004449, Best Loss: 0.000011464 in Epoch 603
Epoch 616
Epoch 616, Loss: 0.000039113, Improvement: 0.000011587, Best Loss: 0.000011464 in Epoch 603
Epoch 617
Epoch 617, Loss: 0.000039708, Improvement: 0.000000595, Best Loss: 0.000011464 in Epoch 603
Epoch 618
Epoch 618, Loss: 0.000031332, Improvement: -0.000008376, Best Loss: 0.000011464 in Epoch 603
Epoch 619
Epoch 619, Loss: 0.000034754, Improvement: 0.000003422, Best Loss: 0.000011464 in Epoch 603
Epoch 620
Epoch 620, Loss: 0.000061515, Improvement: 0.000026761, Best Loss: 0.000011464 in Epoch 603
Epoch 621
Epoch 621, Loss: 0.000148424, Improvement: 0.000086908, Best Loss: 0.000011464 in Epoch 603
Epoch 622
Epoch 622, Loss: 0.000050566, Improvement: -0.000097857, Best Loss: 0.000011464 in Epoch 603
Epoch 623
Epoch 623, Loss: 0.000028087, Improvement: -0.000022479, Best Loss: 0.000011464 in Epoch 603
Epoch 624
Epoch 624, Loss: 0.000021389, Improvement: -0.000006698, Best Loss: 0.000011464 in Epoch 603
Epoch 625
A best model at epoch 625 has been saved with training error 0.000010768.
Epoch 625, Loss: 0.000018668, Improvement: -0.000002721, Best Loss: 0.000010768 in Epoch 625
Epoch 626
A best model at epoch 626 has been saved with training error 0.000010305.
Epoch 626, Loss: 0.000016188, Improvement: -0.000002480, Best Loss: 0.000010305 in Epoch 626
Epoch 627
Epoch 627, Loss: 0.000015620, Improvement: -0.000000568, Best Loss: 0.000010305 in Epoch 626
Epoch 628
Epoch 628, Loss: 0.000014921, Improvement: -0.000000698, Best Loss: 0.000010305 in Epoch 626
Epoch 629
Epoch 629, Loss: 0.000014636, Improvement: -0.000000286, Best Loss: 0.000010305 in Epoch 626
Epoch 630
Epoch 630, Loss: 0.000015320, Improvement: 0.000000684, Best Loss: 0.000010305 in Epoch 626
Epoch 631
A best model at epoch 631 has been saved with training error 0.000009323.
Epoch 631, Loss: 0.000014835, Improvement: -0.000000485, Best Loss: 0.000009323 in Epoch 631
Epoch 632
Epoch 632, Loss: 0.000014664, Improvement: -0.000000171, Best Loss: 0.000009323 in Epoch 631
Epoch 633
Epoch 633, Loss: 0.000015013, Improvement: 0.000000349, Best Loss: 0.000009323 in Epoch 631
Epoch 634
Epoch 634, Loss: 0.000015793, Improvement: 0.000000779, Best Loss: 0.000009323 in Epoch 631
Epoch 635
Epoch 635, Loss: 0.000028509, Improvement: 0.000012716, Best Loss: 0.000009323 in Epoch 631
Epoch 636
Epoch 636, Loss: 0.000044896, Improvement: 0.000016387, Best Loss: 0.000009323 in Epoch 631
Epoch 637
Epoch 637, Loss: 0.000039264, Improvement: -0.000005632, Best Loss: 0.000009323 in Epoch 631
Epoch 638
Epoch 638, Loss: 0.000029353, Improvement: -0.000009911, Best Loss: 0.000009323 in Epoch 631
Epoch 639
Epoch 639, Loss: 0.000021202, Improvement: -0.000008151, Best Loss: 0.000009323 in Epoch 631
Epoch 640
Epoch 640, Loss: 0.000023262, Improvement: 0.000002060, Best Loss: 0.000009323 in Epoch 631
Epoch 641
Epoch 641, Loss: 0.000031919, Improvement: 0.000008657, Best Loss: 0.000009323 in Epoch 631
Epoch 642
Epoch 642, Loss: 0.000026103, Improvement: -0.000005816, Best Loss: 0.000009323 in Epoch 631
Epoch 643
Epoch 643, Loss: 0.000026120, Improvement: 0.000000017, Best Loss: 0.000009323 in Epoch 631
Epoch 644
Epoch 644, Loss: 0.000035965, Improvement: 0.000009845, Best Loss: 0.000009323 in Epoch 631
Epoch 645
Epoch 645, Loss: 0.000026964, Improvement: -0.000009000, Best Loss: 0.000009323 in Epoch 631
Epoch 646
Epoch 646, Loss: 0.000019281, Improvement: -0.000007684, Best Loss: 0.000009323 in Epoch 631
Epoch 647
Epoch 647, Loss: 0.000017167, Improvement: -0.000002113, Best Loss: 0.000009323 in Epoch 631
Epoch 648
Epoch 648, Loss: 0.000019612, Improvement: 0.000002444, Best Loss: 0.000009323 in Epoch 631
Epoch 649
Epoch 649, Loss: 0.000017142, Improvement: -0.000002470, Best Loss: 0.000009323 in Epoch 631
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.000021467, Improvement: 0.000004326, Best Loss: 0.000009323 in Epoch 631
Epoch 651
Epoch 651, Loss: 0.000027600, Improvement: 0.000006133, Best Loss: 0.000009323 in Epoch 631
Epoch 652
Epoch 652, Loss: 0.000019661, Improvement: -0.000007939, Best Loss: 0.000009323 in Epoch 631
Epoch 653
Epoch 653, Loss: 0.000019525, Improvement: -0.000000136, Best Loss: 0.000009323 in Epoch 631
Epoch 654
Epoch 654, Loss: 0.000022698, Improvement: 0.000003173, Best Loss: 0.000009323 in Epoch 631
Epoch 655
Epoch 655, Loss: 0.000039698, Improvement: 0.000017000, Best Loss: 0.000009323 in Epoch 631
Epoch 656
Epoch 656, Loss: 0.000039504, Improvement: -0.000000194, Best Loss: 0.000009323 in Epoch 631
Epoch 657
Epoch 657, Loss: 0.000041762, Improvement: 0.000002257, Best Loss: 0.000009323 in Epoch 631
Epoch 658
Epoch 658, Loss: 0.000038262, Improvement: -0.000003500, Best Loss: 0.000009323 in Epoch 631
Epoch 659
Epoch 659, Loss: 0.000030735, Improvement: -0.000007526, Best Loss: 0.000009323 in Epoch 631
Epoch 660
Epoch 660, Loss: 0.000037146, Improvement: 0.000006410, Best Loss: 0.000009323 in Epoch 631
Epoch 661
Epoch 661, Loss: 0.000033777, Improvement: -0.000003369, Best Loss: 0.000009323 in Epoch 631
Epoch 662
Epoch 662, Loss: 0.000071521, Improvement: 0.000037745, Best Loss: 0.000009323 in Epoch 631
Epoch 663
Epoch 663, Loss: 0.000081345, Improvement: 0.000009824, Best Loss: 0.000009323 in Epoch 631
Epoch 664
Epoch 664, Loss: 0.000079896, Improvement: -0.000001449, Best Loss: 0.000009323 in Epoch 631
Epoch 665
Epoch 665, Loss: 0.000051591, Improvement: -0.000028306, Best Loss: 0.000009323 in Epoch 631
Epoch 666
Epoch 666, Loss: 0.000049926, Improvement: -0.000001665, Best Loss: 0.000009323 in Epoch 631
Epoch 667
Epoch 667, Loss: 0.000025466, Improvement: -0.000024460, Best Loss: 0.000009323 in Epoch 631
Epoch 668
Epoch 668, Loss: 0.000022675, Improvement: -0.000002791, Best Loss: 0.000009323 in Epoch 631
Epoch 669
Epoch 669, Loss: 0.000017828, Improvement: -0.000004847, Best Loss: 0.000009323 in Epoch 631
Epoch 670
Epoch 670, Loss: 0.000018639, Improvement: 0.000000811, Best Loss: 0.000009323 in Epoch 631
Epoch 671
Epoch 671, Loss: 0.000026675, Improvement: 0.000008036, Best Loss: 0.000009323 in Epoch 631
Epoch 672
Epoch 672, Loss: 0.000051287, Improvement: 0.000024612, Best Loss: 0.000009323 in Epoch 631
Epoch 673
Epoch 673, Loss: 0.000046419, Improvement: -0.000004869, Best Loss: 0.000009323 in Epoch 631
Epoch 674
Epoch 674, Loss: 0.000091981, Improvement: 0.000045562, Best Loss: 0.000009323 in Epoch 631
Epoch 675
Epoch 675, Loss: 0.000127428, Improvement: 0.000035447, Best Loss: 0.000009323 in Epoch 631
Epoch 676
Epoch 676, Loss: 0.000098665, Improvement: -0.000028763, Best Loss: 0.000009323 in Epoch 631
Epoch 677
Epoch 677, Loss: 0.000041904, Improvement: -0.000056761, Best Loss: 0.000009323 in Epoch 631
Epoch 678
Epoch 678, Loss: 0.000026560, Improvement: -0.000015343, Best Loss: 0.000009323 in Epoch 631
Epoch 679
Epoch 679, Loss: 0.000019667, Improvement: -0.000006893, Best Loss: 0.000009323 in Epoch 631
Epoch 680
Epoch 680, Loss: 0.000017305, Improvement: -0.000002363, Best Loss: 0.000009323 in Epoch 631
Epoch 681
A best model at epoch 681 has been saved with training error 0.000008311.
Epoch 681, Loss: 0.000013859, Improvement: -0.000003446, Best Loss: 0.000008311 in Epoch 681
Epoch 682
Epoch 682, Loss: 0.000013268, Improvement: -0.000000591, Best Loss: 0.000008311 in Epoch 681
Epoch 683
Epoch 683, Loss: 0.000012982, Improvement: -0.000000286, Best Loss: 0.000008311 in Epoch 681
Epoch 684
A best model at epoch 684 has been saved with training error 0.000008120.
Epoch 684, Loss: 0.000012142, Improvement: -0.000000840, Best Loss: 0.000008120 in Epoch 684
Epoch 685
Epoch 685, Loss: 0.000012275, Improvement: 0.000000133, Best Loss: 0.000008120 in Epoch 684
Epoch 686
Epoch 686, Loss: 0.000013595, Improvement: 0.000001320, Best Loss: 0.000008120 in Epoch 684
Epoch 687
Epoch 687, Loss: 0.000013660, Improvement: 0.000000065, Best Loss: 0.000008120 in Epoch 684
Epoch 688
Epoch 688, Loss: 0.000017813, Improvement: 0.000004153, Best Loss: 0.000008120 in Epoch 684
Epoch 689
Epoch 689, Loss: 0.000046213, Improvement: 0.000028400, Best Loss: 0.000008120 in Epoch 684
Epoch 690
Epoch 690, Loss: 0.000072295, Improvement: 0.000026082, Best Loss: 0.000008120 in Epoch 684
Epoch 691
Epoch 691, Loss: 0.000106002, Improvement: 0.000033708, Best Loss: 0.000008120 in Epoch 684
Epoch 692
Epoch 692, Loss: 0.000087352, Improvement: -0.000018651, Best Loss: 0.000008120 in Epoch 684
Epoch 693
Epoch 693, Loss: 0.000041776, Improvement: -0.000045575, Best Loss: 0.000008120 in Epoch 684
Epoch 694
Epoch 694, Loss: 0.000020949, Improvement: -0.000020827, Best Loss: 0.000008120 in Epoch 684
Epoch 695
Epoch 695, Loss: 0.000015770, Improvement: -0.000005179, Best Loss: 0.000008120 in Epoch 684
Epoch 696
Epoch 696, Loss: 0.000014422, Improvement: -0.000001347, Best Loss: 0.000008120 in Epoch 684
Epoch 697
Epoch 697, Loss: 0.000023693, Improvement: 0.000009270, Best Loss: 0.000008120 in Epoch 684
Epoch 698
Epoch 698, Loss: 0.000021621, Improvement: -0.000002072, Best Loss: 0.000008120 in Epoch 684
Epoch 699
Epoch 699, Loss: 0.000030285, Improvement: 0.000008664, Best Loss: 0.000008120 in Epoch 684
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.000021076, Improvement: -0.000009210, Best Loss: 0.000008120 in Epoch 684
Epoch 701
Epoch 701, Loss: 0.000018173, Improvement: -0.000002902, Best Loss: 0.000008120 in Epoch 684
Epoch 702
Epoch 702, Loss: 0.000022446, Improvement: 0.000004272, Best Loss: 0.000008120 in Epoch 684
Epoch 703
Epoch 703, Loss: 0.000029666, Improvement: 0.000007220, Best Loss: 0.000008120 in Epoch 684
Epoch 704
Epoch 704, Loss: 0.000063542, Improvement: 0.000033876, Best Loss: 0.000008120 in Epoch 684
Epoch 705
Epoch 705, Loss: 0.000052844, Improvement: -0.000010698, Best Loss: 0.000008120 in Epoch 684
Epoch 706
Epoch 706, Loss: 0.000042920, Improvement: -0.000009924, Best Loss: 0.000008120 in Epoch 684
Epoch 707
Epoch 707, Loss: 0.000030166, Improvement: -0.000012753, Best Loss: 0.000008120 in Epoch 684
Epoch 708
Epoch 708, Loss: 0.000030365, Improvement: 0.000000198, Best Loss: 0.000008120 in Epoch 684
Epoch 709
Epoch 709, Loss: 0.000049622, Improvement: 0.000019257, Best Loss: 0.000008120 in Epoch 684
Epoch 710
Epoch 710, Loss: 0.000073548, Improvement: 0.000023926, Best Loss: 0.000008120 in Epoch 684
Epoch 711
Epoch 711, Loss: 0.000087902, Improvement: 0.000014354, Best Loss: 0.000008120 in Epoch 684
Epoch 712
Epoch 712, Loss: 0.000042799, Improvement: -0.000045103, Best Loss: 0.000008120 in Epoch 684
Epoch 713
Epoch 713, Loss: 0.000027217, Improvement: -0.000015582, Best Loss: 0.000008120 in Epoch 684
Epoch 714
Epoch 714, Loss: 0.000021007, Improvement: -0.000006210, Best Loss: 0.000008120 in Epoch 684
Epoch 715
Epoch 715, Loss: 0.000018650, Improvement: -0.000002357, Best Loss: 0.000008120 in Epoch 684
Epoch 716
Epoch 716, Loss: 0.000013827, Improvement: -0.000004823, Best Loss: 0.000008120 in Epoch 684
Epoch 717
Epoch 717, Loss: 0.000014662, Improvement: 0.000000835, Best Loss: 0.000008120 in Epoch 684
Epoch 718
Epoch 718, Loss: 0.000013910, Improvement: -0.000000752, Best Loss: 0.000008120 in Epoch 684
Epoch 719
Epoch 719, Loss: 0.000013131, Improvement: -0.000000779, Best Loss: 0.000008120 in Epoch 684
Epoch 720
Epoch 720, Loss: 0.000016609, Improvement: 0.000003477, Best Loss: 0.000008120 in Epoch 684
Epoch 721
Epoch 721, Loss: 0.000018569, Improvement: 0.000001961, Best Loss: 0.000008120 in Epoch 684
Epoch 722
Epoch 722, Loss: 0.000016737, Improvement: -0.000001832, Best Loss: 0.000008120 in Epoch 684
Epoch 723
Epoch 723, Loss: 0.000019674, Improvement: 0.000002937, Best Loss: 0.000008120 in Epoch 684
Epoch 724
Epoch 724, Loss: 0.000014685, Improvement: -0.000004988, Best Loss: 0.000008120 in Epoch 684
Epoch 725
Epoch 725, Loss: 0.000011498, Improvement: -0.000003187, Best Loss: 0.000008120 in Epoch 684
Epoch 726
A best model at epoch 726 has been saved with training error 0.000007930.
Epoch 726, Loss: 0.000010514, Improvement: -0.000000984, Best Loss: 0.000007930 in Epoch 726
Epoch 727
Epoch 727, Loss: 0.000013937, Improvement: 0.000003423, Best Loss: 0.000007930 in Epoch 726
Epoch 728
Epoch 728, Loss: 0.000046470, Improvement: 0.000032533, Best Loss: 0.000007930 in Epoch 726
Epoch 729
Epoch 729, Loss: 0.000055073, Improvement: 0.000008603, Best Loss: 0.000007930 in Epoch 726
Epoch 730
Epoch 730, Loss: 0.000094746, Improvement: 0.000039674, Best Loss: 0.000007930 in Epoch 726
Epoch 731
Epoch 731, Loss: 0.000042259, Improvement: -0.000052487, Best Loss: 0.000007930 in Epoch 726
Epoch 732
Epoch 732, Loss: 0.000022828, Improvement: -0.000019432, Best Loss: 0.000007930 in Epoch 726
Epoch 733
Epoch 733, Loss: 0.000028011, Improvement: 0.000005184, Best Loss: 0.000007930 in Epoch 726
Epoch 734
Epoch 734, Loss: 0.000042553, Improvement: 0.000014542, Best Loss: 0.000007930 in Epoch 726
Epoch 735
Epoch 735, Loss: 0.000028462, Improvement: -0.000014091, Best Loss: 0.000007930 in Epoch 726
Epoch 736
Epoch 736, Loss: 0.000021009, Improvement: -0.000007453, Best Loss: 0.000007930 in Epoch 726
Epoch 737
Epoch 737, Loss: 0.000038634, Improvement: 0.000017625, Best Loss: 0.000007930 in Epoch 726
Epoch 738
Epoch 738, Loss: 0.000043001, Improvement: 0.000004366, Best Loss: 0.000007930 in Epoch 726
Epoch 739
Epoch 739, Loss: 0.000023148, Improvement: -0.000019853, Best Loss: 0.000007930 in Epoch 726
Epoch 740
Epoch 740, Loss: 0.000019086, Improvement: -0.000004061, Best Loss: 0.000007930 in Epoch 726
Epoch 741
Epoch 741, Loss: 0.000028218, Improvement: 0.000009132, Best Loss: 0.000007930 in Epoch 726
Epoch 742
Epoch 742, Loss: 0.000047938, Improvement: 0.000019719, Best Loss: 0.000007930 in Epoch 726
Epoch 743
Epoch 743, Loss: 0.000027171, Improvement: -0.000020767, Best Loss: 0.000007930 in Epoch 726
Epoch 744
Epoch 744, Loss: 0.000015214, Improvement: -0.000011957, Best Loss: 0.000007930 in Epoch 726
Epoch 745
Epoch 745, Loss: 0.000013935, Improvement: -0.000001279, Best Loss: 0.000007930 in Epoch 726
Epoch 746
A best model at epoch 746 has been saved with training error 0.000007601.
Epoch 746, Loss: 0.000014770, Improvement: 0.000000835, Best Loss: 0.000007601 in Epoch 746
Epoch 747
Epoch 747, Loss: 0.000022346, Improvement: 0.000007576, Best Loss: 0.000007601 in Epoch 746
Epoch 748
Epoch 748, Loss: 0.000022013, Improvement: -0.000000333, Best Loss: 0.000007601 in Epoch 746
Epoch 749
Epoch 749, Loss: 0.000021818, Improvement: -0.000000194, Best Loss: 0.000007601 in Epoch 746
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.000023523, Improvement: 0.000001705, Best Loss: 0.000007601 in Epoch 746
Epoch 751
Epoch 751, Loss: 0.000025813, Improvement: 0.000002290, Best Loss: 0.000007601 in Epoch 746
Epoch 752
Epoch 752, Loss: 0.000034672, Improvement: 0.000008859, Best Loss: 0.000007601 in Epoch 746
Epoch 753
Epoch 753, Loss: 0.000036817, Improvement: 0.000002145, Best Loss: 0.000007601 in Epoch 746
Epoch 754
Epoch 754, Loss: 0.000025254, Improvement: -0.000011563, Best Loss: 0.000007601 in Epoch 746
Epoch 755
Epoch 755, Loss: 0.000032085, Improvement: 0.000006831, Best Loss: 0.000007601 in Epoch 746
Epoch 756
Epoch 756, Loss: 0.000024381, Improvement: -0.000007704, Best Loss: 0.000007601 in Epoch 746
Epoch 757
Epoch 757, Loss: 0.000035840, Improvement: 0.000011459, Best Loss: 0.000007601 in Epoch 746
Epoch 758
Epoch 758, Loss: 0.000025772, Improvement: -0.000010067, Best Loss: 0.000007601 in Epoch 746
Epoch 759
Epoch 759, Loss: 0.000015486, Improvement: -0.000010286, Best Loss: 0.000007601 in Epoch 746
Epoch 760
Epoch 760, Loss: 0.000013486, Improvement: -0.000002001, Best Loss: 0.000007601 in Epoch 746
Epoch 761
Epoch 761, Loss: 0.000014226, Improvement: 0.000000741, Best Loss: 0.000007601 in Epoch 746
Epoch 762
Epoch 762, Loss: 0.000015121, Improvement: 0.000000895, Best Loss: 0.000007601 in Epoch 746
Epoch 763
A best model at epoch 763 has been saved with training error 0.000006603.
Epoch 763, Loss: 0.000012409, Improvement: -0.000002712, Best Loss: 0.000006603 in Epoch 763
Epoch 764
Epoch 764, Loss: 0.000016578, Improvement: 0.000004169, Best Loss: 0.000006603 in Epoch 763
Epoch 765
Epoch 765, Loss: 0.000035688, Improvement: 0.000019111, Best Loss: 0.000006603 in Epoch 763
Epoch 766
Epoch 766, Loss: 0.000039708, Improvement: 0.000004020, Best Loss: 0.000006603 in Epoch 763
Epoch 767
Epoch 767, Loss: 0.000151767, Improvement: 0.000112059, Best Loss: 0.000006603 in Epoch 763
Epoch 768
Epoch 768, Loss: 0.000077528, Improvement: -0.000074238, Best Loss: 0.000006603 in Epoch 763
Epoch 769
Epoch 769, Loss: 0.000029604, Improvement: -0.000047925, Best Loss: 0.000006603 in Epoch 763
Epoch 770
Epoch 770, Loss: 0.000016387, Improvement: -0.000013216, Best Loss: 0.000006603 in Epoch 763
Epoch 771
Epoch 771, Loss: 0.000011528, Improvement: -0.000004859, Best Loss: 0.000006603 in Epoch 763
Epoch 772
A best model at epoch 772 has been saved with training error 0.000006216.
Epoch 772, Loss: 0.000009817, Improvement: -0.000001712, Best Loss: 0.000006216 in Epoch 772
Epoch 773
A best model at epoch 773 has been saved with training error 0.000005817.
Epoch 773, Loss: 0.000009167, Improvement: -0.000000649, Best Loss: 0.000005817 in Epoch 773
Epoch 774
Epoch 774, Loss: 0.000008838, Improvement: -0.000000330, Best Loss: 0.000005817 in Epoch 773
Epoch 775
Epoch 775, Loss: 0.000008596, Improvement: -0.000000241, Best Loss: 0.000005817 in Epoch 773
Epoch 776
Epoch 776, Loss: 0.000009072, Improvement: 0.000000475, Best Loss: 0.000005817 in Epoch 773
Epoch 777
Epoch 777, Loss: 0.000008215, Improvement: -0.000000857, Best Loss: 0.000005817 in Epoch 773
Epoch 778
A best model at epoch 778 has been saved with training error 0.000005815.
Epoch 778, Loss: 0.000008344, Improvement: 0.000000129, Best Loss: 0.000005815 in Epoch 778
Epoch 779
Epoch 779, Loss: 0.000009252, Improvement: 0.000000908, Best Loss: 0.000005815 in Epoch 778
Epoch 780
Epoch 780, Loss: 0.000024384, Improvement: 0.000015132, Best Loss: 0.000005815 in Epoch 778
Epoch 781
Epoch 781, Loss: 0.000107173, Improvement: 0.000082789, Best Loss: 0.000005815 in Epoch 778
Epoch 782
Epoch 782, Loss: 0.000068123, Improvement: -0.000039051, Best Loss: 0.000005815 in Epoch 778
Epoch 783
Epoch 783, Loss: 0.000026295, Improvement: -0.000041827, Best Loss: 0.000005815 in Epoch 778
Epoch 784
Epoch 784, Loss: 0.000013717, Improvement: -0.000012578, Best Loss: 0.000005815 in Epoch 778
Epoch 785
Epoch 785, Loss: 0.000010706, Improvement: -0.000003011, Best Loss: 0.000005815 in Epoch 778
Epoch 786
A best model at epoch 786 has been saved with training error 0.000005614.
Epoch 786, Loss: 0.000008773, Improvement: -0.000001933, Best Loss: 0.000005614 in Epoch 786
Epoch 787
Epoch 787, Loss: 0.000008991, Improvement: 0.000000219, Best Loss: 0.000005614 in Epoch 786
Epoch 788
Epoch 788, Loss: 0.000008616, Improvement: -0.000000376, Best Loss: 0.000005614 in Epoch 786
Epoch 789
Epoch 789, Loss: 0.000008053, Improvement: -0.000000563, Best Loss: 0.000005614 in Epoch 786
Epoch 790
A best model at epoch 790 has been saved with training error 0.000004580.
Epoch 790, Loss: 0.000008331, Improvement: 0.000000278, Best Loss: 0.000004580 in Epoch 790
Epoch 791
Epoch 791, Loss: 0.000009341, Improvement: 0.000001010, Best Loss: 0.000004580 in Epoch 790
Epoch 792
Epoch 792, Loss: 0.000010986, Improvement: 0.000001646, Best Loss: 0.000004580 in Epoch 790
Epoch 793
Epoch 793, Loss: 0.000009468, Improvement: -0.000001518, Best Loss: 0.000004580 in Epoch 790
Epoch 794
Epoch 794, Loss: 0.000010809, Improvement: 0.000001341, Best Loss: 0.000004580 in Epoch 790
Epoch 795
Epoch 795, Loss: 0.000009366, Improvement: -0.000001443, Best Loss: 0.000004580 in Epoch 790
Epoch 796
Epoch 796, Loss: 0.000008972, Improvement: -0.000000393, Best Loss: 0.000004580 in Epoch 790
Epoch 797
Epoch 797, Loss: 0.000008464, Improvement: -0.000000509, Best Loss: 0.000004580 in Epoch 790
Epoch 798
Epoch 798, Loss: 0.000010119, Improvement: 0.000001656, Best Loss: 0.000004580 in Epoch 790
Epoch 799
Epoch 799, Loss: 0.000012050, Improvement: 0.000001931, Best Loss: 0.000004580 in Epoch 790
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.000013031, Improvement: 0.000000981, Best Loss: 0.000004580 in Epoch 790
Epoch 801
Epoch 801, Loss: 0.000048717, Improvement: 0.000035685, Best Loss: 0.000004580 in Epoch 790
Epoch 802
Epoch 802, Loss: 0.000051847, Improvement: 0.000003131, Best Loss: 0.000004580 in Epoch 790
Epoch 803
Epoch 803, Loss: 0.000029431, Improvement: -0.000022416, Best Loss: 0.000004580 in Epoch 790
Epoch 804
Epoch 804, Loss: 0.000025752, Improvement: -0.000003679, Best Loss: 0.000004580 in Epoch 790
Epoch 805
Epoch 805, Loss: 0.000028984, Improvement: 0.000003232, Best Loss: 0.000004580 in Epoch 790
Epoch 806
Epoch 806, Loss: 0.000021697, Improvement: -0.000007287, Best Loss: 0.000004580 in Epoch 790
Epoch 807
Epoch 807, Loss: 0.000021601, Improvement: -0.000000097, Best Loss: 0.000004580 in Epoch 790
Epoch 808
Epoch 808, Loss: 0.000018422, Improvement: -0.000003178, Best Loss: 0.000004580 in Epoch 790
Epoch 809
Epoch 809, Loss: 0.000017980, Improvement: -0.000000442, Best Loss: 0.000004580 in Epoch 790
Epoch 810
Epoch 810, Loss: 0.000025608, Improvement: 0.000007628, Best Loss: 0.000004580 in Epoch 790
Epoch 811
Epoch 811, Loss: 0.000039167, Improvement: 0.000013560, Best Loss: 0.000004580 in Epoch 790
Epoch 812
Epoch 812, Loss: 0.000024624, Improvement: -0.000014544, Best Loss: 0.000004580 in Epoch 790
Epoch 813
Epoch 813, Loss: 0.000016880, Improvement: -0.000007744, Best Loss: 0.000004580 in Epoch 790
Epoch 814
Epoch 814, Loss: 0.000015666, Improvement: -0.000001213, Best Loss: 0.000004580 in Epoch 790
Epoch 815
Epoch 815, Loss: 0.000015938, Improvement: 0.000000271, Best Loss: 0.000004580 in Epoch 790
Epoch 816
Epoch 816, Loss: 0.000017476, Improvement: 0.000001538, Best Loss: 0.000004580 in Epoch 790
Epoch 817
Epoch 817, Loss: 0.000013125, Improvement: -0.000004351, Best Loss: 0.000004580 in Epoch 790
Epoch 818
Epoch 818, Loss: 0.000011320, Improvement: -0.000001805, Best Loss: 0.000004580 in Epoch 790
Epoch 819
Epoch 819, Loss: 0.000008757, Improvement: -0.000002563, Best Loss: 0.000004580 in Epoch 790
Epoch 820
Epoch 820, Loss: 0.000013321, Improvement: 0.000004564, Best Loss: 0.000004580 in Epoch 790
Epoch 821
Epoch 821, Loss: 0.000024189, Improvement: 0.000010868, Best Loss: 0.000004580 in Epoch 790
Epoch 822
Epoch 822, Loss: 0.000030287, Improvement: 0.000006098, Best Loss: 0.000004580 in Epoch 790
Epoch 823
Epoch 823, Loss: 0.000047975, Improvement: 0.000017688, Best Loss: 0.000004580 in Epoch 790
Epoch 824
Epoch 824, Loss: 0.000067883, Improvement: 0.000019908, Best Loss: 0.000004580 in Epoch 790
Epoch 825
Epoch 825, Loss: 0.000061177, Improvement: -0.000006706, Best Loss: 0.000004580 in Epoch 790
Epoch 826
Epoch 826, Loss: 0.000030973, Improvement: -0.000030203, Best Loss: 0.000004580 in Epoch 790
Epoch 827
Epoch 827, Loss: 0.000023377, Improvement: -0.000007597, Best Loss: 0.000004580 in Epoch 790
Epoch 828
Epoch 828, Loss: 0.000019785, Improvement: -0.000003591, Best Loss: 0.000004580 in Epoch 790
Epoch 829
Epoch 829, Loss: 0.000026944, Improvement: 0.000007159, Best Loss: 0.000004580 in Epoch 790
Epoch 830
Epoch 830, Loss: 0.000036915, Improvement: 0.000009970, Best Loss: 0.000004580 in Epoch 790
Epoch 831
Epoch 831, Loss: 0.000019820, Improvement: -0.000017094, Best Loss: 0.000004580 in Epoch 790
Epoch 832
Epoch 832, Loss: 0.000017948, Improvement: -0.000001872, Best Loss: 0.000004580 in Epoch 790
Epoch 833
Epoch 833, Loss: 0.000015935, Improvement: -0.000002013, Best Loss: 0.000004580 in Epoch 790
Epoch 834
Epoch 834, Loss: 0.000015536, Improvement: -0.000000399, Best Loss: 0.000004580 in Epoch 790
Epoch 835
Epoch 835, Loss: 0.000022648, Improvement: 0.000007112, Best Loss: 0.000004580 in Epoch 790
Epoch 836
Epoch 836, Loss: 0.000040985, Improvement: 0.000018338, Best Loss: 0.000004580 in Epoch 790
Epoch 837
Epoch 837, Loss: 0.000034338, Improvement: -0.000006648, Best Loss: 0.000004580 in Epoch 790
Epoch 838
Epoch 838, Loss: 0.000019009, Improvement: -0.000015328, Best Loss: 0.000004580 in Epoch 790
Epoch 839
Epoch 839, Loss: 0.000013173, Improvement: -0.000005836, Best Loss: 0.000004580 in Epoch 790
Epoch 840
Epoch 840, Loss: 0.000010899, Improvement: -0.000002274, Best Loss: 0.000004580 in Epoch 790
Epoch 841
Epoch 841, Loss: 0.000009074, Improvement: -0.000001825, Best Loss: 0.000004580 in Epoch 790
Epoch 842
Epoch 842, Loss: 0.000009220, Improvement: 0.000000146, Best Loss: 0.000004580 in Epoch 790
Epoch 843
Epoch 843, Loss: 0.000014021, Improvement: 0.000004800, Best Loss: 0.000004580 in Epoch 790
Epoch 844
Epoch 844, Loss: 0.000018137, Improvement: 0.000004116, Best Loss: 0.000004580 in Epoch 790
Epoch 845
Epoch 845, Loss: 0.000015057, Improvement: -0.000003079, Best Loss: 0.000004580 in Epoch 790
Epoch 846
Epoch 846, Loss: 0.000014940, Improvement: -0.000000117, Best Loss: 0.000004580 in Epoch 790
Epoch 847
Epoch 847, Loss: 0.000013493, Improvement: -0.000001447, Best Loss: 0.000004580 in Epoch 790
Epoch 848
Epoch 848, Loss: 0.000010082, Improvement: -0.000003411, Best Loss: 0.000004580 in Epoch 790
Epoch 849
Epoch 849, Loss: 0.000013607, Improvement: 0.000003525, Best Loss: 0.000004580 in Epoch 790
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.000016501, Improvement: 0.000002894, Best Loss: 0.000004580 in Epoch 790
Epoch 851
Epoch 851, Loss: 0.000044318, Improvement: 0.000027817, Best Loss: 0.000004580 in Epoch 790
Epoch 852
Epoch 852, Loss: 0.000069017, Improvement: 0.000024699, Best Loss: 0.000004580 in Epoch 790
Epoch 853
Epoch 853, Loss: 0.000085557, Improvement: 0.000016540, Best Loss: 0.000004580 in Epoch 790
Epoch 854
Epoch 854, Loss: 0.000062618, Improvement: -0.000022938, Best Loss: 0.000004580 in Epoch 790
Epoch 855
Epoch 855, Loss: 0.000034858, Improvement: -0.000027761, Best Loss: 0.000004580 in Epoch 790
Epoch 856
Epoch 856, Loss: 0.000032312, Improvement: -0.000002546, Best Loss: 0.000004580 in Epoch 790
Epoch 857
Epoch 857, Loss: 0.000017902, Improvement: -0.000014410, Best Loss: 0.000004580 in Epoch 790
Epoch 858
Epoch 858, Loss: 0.000013671, Improvement: -0.000004231, Best Loss: 0.000004580 in Epoch 790
Epoch 859
Epoch 859, Loss: 0.000009735, Improvement: -0.000003936, Best Loss: 0.000004580 in Epoch 790
Epoch 860
Epoch 860, Loss: 0.000021080, Improvement: 0.000011345, Best Loss: 0.000004580 in Epoch 790
Epoch 861
Epoch 861, Loss: 0.000027015, Improvement: 0.000005936, Best Loss: 0.000004580 in Epoch 790
Epoch 862
Epoch 862, Loss: 0.000044559, Improvement: 0.000017543, Best Loss: 0.000004580 in Epoch 790
Epoch 863
Epoch 863, Loss: 0.000027434, Improvement: -0.000017124, Best Loss: 0.000004580 in Epoch 790
Epoch 864
Epoch 864, Loss: 0.000018072, Improvement: -0.000009362, Best Loss: 0.000004580 in Epoch 790
Epoch 865
Epoch 865, Loss: 0.000013968, Improvement: -0.000004104, Best Loss: 0.000004580 in Epoch 790
Epoch 866
Epoch 866, Loss: 0.000010187, Improvement: -0.000003781, Best Loss: 0.000004580 in Epoch 790
Epoch 867
Epoch 867, Loss: 0.000008158, Improvement: -0.000002029, Best Loss: 0.000004580 in Epoch 790
Epoch 868
Epoch 868, Loss: 0.000007595, Improvement: -0.000000563, Best Loss: 0.000004580 in Epoch 790
Epoch 869
Epoch 869, Loss: 0.000006727, Improvement: -0.000000869, Best Loss: 0.000004580 in Epoch 790
Epoch 870
A best model at epoch 870 has been saved with training error 0.000004242.
Epoch 870, Loss: 0.000006075, Improvement: -0.000000652, Best Loss: 0.000004242 in Epoch 870
Epoch 871
A best model at epoch 871 has been saved with training error 0.000004021.
Epoch 871, Loss: 0.000006225, Improvement: 0.000000151, Best Loss: 0.000004021 in Epoch 871
Epoch 872
Epoch 872, Loss: 0.000006966, Improvement: 0.000000741, Best Loss: 0.000004021 in Epoch 871
Epoch 873
Epoch 873, Loss: 0.000010317, Improvement: 0.000003351, Best Loss: 0.000004021 in Epoch 871
Epoch 874
Epoch 874, Loss: 0.000009683, Improvement: -0.000000633, Best Loss: 0.000004021 in Epoch 871
Epoch 875
Epoch 875, Loss: 0.000010896, Improvement: 0.000001213, Best Loss: 0.000004021 in Epoch 871
Epoch 876
Epoch 876, Loss: 0.000025251, Improvement: 0.000014355, Best Loss: 0.000004021 in Epoch 871
Epoch 877
Epoch 877, Loss: 0.000052907, Improvement: 0.000027655, Best Loss: 0.000004021 in Epoch 871
Epoch 878
Epoch 878, Loss: 0.000029154, Improvement: -0.000023752, Best Loss: 0.000004021 in Epoch 871
Epoch 879
Epoch 879, Loss: 0.000015848, Improvement: -0.000013306, Best Loss: 0.000004021 in Epoch 871
Epoch 880
Epoch 880, Loss: 0.000011925, Improvement: -0.000003923, Best Loss: 0.000004021 in Epoch 871
Epoch 881
Epoch 881, Loss: 0.000010410, Improvement: -0.000001516, Best Loss: 0.000004021 in Epoch 871
Epoch 882
Epoch 882, Loss: 0.000008640, Improvement: -0.000001770, Best Loss: 0.000004021 in Epoch 871
Epoch 883
Epoch 883, Loss: 0.000007730, Improvement: -0.000000910, Best Loss: 0.000004021 in Epoch 871
Epoch 884
Epoch 884, Loss: 0.000007284, Improvement: -0.000000446, Best Loss: 0.000004021 in Epoch 871
Epoch 885
Epoch 885, Loss: 0.000006329, Improvement: -0.000000956, Best Loss: 0.000004021 in Epoch 871
Epoch 886
Epoch 886, Loss: 0.000008864, Improvement: 0.000002535, Best Loss: 0.000004021 in Epoch 871
Epoch 887
Epoch 887, Loss: 0.000012483, Improvement: 0.000003619, Best Loss: 0.000004021 in Epoch 871
Epoch 888
Epoch 888, Loss: 0.000023584, Improvement: 0.000011101, Best Loss: 0.000004021 in Epoch 871
Epoch 889
Epoch 889, Loss: 0.000022131, Improvement: -0.000001453, Best Loss: 0.000004021 in Epoch 871
Epoch 890
Epoch 890, Loss: 0.000025411, Improvement: 0.000003280, Best Loss: 0.000004021 in Epoch 871
Epoch 891
Epoch 891, Loss: 0.000023738, Improvement: -0.000001673, Best Loss: 0.000004021 in Epoch 871
Epoch 892
Epoch 892, Loss: 0.000015030, Improvement: -0.000008708, Best Loss: 0.000004021 in Epoch 871
Epoch 893
Epoch 893, Loss: 0.000028520, Improvement: 0.000013490, Best Loss: 0.000004021 in Epoch 871
Epoch 894
Epoch 894, Loss: 0.000047326, Improvement: 0.000018806, Best Loss: 0.000004021 in Epoch 871
Epoch 895
Epoch 895, Loss: 0.000028033, Improvement: -0.000019293, Best Loss: 0.000004021 in Epoch 871
Epoch 896
Epoch 896, Loss: 0.000051655, Improvement: 0.000023623, Best Loss: 0.000004021 in Epoch 871
Epoch 897
Epoch 897, Loss: 0.000029450, Improvement: -0.000022206, Best Loss: 0.000004021 in Epoch 871
Epoch 898
Epoch 898, Loss: 0.000021135, Improvement: -0.000008315, Best Loss: 0.000004021 in Epoch 871
Epoch 899
Epoch 899, Loss: 0.000014235, Improvement: -0.000006900, Best Loss: 0.000004021 in Epoch 871
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.000014181, Improvement: -0.000000054, Best Loss: 0.000004021 in Epoch 871
Epoch 901
Epoch 901, Loss: 0.000017188, Improvement: 0.000003007, Best Loss: 0.000004021 in Epoch 871
Epoch 902
Epoch 902, Loss: 0.000028829, Improvement: 0.000011641, Best Loss: 0.000004021 in Epoch 871
Epoch 903
Epoch 903, Loss: 0.000025077, Improvement: -0.000003752, Best Loss: 0.000004021 in Epoch 871
Epoch 904
Epoch 904, Loss: 0.000015865, Improvement: -0.000009212, Best Loss: 0.000004021 in Epoch 871
Epoch 905
Epoch 905, Loss: 0.000021230, Improvement: 0.000005366, Best Loss: 0.000004021 in Epoch 871
Epoch 906
Epoch 906, Loss: 0.000016611, Improvement: -0.000004619, Best Loss: 0.000004021 in Epoch 871
Epoch 907
Epoch 907, Loss: 0.000013885, Improvement: -0.000002726, Best Loss: 0.000004021 in Epoch 871
Epoch 908
Epoch 908, Loss: 0.000016540, Improvement: 0.000002655, Best Loss: 0.000004021 in Epoch 871
Epoch 909
Epoch 909, Loss: 0.000020044, Improvement: 0.000003504, Best Loss: 0.000004021 in Epoch 871
Epoch 910
Epoch 910, Loss: 0.000021090, Improvement: 0.000001046, Best Loss: 0.000004021 in Epoch 871
Epoch 911
Epoch 911, Loss: 0.000016176, Improvement: -0.000004914, Best Loss: 0.000004021 in Epoch 871
Epoch 912
Epoch 912, Loss: 0.000011128, Improvement: -0.000005048, Best Loss: 0.000004021 in Epoch 871
Epoch 913
Epoch 913, Loss: 0.000012713, Improvement: 0.000001585, Best Loss: 0.000004021 in Epoch 871
Epoch 914
Epoch 914, Loss: 0.000021054, Improvement: 0.000008342, Best Loss: 0.000004021 in Epoch 871
Epoch 915
Epoch 915, Loss: 0.000028338, Improvement: 0.000007284, Best Loss: 0.000004021 in Epoch 871
Epoch 916
Epoch 916, Loss: 0.000031718, Improvement: 0.000003380, Best Loss: 0.000004021 in Epoch 871
Epoch 917
Epoch 917, Loss: 0.000022402, Improvement: -0.000009316, Best Loss: 0.000004021 in Epoch 871
Epoch 918
Epoch 918, Loss: 0.000034589, Improvement: 0.000012187, Best Loss: 0.000004021 in Epoch 871
Epoch 919
Epoch 919, Loss: 0.000030339, Improvement: -0.000004250, Best Loss: 0.000004021 in Epoch 871
Epoch 920
Epoch 920, Loss: 0.000025708, Improvement: -0.000004631, Best Loss: 0.000004021 in Epoch 871
Epoch 921
Epoch 921, Loss: 0.000038909, Improvement: 0.000013201, Best Loss: 0.000004021 in Epoch 871
Epoch 922
Epoch 922, Loss: 0.000036137, Improvement: -0.000002772, Best Loss: 0.000004021 in Epoch 871
Epoch 923
Epoch 923, Loss: 0.000032359, Improvement: -0.000003778, Best Loss: 0.000004021 in Epoch 871
Epoch 924
Epoch 924, Loss: 0.000029665, Improvement: -0.000002694, Best Loss: 0.000004021 in Epoch 871
Epoch 925
Epoch 925, Loss: 0.000018315, Improvement: -0.000011349, Best Loss: 0.000004021 in Epoch 871
Epoch 926
Epoch 926, Loss: 0.000029204, Improvement: 0.000010889, Best Loss: 0.000004021 in Epoch 871
Epoch 927
Epoch 927, Loss: 0.000024429, Improvement: -0.000004775, Best Loss: 0.000004021 in Epoch 871
Epoch 928
Epoch 928, Loss: 0.000024148, Improvement: -0.000000280, Best Loss: 0.000004021 in Epoch 871
Epoch 929
Epoch 929, Loss: 0.000040561, Improvement: 0.000016413, Best Loss: 0.000004021 in Epoch 871
Epoch 930
Epoch 930, Loss: 0.000050059, Improvement: 0.000009498, Best Loss: 0.000004021 in Epoch 871
Epoch 931
Epoch 931, Loss: 0.000050895, Improvement: 0.000000836, Best Loss: 0.000004021 in Epoch 871
Epoch 932
Epoch 932, Loss: 0.000026255, Improvement: -0.000024640, Best Loss: 0.000004021 in Epoch 871
Epoch 933
Epoch 933, Loss: 0.000051590, Improvement: 0.000025335, Best Loss: 0.000004021 in Epoch 871
Epoch 934
Epoch 934, Loss: 0.000045968, Improvement: -0.000005622, Best Loss: 0.000004021 in Epoch 871
Epoch 935
Epoch 935, Loss: 0.000033568, Improvement: -0.000012401, Best Loss: 0.000004021 in Epoch 871
Epoch 936
Epoch 936, Loss: 0.000023349, Improvement: -0.000010219, Best Loss: 0.000004021 in Epoch 871
Epoch 937
Epoch 937, Loss: 0.000040741, Improvement: 0.000017392, Best Loss: 0.000004021 in Epoch 871
Epoch 938
Epoch 938, Loss: 0.000020455, Improvement: -0.000020285, Best Loss: 0.000004021 in Epoch 871
Epoch 939
Epoch 939, Loss: 0.000012248, Improvement: -0.000008208, Best Loss: 0.000004021 in Epoch 871
Epoch 940
Epoch 940, Loss: 0.000011220, Improvement: -0.000001028, Best Loss: 0.000004021 in Epoch 871
Epoch 941
Epoch 941, Loss: 0.000008116, Improvement: -0.000003104, Best Loss: 0.000004021 in Epoch 871
Epoch 942
Epoch 942, Loss: 0.000008130, Improvement: 0.000000013, Best Loss: 0.000004021 in Epoch 871
Epoch 943
Epoch 943, Loss: 0.000007264, Improvement: -0.000000866, Best Loss: 0.000004021 in Epoch 871
Epoch 944
Epoch 944, Loss: 0.000006392, Improvement: -0.000000871, Best Loss: 0.000004021 in Epoch 871
Epoch 945
Epoch 945, Loss: 0.000007222, Improvement: 0.000000830, Best Loss: 0.000004021 in Epoch 871
Epoch 946
Epoch 946, Loss: 0.000006782, Improvement: -0.000000440, Best Loss: 0.000004021 in Epoch 871
Epoch 947
Epoch 947, Loss: 0.000007695, Improvement: 0.000000913, Best Loss: 0.000004021 in Epoch 871
Epoch 948
Epoch 948, Loss: 0.000009761, Improvement: 0.000002065, Best Loss: 0.000004021 in Epoch 871
Epoch 949
Epoch 949, Loss: 0.000012388, Improvement: 0.000002627, Best Loss: 0.000004021 in Epoch 871
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.000013302, Improvement: 0.000000914, Best Loss: 0.000004021 in Epoch 871
Epoch 951
Epoch 951, Loss: 0.000019172, Improvement: 0.000005870, Best Loss: 0.000004021 in Epoch 871
Epoch 952
Epoch 952, Loss: 0.000011861, Improvement: -0.000007312, Best Loss: 0.000004021 in Epoch 871
Epoch 953
Epoch 953, Loss: 0.000013221, Improvement: 0.000001361, Best Loss: 0.000004021 in Epoch 871
Epoch 954
Epoch 954, Loss: 0.000047757, Improvement: 0.000034535, Best Loss: 0.000004021 in Epoch 871
Epoch 955
Epoch 955, Loss: 0.000117799, Improvement: 0.000070042, Best Loss: 0.000004021 in Epoch 871
Epoch 956
Epoch 956, Loss: 0.000060397, Improvement: -0.000057402, Best Loss: 0.000004021 in Epoch 871
Epoch 957
Epoch 957, Loss: 0.000028699, Improvement: -0.000031698, Best Loss: 0.000004021 in Epoch 871
Epoch 958
Epoch 958, Loss: 0.000017729, Improvement: -0.000010969, Best Loss: 0.000004021 in Epoch 871
Epoch 959
Epoch 959, Loss: 0.000010255, Improvement: -0.000007474, Best Loss: 0.000004021 in Epoch 871
Epoch 960
Epoch 960, Loss: 0.000007552, Improvement: -0.000002702, Best Loss: 0.000004021 in Epoch 871
Epoch 961
Epoch 961, Loss: 0.000006575, Improvement: -0.000000977, Best Loss: 0.000004021 in Epoch 871
Epoch 962
Epoch 962, Loss: 0.000005820, Improvement: -0.000000755, Best Loss: 0.000004021 in Epoch 871
Epoch 963
A best model at epoch 963 has been saved with training error 0.000003872.
Epoch 963, Loss: 0.000005465, Improvement: -0.000000355, Best Loss: 0.000003872 in Epoch 963
Epoch 964
A best model at epoch 964 has been saved with training error 0.000003515.
Epoch 964, Loss: 0.000005221, Improvement: -0.000000244, Best Loss: 0.000003515 in Epoch 964
Epoch 965
A best model at epoch 965 has been saved with training error 0.000003217.
Epoch 965, Loss: 0.000005035, Improvement: -0.000000186, Best Loss: 0.000003217 in Epoch 965
Epoch 966
Epoch 966, Loss: 0.000004940, Improvement: -0.000000095, Best Loss: 0.000003217 in Epoch 965
Epoch 967
A best model at epoch 967 has been saved with training error 0.000002924.
Epoch 967, Loss: 0.000004989, Improvement: 0.000000049, Best Loss: 0.000002924 in Epoch 967
Epoch 968
Epoch 968, Loss: 0.000005680, Improvement: 0.000000691, Best Loss: 0.000002924 in Epoch 967
Epoch 969
Epoch 969, Loss: 0.000005587, Improvement: -0.000000093, Best Loss: 0.000002924 in Epoch 967
Epoch 970
Epoch 970, Loss: 0.000005444, Improvement: -0.000000143, Best Loss: 0.000002924 in Epoch 967
Epoch 971
Epoch 971, Loss: 0.000006846, Improvement: 0.000001402, Best Loss: 0.000002924 in Epoch 967
Epoch 972
Epoch 972, Loss: 0.000006270, Improvement: -0.000000576, Best Loss: 0.000002924 in Epoch 967
Epoch 973
Epoch 973, Loss: 0.000011362, Improvement: 0.000005092, Best Loss: 0.000002924 in Epoch 967
Epoch 974
Epoch 974, Loss: 0.000017762, Improvement: 0.000006400, Best Loss: 0.000002924 in Epoch 967
Epoch 975
Epoch 975, Loss: 0.000022473, Improvement: 0.000004710, Best Loss: 0.000002924 in Epoch 967
Epoch 976
Epoch 976, Loss: 0.000024620, Improvement: 0.000002147, Best Loss: 0.000002924 in Epoch 967
Epoch 977
Epoch 977, Loss: 0.000078808, Improvement: 0.000054189, Best Loss: 0.000002924 in Epoch 967
Epoch 978
Epoch 978, Loss: 0.000030226, Improvement: -0.000048582, Best Loss: 0.000002924 in Epoch 967
Epoch 979
Epoch 979, Loss: 0.000014601, Improvement: -0.000015626, Best Loss: 0.000002924 in Epoch 967
Epoch 980
Epoch 980, Loss: 0.000010755, Improvement: -0.000003845, Best Loss: 0.000002924 in Epoch 967
Epoch 981
Epoch 981, Loss: 0.000007602, Improvement: -0.000003153, Best Loss: 0.000002924 in Epoch 967
Epoch 982
Epoch 982, Loss: 0.000006784, Improvement: -0.000000819, Best Loss: 0.000002924 in Epoch 967
Epoch 983
Epoch 983, Loss: 0.000007101, Improvement: 0.000000318, Best Loss: 0.000002924 in Epoch 967
Epoch 984
Epoch 984, Loss: 0.000006171, Improvement: -0.000000931, Best Loss: 0.000002924 in Epoch 967
Epoch 985
Epoch 985, Loss: 0.000005944, Improvement: -0.000000226, Best Loss: 0.000002924 in Epoch 967
Epoch 986
Epoch 986, Loss: 0.000005809, Improvement: -0.000000135, Best Loss: 0.000002924 in Epoch 967
Epoch 987
Epoch 987, Loss: 0.000005926, Improvement: 0.000000117, Best Loss: 0.000002924 in Epoch 967
Epoch 988
Epoch 988, Loss: 0.000008565, Improvement: 0.000002639, Best Loss: 0.000002924 in Epoch 967
Epoch 989
Epoch 989, Loss: 0.000006891, Improvement: -0.000001674, Best Loss: 0.000002924 in Epoch 967
Epoch 990
Epoch 990, Loss: 0.000007774, Improvement: 0.000000883, Best Loss: 0.000002924 in Epoch 967
Epoch 991
Epoch 991, Loss: 0.000006246, Improvement: -0.000001528, Best Loss: 0.000002924 in Epoch 967
Epoch 992
Epoch 992, Loss: 0.000006198, Improvement: -0.000000048, Best Loss: 0.000002924 in Epoch 967
Epoch 993
Epoch 993, Loss: 0.000008622, Improvement: 0.000002424, Best Loss: 0.000002924 in Epoch 967
Epoch 994
Epoch 994, Loss: 0.000009234, Improvement: 0.000000611, Best Loss: 0.000002924 in Epoch 967
Epoch 995
Epoch 995, Loss: 0.000009376, Improvement: 0.000000142, Best Loss: 0.000002924 in Epoch 967
Epoch 996
Epoch 996, Loss: 0.000018297, Improvement: 0.000008920, Best Loss: 0.000002924 in Epoch 967
Epoch 997
Epoch 997, Loss: 0.000057027, Improvement: 0.000038731, Best Loss: 0.000002924 in Epoch 967
Epoch 998
Epoch 998, Loss: 0.000044968, Improvement: -0.000012059, Best Loss: 0.000002924 in Epoch 967
Epoch 999
Epoch 999, Loss: 0.000034524, Improvement: -0.000010443, Best Loss: 0.000002924 in Epoch 967
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.000021467, Improvement: -0.000013058, Best Loss: 0.000002924 in Epoch 967
Epoch 1001
Epoch 1001, Loss: 0.000017084, Improvement: -0.000004382, Best Loss: 0.000002924 in Epoch 967
Epoch 1002
Epoch 1002, Loss: 0.000014362, Improvement: -0.000002722, Best Loss: 0.000002924 in Epoch 967
Epoch 1003
Epoch 1003, Loss: 0.000010107, Improvement: -0.000004255, Best Loss: 0.000002924 in Epoch 967
Epoch 1004
Epoch 1004, Loss: 0.000007516, Improvement: -0.000002591, Best Loss: 0.000002924 in Epoch 967
Epoch 1005
Epoch 1005, Loss: 0.000007548, Improvement: 0.000000032, Best Loss: 0.000002924 in Epoch 967
Epoch 1006
Epoch 1006, Loss: 0.000008656, Improvement: 0.000001108, Best Loss: 0.000002924 in Epoch 967
Epoch 1007
Epoch 1007, Loss: 0.000007789, Improvement: -0.000000867, Best Loss: 0.000002924 in Epoch 967
Epoch 1008
Epoch 1008, Loss: 0.000006853, Improvement: -0.000000937, Best Loss: 0.000002924 in Epoch 967
Epoch 1009
Epoch 1009, Loss: 0.000005254, Improvement: -0.000001599, Best Loss: 0.000002924 in Epoch 967
Epoch 1010
Epoch 1010, Loss: 0.000005115, Improvement: -0.000000138, Best Loss: 0.000002924 in Epoch 967
Epoch 1011
Epoch 1011, Loss: 0.000004722, Improvement: -0.000000393, Best Loss: 0.000002924 in Epoch 967
Epoch 1012
Epoch 1012, Loss: 0.000007531, Improvement: 0.000002809, Best Loss: 0.000002924 in Epoch 967
Epoch 1013
Epoch 1013, Loss: 0.000008325, Improvement: 0.000000794, Best Loss: 0.000002924 in Epoch 967
Epoch 1014
Epoch 1014, Loss: 0.000007909, Improvement: -0.000000416, Best Loss: 0.000002924 in Epoch 967
Epoch 1015
Epoch 1015, Loss: 0.000007992, Improvement: 0.000000084, Best Loss: 0.000002924 in Epoch 967
Epoch 1016
Epoch 1016, Loss: 0.000018100, Improvement: 0.000010108, Best Loss: 0.000002924 in Epoch 967
Epoch 1017
Epoch 1017, Loss: 0.000023949, Improvement: 0.000005849, Best Loss: 0.000002924 in Epoch 967
Epoch 1018
Epoch 1018, Loss: 0.000023275, Improvement: -0.000000674, Best Loss: 0.000002924 in Epoch 967
Epoch 1019
Epoch 1019, Loss: 0.000011563, Improvement: -0.000011712, Best Loss: 0.000002924 in Epoch 967
Epoch 1020
Epoch 1020, Loss: 0.000008453, Improvement: -0.000003110, Best Loss: 0.000002924 in Epoch 967
Epoch 1021
Epoch 1021, Loss: 0.000006267, Improvement: -0.000002186, Best Loss: 0.000002924 in Epoch 967
Epoch 1022
Epoch 1022, Loss: 0.000007170, Improvement: 0.000000903, Best Loss: 0.000002924 in Epoch 967
Epoch 1023
Epoch 1023, Loss: 0.000008007, Improvement: 0.000000837, Best Loss: 0.000002924 in Epoch 967
Epoch 1024
Epoch 1024, Loss: 0.000009306, Improvement: 0.000001298, Best Loss: 0.000002924 in Epoch 967
Epoch 1025
Epoch 1025, Loss: 0.000029314, Improvement: 0.000020008, Best Loss: 0.000002924 in Epoch 967
Epoch 1026
Epoch 1026, Loss: 0.000047303, Improvement: 0.000017988, Best Loss: 0.000002924 in Epoch 967
Epoch 1027
Epoch 1027, Loss: 0.000037870, Improvement: -0.000009433, Best Loss: 0.000002924 in Epoch 967
Epoch 1028
Epoch 1028, Loss: 0.000043398, Improvement: 0.000005528, Best Loss: 0.000002924 in Epoch 967
Epoch 1029
Epoch 1029, Loss: 0.000052321, Improvement: 0.000008923, Best Loss: 0.000002924 in Epoch 967
Epoch 1030
Epoch 1030, Loss: 0.000046212, Improvement: -0.000006108, Best Loss: 0.000002924 in Epoch 967
Epoch 1031
Epoch 1031, Loss: 0.000045819, Improvement: -0.000000394, Best Loss: 0.000002924 in Epoch 967
Epoch 1032
Epoch 1032, Loss: 0.000031825, Improvement: -0.000013994, Best Loss: 0.000002924 in Epoch 967
Epoch 1033
Epoch 1033, Loss: 0.000015446, Improvement: -0.000016378, Best Loss: 0.000002924 in Epoch 967
Epoch 1034
Epoch 1034, Loss: 0.000009447, Improvement: -0.000006000, Best Loss: 0.000002924 in Epoch 967
Epoch 1035
Epoch 1035, Loss: 0.000006338, Improvement: -0.000003109, Best Loss: 0.000002924 in Epoch 967
Epoch 1036
Epoch 1036, Loss: 0.000005623, Improvement: -0.000000715, Best Loss: 0.000002924 in Epoch 967
Epoch 1037
Epoch 1037, Loss: 0.000005377, Improvement: -0.000000246, Best Loss: 0.000002924 in Epoch 967
Epoch 1038
Epoch 1038, Loss: 0.000004998, Improvement: -0.000000380, Best Loss: 0.000002924 in Epoch 967
Epoch 1039
Epoch 1039, Loss: 0.000004502, Improvement: -0.000000495, Best Loss: 0.000002924 in Epoch 967
Epoch 1040
Epoch 1040, Loss: 0.000005092, Improvement: 0.000000590, Best Loss: 0.000002924 in Epoch 967
Epoch 1041
Epoch 1041, Loss: 0.000005728, Improvement: 0.000000636, Best Loss: 0.000002924 in Epoch 967
Epoch 1042
Epoch 1042, Loss: 0.000010629, Improvement: 0.000004901, Best Loss: 0.000002924 in Epoch 967
Epoch 1043
Epoch 1043, Loss: 0.000011470, Improvement: 0.000000841, Best Loss: 0.000002924 in Epoch 967
Epoch 1044
Epoch 1044, Loss: 0.000009106, Improvement: -0.000002364, Best Loss: 0.000002924 in Epoch 967
Epoch 1045
Epoch 1045, Loss: 0.000009321, Improvement: 0.000000215, Best Loss: 0.000002924 in Epoch 967
Epoch 1046
Epoch 1046, Loss: 0.000007801, Improvement: -0.000001520, Best Loss: 0.000002924 in Epoch 967
Epoch 1047
Epoch 1047, Loss: 0.000010354, Improvement: 0.000002553, Best Loss: 0.000002924 in Epoch 967
Epoch 1048
Epoch 1048, Loss: 0.000009435, Improvement: -0.000000919, Best Loss: 0.000002924 in Epoch 967
Epoch 1049
Epoch 1049, Loss: 0.000017992, Improvement: 0.000008557, Best Loss: 0.000002924 in Epoch 967
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.000023331, Improvement: 0.000005339, Best Loss: 0.000002924 in Epoch 967
Epoch 1051
Epoch 1051, Loss: 0.000017942, Improvement: -0.000005389, Best Loss: 0.000002924 in Epoch 967
Epoch 1052
Epoch 1052, Loss: 0.000021179, Improvement: 0.000003237, Best Loss: 0.000002924 in Epoch 967
Epoch 1053
Epoch 1053, Loss: 0.000012740, Improvement: -0.000008439, Best Loss: 0.000002924 in Epoch 967
Epoch 1054
Epoch 1054, Loss: 0.000028914, Improvement: 0.000016173, Best Loss: 0.000002924 in Epoch 967
Epoch 1055
Epoch 1055, Loss: 0.000045928, Improvement: 0.000017014, Best Loss: 0.000002924 in Epoch 967
Epoch 1056
Epoch 1056, Loss: 0.000018014, Improvement: -0.000027914, Best Loss: 0.000002924 in Epoch 967
Epoch 1057
Epoch 1057, Loss: 0.000017039, Improvement: -0.000000975, Best Loss: 0.000002924 in Epoch 967
Epoch 1058
Epoch 1058, Loss: 0.000008328, Improvement: -0.000008711, Best Loss: 0.000002924 in Epoch 967
Epoch 1059
Epoch 1059, Loss: 0.000006813, Improvement: -0.000001515, Best Loss: 0.000002924 in Epoch 967
Epoch 1060
Epoch 1060, Loss: 0.000012025, Improvement: 0.000005212, Best Loss: 0.000002924 in Epoch 967
Epoch 1061
Epoch 1061, Loss: 0.000022094, Improvement: 0.000010070, Best Loss: 0.000002924 in Epoch 967
Epoch 1062
Epoch 1062, Loss: 0.000023088, Improvement: 0.000000994, Best Loss: 0.000002924 in Epoch 967
Epoch 1063
Epoch 1063, Loss: 0.000021479, Improvement: -0.000001609, Best Loss: 0.000002924 in Epoch 967
Epoch 1064
Epoch 1064, Loss: 0.000040238, Improvement: 0.000018759, Best Loss: 0.000002924 in Epoch 967
Epoch 1065
Epoch 1065, Loss: 0.000040436, Improvement: 0.000000198, Best Loss: 0.000002924 in Epoch 967
Epoch 1066
Epoch 1066, Loss: 0.000017534, Improvement: -0.000022902, Best Loss: 0.000002924 in Epoch 967
Epoch 1067
Epoch 1067, Loss: 0.000012233, Improvement: -0.000005301, Best Loss: 0.000002924 in Epoch 967
Epoch 1068
Epoch 1068, Loss: 0.000007103, Improvement: -0.000005130, Best Loss: 0.000002924 in Epoch 967
Epoch 1069
Epoch 1069, Loss: 0.000005233, Improvement: -0.000001870, Best Loss: 0.000002924 in Epoch 967
Epoch 1070
Epoch 1070, Loss: 0.000005289, Improvement: 0.000000056, Best Loss: 0.000002924 in Epoch 967
Epoch 1071
Epoch 1071, Loss: 0.000007947, Improvement: 0.000002658, Best Loss: 0.000002924 in Epoch 967
Epoch 1072
Epoch 1072, Loss: 0.000024211, Improvement: 0.000016265, Best Loss: 0.000002924 in Epoch 967
Epoch 1073
Epoch 1073, Loss: 0.000026541, Improvement: 0.000002330, Best Loss: 0.000002924 in Epoch 967
Epoch 1074
Epoch 1074, Loss: 0.000072938, Improvement: 0.000046396, Best Loss: 0.000002924 in Epoch 967
Epoch 1075
Epoch 1075, Loss: 0.000031097, Improvement: -0.000041840, Best Loss: 0.000002924 in Epoch 967
Epoch 1076
Epoch 1076, Loss: 0.000009478, Improvement: -0.000021619, Best Loss: 0.000002924 in Epoch 967
Epoch 1077
Epoch 1077, Loss: 0.000005585, Improvement: -0.000003893, Best Loss: 0.000002924 in Epoch 967
Epoch 1078
Epoch 1078, Loss: 0.000004493, Improvement: -0.000001092, Best Loss: 0.000002924 in Epoch 967
Epoch 1079
Epoch 1079, Loss: 0.000004093, Improvement: -0.000000400, Best Loss: 0.000002924 in Epoch 967
Epoch 1080
Epoch 1080, Loss: 0.000004441, Improvement: 0.000000347, Best Loss: 0.000002924 in Epoch 967
Epoch 1081
A best model at epoch 1081 has been saved with training error 0.000002888.
A best model at epoch 1081 has been saved with training error 0.000002884.
A best model at epoch 1081 has been saved with training error 0.000002720.
Epoch 1081, Loss: 0.000003860, Improvement: -0.000000580, Best Loss: 0.000002720 in Epoch 1081
Epoch 1082
Epoch 1082, Loss: 0.000003725, Improvement: -0.000000135, Best Loss: 0.000002720 in Epoch 1081
Epoch 1083
A best model at epoch 1083 has been saved with training error 0.000002667.
Epoch 1083, Loss: 0.000003653, Improvement: -0.000000072, Best Loss: 0.000002667 in Epoch 1083
Epoch 1084
A best model at epoch 1084 has been saved with training error 0.000002638.
Epoch 1084, Loss: 0.000003910, Improvement: 0.000000257, Best Loss: 0.000002638 in Epoch 1084
Epoch 1085
Epoch 1085, Loss: 0.000004102, Improvement: 0.000000192, Best Loss: 0.000002638 in Epoch 1084
Epoch 1086
Epoch 1086, Loss: 0.000003706, Improvement: -0.000000396, Best Loss: 0.000002638 in Epoch 1084
Epoch 1087
Epoch 1087, Loss: 0.000004179, Improvement: 0.000000473, Best Loss: 0.000002638 in Epoch 1084
Epoch 1088
Epoch 1088, Loss: 0.000004532, Improvement: 0.000000353, Best Loss: 0.000002638 in Epoch 1084
Epoch 1089
Epoch 1089, Loss: 0.000005553, Improvement: 0.000001021, Best Loss: 0.000002638 in Epoch 1084
Epoch 1090
Epoch 1090, Loss: 0.000005572, Improvement: 0.000000018, Best Loss: 0.000002638 in Epoch 1084
Epoch 1091
Epoch 1091, Loss: 0.000004117, Improvement: -0.000001455, Best Loss: 0.000002638 in Epoch 1084
Epoch 1092
Epoch 1092, Loss: 0.000004782, Improvement: 0.000000665, Best Loss: 0.000002638 in Epoch 1084
Epoch 1093
Epoch 1093, Loss: 0.000005722, Improvement: 0.000000940, Best Loss: 0.000002638 in Epoch 1084
Epoch 1094
Epoch 1094, Loss: 0.000006771, Improvement: 0.000001049, Best Loss: 0.000002638 in Epoch 1084
Epoch 1095
Epoch 1095, Loss: 0.000009986, Improvement: 0.000003216, Best Loss: 0.000002638 in Epoch 1084
Epoch 1096
Epoch 1096, Loss: 0.000007592, Improvement: -0.000002394, Best Loss: 0.000002638 in Epoch 1084
Epoch 1097
Epoch 1097, Loss: 0.000007176, Improvement: -0.000000416, Best Loss: 0.000002638 in Epoch 1084
Epoch 1098
Epoch 1098, Loss: 0.000010112, Improvement: 0.000002936, Best Loss: 0.000002638 in Epoch 1084
Epoch 1099
Epoch 1099, Loss: 0.000014145, Improvement: 0.000004033, Best Loss: 0.000002638 in Epoch 1084
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.000015297, Improvement: 0.000001152, Best Loss: 0.000002638 in Epoch 1084
Epoch 1101
Epoch 1101, Loss: 0.000030296, Improvement: 0.000015000, Best Loss: 0.000002638 in Epoch 1084
Epoch 1102
Epoch 1102, Loss: 0.000059749, Improvement: 0.000029452, Best Loss: 0.000002638 in Epoch 1084
Epoch 1103
Epoch 1103, Loss: 0.000044322, Improvement: -0.000015427, Best Loss: 0.000002638 in Epoch 1084
Epoch 1104
Epoch 1104, Loss: 0.000029872, Improvement: -0.000014450, Best Loss: 0.000002638 in Epoch 1084
Epoch 1105
Epoch 1105, Loss: 0.000017187, Improvement: -0.000012685, Best Loss: 0.000002638 in Epoch 1084
Epoch 1106
Epoch 1106, Loss: 0.000012322, Improvement: -0.000004865, Best Loss: 0.000002638 in Epoch 1084
Epoch 1107
Epoch 1107, Loss: 0.000007481, Improvement: -0.000004841, Best Loss: 0.000002638 in Epoch 1084
Epoch 1108
Epoch 1108, Loss: 0.000004786, Improvement: -0.000002695, Best Loss: 0.000002638 in Epoch 1084
Epoch 1109
Epoch 1109, Loss: 0.000004092, Improvement: -0.000000693, Best Loss: 0.000002638 in Epoch 1084
Epoch 1110
Epoch 1110, Loss: 0.000003836, Improvement: -0.000000256, Best Loss: 0.000002638 in Epoch 1084
Epoch 1111
Epoch 1111, Loss: 0.000003625, Improvement: -0.000000211, Best Loss: 0.000002638 in Epoch 1084
Epoch 1112
Epoch 1112, Loss: 0.000005367, Improvement: 0.000001742, Best Loss: 0.000002638 in Epoch 1084
Epoch 1113
Epoch 1113, Loss: 0.000006414, Improvement: 0.000001047, Best Loss: 0.000002638 in Epoch 1084
Epoch 1114
Epoch 1114, Loss: 0.000015140, Improvement: 0.000008726, Best Loss: 0.000002638 in Epoch 1084
Epoch 1115
Epoch 1115, Loss: 0.000026092, Improvement: 0.000010953, Best Loss: 0.000002638 in Epoch 1084
Epoch 1116
Epoch 1116, Loss: 0.000025094, Improvement: -0.000000999, Best Loss: 0.000002638 in Epoch 1084
Epoch 1117
Epoch 1117, Loss: 0.000012990, Improvement: -0.000012104, Best Loss: 0.000002638 in Epoch 1084
Epoch 1118
Epoch 1118, Loss: 0.000030142, Improvement: 0.000017152, Best Loss: 0.000002638 in Epoch 1084
Epoch 1119
Epoch 1119, Loss: 0.000043908, Improvement: 0.000013766, Best Loss: 0.000002638 in Epoch 1084
Epoch 1120
Epoch 1120, Loss: 0.000021964, Improvement: -0.000021944, Best Loss: 0.000002638 in Epoch 1084
Epoch 1121
Epoch 1121, Loss: 0.000013396, Improvement: -0.000008569, Best Loss: 0.000002638 in Epoch 1084
Epoch 1122
Epoch 1122, Loss: 0.000010278, Improvement: -0.000003117, Best Loss: 0.000002638 in Epoch 1084
Epoch 1123
Epoch 1123, Loss: 0.000006750, Improvement: -0.000003529, Best Loss: 0.000002638 in Epoch 1084
Epoch 1124
Epoch 1124, Loss: 0.000005971, Improvement: -0.000000779, Best Loss: 0.000002638 in Epoch 1084
Epoch 1125
Epoch 1125, Loss: 0.000004879, Improvement: -0.000001092, Best Loss: 0.000002638 in Epoch 1084
Epoch 1126
Epoch 1126, Loss: 0.000005322, Improvement: 0.000000443, Best Loss: 0.000002638 in Epoch 1084
Epoch 1127
Epoch 1127, Loss: 0.000005946, Improvement: 0.000000624, Best Loss: 0.000002638 in Epoch 1084
Epoch 1128
Epoch 1128, Loss: 0.000006188, Improvement: 0.000000242, Best Loss: 0.000002638 in Epoch 1084
Epoch 1129
Epoch 1129, Loss: 0.000006772, Improvement: 0.000000584, Best Loss: 0.000002638 in Epoch 1084
Epoch 1130
Epoch 1130, Loss: 0.000004143, Improvement: -0.000002629, Best Loss: 0.000002638 in Epoch 1084
Epoch 1131
Epoch 1131, Loss: 0.000004200, Improvement: 0.000000057, Best Loss: 0.000002638 in Epoch 1084
Epoch 1132
Epoch 1132, Loss: 0.000005705, Improvement: 0.000001504, Best Loss: 0.000002638 in Epoch 1084
Epoch 1133
Epoch 1133, Loss: 0.000007630, Improvement: 0.000001925, Best Loss: 0.000002638 in Epoch 1084
Epoch 1134
Epoch 1134, Loss: 0.000021707, Improvement: 0.000014076, Best Loss: 0.000002638 in Epoch 1084
Epoch 1135
Epoch 1135, Loss: 0.000026542, Improvement: 0.000004835, Best Loss: 0.000002638 in Epoch 1084
Epoch 1136
Epoch 1136, Loss: 0.000036846, Improvement: 0.000010305, Best Loss: 0.000002638 in Epoch 1084
Epoch 1137
Epoch 1137, Loss: 0.000057225, Improvement: 0.000020379, Best Loss: 0.000002638 in Epoch 1084
Epoch 1138
Epoch 1138, Loss: 0.000047461, Improvement: -0.000009764, Best Loss: 0.000002638 in Epoch 1084
Epoch 1139
Epoch 1139, Loss: 0.000034735, Improvement: -0.000012726, Best Loss: 0.000002638 in Epoch 1084
Epoch 1140
Epoch 1140, Loss: 0.000019722, Improvement: -0.000015013, Best Loss: 0.000002638 in Epoch 1084
Epoch 1141
Epoch 1141, Loss: 0.000016400, Improvement: -0.000003322, Best Loss: 0.000002638 in Epoch 1084
Epoch 1142
Epoch 1142, Loss: 0.000013808, Improvement: -0.000002592, Best Loss: 0.000002638 in Epoch 1084
Epoch 1143
Epoch 1143, Loss: 0.000007386, Improvement: -0.000006422, Best Loss: 0.000002638 in Epoch 1084
Epoch 1144
Epoch 1144, Loss: 0.000004879, Improvement: -0.000002506, Best Loss: 0.000002638 in Epoch 1084
Epoch 1145
Epoch 1145, Loss: 0.000005581, Improvement: 0.000000702, Best Loss: 0.000002638 in Epoch 1084
Epoch 1146
Epoch 1146, Loss: 0.000007166, Improvement: 0.000001585, Best Loss: 0.000002638 in Epoch 1084
Epoch 1147
Epoch 1147, Loss: 0.000008595, Improvement: 0.000001429, Best Loss: 0.000002638 in Epoch 1084
Epoch 1148
Epoch 1148, Loss: 0.000007396, Improvement: -0.000001199, Best Loss: 0.000002638 in Epoch 1084
Epoch 1149
Epoch 1149, Loss: 0.000007602, Improvement: 0.000000206, Best Loss: 0.000002638 in Epoch 1084
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.000009892, Improvement: 0.000002290, Best Loss: 0.000002638 in Epoch 1084
Epoch 1151
Epoch 1151, Loss: 0.000017950, Improvement: 0.000008058, Best Loss: 0.000002638 in Epoch 1084
Epoch 1152
Epoch 1152, Loss: 0.000008699, Improvement: -0.000009251, Best Loss: 0.000002638 in Epoch 1084
Epoch 1153
Epoch 1153, Loss: 0.000007356, Improvement: -0.000001342, Best Loss: 0.000002638 in Epoch 1084
Epoch 1154
Epoch 1154, Loss: 0.000014935, Improvement: 0.000007579, Best Loss: 0.000002638 in Epoch 1084
Epoch 1155
Epoch 1155, Loss: 0.000008158, Improvement: -0.000006777, Best Loss: 0.000002638 in Epoch 1084
Epoch 1156
Epoch 1156, Loss: 0.000008748, Improvement: 0.000000590, Best Loss: 0.000002638 in Epoch 1084
Epoch 1157
Epoch 1157, Loss: 0.000011206, Improvement: 0.000002458, Best Loss: 0.000002638 in Epoch 1084
Epoch 1158
Epoch 1158, Loss: 0.000009037, Improvement: -0.000002170, Best Loss: 0.000002638 in Epoch 1084
Epoch 1159
Epoch 1159, Loss: 0.000016529, Improvement: 0.000007492, Best Loss: 0.000002638 in Epoch 1084
Epoch 1160
Epoch 1160, Loss: 0.000040051, Improvement: 0.000023523, Best Loss: 0.000002638 in Epoch 1084
Epoch 1161
Epoch 1161, Loss: 0.000048973, Improvement: 0.000008922, Best Loss: 0.000002638 in Epoch 1084
Epoch 1162
Epoch 1162, Loss: 0.000033786, Improvement: -0.000015187, Best Loss: 0.000002638 in Epoch 1084
Epoch 1163
Epoch 1163, Loss: 0.000036902, Improvement: 0.000003116, Best Loss: 0.000002638 in Epoch 1084
Epoch 1164
Epoch 1164, Loss: 0.000029214, Improvement: -0.000007689, Best Loss: 0.000002638 in Epoch 1084
Epoch 1165
Epoch 1165, Loss: 0.000020132, Improvement: -0.000009081, Best Loss: 0.000002638 in Epoch 1084
Epoch 1166
Epoch 1166, Loss: 0.000015482, Improvement: -0.000004651, Best Loss: 0.000002638 in Epoch 1084
Epoch 1167
Epoch 1167, Loss: 0.000018166, Improvement: 0.000002685, Best Loss: 0.000002638 in Epoch 1084
Epoch 1168
Epoch 1168, Loss: 0.000011969, Improvement: -0.000006198, Best Loss: 0.000002638 in Epoch 1084
Epoch 1169
Epoch 1169, Loss: 0.000033363, Improvement: 0.000021394, Best Loss: 0.000002638 in Epoch 1084
Epoch 1170
Epoch 1170, Loss: 0.000073952, Improvement: 0.000040589, Best Loss: 0.000002638 in Epoch 1084
Epoch 1171
Epoch 1171, Loss: 0.000067111, Improvement: -0.000006841, Best Loss: 0.000002638 in Epoch 1084
Epoch 1172
Epoch 1172, Loss: 0.000049926, Improvement: -0.000017185, Best Loss: 0.000002638 in Epoch 1084
Epoch 1173
Epoch 1173, Loss: 0.000019242, Improvement: -0.000030685, Best Loss: 0.000002638 in Epoch 1084
Epoch 1174
Epoch 1174, Loss: 0.000017064, Improvement: -0.000002178, Best Loss: 0.000002638 in Epoch 1084
Epoch 1175
Epoch 1175, Loss: 0.000007123, Improvement: -0.000009940, Best Loss: 0.000002638 in Epoch 1084
Epoch 1176
Epoch 1176, Loss: 0.000004798, Improvement: -0.000002325, Best Loss: 0.000002638 in Epoch 1084
Epoch 1177
Epoch 1177, Loss: 0.000004614, Improvement: -0.000000184, Best Loss: 0.000002638 in Epoch 1084
Epoch 1178
Epoch 1178, Loss: 0.000006212, Improvement: 0.000001598, Best Loss: 0.000002638 in Epoch 1084
Epoch 1179
Epoch 1179, Loss: 0.000005951, Improvement: -0.000000261, Best Loss: 0.000002638 in Epoch 1084
Epoch 1180
Epoch 1180, Loss: 0.000005274, Improvement: -0.000000677, Best Loss: 0.000002638 in Epoch 1084
Epoch 1181
A best model at epoch 1181 has been saved with training error 0.000002591.
Epoch 1181, Loss: 0.000003583, Improvement: -0.000001691, Best Loss: 0.000002591 in Epoch 1181
Epoch 1182
A best model at epoch 1182 has been saved with training error 0.000002421.
Epoch 1182, Loss: 0.000003282, Improvement: -0.000000301, Best Loss: 0.000002421 in Epoch 1182
Epoch 1183
A best model at epoch 1183 has been saved with training error 0.000002210.
Epoch 1183, Loss: 0.000003731, Improvement: 0.000000449, Best Loss: 0.000002210 in Epoch 1183
Epoch 1184
Epoch 1184, Loss: 0.000003485, Improvement: -0.000000246, Best Loss: 0.000002210 in Epoch 1183
Epoch 1185
A best model at epoch 1185 has been saved with training error 0.000002159.
Epoch 1185, Loss: 0.000003288, Improvement: -0.000000198, Best Loss: 0.000002159 in Epoch 1185
Epoch 1186
A best model at epoch 1186 has been saved with training error 0.000002142.
Epoch 1186, Loss: 0.000004809, Improvement: 0.000001521, Best Loss: 0.000002142 in Epoch 1186
Epoch 1187
Epoch 1187, Loss: 0.000004963, Improvement: 0.000000155, Best Loss: 0.000002142 in Epoch 1186
Epoch 1188
Epoch 1188, Loss: 0.000003578, Improvement: -0.000001385, Best Loss: 0.000002142 in Epoch 1186
Epoch 1189
A best model at epoch 1189 has been saved with training error 0.000002114.
Epoch 1189, Loss: 0.000003612, Improvement: 0.000000033, Best Loss: 0.000002114 in Epoch 1189
Epoch 1190
Epoch 1190, Loss: 0.000005960, Improvement: 0.000002348, Best Loss: 0.000002114 in Epoch 1189
Epoch 1191
Epoch 1191, Loss: 0.000007777, Improvement: 0.000001818, Best Loss: 0.000002114 in Epoch 1189
Epoch 1192
Epoch 1192, Loss: 0.000009553, Improvement: 0.000001776, Best Loss: 0.000002114 in Epoch 1189
Epoch 1193
Epoch 1193, Loss: 0.000007047, Improvement: -0.000002506, Best Loss: 0.000002114 in Epoch 1189
Epoch 1194
Epoch 1194, Loss: 0.000005349, Improvement: -0.000001699, Best Loss: 0.000002114 in Epoch 1189
Epoch 1195
Epoch 1195, Loss: 0.000005295, Improvement: -0.000000053, Best Loss: 0.000002114 in Epoch 1189
Epoch 1196
Epoch 1196, Loss: 0.000005423, Improvement: 0.000000127, Best Loss: 0.000002114 in Epoch 1189
Epoch 1197
Epoch 1197, Loss: 0.000013408, Improvement: 0.000007985, Best Loss: 0.000002114 in Epoch 1189
Epoch 1198
Epoch 1198, Loss: 0.000021893, Improvement: 0.000008485, Best Loss: 0.000002114 in Epoch 1189
Epoch 1199
Epoch 1199, Loss: 0.000030623, Improvement: 0.000008730, Best Loss: 0.000002114 in Epoch 1189
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.000020400, Improvement: -0.000010223, Best Loss: 0.000002114 in Epoch 1189
Epoch 1201
Epoch 1201, Loss: 0.000036786, Improvement: 0.000016386, Best Loss: 0.000002114 in Epoch 1189
Epoch 1202
Epoch 1202, Loss: 0.000020788, Improvement: -0.000015998, Best Loss: 0.000002114 in Epoch 1189
Epoch 1203
Epoch 1203, Loss: 0.000012373, Improvement: -0.000008415, Best Loss: 0.000002114 in Epoch 1189
Epoch 1204
Epoch 1204, Loss: 0.000009101, Improvement: -0.000003272, Best Loss: 0.000002114 in Epoch 1189
Epoch 1205
Epoch 1205, Loss: 0.000012341, Improvement: 0.000003240, Best Loss: 0.000002114 in Epoch 1189
Epoch 1206
Epoch 1206, Loss: 0.000024994, Improvement: 0.000012653, Best Loss: 0.000002114 in Epoch 1189
Epoch 1207
Epoch 1207, Loss: 0.000019786, Improvement: -0.000005208, Best Loss: 0.000002114 in Epoch 1189
Epoch 1208
Epoch 1208, Loss: 0.000013926, Improvement: -0.000005860, Best Loss: 0.000002114 in Epoch 1189
Epoch 1209
Epoch 1209, Loss: 0.000016350, Improvement: 0.000002425, Best Loss: 0.000002114 in Epoch 1189
Epoch 1210
Epoch 1210, Loss: 0.000024278, Improvement: 0.000007928, Best Loss: 0.000002114 in Epoch 1189
Epoch 1211
Epoch 1211, Loss: 0.000016814, Improvement: -0.000007464, Best Loss: 0.000002114 in Epoch 1189
Epoch 1212
Epoch 1212, Loss: 0.000011821, Improvement: -0.000004992, Best Loss: 0.000002114 in Epoch 1189
Epoch 1213
Epoch 1213, Loss: 0.000034753, Improvement: 0.000022932, Best Loss: 0.000002114 in Epoch 1189
Epoch 1214
Epoch 1214, Loss: 0.000058006, Improvement: 0.000023253, Best Loss: 0.000002114 in Epoch 1189
Epoch 1215
Epoch 1215, Loss: 0.000030889, Improvement: -0.000027117, Best Loss: 0.000002114 in Epoch 1189
Epoch 1216
Epoch 1216, Loss: 0.000022382, Improvement: -0.000008506, Best Loss: 0.000002114 in Epoch 1189
Epoch 1217
Epoch 1217, Loss: 0.000011720, Improvement: -0.000010662, Best Loss: 0.000002114 in Epoch 1189
Epoch 1218
Epoch 1218, Loss: 0.000008988, Improvement: -0.000002733, Best Loss: 0.000002114 in Epoch 1189
Epoch 1219
Epoch 1219, Loss: 0.000006418, Improvement: -0.000002570, Best Loss: 0.000002114 in Epoch 1189
Epoch 1220
Epoch 1220, Loss: 0.000004520, Improvement: -0.000001898, Best Loss: 0.000002114 in Epoch 1189
Epoch 1221
Epoch 1221, Loss: 0.000003379, Improvement: -0.000001140, Best Loss: 0.000002114 in Epoch 1189
Epoch 1222
Epoch 1222, Loss: 0.000003317, Improvement: -0.000000062, Best Loss: 0.000002114 in Epoch 1189
Epoch 1223
A best model at epoch 1223 has been saved with training error 0.000002084.
Epoch 1223, Loss: 0.000003695, Improvement: 0.000000378, Best Loss: 0.000002084 in Epoch 1223
Epoch 1224
Epoch 1224, Loss: 0.000003433, Improvement: -0.000000262, Best Loss: 0.000002084 in Epoch 1223
Epoch 1225
Epoch 1225, Loss: 0.000004398, Improvement: 0.000000966, Best Loss: 0.000002084 in Epoch 1223
Epoch 1226
Epoch 1226, Loss: 0.000005817, Improvement: 0.000001418, Best Loss: 0.000002084 in Epoch 1223
Epoch 1227
Epoch 1227, Loss: 0.000009515, Improvement: 0.000003698, Best Loss: 0.000002084 in Epoch 1223
Epoch 1228
Epoch 1228, Loss: 0.000012052, Improvement: 0.000002537, Best Loss: 0.000002084 in Epoch 1223
Epoch 1229
Epoch 1229, Loss: 0.000012096, Improvement: 0.000000044, Best Loss: 0.000002084 in Epoch 1223
Epoch 1230
Epoch 1230, Loss: 0.000007652, Improvement: -0.000004444, Best Loss: 0.000002084 in Epoch 1223
Epoch 1231
Epoch 1231, Loss: 0.000011356, Improvement: 0.000003704, Best Loss: 0.000002084 in Epoch 1223
Epoch 1232
Epoch 1232, Loss: 0.000010968, Improvement: -0.000000388, Best Loss: 0.000002084 in Epoch 1223
Epoch 1233
Epoch 1233, Loss: 0.000013295, Improvement: 0.000002327, Best Loss: 0.000002084 in Epoch 1223
Epoch 1234
Epoch 1234, Loss: 0.000014814, Improvement: 0.000001519, Best Loss: 0.000002084 in Epoch 1223
Epoch 1235
Epoch 1235, Loss: 0.000009303, Improvement: -0.000005511, Best Loss: 0.000002084 in Epoch 1223
Epoch 1236
Epoch 1236, Loss: 0.000010695, Improvement: 0.000001392, Best Loss: 0.000002084 in Epoch 1223
Epoch 1237
Epoch 1237, Loss: 0.000021686, Improvement: 0.000010991, Best Loss: 0.000002084 in Epoch 1223
Epoch 1238
Epoch 1238, Loss: 0.000020580, Improvement: -0.000001106, Best Loss: 0.000002084 in Epoch 1223
Epoch 1239
Epoch 1239, Loss: 0.000022037, Improvement: 0.000001458, Best Loss: 0.000002084 in Epoch 1223
Epoch 1240
Epoch 1240, Loss: 0.000047663, Improvement: 0.000025626, Best Loss: 0.000002084 in Epoch 1223
Epoch 1241
Epoch 1241, Loss: 0.000024238, Improvement: -0.000023425, Best Loss: 0.000002084 in Epoch 1223
Epoch 1242
Epoch 1242, Loss: 0.000013602, Improvement: -0.000010636, Best Loss: 0.000002084 in Epoch 1223
Epoch 1243
Epoch 1243, Loss: 0.000009536, Improvement: -0.000004066, Best Loss: 0.000002084 in Epoch 1223
Epoch 1244
Epoch 1244, Loss: 0.000010256, Improvement: 0.000000720, Best Loss: 0.000002084 in Epoch 1223
Epoch 1245
Epoch 1245, Loss: 0.000009119, Improvement: -0.000001137, Best Loss: 0.000002084 in Epoch 1223
Epoch 1246
Epoch 1246, Loss: 0.000007966, Improvement: -0.000001153, Best Loss: 0.000002084 in Epoch 1223
Epoch 1247
Epoch 1247, Loss: 0.000008668, Improvement: 0.000000701, Best Loss: 0.000002084 in Epoch 1223
Epoch 1248
Epoch 1248, Loss: 0.000007408, Improvement: -0.000001260, Best Loss: 0.000002084 in Epoch 1223
Epoch 1249
Epoch 1249, Loss: 0.000013421, Improvement: 0.000006013, Best Loss: 0.000002084 in Epoch 1223
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.000058075, Improvement: 0.000044655, Best Loss: 0.000002084 in Epoch 1223
Epoch 1251
Epoch 1251, Loss: 0.000023649, Improvement: -0.000034427, Best Loss: 0.000002084 in Epoch 1223
Epoch 1252
Epoch 1252, Loss: 0.000014039, Improvement: -0.000009610, Best Loss: 0.000002084 in Epoch 1223
Epoch 1253
Epoch 1253, Loss: 0.000008436, Improvement: -0.000005603, Best Loss: 0.000002084 in Epoch 1223
Epoch 1254
Epoch 1254, Loss: 0.000006876, Improvement: -0.000001560, Best Loss: 0.000002084 in Epoch 1223
Epoch 1255
Epoch 1255, Loss: 0.000005114, Improvement: -0.000001762, Best Loss: 0.000002084 in Epoch 1223
Epoch 1256
Epoch 1256, Loss: 0.000003765, Improvement: -0.000001349, Best Loss: 0.000002084 in Epoch 1223
Epoch 1257
Epoch 1257, Loss: 0.000003277, Improvement: -0.000000488, Best Loss: 0.000002084 in Epoch 1223
Epoch 1258
Epoch 1258, Loss: 0.000003368, Improvement: 0.000000091, Best Loss: 0.000002084 in Epoch 1223
Epoch 1259
Epoch 1259, Loss: 0.000003322, Improvement: -0.000000046, Best Loss: 0.000002084 in Epoch 1223
Epoch 1260
Epoch 1260, Loss: 0.000003667, Improvement: 0.000000344, Best Loss: 0.000002084 in Epoch 1223
Epoch 1261
A best model at epoch 1261 has been saved with training error 0.000001971.
Epoch 1261, Loss: 0.000003307, Improvement: -0.000000360, Best Loss: 0.000001971 in Epoch 1261
Epoch 1262
Epoch 1262, Loss: 0.000002863, Improvement: -0.000000444, Best Loss: 0.000001971 in Epoch 1261
Epoch 1263
Epoch 1263, Loss: 0.000002787, Improvement: -0.000000076, Best Loss: 0.000001971 in Epoch 1261
Epoch 1264
A best model at epoch 1264 has been saved with training error 0.000001889.
Epoch 1264, Loss: 0.000002651, Improvement: -0.000000136, Best Loss: 0.000001889 in Epoch 1264
Epoch 1265
Epoch 1265, Loss: 0.000002488, Improvement: -0.000000163, Best Loss: 0.000001889 in Epoch 1264
Epoch 1266
A best model at epoch 1266 has been saved with training error 0.000001628.
Epoch 1266, Loss: 0.000002530, Improvement: 0.000000043, Best Loss: 0.000001628 in Epoch 1266
Epoch 1267
Epoch 1267, Loss: 0.000002607, Improvement: 0.000000076, Best Loss: 0.000001628 in Epoch 1266
Epoch 1268
Epoch 1268, Loss: 0.000003579, Improvement: 0.000000972, Best Loss: 0.000001628 in Epoch 1266
Epoch 1269
Epoch 1269, Loss: 0.000004053, Improvement: 0.000000473, Best Loss: 0.000001628 in Epoch 1266
Epoch 1270
Epoch 1270, Loss: 0.000004160, Improvement: 0.000000108, Best Loss: 0.000001628 in Epoch 1266
Epoch 1271
Epoch 1271, Loss: 0.000004226, Improvement: 0.000000066, Best Loss: 0.000001628 in Epoch 1266
Epoch 1272
Epoch 1272, Loss: 0.000005259, Improvement: 0.000001033, Best Loss: 0.000001628 in Epoch 1266
Epoch 1273
Epoch 1273, Loss: 0.000004933, Improvement: -0.000000326, Best Loss: 0.000001628 in Epoch 1266
Epoch 1274
Epoch 1274, Loss: 0.000003759, Improvement: -0.000001174, Best Loss: 0.000001628 in Epoch 1266
Epoch 1275
Epoch 1275, Loss: 0.000007147, Improvement: 0.000003388, Best Loss: 0.000001628 in Epoch 1266
Epoch 1276
Epoch 1276, Loss: 0.000018483, Improvement: 0.000011336, Best Loss: 0.000001628 in Epoch 1266
Epoch 1277
Epoch 1277, Loss: 0.000021712, Improvement: 0.000003229, Best Loss: 0.000001628 in Epoch 1266
Epoch 1278
Epoch 1278, Loss: 0.000022102, Improvement: 0.000000390, Best Loss: 0.000001628 in Epoch 1266
Epoch 1279
Epoch 1279, Loss: 0.000011527, Improvement: -0.000010575, Best Loss: 0.000001628 in Epoch 1266
Epoch 1280
Epoch 1280, Loss: 0.000006708, Improvement: -0.000004820, Best Loss: 0.000001628 in Epoch 1266
Epoch 1281
Epoch 1281, Loss: 0.000007946, Improvement: 0.000001239, Best Loss: 0.000001628 in Epoch 1266
Epoch 1282
Epoch 1282, Loss: 0.000005675, Improvement: -0.000002272, Best Loss: 0.000001628 in Epoch 1266
Epoch 1283
Epoch 1283, Loss: 0.000004031, Improvement: -0.000001644, Best Loss: 0.000001628 in Epoch 1266
Epoch 1284
Epoch 1284, Loss: 0.000003988, Improvement: -0.000000043, Best Loss: 0.000001628 in Epoch 1266
Epoch 1285
Epoch 1285, Loss: 0.000004478, Improvement: 0.000000490, Best Loss: 0.000001628 in Epoch 1266
Epoch 1286
Epoch 1286, Loss: 0.000004086, Improvement: -0.000000392, Best Loss: 0.000001628 in Epoch 1266
Epoch 1287
Epoch 1287, Loss: 0.000004327, Improvement: 0.000000242, Best Loss: 0.000001628 in Epoch 1266
Epoch 1288
Epoch 1288, Loss: 0.000006100, Improvement: 0.000001773, Best Loss: 0.000001628 in Epoch 1266
Epoch 1289
Epoch 1289, Loss: 0.000006233, Improvement: 0.000000133, Best Loss: 0.000001628 in Epoch 1266
Epoch 1290
Epoch 1290, Loss: 0.000005725, Improvement: -0.000000508, Best Loss: 0.000001628 in Epoch 1266
Epoch 1291
Epoch 1291, Loss: 0.000009415, Improvement: 0.000003690, Best Loss: 0.000001628 in Epoch 1266
Epoch 1292
Epoch 1292, Loss: 0.000021729, Improvement: 0.000012313, Best Loss: 0.000001628 in Epoch 1266
Epoch 1293
Epoch 1293, Loss: 0.000061733, Improvement: 0.000040004, Best Loss: 0.000001628 in Epoch 1266
Epoch 1294
Epoch 1294, Loss: 0.000036865, Improvement: -0.000024868, Best Loss: 0.000001628 in Epoch 1266
Epoch 1295
Epoch 1295, Loss: 0.000013566, Improvement: -0.000023299, Best Loss: 0.000001628 in Epoch 1266
Epoch 1296
Epoch 1296, Loss: 0.000006347, Improvement: -0.000007219, Best Loss: 0.000001628 in Epoch 1266
Epoch 1297
Epoch 1297, Loss: 0.000004311, Improvement: -0.000002036, Best Loss: 0.000001628 in Epoch 1266
Epoch 1298
Epoch 1298, Loss: 0.000003739, Improvement: -0.000000572, Best Loss: 0.000001628 in Epoch 1266
Epoch 1299
Epoch 1299, Loss: 0.000003257, Improvement: -0.000000482, Best Loss: 0.000001628 in Epoch 1266
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.000002800, Improvement: -0.000000457, Best Loss: 0.000001628 in Epoch 1266
Epoch 1301
Epoch 1301, Loss: 0.000002582, Improvement: -0.000000218, Best Loss: 0.000001628 in Epoch 1266
Epoch 1302
Epoch 1302, Loss: 0.000002617, Improvement: 0.000000036, Best Loss: 0.000001628 in Epoch 1266
Epoch 1303
Epoch 1303, Loss: 0.000002658, Improvement: 0.000000041, Best Loss: 0.000001628 in Epoch 1266
Epoch 1304
Epoch 1304, Loss: 0.000002556, Improvement: -0.000000101, Best Loss: 0.000001628 in Epoch 1266
Epoch 1305
Epoch 1305, Loss: 0.000002446, Improvement: -0.000000110, Best Loss: 0.000001628 in Epoch 1266
Epoch 1306
Epoch 1306, Loss: 0.000002403, Improvement: -0.000000043, Best Loss: 0.000001628 in Epoch 1266
Epoch 1307
Epoch 1307, Loss: 0.000002477, Improvement: 0.000000073, Best Loss: 0.000001628 in Epoch 1266
Epoch 1308
Epoch 1308, Loss: 0.000002512, Improvement: 0.000000036, Best Loss: 0.000001628 in Epoch 1266
Epoch 1309
A best model at epoch 1309 has been saved with training error 0.000001600.
Epoch 1309, Loss: 0.000002705, Improvement: 0.000000193, Best Loss: 0.000001600 in Epoch 1309
Epoch 1310
Epoch 1310, Loss: 0.000004919, Improvement: 0.000002214, Best Loss: 0.000001600 in Epoch 1309
Epoch 1311
Epoch 1311, Loss: 0.000005173, Improvement: 0.000000254, Best Loss: 0.000001600 in Epoch 1309
Epoch 1312
Epoch 1312, Loss: 0.000006055, Improvement: 0.000000882, Best Loss: 0.000001600 in Epoch 1309
Epoch 1313
Epoch 1313, Loss: 0.000006355, Improvement: 0.000000300, Best Loss: 0.000001600 in Epoch 1309
Epoch 1314
Epoch 1314, Loss: 0.000005324, Improvement: -0.000001031, Best Loss: 0.000001600 in Epoch 1309
Epoch 1315
Epoch 1315, Loss: 0.000006177, Improvement: 0.000000853, Best Loss: 0.000001600 in Epoch 1309
Epoch 1316
Epoch 1316, Loss: 0.000005323, Improvement: -0.000000854, Best Loss: 0.000001600 in Epoch 1309
Epoch 1317
Epoch 1317, Loss: 0.000006916, Improvement: 0.000001593, Best Loss: 0.000001600 in Epoch 1309
Epoch 1318
Epoch 1318, Loss: 0.000014240, Improvement: 0.000007324, Best Loss: 0.000001600 in Epoch 1309
Epoch 1319
Epoch 1319, Loss: 0.000019211, Improvement: 0.000004970, Best Loss: 0.000001600 in Epoch 1309
Epoch 1320
Epoch 1320, Loss: 0.000056555, Improvement: 0.000037344, Best Loss: 0.000001600 in Epoch 1309
Epoch 1321
Epoch 1321, Loss: 0.000066675, Improvement: 0.000010120, Best Loss: 0.000001600 in Epoch 1309
Epoch 1322
Epoch 1322, Loss: 0.000022194, Improvement: -0.000044480, Best Loss: 0.000001600 in Epoch 1309
Epoch 1323
Epoch 1323, Loss: 0.000012624, Improvement: -0.000009570, Best Loss: 0.000001600 in Epoch 1309
Epoch 1324
Epoch 1324, Loss: 0.000005848, Improvement: -0.000006776, Best Loss: 0.000001600 in Epoch 1309
Epoch 1325
Epoch 1325, Loss: 0.000004387, Improvement: -0.000001461, Best Loss: 0.000001600 in Epoch 1309
Epoch 1326
Epoch 1326, Loss: 0.000003504, Improvement: -0.000000883, Best Loss: 0.000001600 in Epoch 1309
Epoch 1327
Epoch 1327, Loss: 0.000003188, Improvement: -0.000000316, Best Loss: 0.000001600 in Epoch 1309
Epoch 1328
Epoch 1328, Loss: 0.000002653, Improvement: -0.000000535, Best Loss: 0.000001600 in Epoch 1309
Epoch 1329
Epoch 1329, Loss: 0.000002426, Improvement: -0.000000226, Best Loss: 0.000001600 in Epoch 1309
Epoch 1330
Epoch 1330, Loss: 0.000002925, Improvement: 0.000000499, Best Loss: 0.000001600 in Epoch 1309
Epoch 1331
Epoch 1331, Loss: 0.000003044, Improvement: 0.000000118, Best Loss: 0.000001600 in Epoch 1309
Epoch 1332
Epoch 1332, Loss: 0.000002504, Improvement: -0.000000540, Best Loss: 0.000001600 in Epoch 1309
Epoch 1333
Epoch 1333, Loss: 0.000002539, Improvement: 0.000000035, Best Loss: 0.000001600 in Epoch 1309
Epoch 1334
A best model at epoch 1334 has been saved with training error 0.000001553.
Epoch 1334, Loss: 0.000002432, Improvement: -0.000000107, Best Loss: 0.000001553 in Epoch 1334
Epoch 1335
Epoch 1335, Loss: 0.000002521, Improvement: 0.000000089, Best Loss: 0.000001553 in Epoch 1334
Epoch 1336
Epoch 1336, Loss: 0.000002505, Improvement: -0.000000016, Best Loss: 0.000001553 in Epoch 1334
Epoch 1337
Epoch 1337, Loss: 0.000002227, Improvement: -0.000000278, Best Loss: 0.000001553 in Epoch 1334
Epoch 1338
A best model at epoch 1338 has been saved with training error 0.000001509.
Epoch 1338, Loss: 0.000002117, Improvement: -0.000000110, Best Loss: 0.000001509 in Epoch 1338
Epoch 1339
A best model at epoch 1339 has been saved with training error 0.000001416.
Epoch 1339, Loss: 0.000002028, Improvement: -0.000000089, Best Loss: 0.000001416 in Epoch 1339
Epoch 1340
Epoch 1340, Loss: 0.000002140, Improvement: 0.000000112, Best Loss: 0.000001416 in Epoch 1339
Epoch 1341
Epoch 1341, Loss: 0.000002250, Improvement: 0.000000110, Best Loss: 0.000001416 in Epoch 1339
Epoch 1342
Epoch 1342, Loss: 0.000002223, Improvement: -0.000000027, Best Loss: 0.000001416 in Epoch 1339
Epoch 1343
Epoch 1343, Loss: 0.000003912, Improvement: 0.000001689, Best Loss: 0.000001416 in Epoch 1339
Epoch 1344
Epoch 1344, Loss: 0.000007900, Improvement: 0.000003988, Best Loss: 0.000001416 in Epoch 1339
Epoch 1345
Epoch 1345, Loss: 0.000007725, Improvement: -0.000000175, Best Loss: 0.000001416 in Epoch 1339
Epoch 1346
Epoch 1346, Loss: 0.000007106, Improvement: -0.000000619, Best Loss: 0.000001416 in Epoch 1339
Epoch 1347
Epoch 1347, Loss: 0.000003787, Improvement: -0.000003319, Best Loss: 0.000001416 in Epoch 1339
Epoch 1348
Epoch 1348, Loss: 0.000004537, Improvement: 0.000000750, Best Loss: 0.000001416 in Epoch 1339
Epoch 1349
Epoch 1349, Loss: 0.000003823, Improvement: -0.000000714, Best Loss: 0.000001416 in Epoch 1339
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.000019156, Improvement: 0.000015333, Best Loss: 0.000001416 in Epoch 1339
Epoch 1351
Epoch 1351, Loss: 0.000012354, Improvement: -0.000006802, Best Loss: 0.000001416 in Epoch 1339
Epoch 1352
Epoch 1352, Loss: 0.000029605, Improvement: 0.000017251, Best Loss: 0.000001416 in Epoch 1339
Epoch 1353
Epoch 1353, Loss: 0.000019237, Improvement: -0.000010367, Best Loss: 0.000001416 in Epoch 1339
Epoch 1354
Epoch 1354, Loss: 0.000021917, Improvement: 0.000002680, Best Loss: 0.000001416 in Epoch 1339
Epoch 1355
Epoch 1355, Loss: 0.000026167, Improvement: 0.000004250, Best Loss: 0.000001416 in Epoch 1339
Epoch 1356
Epoch 1356, Loss: 0.000021129, Improvement: -0.000005038, Best Loss: 0.000001416 in Epoch 1339
Epoch 1357
Epoch 1357, Loss: 0.000027888, Improvement: 0.000006760, Best Loss: 0.000001416 in Epoch 1339
Epoch 1358
Epoch 1358, Loss: 0.000035151, Improvement: 0.000007262, Best Loss: 0.000001416 in Epoch 1339
Epoch 1359
Epoch 1359, Loss: 0.000013191, Improvement: -0.000021959, Best Loss: 0.000001416 in Epoch 1339
Epoch 1360
Epoch 1360, Loss: 0.000007656, Improvement: -0.000005535, Best Loss: 0.000001416 in Epoch 1339
Epoch 1361
Epoch 1361, Loss: 0.000006855, Improvement: -0.000000802, Best Loss: 0.000001416 in Epoch 1339
Epoch 1362
Epoch 1362, Loss: 0.000004556, Improvement: -0.000002298, Best Loss: 0.000001416 in Epoch 1339
Epoch 1363
Epoch 1363, Loss: 0.000003304, Improvement: -0.000001252, Best Loss: 0.000001416 in Epoch 1339
Epoch 1364
Epoch 1364, Loss: 0.000002671, Improvement: -0.000000633, Best Loss: 0.000001416 in Epoch 1339
Epoch 1365
Epoch 1365, Loss: 0.000002783, Improvement: 0.000000112, Best Loss: 0.000001416 in Epoch 1339
Epoch 1366
Epoch 1366, Loss: 0.000003083, Improvement: 0.000000299, Best Loss: 0.000001416 in Epoch 1339
Epoch 1367
Epoch 1367, Loss: 0.000003477, Improvement: 0.000000394, Best Loss: 0.000001416 in Epoch 1339
Epoch 1368
Epoch 1368, Loss: 0.000002973, Improvement: -0.000000503, Best Loss: 0.000001416 in Epoch 1339
Epoch 1369
Epoch 1369, Loss: 0.000003657, Improvement: 0.000000684, Best Loss: 0.000001416 in Epoch 1339
Epoch 1370
Epoch 1370, Loss: 0.000003460, Improvement: -0.000000197, Best Loss: 0.000001416 in Epoch 1339
Epoch 1371
Epoch 1371, Loss: 0.000007361, Improvement: 0.000003901, Best Loss: 0.000001416 in Epoch 1339
Epoch 1372
Epoch 1372, Loss: 0.000011892, Improvement: 0.000004531, Best Loss: 0.000001416 in Epoch 1339
Epoch 1373
Epoch 1373, Loss: 0.000015322, Improvement: 0.000003431, Best Loss: 0.000001416 in Epoch 1339
Epoch 1374
Epoch 1374, Loss: 0.000020117, Improvement: 0.000004795, Best Loss: 0.000001416 in Epoch 1339
Epoch 1375
Epoch 1375, Loss: 0.000013400, Improvement: -0.000006717, Best Loss: 0.000001416 in Epoch 1339
Epoch 1376
Epoch 1376, Loss: 0.000009553, Improvement: -0.000003846, Best Loss: 0.000001416 in Epoch 1339
Epoch 1377
Epoch 1377, Loss: 0.000007075, Improvement: -0.000002478, Best Loss: 0.000001416 in Epoch 1339
Epoch 1378
Epoch 1378, Loss: 0.000006084, Improvement: -0.000000991, Best Loss: 0.000001416 in Epoch 1339
Epoch 1379
Epoch 1379, Loss: 0.000008544, Improvement: 0.000002460, Best Loss: 0.000001416 in Epoch 1339
Epoch 1380
Epoch 1380, Loss: 0.000006051, Improvement: -0.000002493, Best Loss: 0.000001416 in Epoch 1339
Epoch 1381
Epoch 1381, Loss: 0.000005366, Improvement: -0.000000686, Best Loss: 0.000001416 in Epoch 1339
Epoch 1382
Epoch 1382, Loss: 0.000006982, Improvement: 0.000001616, Best Loss: 0.000001416 in Epoch 1339
Epoch 1383
Epoch 1383, Loss: 0.000025686, Improvement: 0.000018704, Best Loss: 0.000001416 in Epoch 1339
Epoch 1384
Epoch 1384, Loss: 0.000024068, Improvement: -0.000001618, Best Loss: 0.000001416 in Epoch 1339
Epoch 1385
Epoch 1385, Loss: 0.000027936, Improvement: 0.000003868, Best Loss: 0.000001416 in Epoch 1339
Epoch 1386
Epoch 1386, Loss: 0.000021500, Improvement: -0.000006435, Best Loss: 0.000001416 in Epoch 1339
Epoch 1387
Epoch 1387, Loss: 0.000026921, Improvement: 0.000005420, Best Loss: 0.000001416 in Epoch 1339
Epoch 1388
Epoch 1388, Loss: 0.000013079, Improvement: -0.000013841, Best Loss: 0.000001416 in Epoch 1339
Epoch 1389
Epoch 1389, Loss: 0.000010520, Improvement: -0.000002559, Best Loss: 0.000001416 in Epoch 1339
Epoch 1390
Epoch 1390, Loss: 0.000006246, Improvement: -0.000004274, Best Loss: 0.000001416 in Epoch 1339
Epoch 1391
Epoch 1391, Loss: 0.000006763, Improvement: 0.000000517, Best Loss: 0.000001416 in Epoch 1339
Epoch 1392
Epoch 1392, Loss: 0.000006887, Improvement: 0.000000124, Best Loss: 0.000001416 in Epoch 1339
Epoch 1393
Epoch 1393, Loss: 0.000006727, Improvement: -0.000000160, Best Loss: 0.000001416 in Epoch 1339
Epoch 1394
Epoch 1394, Loss: 0.000010439, Improvement: 0.000003712, Best Loss: 0.000001416 in Epoch 1339
Epoch 1395
Epoch 1395, Loss: 0.000013480, Improvement: 0.000003041, Best Loss: 0.000001416 in Epoch 1339
Epoch 1396
Epoch 1396, Loss: 0.000018488, Improvement: 0.000005008, Best Loss: 0.000001416 in Epoch 1339
Epoch 1397
Epoch 1397, Loss: 0.000012605, Improvement: -0.000005883, Best Loss: 0.000001416 in Epoch 1339
Epoch 1398
Epoch 1398, Loss: 0.000018674, Improvement: 0.000006069, Best Loss: 0.000001416 in Epoch 1339
Epoch 1399
Epoch 1399, Loss: 0.000047487, Improvement: 0.000028813, Best Loss: 0.000001416 in Epoch 1339
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.000033835, Improvement: -0.000013652, Best Loss: 0.000001416 in Epoch 1339
Epoch 1401
Epoch 1401, Loss: 0.000030692, Improvement: -0.000003143, Best Loss: 0.000001416 in Epoch 1339
Epoch 1402
Epoch 1402, Loss: 0.000014904, Improvement: -0.000015788, Best Loss: 0.000001416 in Epoch 1339
Epoch 1403
Epoch 1403, Loss: 0.000009139, Improvement: -0.000005765, Best Loss: 0.000001416 in Epoch 1339
Epoch 1404
Epoch 1404, Loss: 0.000007312, Improvement: -0.000001827, Best Loss: 0.000001416 in Epoch 1339
Epoch 1405
Epoch 1405, Loss: 0.000007464, Improvement: 0.000000152, Best Loss: 0.000001416 in Epoch 1339
Epoch 1406
Epoch 1406, Loss: 0.000005700, Improvement: -0.000001763, Best Loss: 0.000001416 in Epoch 1339
Epoch 1407
Epoch 1407, Loss: 0.000003691, Improvement: -0.000002009, Best Loss: 0.000001416 in Epoch 1339
Epoch 1408
Epoch 1408, Loss: 0.000003415, Improvement: -0.000000276, Best Loss: 0.000001416 in Epoch 1339
Epoch 1409
Epoch 1409, Loss: 0.000003547, Improvement: 0.000000132, Best Loss: 0.000001416 in Epoch 1339
Epoch 1410
Epoch 1410, Loss: 0.000004441, Improvement: 0.000000894, Best Loss: 0.000001416 in Epoch 1339
Epoch 1411
Epoch 1411, Loss: 0.000008857, Improvement: 0.000004416, Best Loss: 0.000001416 in Epoch 1339
Epoch 1412
Epoch 1412, Loss: 0.000020529, Improvement: 0.000011673, Best Loss: 0.000001416 in Epoch 1339
Epoch 1413
Epoch 1413, Loss: 0.000025838, Improvement: 0.000005309, Best Loss: 0.000001416 in Epoch 1339
Epoch 1414
Epoch 1414, Loss: 0.000013551, Improvement: -0.000012286, Best Loss: 0.000001416 in Epoch 1339
Epoch 1415
Epoch 1415, Loss: 0.000021587, Improvement: 0.000008036, Best Loss: 0.000001416 in Epoch 1339
Epoch 1416
Epoch 1416, Loss: 0.000010984, Improvement: -0.000010603, Best Loss: 0.000001416 in Epoch 1339
Epoch 1417
Epoch 1417, Loss: 0.000004825, Improvement: -0.000006159, Best Loss: 0.000001416 in Epoch 1339
Epoch 1418
Epoch 1418, Loss: 0.000003565, Improvement: -0.000001260, Best Loss: 0.000001416 in Epoch 1339
Epoch 1419
Epoch 1419, Loss: 0.000003415, Improvement: -0.000000150, Best Loss: 0.000001416 in Epoch 1339
Epoch 1420
Epoch 1420, Loss: 0.000005834, Improvement: 0.000002419, Best Loss: 0.000001416 in Epoch 1339
Epoch 1421
Epoch 1421, Loss: 0.000005013, Improvement: -0.000000822, Best Loss: 0.000001416 in Epoch 1339
Epoch 1422
Epoch 1422, Loss: 0.000004483, Improvement: -0.000000530, Best Loss: 0.000001416 in Epoch 1339
Epoch 1423
Epoch 1423, Loss: 0.000005747, Improvement: 0.000001264, Best Loss: 0.000001416 in Epoch 1339
Epoch 1424
Epoch 1424, Loss: 0.000007808, Improvement: 0.000002061, Best Loss: 0.000001416 in Epoch 1339
Epoch 1425
Epoch 1425, Loss: 0.000008216, Improvement: 0.000000409, Best Loss: 0.000001416 in Epoch 1339
Epoch 1426
Epoch 1426, Loss: 0.000006253, Improvement: -0.000001963, Best Loss: 0.000001416 in Epoch 1339
Epoch 1427
Epoch 1427, Loss: 0.000010795, Improvement: 0.000004542, Best Loss: 0.000001416 in Epoch 1339
Epoch 1428
Epoch 1428, Loss: 0.000014345, Improvement: 0.000003550, Best Loss: 0.000001416 in Epoch 1339
Epoch 1429
Epoch 1429, Loss: 0.000044489, Improvement: 0.000030144, Best Loss: 0.000001416 in Epoch 1339
Epoch 1430
Epoch 1430, Loss: 0.000056052, Improvement: 0.000011563, Best Loss: 0.000001416 in Epoch 1339
Epoch 1431
Epoch 1431, Loss: 0.000130307, Improvement: 0.000074255, Best Loss: 0.000001416 in Epoch 1339
Epoch 1432
Epoch 1432, Loss: 0.000059645, Improvement: -0.000070662, Best Loss: 0.000001416 in Epoch 1339
Epoch 1433
Epoch 1433, Loss: 0.000022560, Improvement: -0.000037085, Best Loss: 0.000001416 in Epoch 1339
Epoch 1434
Epoch 1434, Loss: 0.000010193, Improvement: -0.000012366, Best Loss: 0.000001416 in Epoch 1339
Epoch 1435
Epoch 1435, Loss: 0.000005673, Improvement: -0.000004521, Best Loss: 0.000001416 in Epoch 1339
Epoch 1436
Epoch 1436, Loss: 0.000004139, Improvement: -0.000001534, Best Loss: 0.000001416 in Epoch 1339
Epoch 1437
Epoch 1437, Loss: 0.000003274, Improvement: -0.000000865, Best Loss: 0.000001416 in Epoch 1339
Epoch 1438
Epoch 1438, Loss: 0.000002846, Improvement: -0.000000428, Best Loss: 0.000001416 in Epoch 1339
Epoch 1439
Epoch 1439, Loss: 0.000002526, Improvement: -0.000000320, Best Loss: 0.000001416 in Epoch 1339
Epoch 1440
Epoch 1440, Loss: 0.000002390, Improvement: -0.000000136, Best Loss: 0.000001416 in Epoch 1339
Epoch 1441
Epoch 1441, Loss: 0.000002323, Improvement: -0.000000067, Best Loss: 0.000001416 in Epoch 1339
Epoch 1442
Epoch 1442, Loss: 0.000002292, Improvement: -0.000000031, Best Loss: 0.000001416 in Epoch 1339
Epoch 1443
Epoch 1443, Loss: 0.000002279, Improvement: -0.000000013, Best Loss: 0.000001416 in Epoch 1339
Epoch 1444
Epoch 1444, Loss: 0.000002220, Improvement: -0.000000059, Best Loss: 0.000001416 in Epoch 1339
Epoch 1445
Epoch 1445, Loss: 0.000002339, Improvement: 0.000000119, Best Loss: 0.000001416 in Epoch 1339
Epoch 1446
Epoch 1446, Loss: 0.000002292, Improvement: -0.000000047, Best Loss: 0.000001416 in Epoch 1339
Epoch 1447
A best model at epoch 1447 has been saved with training error 0.000001363.
Epoch 1447, Loss: 0.000002107, Improvement: -0.000000185, Best Loss: 0.000001363 in Epoch 1447
Epoch 1448
Epoch 1448, Loss: 0.000002310, Improvement: 0.000000204, Best Loss: 0.000001363 in Epoch 1447
Epoch 1449
Epoch 1449, Loss: 0.000002416, Improvement: 0.000000106, Best Loss: 0.000001363 in Epoch 1447
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.000002570, Improvement: 0.000000154, Best Loss: 0.000001363 in Epoch 1447
Epoch 1451
Epoch 1451, Loss: 0.000003664, Improvement: 0.000001093, Best Loss: 0.000001363 in Epoch 1447
Epoch 1452
Epoch 1452, Loss: 0.000003420, Improvement: -0.000000244, Best Loss: 0.000001363 in Epoch 1447
Epoch 1453
Epoch 1453, Loss: 0.000002233, Improvement: -0.000001187, Best Loss: 0.000001363 in Epoch 1447
Epoch 1454
Epoch 1454, Loss: 0.000002056, Improvement: -0.000000177, Best Loss: 0.000001363 in Epoch 1447
Epoch 1455
A best model at epoch 1455 has been saved with training error 0.000001297.
Epoch 1455, Loss: 0.000002067, Improvement: 0.000000011, Best Loss: 0.000001297 in Epoch 1455
Epoch 1456
A best model at epoch 1456 has been saved with training error 0.000001134.
Epoch 1456, Loss: 0.000002011, Improvement: -0.000000056, Best Loss: 0.000001134 in Epoch 1456
Epoch 1457
Epoch 1457, Loss: 0.000002007, Improvement: -0.000000004, Best Loss: 0.000001134 in Epoch 1456
Epoch 1458
Epoch 1458, Loss: 0.000002083, Improvement: 0.000000076, Best Loss: 0.000001134 in Epoch 1456
Epoch 1459
Epoch 1459, Loss: 0.000001916, Improvement: -0.000000167, Best Loss: 0.000001134 in Epoch 1456
Epoch 1460
Epoch 1460, Loss: 0.000002075, Improvement: 0.000000158, Best Loss: 0.000001134 in Epoch 1456
Epoch 1461
Epoch 1461, Loss: 0.000001980, Improvement: -0.000000095, Best Loss: 0.000001134 in Epoch 1456
Epoch 1462
Epoch 1462, Loss: 0.000002198, Improvement: 0.000000218, Best Loss: 0.000001134 in Epoch 1456
Epoch 1463
Epoch 1463, Loss: 0.000002196, Improvement: -0.000000003, Best Loss: 0.000001134 in Epoch 1456
Epoch 1464
Epoch 1464, Loss: 0.000001940, Improvement: -0.000000255, Best Loss: 0.000001134 in Epoch 1456
Epoch 1465
Epoch 1465, Loss: 0.000002206, Improvement: 0.000000266, Best Loss: 0.000001134 in Epoch 1456
Epoch 1466
Epoch 1466, Loss: 0.000004561, Improvement: 0.000002354, Best Loss: 0.000001134 in Epoch 1456
Epoch 1467
Epoch 1467, Loss: 0.000014989, Improvement: 0.000010428, Best Loss: 0.000001134 in Epoch 1456
Epoch 1468
Epoch 1468, Loss: 0.000012015, Improvement: -0.000002974, Best Loss: 0.000001134 in Epoch 1456
Epoch 1469
Epoch 1469, Loss: 0.000007497, Improvement: -0.000004517, Best Loss: 0.000001134 in Epoch 1456
Epoch 1470
Epoch 1470, Loss: 0.000014796, Improvement: 0.000007299, Best Loss: 0.000001134 in Epoch 1456
Epoch 1471
Epoch 1471, Loss: 0.000047317, Improvement: 0.000032520, Best Loss: 0.000001134 in Epoch 1456
Epoch 1472
Epoch 1472, Loss: 0.000032648, Improvement: -0.000014668, Best Loss: 0.000001134 in Epoch 1456
Epoch 1473
Epoch 1473, Loss: 0.000011917, Improvement: -0.000020732, Best Loss: 0.000001134 in Epoch 1456
Epoch 1474
Epoch 1474, Loss: 0.000008485, Improvement: -0.000003432, Best Loss: 0.000001134 in Epoch 1456
Epoch 1475
Epoch 1475, Loss: 0.000019353, Improvement: 0.000010868, Best Loss: 0.000001134 in Epoch 1456
Epoch 1476
Epoch 1476, Loss: 0.000017805, Improvement: -0.000001548, Best Loss: 0.000001134 in Epoch 1456
Epoch 1477
Epoch 1477, Loss: 0.000014216, Improvement: -0.000003589, Best Loss: 0.000001134 in Epoch 1456
Epoch 1478
Epoch 1478, Loss: 0.000008594, Improvement: -0.000005622, Best Loss: 0.000001134 in Epoch 1456
Epoch 1479
Epoch 1479, Loss: 0.000009757, Improvement: 0.000001164, Best Loss: 0.000001134 in Epoch 1456
Epoch 1480
Epoch 1480, Loss: 0.000017066, Improvement: 0.000007309, Best Loss: 0.000001134 in Epoch 1456
Epoch 1481
Epoch 1481, Loss: 0.000010448, Improvement: -0.000006618, Best Loss: 0.000001134 in Epoch 1456
Epoch 1482
Epoch 1482, Loss: 0.000012014, Improvement: 0.000001566, Best Loss: 0.000001134 in Epoch 1456
Epoch 1483
Epoch 1483, Loss: 0.000010721, Improvement: -0.000001293, Best Loss: 0.000001134 in Epoch 1456
Epoch 1484
Epoch 1484, Loss: 0.000005788, Improvement: -0.000004933, Best Loss: 0.000001134 in Epoch 1456
Epoch 1485
Epoch 1485, Loss: 0.000007128, Improvement: 0.000001340, Best Loss: 0.000001134 in Epoch 1456
Epoch 1486
Epoch 1486, Loss: 0.000006185, Improvement: -0.000000944, Best Loss: 0.000001134 in Epoch 1456
Epoch 1487
Epoch 1487, Loss: 0.000005039, Improvement: -0.000001146, Best Loss: 0.000001134 in Epoch 1456
Epoch 1488
Epoch 1488, Loss: 0.000003801, Improvement: -0.000001238, Best Loss: 0.000001134 in Epoch 1456
Epoch 1489
Epoch 1489, Loss: 0.000004917, Improvement: 0.000001116, Best Loss: 0.000001134 in Epoch 1456
Epoch 1490
Epoch 1490, Loss: 0.000005873, Improvement: 0.000000957, Best Loss: 0.000001134 in Epoch 1456
Epoch 1491
Epoch 1491, Loss: 0.000005116, Improvement: -0.000000758, Best Loss: 0.000001134 in Epoch 1456
Epoch 1492
Epoch 1492, Loss: 0.000004603, Improvement: -0.000000512, Best Loss: 0.000001134 in Epoch 1456
Epoch 1493
Epoch 1493, Loss: 0.000005654, Improvement: 0.000001051, Best Loss: 0.000001134 in Epoch 1456
Epoch 1494
Epoch 1494, Loss: 0.000006478, Improvement: 0.000000824, Best Loss: 0.000001134 in Epoch 1456
Epoch 1495
Epoch 1495, Loss: 0.000015944, Improvement: 0.000009465, Best Loss: 0.000001134 in Epoch 1456
Epoch 1496
Epoch 1496, Loss: 0.000022050, Improvement: 0.000006106, Best Loss: 0.000001134 in Epoch 1456
Epoch 1497
Epoch 1497, Loss: 0.000015231, Improvement: -0.000006818, Best Loss: 0.000001134 in Epoch 1456
Epoch 1498
Epoch 1498, Loss: 0.000013085, Improvement: -0.000002146, Best Loss: 0.000001134 in Epoch 1456
Epoch 1499
Epoch 1499, Loss: 0.000041289, Improvement: 0.000028204, Best Loss: 0.000001134 in Epoch 1456
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.000032441, Improvement: -0.000008849, Best Loss: 0.000001134 in Epoch 1456
Epoch 1501
Epoch 1501, Loss: 0.000012741, Improvement: -0.000019699, Best Loss: 0.000001134 in Epoch 1456
Epoch 1502
Epoch 1502, Loss: 0.000008485, Improvement: -0.000004257, Best Loss: 0.000001134 in Epoch 1456
Epoch 1503
Epoch 1503, Loss: 0.000004259, Improvement: -0.000004226, Best Loss: 0.000001134 in Epoch 1456
Epoch 1504
Epoch 1504, Loss: 0.000003856, Improvement: -0.000000402, Best Loss: 0.000001134 in Epoch 1456
Epoch 1505
Epoch 1505, Loss: 0.000003018, Improvement: -0.000000838, Best Loss: 0.000001134 in Epoch 1456
Epoch 1506
Epoch 1506, Loss: 0.000003064, Improvement: 0.000000046, Best Loss: 0.000001134 in Epoch 1456
Epoch 1507
Epoch 1507, Loss: 0.000002943, Improvement: -0.000000121, Best Loss: 0.000001134 in Epoch 1456
Epoch 1508
Epoch 1508, Loss: 0.000004219, Improvement: 0.000001275, Best Loss: 0.000001134 in Epoch 1456
Epoch 1509
Epoch 1509, Loss: 0.000003797, Improvement: -0.000000422, Best Loss: 0.000001134 in Epoch 1456
Epoch 1510
Epoch 1510, Loss: 0.000004918, Improvement: 0.000001121, Best Loss: 0.000001134 in Epoch 1456
Epoch 1511
Epoch 1511, Loss: 0.000002975, Improvement: -0.000001943, Best Loss: 0.000001134 in Epoch 1456
Epoch 1512
Epoch 1512, Loss: 0.000002484, Improvement: -0.000000491, Best Loss: 0.000001134 in Epoch 1456
Epoch 1513
Epoch 1513, Loss: 0.000002621, Improvement: 0.000000137, Best Loss: 0.000001134 in Epoch 1456
Epoch 1514
Epoch 1514, Loss: 0.000004817, Improvement: 0.000002195, Best Loss: 0.000001134 in Epoch 1456
Epoch 1515
Epoch 1515, Loss: 0.000007123, Improvement: 0.000002306, Best Loss: 0.000001134 in Epoch 1456
Epoch 1516
Epoch 1516, Loss: 0.000006670, Improvement: -0.000000453, Best Loss: 0.000001134 in Epoch 1456
Epoch 1517
Epoch 1517, Loss: 0.000006414, Improvement: -0.000000256, Best Loss: 0.000001134 in Epoch 1456
Epoch 1518
Epoch 1518, Loss: 0.000013826, Improvement: 0.000007412, Best Loss: 0.000001134 in Epoch 1456
Epoch 1519
Epoch 1519, Loss: 0.000032106, Improvement: 0.000018280, Best Loss: 0.000001134 in Epoch 1456
Epoch 1520
Epoch 1520, Loss: 0.000032600, Improvement: 0.000000494, Best Loss: 0.000001134 in Epoch 1456
Epoch 1521
Epoch 1521, Loss: 0.000029515, Improvement: -0.000003085, Best Loss: 0.000001134 in Epoch 1456
Epoch 1522
Epoch 1522, Loss: 0.000024340, Improvement: -0.000005175, Best Loss: 0.000001134 in Epoch 1456
Epoch 1523
Epoch 1523, Loss: 0.000012055, Improvement: -0.000012285, Best Loss: 0.000001134 in Epoch 1456
Epoch 1524
Epoch 1524, Loss: 0.000021371, Improvement: 0.000009315, Best Loss: 0.000001134 in Epoch 1456
Epoch 1525
Epoch 1525, Loss: 0.000019142, Improvement: -0.000002229, Best Loss: 0.000001134 in Epoch 1456
Epoch 1526
Epoch 1526, Loss: 0.000015132, Improvement: -0.000004010, Best Loss: 0.000001134 in Epoch 1456
Epoch 1527
Epoch 1527, Loss: 0.000005807, Improvement: -0.000009325, Best Loss: 0.000001134 in Epoch 1456
Epoch 1528
Epoch 1528, Loss: 0.000007087, Improvement: 0.000001280, Best Loss: 0.000001134 in Epoch 1456
Epoch 1529
Epoch 1529, Loss: 0.000005301, Improvement: -0.000001786, Best Loss: 0.000001134 in Epoch 1456
Epoch 1530
Epoch 1530, Loss: 0.000004563, Improvement: -0.000000738, Best Loss: 0.000001134 in Epoch 1456
Epoch 1531
Epoch 1531, Loss: 0.000008928, Improvement: 0.000004365, Best Loss: 0.000001134 in Epoch 1456
Epoch 1532
Epoch 1532, Loss: 0.000014339, Improvement: 0.000005411, Best Loss: 0.000001134 in Epoch 1456
Epoch 1533
Epoch 1533, Loss: 0.000009422, Improvement: -0.000004917, Best Loss: 0.000001134 in Epoch 1456
Epoch 1534
Epoch 1534, Loss: 0.000005288, Improvement: -0.000004134, Best Loss: 0.000001134 in Epoch 1456
Epoch 1535
Epoch 1535, Loss: 0.000003844, Improvement: -0.000001444, Best Loss: 0.000001134 in Epoch 1456
Epoch 1536
Epoch 1536, Loss: 0.000003678, Improvement: -0.000000166, Best Loss: 0.000001134 in Epoch 1456
Epoch 1537
Epoch 1537, Loss: 0.000007425, Improvement: 0.000003748, Best Loss: 0.000001134 in Epoch 1456
Epoch 1538
Epoch 1538, Loss: 0.000010794, Improvement: 0.000003369, Best Loss: 0.000001134 in Epoch 1456
Epoch 1539
Epoch 1539, Loss: 0.000019691, Improvement: 0.000008897, Best Loss: 0.000001134 in Epoch 1456
Epoch 1540
Epoch 1540, Loss: 0.000015338, Improvement: -0.000004353, Best Loss: 0.000001134 in Epoch 1456
Epoch 1541
Epoch 1541, Loss: 0.000007421, Improvement: -0.000007917, Best Loss: 0.000001134 in Epoch 1456
Epoch 1542
Epoch 1542, Loss: 0.000007722, Improvement: 0.000000301, Best Loss: 0.000001134 in Epoch 1456
Epoch 1543
Epoch 1543, Loss: 0.000005434, Improvement: -0.000002288, Best Loss: 0.000001134 in Epoch 1456
Epoch 1544
Epoch 1544, Loss: 0.000004776, Improvement: -0.000000658, Best Loss: 0.000001134 in Epoch 1456
Epoch 1545
Epoch 1545, Loss: 0.000006498, Improvement: 0.000001721, Best Loss: 0.000001134 in Epoch 1456
Epoch 1546
Epoch 1546, Loss: 0.000006178, Improvement: -0.000000319, Best Loss: 0.000001134 in Epoch 1456
Epoch 1547
Epoch 1547, Loss: 0.000007206, Improvement: 0.000001028, Best Loss: 0.000001134 in Epoch 1456
Epoch 1548
Epoch 1548, Loss: 0.000011045, Improvement: 0.000003839, Best Loss: 0.000001134 in Epoch 1456
Epoch 1549
Epoch 1549, Loss: 0.000027219, Improvement: 0.000016174, Best Loss: 0.000001134 in Epoch 1456
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.000010942, Improvement: -0.000016277, Best Loss: 0.000001134 in Epoch 1456
Epoch 1551
Epoch 1551, Loss: 0.000006724, Improvement: -0.000004218, Best Loss: 0.000001134 in Epoch 1456
Epoch 1552
Epoch 1552, Loss: 0.000009788, Improvement: 0.000003064, Best Loss: 0.000001134 in Epoch 1456
Epoch 1553
Epoch 1553, Loss: 0.000010389, Improvement: 0.000000602, Best Loss: 0.000001134 in Epoch 1456
Epoch 1554
Epoch 1554, Loss: 0.000011518, Improvement: 0.000001128, Best Loss: 0.000001134 in Epoch 1456
Epoch 1555
Epoch 1555, Loss: 0.000009413, Improvement: -0.000002105, Best Loss: 0.000001134 in Epoch 1456
Epoch 1556
Epoch 1556, Loss: 0.000011905, Improvement: 0.000002492, Best Loss: 0.000001134 in Epoch 1456
Epoch 1557
Epoch 1557, Loss: 0.000011198, Improvement: -0.000000707, Best Loss: 0.000001134 in Epoch 1456
Epoch 1558
Epoch 1558, Loss: 0.000037922, Improvement: 0.000026724, Best Loss: 0.000001134 in Epoch 1456
Epoch 1559
Epoch 1559, Loss: 0.000028593, Improvement: -0.000009328, Best Loss: 0.000001134 in Epoch 1456
Epoch 1560
Epoch 1560, Loss: 0.000009731, Improvement: -0.000018862, Best Loss: 0.000001134 in Epoch 1456
Epoch 1561
Epoch 1561, Loss: 0.000005681, Improvement: -0.000004051, Best Loss: 0.000001134 in Epoch 1456
Epoch 1562
Epoch 1562, Loss: 0.000004778, Improvement: -0.000000903, Best Loss: 0.000001134 in Epoch 1456
Epoch 1563
Epoch 1563, Loss: 0.000004323, Improvement: -0.000000455, Best Loss: 0.000001134 in Epoch 1456
Epoch 1564
Epoch 1564, Loss: 0.000004814, Improvement: 0.000000492, Best Loss: 0.000001134 in Epoch 1456
Epoch 1565
Epoch 1565, Loss: 0.000005719, Improvement: 0.000000905, Best Loss: 0.000001134 in Epoch 1456
Epoch 1566
Epoch 1566, Loss: 0.000005831, Improvement: 0.000000112, Best Loss: 0.000001134 in Epoch 1456
Epoch 1567
Epoch 1567, Loss: 0.000016279, Improvement: 0.000010449, Best Loss: 0.000001134 in Epoch 1456
Epoch 1568
Epoch 1568, Loss: 0.000025103, Improvement: 0.000008824, Best Loss: 0.000001134 in Epoch 1456
Epoch 1569
Epoch 1569, Loss: 0.000028703, Improvement: 0.000003600, Best Loss: 0.000001134 in Epoch 1456
Epoch 1570
Epoch 1570, Loss: 0.000019540, Improvement: -0.000009162, Best Loss: 0.000001134 in Epoch 1456
Epoch 1571
Epoch 1571, Loss: 0.000018154, Improvement: -0.000001386, Best Loss: 0.000001134 in Epoch 1456
Epoch 1572
Epoch 1572, Loss: 0.000019351, Improvement: 0.000001196, Best Loss: 0.000001134 in Epoch 1456
Epoch 1573
Epoch 1573, Loss: 0.000018582, Improvement: -0.000000768, Best Loss: 0.000001134 in Epoch 1456
Epoch 1574
Epoch 1574, Loss: 0.000012405, Improvement: -0.000006178, Best Loss: 0.000001134 in Epoch 1456
Epoch 1575
Epoch 1575, Loss: 0.000009407, Improvement: -0.000002998, Best Loss: 0.000001134 in Epoch 1456
Epoch 1576
Epoch 1576, Loss: 0.000012617, Improvement: 0.000003210, Best Loss: 0.000001134 in Epoch 1456
Epoch 1577
Epoch 1577, Loss: 0.000013746, Improvement: 0.000001129, Best Loss: 0.000001134 in Epoch 1456
Epoch 1578
Epoch 1578, Loss: 0.000017061, Improvement: 0.000003315, Best Loss: 0.000001134 in Epoch 1456
Epoch 1579
Epoch 1579, Loss: 0.000027859, Improvement: 0.000010799, Best Loss: 0.000001134 in Epoch 1456
Epoch 1580
Epoch 1580, Loss: 0.000017956, Improvement: -0.000009903, Best Loss: 0.000001134 in Epoch 1456
Epoch 1581
Epoch 1581, Loss: 0.000045988, Improvement: 0.000028032, Best Loss: 0.000001134 in Epoch 1456
Epoch 1582
Epoch 1582, Loss: 0.000020859, Improvement: -0.000025129, Best Loss: 0.000001134 in Epoch 1456
Epoch 1583
Epoch 1583, Loss: 0.000021590, Improvement: 0.000000731, Best Loss: 0.000001134 in Epoch 1456
Epoch 1584
Epoch 1584, Loss: 0.000046335, Improvement: 0.000024745, Best Loss: 0.000001134 in Epoch 1456
Epoch 1585
Epoch 1585, Loss: 0.000027458, Improvement: -0.000018878, Best Loss: 0.000001134 in Epoch 1456
Epoch 1586
Epoch 1586, Loss: 0.000016231, Improvement: -0.000011227, Best Loss: 0.000001134 in Epoch 1456
Epoch 1587
Epoch 1587, Loss: 0.000007711, Improvement: -0.000008520, Best Loss: 0.000001134 in Epoch 1456
Epoch 1588
Epoch 1588, Loss: 0.000004778, Improvement: -0.000002933, Best Loss: 0.000001134 in Epoch 1456
Epoch 1589
Epoch 1589, Loss: 0.000003349, Improvement: -0.000001429, Best Loss: 0.000001134 in Epoch 1456
Epoch 1590
Epoch 1590, Loss: 0.000002766, Improvement: -0.000000583, Best Loss: 0.000001134 in Epoch 1456
Epoch 1591
Epoch 1591, Loss: 0.000002914, Improvement: 0.000000148, Best Loss: 0.000001134 in Epoch 1456
Epoch 1592
Epoch 1592, Loss: 0.000002426, Improvement: -0.000000488, Best Loss: 0.000001134 in Epoch 1456
Epoch 1593
Epoch 1593, Loss: 0.000002252, Improvement: -0.000000173, Best Loss: 0.000001134 in Epoch 1456
Epoch 1594
Epoch 1594, Loss: 0.000002129, Improvement: -0.000000123, Best Loss: 0.000001134 in Epoch 1456
Epoch 1595
Epoch 1595, Loss: 0.000002038, Improvement: -0.000000091, Best Loss: 0.000001134 in Epoch 1456
Epoch 1596
Epoch 1596, Loss: 0.000001909, Improvement: -0.000000129, Best Loss: 0.000001134 in Epoch 1456
Epoch 1597
Epoch 1597, Loss: 0.000001897, Improvement: -0.000000012, Best Loss: 0.000001134 in Epoch 1456
Epoch 1598
Epoch 1598, Loss: 0.000001716, Improvement: -0.000000181, Best Loss: 0.000001134 in Epoch 1456
Epoch 1599
Epoch 1599, Loss: 0.000001748, Improvement: 0.000000031, Best Loss: 0.000001134 in Epoch 1456
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.000001929, Improvement: 0.000000181, Best Loss: 0.000001134 in Epoch 1456
Epoch 1601
Epoch 1601, Loss: 0.000001898, Improvement: -0.000000031, Best Loss: 0.000001134 in Epoch 1456
Epoch 1602
Epoch 1602, Loss: 0.000001713, Improvement: -0.000000185, Best Loss: 0.000001134 in Epoch 1456
Epoch 1603
Epoch 1603, Loss: 0.000001726, Improvement: 0.000000013, Best Loss: 0.000001134 in Epoch 1456
Epoch 1604
A best model at epoch 1604 has been saved with training error 0.000001111.
Epoch 1604, Loss: 0.000001714, Improvement: -0.000000012, Best Loss: 0.000001111 in Epoch 1604
Epoch 1605
Epoch 1605, Loss: 0.000001916, Improvement: 0.000000202, Best Loss: 0.000001111 in Epoch 1604
Epoch 1606
Epoch 1606, Loss: 0.000002749, Improvement: 0.000000832, Best Loss: 0.000001111 in Epoch 1604
Epoch 1607
Epoch 1607, Loss: 0.000003074, Improvement: 0.000000325, Best Loss: 0.000001111 in Epoch 1604
Epoch 1608
Epoch 1608, Loss: 0.000003415, Improvement: 0.000000341, Best Loss: 0.000001111 in Epoch 1604
Epoch 1609
Epoch 1609, Loss: 0.000007526, Improvement: 0.000004111, Best Loss: 0.000001111 in Epoch 1604
Epoch 1610
Epoch 1610, Loss: 0.000006936, Improvement: -0.000000590, Best Loss: 0.000001111 in Epoch 1604
Epoch 1611
Epoch 1611, Loss: 0.000006137, Improvement: -0.000000799, Best Loss: 0.000001111 in Epoch 1604
Epoch 1612
Epoch 1612, Loss: 0.000005120, Improvement: -0.000001017, Best Loss: 0.000001111 in Epoch 1604
Epoch 1613
Epoch 1613, Loss: 0.000019343, Improvement: 0.000014224, Best Loss: 0.000001111 in Epoch 1604
Epoch 1614
Epoch 1614, Loss: 0.000019189, Improvement: -0.000000155, Best Loss: 0.000001111 in Epoch 1604
Epoch 1615
Epoch 1615, Loss: 0.000016117, Improvement: -0.000003072, Best Loss: 0.000001111 in Epoch 1604
Epoch 1616
Epoch 1616, Loss: 0.000025667, Improvement: 0.000009550, Best Loss: 0.000001111 in Epoch 1604
Epoch 1617
Epoch 1617, Loss: 0.000018525, Improvement: -0.000007142, Best Loss: 0.000001111 in Epoch 1604
Epoch 1618
Epoch 1618, Loss: 0.000007929, Improvement: -0.000010596, Best Loss: 0.000001111 in Epoch 1604
Epoch 1619
Epoch 1619, Loss: 0.000005203, Improvement: -0.000002726, Best Loss: 0.000001111 in Epoch 1604
Epoch 1620
Epoch 1620, Loss: 0.000007258, Improvement: 0.000002055, Best Loss: 0.000001111 in Epoch 1604
Epoch 1621
Epoch 1621, Loss: 0.000005081, Improvement: -0.000002177, Best Loss: 0.000001111 in Epoch 1604
Epoch 1622
Epoch 1622, Loss: 0.000007532, Improvement: 0.000002451, Best Loss: 0.000001111 in Epoch 1604
Epoch 1623
Epoch 1623, Loss: 0.000012455, Improvement: 0.000004924, Best Loss: 0.000001111 in Epoch 1604
Epoch 1624
Epoch 1624, Loss: 0.000031559, Improvement: 0.000019103, Best Loss: 0.000001111 in Epoch 1604
Epoch 1625
Epoch 1625, Loss: 0.000015366, Improvement: -0.000016193, Best Loss: 0.000001111 in Epoch 1604
Epoch 1626
Epoch 1626, Loss: 0.000015008, Improvement: -0.000000357, Best Loss: 0.000001111 in Epoch 1604
Epoch 1627
Epoch 1627, Loss: 0.000008196, Improvement: -0.000006812, Best Loss: 0.000001111 in Epoch 1604
Epoch 1628
Epoch 1628, Loss: 0.000010211, Improvement: 0.000002014, Best Loss: 0.000001111 in Epoch 1604
Epoch 1629
Epoch 1629, Loss: 0.000006973, Improvement: -0.000003238, Best Loss: 0.000001111 in Epoch 1604
Epoch 1630
Epoch 1630, Loss: 0.000014267, Improvement: 0.000007294, Best Loss: 0.000001111 in Epoch 1604
Epoch 1631
Epoch 1631, Loss: 0.000028021, Improvement: 0.000013754, Best Loss: 0.000001111 in Epoch 1604
Epoch 1632
Epoch 1632, Loss: 0.000036171, Improvement: 0.000008150, Best Loss: 0.000001111 in Epoch 1604
Epoch 1633
Epoch 1633, Loss: 0.000024370, Improvement: -0.000011801, Best Loss: 0.000001111 in Epoch 1604
Epoch 1634
Epoch 1634, Loss: 0.000010822, Improvement: -0.000013549, Best Loss: 0.000001111 in Epoch 1604
Epoch 1635
Epoch 1635, Loss: 0.000007869, Improvement: -0.000002952, Best Loss: 0.000001111 in Epoch 1604
Epoch 1636
Epoch 1636, Loss: 0.000005467, Improvement: -0.000002402, Best Loss: 0.000001111 in Epoch 1604
Epoch 1637
Epoch 1637, Loss: 0.000005113, Improvement: -0.000000355, Best Loss: 0.000001111 in Epoch 1604
Epoch 1638
Epoch 1638, Loss: 0.000003515, Improvement: -0.000001598, Best Loss: 0.000001111 in Epoch 1604
Epoch 1639
Epoch 1639, Loss: 0.000003456, Improvement: -0.000000058, Best Loss: 0.000001111 in Epoch 1604
Epoch 1640
Epoch 1640, Loss: 0.000004196, Improvement: 0.000000739, Best Loss: 0.000001111 in Epoch 1604
Epoch 1641
Epoch 1641, Loss: 0.000004266, Improvement: 0.000000070, Best Loss: 0.000001111 in Epoch 1604
Epoch 1642
Epoch 1642, Loss: 0.000003633, Improvement: -0.000000633, Best Loss: 0.000001111 in Epoch 1604
Epoch 1643
Epoch 1643, Loss: 0.000003759, Improvement: 0.000000126, Best Loss: 0.000001111 in Epoch 1604
Epoch 1644
Epoch 1644, Loss: 0.000002561, Improvement: -0.000001198, Best Loss: 0.000001111 in Epoch 1604
Epoch 1645
Epoch 1645, Loss: 0.000003061, Improvement: 0.000000499, Best Loss: 0.000001111 in Epoch 1604
Epoch 1646
Epoch 1646, Loss: 0.000010354, Improvement: 0.000007293, Best Loss: 0.000001111 in Epoch 1604
Epoch 1647
Epoch 1647, Loss: 0.000024220, Improvement: 0.000013866, Best Loss: 0.000001111 in Epoch 1604
Epoch 1648
Epoch 1648, Loss: 0.000036796, Improvement: 0.000012577, Best Loss: 0.000001111 in Epoch 1604
Epoch 1649
Epoch 1649, Loss: 0.000038433, Improvement: 0.000001637, Best Loss: 0.000001111 in Epoch 1604
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.000042363, Improvement: 0.000003930, Best Loss: 0.000001111 in Epoch 1604
Epoch 1651
Epoch 1651, Loss: 0.000011417, Improvement: -0.000030947, Best Loss: 0.000001111 in Epoch 1604
Epoch 1652
Epoch 1652, Loss: 0.000008055, Improvement: -0.000003361, Best Loss: 0.000001111 in Epoch 1604
Epoch 1653
Epoch 1653, Loss: 0.000004722, Improvement: -0.000003333, Best Loss: 0.000001111 in Epoch 1604
Epoch 1654
Epoch 1654, Loss: 0.000002958, Improvement: -0.000001764, Best Loss: 0.000001111 in Epoch 1604
Epoch 1655
Epoch 1655, Loss: 0.000002666, Improvement: -0.000000293, Best Loss: 0.000001111 in Epoch 1604
Epoch 1656
Epoch 1656, Loss: 0.000002261, Improvement: -0.000000405, Best Loss: 0.000001111 in Epoch 1604
Epoch 1657
Epoch 1657, Loss: 0.000002047, Improvement: -0.000000214, Best Loss: 0.000001111 in Epoch 1604
Epoch 1658
Epoch 1658, Loss: 0.000001833, Improvement: -0.000000215, Best Loss: 0.000001111 in Epoch 1604
Epoch 1659
Epoch 1659, Loss: 0.000001846, Improvement: 0.000000013, Best Loss: 0.000001111 in Epoch 1604
Epoch 1660
Epoch 1660, Loss: 0.000002052, Improvement: 0.000000206, Best Loss: 0.000001111 in Epoch 1604
Epoch 1661
Epoch 1661, Loss: 0.000001844, Improvement: -0.000000207, Best Loss: 0.000001111 in Epoch 1604
Epoch 1662
Epoch 1662, Loss: 0.000001712, Improvement: -0.000000133, Best Loss: 0.000001111 in Epoch 1604
Epoch 1663
Epoch 1663, Loss: 0.000001786, Improvement: 0.000000074, Best Loss: 0.000001111 in Epoch 1604
Epoch 1664
A best model at epoch 1664 has been saved with training error 0.000001078.
Epoch 1664, Loss: 0.000001758, Improvement: -0.000000028, Best Loss: 0.000001078 in Epoch 1664
Epoch 1665
Epoch 1665, Loss: 0.000003396, Improvement: 0.000001638, Best Loss: 0.000001078 in Epoch 1664
Epoch 1666
Epoch 1666, Loss: 0.000015178, Improvement: 0.000011782, Best Loss: 0.000001078 in Epoch 1664
Epoch 1667
Epoch 1667, Loss: 0.000018792, Improvement: 0.000003614, Best Loss: 0.000001078 in Epoch 1664
Epoch 1668
Epoch 1668, Loss: 0.000008575, Improvement: -0.000010217, Best Loss: 0.000001078 in Epoch 1664
Epoch 1669
Epoch 1669, Loss: 0.000004787, Improvement: -0.000003788, Best Loss: 0.000001078 in Epoch 1664
Epoch 1670
Epoch 1670, Loss: 0.000007095, Improvement: 0.000002308, Best Loss: 0.000001078 in Epoch 1664
Epoch 1671
Epoch 1671, Loss: 0.000004359, Improvement: -0.000002736, Best Loss: 0.000001078 in Epoch 1664
Epoch 1672
Epoch 1672, Loss: 0.000003226, Improvement: -0.000001133, Best Loss: 0.000001078 in Epoch 1664
Epoch 1673
Epoch 1673, Loss: 0.000002144, Improvement: -0.000001082, Best Loss: 0.000001078 in Epoch 1664
Epoch 1674
Epoch 1674, Loss: 0.000002863, Improvement: 0.000000719, Best Loss: 0.000001078 in Epoch 1664
Epoch 1675
Epoch 1675, Loss: 0.000003224, Improvement: 0.000000360, Best Loss: 0.000001078 in Epoch 1664
Epoch 1676
Epoch 1676, Loss: 0.000003337, Improvement: 0.000000113, Best Loss: 0.000001078 in Epoch 1664
Epoch 1677
Epoch 1677, Loss: 0.000004103, Improvement: 0.000000766, Best Loss: 0.000001078 in Epoch 1664
Epoch 1678
Epoch 1678, Loss: 0.000005021, Improvement: 0.000000918, Best Loss: 0.000001078 in Epoch 1664
Epoch 1679
Epoch 1679, Loss: 0.000005038, Improvement: 0.000000017, Best Loss: 0.000001078 in Epoch 1664
Epoch 1680
Epoch 1680, Loss: 0.000009760, Improvement: 0.000004722, Best Loss: 0.000001078 in Epoch 1664
Epoch 1681
Epoch 1681, Loss: 0.000011445, Improvement: 0.000001684, Best Loss: 0.000001078 in Epoch 1664
Epoch 1682
Epoch 1682, Loss: 0.000026917, Improvement: 0.000015472, Best Loss: 0.000001078 in Epoch 1664
Epoch 1683
Epoch 1683, Loss: 0.000033467, Improvement: 0.000006550, Best Loss: 0.000001078 in Epoch 1664
Epoch 1684
Epoch 1684, Loss: 0.000014298, Improvement: -0.000019169, Best Loss: 0.000001078 in Epoch 1664
Epoch 1685
Epoch 1685, Loss: 0.000009736, Improvement: -0.000004562, Best Loss: 0.000001078 in Epoch 1664
Epoch 1686
Epoch 1686, Loss: 0.000006688, Improvement: -0.000003049, Best Loss: 0.000001078 in Epoch 1664
Epoch 1687
Epoch 1687, Loss: 0.000021463, Improvement: 0.000014775, Best Loss: 0.000001078 in Epoch 1664
Epoch 1688
Epoch 1688, Loss: 0.000011195, Improvement: -0.000010268, Best Loss: 0.000001078 in Epoch 1664
Epoch 1689
Epoch 1689, Loss: 0.000008452, Improvement: -0.000002743, Best Loss: 0.000001078 in Epoch 1664
Epoch 1690
Epoch 1690, Loss: 0.000005242, Improvement: -0.000003210, Best Loss: 0.000001078 in Epoch 1664
Epoch 1691
Epoch 1691, Loss: 0.000003231, Improvement: -0.000002011, Best Loss: 0.000001078 in Epoch 1664
Epoch 1692
Epoch 1692, Loss: 0.000003910, Improvement: 0.000000679, Best Loss: 0.000001078 in Epoch 1664
Epoch 1693
Epoch 1693, Loss: 0.000003726, Improvement: -0.000000184, Best Loss: 0.000001078 in Epoch 1664
Epoch 1694
Epoch 1694, Loss: 0.000004305, Improvement: 0.000000578, Best Loss: 0.000001078 in Epoch 1664
Epoch 1695
Epoch 1695, Loss: 0.000042037, Improvement: 0.000037732, Best Loss: 0.000001078 in Epoch 1664
Epoch 1696
Epoch 1696, Loss: 0.000020195, Improvement: -0.000021841, Best Loss: 0.000001078 in Epoch 1664
Epoch 1697
Epoch 1697, Loss: 0.000017505, Improvement: -0.000002690, Best Loss: 0.000001078 in Epoch 1664
Epoch 1698
Epoch 1698, Loss: 0.000012223, Improvement: -0.000005283, Best Loss: 0.000001078 in Epoch 1664
Epoch 1699
Epoch 1699, Loss: 0.000004726, Improvement: -0.000007496, Best Loss: 0.000001078 in Epoch 1664
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.000002796, Improvement: -0.000001930, Best Loss: 0.000001078 in Epoch 1664
Epoch 1701
Epoch 1701, Loss: 0.000002110, Improvement: -0.000000686, Best Loss: 0.000001078 in Epoch 1664
Epoch 1702
Epoch 1702, Loss: 0.000001963, Improvement: -0.000000147, Best Loss: 0.000001078 in Epoch 1664
Epoch 1703
A best model at epoch 1703 has been saved with training error 0.000001035.
Epoch 1703, Loss: 0.000001680, Improvement: -0.000000283, Best Loss: 0.000001035 in Epoch 1703
Epoch 1704
Epoch 1704, Loss: 0.000001610, Improvement: -0.000000070, Best Loss: 0.000001035 in Epoch 1703
Epoch 1705
Epoch 1705, Loss: 0.000001895, Improvement: 0.000000285, Best Loss: 0.000001035 in Epoch 1703
Epoch 1706
Epoch 1706, Loss: 0.000001995, Improvement: 0.000000099, Best Loss: 0.000001035 in Epoch 1703
Epoch 1707
Epoch 1707, Loss: 0.000001742, Improvement: -0.000000253, Best Loss: 0.000001035 in Epoch 1703
Epoch 1708
Epoch 1708, Loss: 0.000001596, Improvement: -0.000000146, Best Loss: 0.000001035 in Epoch 1703
Epoch 1709
Epoch 1709, Loss: 0.000001532, Improvement: -0.000000064, Best Loss: 0.000001035 in Epoch 1703
Epoch 1710
Epoch 1710, Loss: 0.000001664, Improvement: 0.000000132, Best Loss: 0.000001035 in Epoch 1703
Epoch 1711
Epoch 1711, Loss: 0.000002007, Improvement: 0.000000343, Best Loss: 0.000001035 in Epoch 1703
Epoch 1712
Epoch 1712, Loss: 0.000002074, Improvement: 0.000000067, Best Loss: 0.000001035 in Epoch 1703
Epoch 1713
Epoch 1713, Loss: 0.000001859, Improvement: -0.000000215, Best Loss: 0.000001035 in Epoch 1703
Epoch 1714
Epoch 1714, Loss: 0.000002153, Improvement: 0.000000294, Best Loss: 0.000001035 in Epoch 1703
Epoch 1715
Epoch 1715, Loss: 0.000002643, Improvement: 0.000000490, Best Loss: 0.000001035 in Epoch 1703
Epoch 1716
Epoch 1716, Loss: 0.000003144, Improvement: 0.000000500, Best Loss: 0.000001035 in Epoch 1703
Epoch 1717
Epoch 1717, Loss: 0.000002220, Improvement: -0.000000923, Best Loss: 0.000001035 in Epoch 1703
Epoch 1718
Epoch 1718, Loss: 0.000002236, Improvement: 0.000000016, Best Loss: 0.000001035 in Epoch 1703
Epoch 1719
Epoch 1719, Loss: 0.000002216, Improvement: -0.000000020, Best Loss: 0.000001035 in Epoch 1703
Epoch 1720
Epoch 1720, Loss: 0.000002382, Improvement: 0.000000166, Best Loss: 0.000001035 in Epoch 1703
Epoch 1721
Epoch 1721, Loss: 0.000002991, Improvement: 0.000000609, Best Loss: 0.000001035 in Epoch 1703
Epoch 1722
Epoch 1722, Loss: 0.000003911, Improvement: 0.000000920, Best Loss: 0.000001035 in Epoch 1703
Epoch 1723
Epoch 1723, Loss: 0.000005070, Improvement: 0.000001159, Best Loss: 0.000001035 in Epoch 1703
Epoch 1724
Epoch 1724, Loss: 0.000004436, Improvement: -0.000000634, Best Loss: 0.000001035 in Epoch 1703
Epoch 1725
Epoch 1725, Loss: 0.000004936, Improvement: 0.000000499, Best Loss: 0.000001035 in Epoch 1703
Epoch 1726
Epoch 1726, Loss: 0.000002631, Improvement: -0.000002305, Best Loss: 0.000001035 in Epoch 1703
Epoch 1727
Epoch 1727, Loss: 0.000002457, Improvement: -0.000000174, Best Loss: 0.000001035 in Epoch 1703
Epoch 1728
Epoch 1728, Loss: 0.000007248, Improvement: 0.000004791, Best Loss: 0.000001035 in Epoch 1703
Epoch 1729
Epoch 1729, Loss: 0.000016948, Improvement: 0.000009700, Best Loss: 0.000001035 in Epoch 1703
Epoch 1730
Epoch 1730, Loss: 0.000017358, Improvement: 0.000000410, Best Loss: 0.000001035 in Epoch 1703
Epoch 1731
Epoch 1731, Loss: 0.000007223, Improvement: -0.000010135, Best Loss: 0.000001035 in Epoch 1703
Epoch 1732
Epoch 1732, Loss: 0.000006849, Improvement: -0.000000374, Best Loss: 0.000001035 in Epoch 1703
Epoch 1733
Epoch 1733, Loss: 0.000004439, Improvement: -0.000002410, Best Loss: 0.000001035 in Epoch 1703
Epoch 1734
Epoch 1734, Loss: 0.000005667, Improvement: 0.000001228, Best Loss: 0.000001035 in Epoch 1703
Epoch 1735
Epoch 1735, Loss: 0.000008235, Improvement: 0.000002567, Best Loss: 0.000001035 in Epoch 1703
Epoch 1736
Epoch 1736, Loss: 0.000007742, Improvement: -0.000000493, Best Loss: 0.000001035 in Epoch 1703
Epoch 1737
Epoch 1737, Loss: 0.000005767, Improvement: -0.000001975, Best Loss: 0.000001035 in Epoch 1703
Epoch 1738
Epoch 1738, Loss: 0.000006407, Improvement: 0.000000640, Best Loss: 0.000001035 in Epoch 1703
Epoch 1739
Epoch 1739, Loss: 0.000005581, Improvement: -0.000000826, Best Loss: 0.000001035 in Epoch 1703
Epoch 1740
Epoch 1740, Loss: 0.000005419, Improvement: -0.000000163, Best Loss: 0.000001035 in Epoch 1703
Epoch 1741
Epoch 1741, Loss: 0.000015766, Improvement: 0.000010347, Best Loss: 0.000001035 in Epoch 1703
Epoch 1742
Epoch 1742, Loss: 0.000042600, Improvement: 0.000026834, Best Loss: 0.000001035 in Epoch 1703
Epoch 1743
Epoch 1743, Loss: 0.000035242, Improvement: -0.000007357, Best Loss: 0.000001035 in Epoch 1703
Epoch 1744
Epoch 1744, Loss: 0.000027009, Improvement: -0.000008234, Best Loss: 0.000001035 in Epoch 1703
Epoch 1745
Epoch 1745, Loss: 0.000009278, Improvement: -0.000017731, Best Loss: 0.000001035 in Epoch 1703
Epoch 1746
Epoch 1746, Loss: 0.000003877, Improvement: -0.000005401, Best Loss: 0.000001035 in Epoch 1703
Epoch 1747
Epoch 1747, Loss: 0.000002630, Improvement: -0.000001247, Best Loss: 0.000001035 in Epoch 1703
Epoch 1748
Epoch 1748, Loss: 0.000002065, Improvement: -0.000000565, Best Loss: 0.000001035 in Epoch 1703
Epoch 1749
Epoch 1749, Loss: 0.000001771, Improvement: -0.000000294, Best Loss: 0.000001035 in Epoch 1703
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.000001676, Improvement: -0.000000096, Best Loss: 0.000001035 in Epoch 1703
Epoch 1751
Epoch 1751, Loss: 0.000001645, Improvement: -0.000000030, Best Loss: 0.000001035 in Epoch 1703
Epoch 1752
Epoch 1752, Loss: 0.000001597, Improvement: -0.000000048, Best Loss: 0.000001035 in Epoch 1703
Epoch 1753
Epoch 1753, Loss: 0.000001550, Improvement: -0.000000047, Best Loss: 0.000001035 in Epoch 1703
Epoch 1754
Epoch 1754, Loss: 0.000001473, Improvement: -0.000000077, Best Loss: 0.000001035 in Epoch 1703
Epoch 1755
Epoch 1755, Loss: 0.000001428, Improvement: -0.000000045, Best Loss: 0.000001035 in Epoch 1703
Epoch 1756
Epoch 1756, Loss: 0.000001446, Improvement: 0.000000017, Best Loss: 0.000001035 in Epoch 1703
Epoch 1757
Epoch 1757, Loss: 0.000001434, Improvement: -0.000000012, Best Loss: 0.000001035 in Epoch 1703
Epoch 1758
Epoch 1758, Loss: 0.000001657, Improvement: 0.000000223, Best Loss: 0.000001035 in Epoch 1703
Epoch 1759
Epoch 1759, Loss: 0.000001625, Improvement: -0.000000032, Best Loss: 0.000001035 in Epoch 1703
Epoch 1760
Epoch 1760, Loss: 0.000001636, Improvement: 0.000000011, Best Loss: 0.000001035 in Epoch 1703
Epoch 1761
Epoch 1761, Loss: 0.000001607, Improvement: -0.000000028, Best Loss: 0.000001035 in Epoch 1703
Epoch 1762
Epoch 1762, Loss: 0.000001664, Improvement: 0.000000057, Best Loss: 0.000001035 in Epoch 1703
Epoch 1763
Epoch 1763, Loss: 0.000001928, Improvement: 0.000000264, Best Loss: 0.000001035 in Epoch 1703
Epoch 1764
Epoch 1764, Loss: 0.000002613, Improvement: 0.000000685, Best Loss: 0.000001035 in Epoch 1703
Epoch 1765
Epoch 1765, Loss: 0.000002892, Improvement: 0.000000279, Best Loss: 0.000001035 in Epoch 1703
Epoch 1766
Epoch 1766, Loss: 0.000005664, Improvement: 0.000002772, Best Loss: 0.000001035 in Epoch 1703
Epoch 1767
Epoch 1767, Loss: 0.000004191, Improvement: -0.000001473, Best Loss: 0.000001035 in Epoch 1703
Epoch 1768
Epoch 1768, Loss: 0.000003415, Improvement: -0.000000776, Best Loss: 0.000001035 in Epoch 1703
Epoch 1769
Epoch 1769, Loss: 0.000002735, Improvement: -0.000000680, Best Loss: 0.000001035 in Epoch 1703
Epoch 1770
Epoch 1770, Loss: 0.000003754, Improvement: 0.000001018, Best Loss: 0.000001035 in Epoch 1703
Epoch 1771
Epoch 1771, Loss: 0.000010776, Improvement: 0.000007022, Best Loss: 0.000001035 in Epoch 1703
Epoch 1772
Epoch 1772, Loss: 0.000011535, Improvement: 0.000000759, Best Loss: 0.000001035 in Epoch 1703
Epoch 1773
Epoch 1773, Loss: 0.000009451, Improvement: -0.000002084, Best Loss: 0.000001035 in Epoch 1703
Epoch 1774
Epoch 1774, Loss: 0.000015152, Improvement: 0.000005702, Best Loss: 0.000001035 in Epoch 1703
Epoch 1775
Epoch 1775, Loss: 0.000019814, Improvement: 0.000004662, Best Loss: 0.000001035 in Epoch 1703
Epoch 1776
Epoch 1776, Loss: 0.000020540, Improvement: 0.000000725, Best Loss: 0.000001035 in Epoch 1703
Epoch 1777
Epoch 1777, Loss: 0.000013591, Improvement: -0.000006949, Best Loss: 0.000001035 in Epoch 1703
Epoch 1778
Epoch 1778, Loss: 0.000008089, Improvement: -0.000005502, Best Loss: 0.000001035 in Epoch 1703
Epoch 1779
Epoch 1779, Loss: 0.000018169, Improvement: 0.000010080, Best Loss: 0.000001035 in Epoch 1703
Epoch 1780
Epoch 1780, Loss: 0.000023240, Improvement: 0.000005071, Best Loss: 0.000001035 in Epoch 1703
Epoch 1781
Epoch 1781, Loss: 0.000018801, Improvement: -0.000004439, Best Loss: 0.000001035 in Epoch 1703
Epoch 1782
Epoch 1782, Loss: 0.000012417, Improvement: -0.000006385, Best Loss: 0.000001035 in Epoch 1703
Epoch 1783
Epoch 1783, Loss: 0.000009312, Improvement: -0.000003105, Best Loss: 0.000001035 in Epoch 1703
Epoch 1784
Epoch 1784, Loss: 0.000005597, Improvement: -0.000003714, Best Loss: 0.000001035 in Epoch 1703
Epoch 1785
Epoch 1785, Loss: 0.000006967, Improvement: 0.000001370, Best Loss: 0.000001035 in Epoch 1703
Epoch 1786
Epoch 1786, Loss: 0.000009568, Improvement: 0.000002600, Best Loss: 0.000001035 in Epoch 1703
Epoch 1787
Epoch 1787, Loss: 0.000008179, Improvement: -0.000001388, Best Loss: 0.000001035 in Epoch 1703
Epoch 1788
Epoch 1788, Loss: 0.000008768, Improvement: 0.000000588, Best Loss: 0.000001035 in Epoch 1703
Epoch 1789
Epoch 1789, Loss: 0.000006248, Improvement: -0.000002520, Best Loss: 0.000001035 in Epoch 1703
Epoch 1790
Epoch 1790, Loss: 0.000004470, Improvement: -0.000001778, Best Loss: 0.000001035 in Epoch 1703
Epoch 1791
Epoch 1791, Loss: 0.000003476, Improvement: -0.000000995, Best Loss: 0.000001035 in Epoch 1703
Epoch 1792
Epoch 1792, Loss: 0.000003164, Improvement: -0.000000312, Best Loss: 0.000001035 in Epoch 1703
Epoch 1793
Epoch 1793, Loss: 0.000004588, Improvement: 0.000001424, Best Loss: 0.000001035 in Epoch 1703
Epoch 1794
Epoch 1794, Loss: 0.000013847, Improvement: 0.000009260, Best Loss: 0.000001035 in Epoch 1703
Epoch 1795
Epoch 1795, Loss: 0.000020351, Improvement: 0.000006504, Best Loss: 0.000001035 in Epoch 1703
Epoch 1796
Epoch 1796, Loss: 0.000022919, Improvement: 0.000002568, Best Loss: 0.000001035 in Epoch 1703
Epoch 1797
Epoch 1797, Loss: 0.000016058, Improvement: -0.000006862, Best Loss: 0.000001035 in Epoch 1703
Epoch 1798
Epoch 1798, Loss: 0.000011951, Improvement: -0.000004107, Best Loss: 0.000001035 in Epoch 1703
Epoch 1799
Epoch 1799, Loss: 0.000012785, Improvement: 0.000000834, Best Loss: 0.000001035 in Epoch 1703
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.000032500, Improvement: 0.000019715, Best Loss: 0.000001035 in Epoch 1703
Epoch 1801
Epoch 1801, Loss: 0.000016494, Improvement: -0.000016006, Best Loss: 0.000001035 in Epoch 1703
Epoch 1802
Epoch 1802, Loss: 0.000019881, Improvement: 0.000003387, Best Loss: 0.000001035 in Epoch 1703
Epoch 1803
Epoch 1803, Loss: 0.000009626, Improvement: -0.000010255, Best Loss: 0.000001035 in Epoch 1703
Epoch 1804
Epoch 1804, Loss: 0.000006467, Improvement: -0.000003159, Best Loss: 0.000001035 in Epoch 1703
Epoch 1805
Epoch 1805, Loss: 0.000004298, Improvement: -0.000002170, Best Loss: 0.000001035 in Epoch 1703
Epoch 1806
Epoch 1806, Loss: 0.000002931, Improvement: -0.000001366, Best Loss: 0.000001035 in Epoch 1703
Epoch 1807
Epoch 1807, Loss: 0.000002678, Improvement: -0.000000253, Best Loss: 0.000001035 in Epoch 1703
Epoch 1808
Epoch 1808, Loss: 0.000002238, Improvement: -0.000000441, Best Loss: 0.000001035 in Epoch 1703
Epoch 1809
Epoch 1809, Loss: 0.000002202, Improvement: -0.000000036, Best Loss: 0.000001035 in Epoch 1703
Epoch 1810
Epoch 1810, Loss: 0.000002026, Improvement: -0.000000176, Best Loss: 0.000001035 in Epoch 1703
Epoch 1811
Epoch 1811, Loss: 0.000002566, Improvement: 0.000000540, Best Loss: 0.000001035 in Epoch 1703
Epoch 1812
Epoch 1812, Loss: 0.000005040, Improvement: 0.000002474, Best Loss: 0.000001035 in Epoch 1703
Epoch 1813
Epoch 1813, Loss: 0.000008588, Improvement: 0.000003547, Best Loss: 0.000001035 in Epoch 1703
Epoch 1814
Epoch 1814, Loss: 0.000007852, Improvement: -0.000000735, Best Loss: 0.000001035 in Epoch 1703
Epoch 1815
Epoch 1815, Loss: 0.000016985, Improvement: 0.000009132, Best Loss: 0.000001035 in Epoch 1703
Epoch 1816
Epoch 1816, Loss: 0.000012854, Improvement: -0.000004131, Best Loss: 0.000001035 in Epoch 1703
Epoch 1817
Epoch 1817, Loss: 0.000019164, Improvement: 0.000006310, Best Loss: 0.000001035 in Epoch 1703
Epoch 1818
Epoch 1818, Loss: 0.000020597, Improvement: 0.000001433, Best Loss: 0.000001035 in Epoch 1703
Epoch 1819
Epoch 1819, Loss: 0.000010225, Improvement: -0.000010373, Best Loss: 0.000001035 in Epoch 1703
Epoch 1820
Epoch 1820, Loss: 0.000004397, Improvement: -0.000005827, Best Loss: 0.000001035 in Epoch 1703
Epoch 1821
Epoch 1821, Loss: 0.000003843, Improvement: -0.000000554, Best Loss: 0.000001035 in Epoch 1703
Epoch 1822
Epoch 1822, Loss: 0.000002905, Improvement: -0.000000938, Best Loss: 0.000001035 in Epoch 1703
Epoch 1823
Epoch 1823, Loss: 0.000002730, Improvement: -0.000000175, Best Loss: 0.000001035 in Epoch 1703
Epoch 1824
Epoch 1824, Loss: 0.000002433, Improvement: -0.000000297, Best Loss: 0.000001035 in Epoch 1703
Epoch 1825
Epoch 1825, Loss: 0.000002416, Improvement: -0.000000017, Best Loss: 0.000001035 in Epoch 1703
Epoch 1826
Epoch 1826, Loss: 0.000002119, Improvement: -0.000000297, Best Loss: 0.000001035 in Epoch 1703
Epoch 1827
Epoch 1827, Loss: 0.000001998, Improvement: -0.000000121, Best Loss: 0.000001035 in Epoch 1703
Epoch 1828
Epoch 1828, Loss: 0.000002454, Improvement: 0.000000456, Best Loss: 0.000001035 in Epoch 1703
Epoch 1829
Epoch 1829, Loss: 0.000005738, Improvement: 0.000003284, Best Loss: 0.000001035 in Epoch 1703
Epoch 1830
Epoch 1830, Loss: 0.000005865, Improvement: 0.000000127, Best Loss: 0.000001035 in Epoch 1703
Epoch 1831
Epoch 1831, Loss: 0.000007203, Improvement: 0.000001338, Best Loss: 0.000001035 in Epoch 1703
Epoch 1832
Epoch 1832, Loss: 0.000015381, Improvement: 0.000008178, Best Loss: 0.000001035 in Epoch 1703
Epoch 1833
Epoch 1833, Loss: 0.000008556, Improvement: -0.000006825, Best Loss: 0.000001035 in Epoch 1703
Epoch 1834
Epoch 1834, Loss: 0.000005226, Improvement: -0.000003330, Best Loss: 0.000001035 in Epoch 1703
Epoch 1835
Epoch 1835, Loss: 0.000022847, Improvement: 0.000017621, Best Loss: 0.000001035 in Epoch 1703
Epoch 1836
Epoch 1836, Loss: 0.000023090, Improvement: 0.000000243, Best Loss: 0.000001035 in Epoch 1703
Epoch 1837
Epoch 1837, Loss: 0.000010143, Improvement: -0.000012947, Best Loss: 0.000001035 in Epoch 1703
Epoch 1838
Epoch 1838, Loss: 0.000004674, Improvement: -0.000005469, Best Loss: 0.000001035 in Epoch 1703
Epoch 1839
Epoch 1839, Loss: 0.000003611, Improvement: -0.000001064, Best Loss: 0.000001035 in Epoch 1703
Epoch 1840
Epoch 1840, Loss: 0.000002712, Improvement: -0.000000899, Best Loss: 0.000001035 in Epoch 1703
Epoch 1841
Epoch 1841, Loss: 0.000002961, Improvement: 0.000000249, Best Loss: 0.000001035 in Epoch 1703
Epoch 1842
Epoch 1842, Loss: 0.000007746, Improvement: 0.000004785, Best Loss: 0.000001035 in Epoch 1703
Epoch 1843
Epoch 1843, Loss: 0.000008033, Improvement: 0.000000288, Best Loss: 0.000001035 in Epoch 1703
Epoch 1844
Epoch 1844, Loss: 0.000005469, Improvement: -0.000002564, Best Loss: 0.000001035 in Epoch 1703
Epoch 1845
Epoch 1845, Loss: 0.000004041, Improvement: -0.000001428, Best Loss: 0.000001035 in Epoch 1703
Epoch 1846
Epoch 1846, Loss: 0.000003325, Improvement: -0.000000716, Best Loss: 0.000001035 in Epoch 1703
Epoch 1847
Epoch 1847, Loss: 0.000007978, Improvement: 0.000004653, Best Loss: 0.000001035 in Epoch 1703
Epoch 1848
Epoch 1848, Loss: 0.000005697, Improvement: -0.000002282, Best Loss: 0.000001035 in Epoch 1703
Epoch 1849
Epoch 1849, Loss: 0.000004604, Improvement: -0.000001093, Best Loss: 0.000001035 in Epoch 1703
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.000007196, Improvement: 0.000002593, Best Loss: 0.000001035 in Epoch 1703
Epoch 1851
Epoch 1851, Loss: 0.000006759, Improvement: -0.000000437, Best Loss: 0.000001035 in Epoch 1703
Epoch 1852
Epoch 1852, Loss: 0.000011305, Improvement: 0.000004545, Best Loss: 0.000001035 in Epoch 1703
Epoch 1853
Epoch 1853, Loss: 0.000013333, Improvement: 0.000002029, Best Loss: 0.000001035 in Epoch 1703
Epoch 1854
Epoch 1854, Loss: 0.000011956, Improvement: -0.000001377, Best Loss: 0.000001035 in Epoch 1703
Epoch 1855
Epoch 1855, Loss: 0.000014155, Improvement: 0.000002199, Best Loss: 0.000001035 in Epoch 1703
Epoch 1856
Epoch 1856, Loss: 0.000020004, Improvement: 0.000005849, Best Loss: 0.000001035 in Epoch 1703
Epoch 1857
Epoch 1857, Loss: 0.000032210, Improvement: 0.000012206, Best Loss: 0.000001035 in Epoch 1703
Epoch 1858
Epoch 1858, Loss: 0.000011738, Improvement: -0.000020472, Best Loss: 0.000001035 in Epoch 1703
Epoch 1859
Epoch 1859, Loss: 0.000007507, Improvement: -0.000004231, Best Loss: 0.000001035 in Epoch 1703
Epoch 1860
Epoch 1860, Loss: 0.000004849, Improvement: -0.000002659, Best Loss: 0.000001035 in Epoch 1703
Epoch 1861
Epoch 1861, Loss: 0.000003166, Improvement: -0.000001682, Best Loss: 0.000001035 in Epoch 1703
Epoch 1862
Epoch 1862, Loss: 0.000003008, Improvement: -0.000000158, Best Loss: 0.000001035 in Epoch 1703
Epoch 1863
Epoch 1863, Loss: 0.000002776, Improvement: -0.000000233, Best Loss: 0.000001035 in Epoch 1703
Epoch 1864
Epoch 1864, Loss: 0.000023884, Improvement: 0.000021108, Best Loss: 0.000001035 in Epoch 1703
Epoch 1865
Epoch 1865, Loss: 0.000037296, Improvement: 0.000013412, Best Loss: 0.000001035 in Epoch 1703
Epoch 1866
Epoch 1866, Loss: 0.000026239, Improvement: -0.000011056, Best Loss: 0.000001035 in Epoch 1703
Epoch 1867
Epoch 1867, Loss: 0.000009891, Improvement: -0.000016348, Best Loss: 0.000001035 in Epoch 1703
Epoch 1868
Epoch 1868, Loss: 0.000004788, Improvement: -0.000005103, Best Loss: 0.000001035 in Epoch 1703
Epoch 1869
Epoch 1869, Loss: 0.000003227, Improvement: -0.000001561, Best Loss: 0.000001035 in Epoch 1703
Epoch 1870
Epoch 1870, Loss: 0.000002509, Improvement: -0.000000717, Best Loss: 0.000001035 in Epoch 1703
Epoch 1871
Epoch 1871, Loss: 0.000002082, Improvement: -0.000000427, Best Loss: 0.000001035 in Epoch 1703
Epoch 1872
Epoch 1872, Loss: 0.000001775, Improvement: -0.000000307, Best Loss: 0.000001035 in Epoch 1703
Epoch 1873
Epoch 1873, Loss: 0.000001642, Improvement: -0.000000133, Best Loss: 0.000001035 in Epoch 1703
Epoch 1874
A best model at epoch 1874 has been saved with training error 0.000001022.
Epoch 1874, Loss: 0.000001675, Improvement: 0.000000034, Best Loss: 0.000001022 in Epoch 1874
Epoch 1875
Epoch 1875, Loss: 0.000001591, Improvement: -0.000000085, Best Loss: 0.000001022 in Epoch 1874
Epoch 1876
A best model at epoch 1876 has been saved with training error 0.000001003.
Epoch 1876, Loss: 0.000001586, Improvement: -0.000000005, Best Loss: 0.000001003 in Epoch 1876
Epoch 1877
A best model at epoch 1877 has been saved with training error 0.000000995.
Epoch 1877, Loss: 0.000001551, Improvement: -0.000000035, Best Loss: 0.000000995 in Epoch 1877
Epoch 1878
Epoch 1878, Loss: 0.000001453, Improvement: -0.000000098, Best Loss: 0.000000995 in Epoch 1877
Epoch 1879
Epoch 1879, Loss: 0.000001490, Improvement: 0.000000037, Best Loss: 0.000000995 in Epoch 1877
Epoch 1880
Epoch 1880, Loss: 0.000001481, Improvement: -0.000000009, Best Loss: 0.000000995 in Epoch 1877
Epoch 1881
Epoch 1881, Loss: 0.000001378, Improvement: -0.000000103, Best Loss: 0.000000995 in Epoch 1877
Epoch 1882
Epoch 1882, Loss: 0.000001639, Improvement: 0.000000261, Best Loss: 0.000000995 in Epoch 1877
Epoch 1883
Epoch 1883, Loss: 0.000001745, Improvement: 0.000000106, Best Loss: 0.000000995 in Epoch 1877
Epoch 1884
A best model at epoch 1884 has been saved with training error 0.000000985.
Epoch 1884, Loss: 0.000001901, Improvement: 0.000000156, Best Loss: 0.000000985 in Epoch 1884
Epoch 1885
Epoch 1885, Loss: 0.000003395, Improvement: 0.000001494, Best Loss: 0.000000985 in Epoch 1884
Epoch 1886
Epoch 1886, Loss: 0.000002714, Improvement: -0.000000681, Best Loss: 0.000000985 in Epoch 1884
Epoch 1887
Epoch 1887, Loss: 0.000003345, Improvement: 0.000000631, Best Loss: 0.000000985 in Epoch 1884
Epoch 1888
Epoch 1888, Loss: 0.000004697, Improvement: 0.000001352, Best Loss: 0.000000985 in Epoch 1884
Epoch 1889
Epoch 1889, Loss: 0.000003612, Improvement: -0.000001085, Best Loss: 0.000000985 in Epoch 1884
Epoch 1890
Epoch 1890, Loss: 0.000004012, Improvement: 0.000000400, Best Loss: 0.000000985 in Epoch 1884
Epoch 1891
Epoch 1891, Loss: 0.000006598, Improvement: 0.000002586, Best Loss: 0.000000985 in Epoch 1884
Epoch 1892
Epoch 1892, Loss: 0.000006451, Improvement: -0.000000147, Best Loss: 0.000000985 in Epoch 1884
Epoch 1893
Epoch 1893, Loss: 0.000006257, Improvement: -0.000000194, Best Loss: 0.000000985 in Epoch 1884
Epoch 1894
Epoch 1894, Loss: 0.000006781, Improvement: 0.000000525, Best Loss: 0.000000985 in Epoch 1884
Epoch 1895
Epoch 1895, Loss: 0.000014225, Improvement: 0.000007444, Best Loss: 0.000000985 in Epoch 1884
Epoch 1896
Epoch 1896, Loss: 0.000017726, Improvement: 0.000003500, Best Loss: 0.000000985 in Epoch 1884
Epoch 1897
Epoch 1897, Loss: 0.000035663, Improvement: 0.000017937, Best Loss: 0.000000985 in Epoch 1884
Epoch 1898
Epoch 1898, Loss: 0.000024788, Improvement: -0.000010874, Best Loss: 0.000000985 in Epoch 1884
Epoch 1899
Epoch 1899, Loss: 0.000011818, Improvement: -0.000012971, Best Loss: 0.000000985 in Epoch 1884
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.000006213, Improvement: -0.000005604, Best Loss: 0.000000985 in Epoch 1884
Epoch 1901
Epoch 1901, Loss: 0.000004758, Improvement: -0.000001455, Best Loss: 0.000000985 in Epoch 1884
Epoch 1902
Epoch 1902, Loss: 0.000003909, Improvement: -0.000000849, Best Loss: 0.000000985 in Epoch 1884
Epoch 1903
Epoch 1903, Loss: 0.000003842, Improvement: -0.000000066, Best Loss: 0.000000985 in Epoch 1884
Epoch 1904
Epoch 1904, Loss: 0.000003019, Improvement: -0.000000824, Best Loss: 0.000000985 in Epoch 1884
Epoch 1905
Epoch 1905, Loss: 0.000002961, Improvement: -0.000000057, Best Loss: 0.000000985 in Epoch 1884
Epoch 1906
Epoch 1906, Loss: 0.000003498, Improvement: 0.000000537, Best Loss: 0.000000985 in Epoch 1884
Epoch 1907
Epoch 1907, Loss: 0.000003079, Improvement: -0.000000419, Best Loss: 0.000000985 in Epoch 1884
Epoch 1908
Epoch 1908, Loss: 0.000002524, Improvement: -0.000000554, Best Loss: 0.000000985 in Epoch 1884
Epoch 1909
Epoch 1909, Loss: 0.000001931, Improvement: -0.000000593, Best Loss: 0.000000985 in Epoch 1884
Epoch 1910
Epoch 1910, Loss: 0.000001682, Improvement: -0.000000250, Best Loss: 0.000000985 in Epoch 1884
Epoch 1911
Epoch 1911, Loss: 0.000002149, Improvement: 0.000000468, Best Loss: 0.000000985 in Epoch 1884
Epoch 1912
Epoch 1912, Loss: 0.000004424, Improvement: 0.000002275, Best Loss: 0.000000985 in Epoch 1884
Epoch 1913
Epoch 1913, Loss: 0.000007515, Improvement: 0.000003091, Best Loss: 0.000000985 in Epoch 1884
Epoch 1914
Epoch 1914, Loss: 0.000024974, Improvement: 0.000017459, Best Loss: 0.000000985 in Epoch 1884
Epoch 1915
Epoch 1915, Loss: 0.000037238, Improvement: 0.000012264, Best Loss: 0.000000985 in Epoch 1884
Epoch 1916
Epoch 1916, Loss: 0.000020233, Improvement: -0.000017005, Best Loss: 0.000000985 in Epoch 1884
Epoch 1917
Epoch 1917, Loss: 0.000007365, Improvement: -0.000012868, Best Loss: 0.000000985 in Epoch 1884
Epoch 1918
Epoch 1918, Loss: 0.000004717, Improvement: -0.000002648, Best Loss: 0.000000985 in Epoch 1884
Epoch 1919
Epoch 1919, Loss: 0.000003476, Improvement: -0.000001241, Best Loss: 0.000000985 in Epoch 1884
Epoch 1920
Epoch 1920, Loss: 0.000002871, Improvement: -0.000000604, Best Loss: 0.000000985 in Epoch 1884
Epoch 1921
Epoch 1921, Loss: 0.000003329, Improvement: 0.000000458, Best Loss: 0.000000985 in Epoch 1884
Epoch 1922
Epoch 1922, Loss: 0.000002820, Improvement: -0.000000508, Best Loss: 0.000000985 in Epoch 1884
Epoch 1923
Epoch 1923, Loss: 0.000002193, Improvement: -0.000000627, Best Loss: 0.000000985 in Epoch 1884
Epoch 1924
A best model at epoch 1924 has been saved with training error 0.000000978.
Epoch 1924, Loss: 0.000001896, Improvement: -0.000000297, Best Loss: 0.000000978 in Epoch 1924
Epoch 1925
Epoch 1925, Loss: 0.000002332, Improvement: 0.000000436, Best Loss: 0.000000978 in Epoch 1924
Epoch 1926
Epoch 1926, Loss: 0.000009503, Improvement: 0.000007171, Best Loss: 0.000000978 in Epoch 1924
Epoch 1927
Epoch 1927, Loss: 0.000009742, Improvement: 0.000000238, Best Loss: 0.000000978 in Epoch 1924
Epoch 1928
Epoch 1928, Loss: 0.000008029, Improvement: -0.000001712, Best Loss: 0.000000978 in Epoch 1924
Epoch 1929
Epoch 1929, Loss: 0.000004264, Improvement: -0.000003765, Best Loss: 0.000000978 in Epoch 1924
Epoch 1930
Epoch 1930, Loss: 0.000003467, Improvement: -0.000000797, Best Loss: 0.000000978 in Epoch 1924
Epoch 1931
Epoch 1931, Loss: 0.000002353, Improvement: -0.000001114, Best Loss: 0.000000978 in Epoch 1924
Epoch 1932
Epoch 1932, Loss: 0.000002248, Improvement: -0.000000106, Best Loss: 0.000000978 in Epoch 1924
Epoch 1933
Epoch 1933, Loss: 0.000006502, Improvement: 0.000004255, Best Loss: 0.000000978 in Epoch 1924
Epoch 1934
Epoch 1934, Loss: 0.000013186, Improvement: 0.000006684, Best Loss: 0.000000978 in Epoch 1924
Epoch 1935
Epoch 1935, Loss: 0.000012260, Improvement: -0.000000925, Best Loss: 0.000000978 in Epoch 1924
Epoch 1936
Epoch 1936, Loss: 0.000008750, Improvement: -0.000003510, Best Loss: 0.000000978 in Epoch 1924
Epoch 1937
Epoch 1937, Loss: 0.000005906, Improvement: -0.000002844, Best Loss: 0.000000978 in Epoch 1924
Epoch 1938
Epoch 1938, Loss: 0.000005482, Improvement: -0.000000424, Best Loss: 0.000000978 in Epoch 1924
Epoch 1939
Epoch 1939, Loss: 0.000003304, Improvement: -0.000002177, Best Loss: 0.000000978 in Epoch 1924
Epoch 1940
Epoch 1940, Loss: 0.000003435, Improvement: 0.000000130, Best Loss: 0.000000978 in Epoch 1924
Epoch 1941
Epoch 1941, Loss: 0.000014000, Improvement: 0.000010566, Best Loss: 0.000000978 in Epoch 1924
Epoch 1942
Epoch 1942, Loss: 0.000010006, Improvement: -0.000003995, Best Loss: 0.000000978 in Epoch 1924
Epoch 1943
Epoch 1943, Loss: 0.000005673, Improvement: -0.000004333, Best Loss: 0.000000978 in Epoch 1924
Epoch 1944
Epoch 1944, Loss: 0.000005739, Improvement: 0.000000066, Best Loss: 0.000000978 in Epoch 1924
Epoch 1945
Epoch 1945, Loss: 0.000003812, Improvement: -0.000001927, Best Loss: 0.000000978 in Epoch 1924
Epoch 1946
Epoch 1946, Loss: 0.000002581, Improvement: -0.000001231, Best Loss: 0.000000978 in Epoch 1924
Epoch 1947
Epoch 1947, Loss: 0.000002744, Improvement: 0.000000163, Best Loss: 0.000000978 in Epoch 1924
Epoch 1948
Epoch 1948, Loss: 0.000003363, Improvement: 0.000000619, Best Loss: 0.000000978 in Epoch 1924
Epoch 1949
Epoch 1949, Loss: 0.000002562, Improvement: -0.000000801, Best Loss: 0.000000978 in Epoch 1924
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.000002781, Improvement: 0.000000219, Best Loss: 0.000000978 in Epoch 1924
Epoch 1951
Epoch 1951, Loss: 0.000004481, Improvement: 0.000001700, Best Loss: 0.000000978 in Epoch 1924
Epoch 1952
Epoch 1952, Loss: 0.000009723, Improvement: 0.000005242, Best Loss: 0.000000978 in Epoch 1924
Epoch 1953
Epoch 1953, Loss: 0.000020385, Improvement: 0.000010662, Best Loss: 0.000000978 in Epoch 1924
Epoch 1954
Epoch 1954, Loss: 0.000032537, Improvement: 0.000012152, Best Loss: 0.000000978 in Epoch 1924
Epoch 1955
Epoch 1955, Loss: 0.000054830, Improvement: 0.000022293, Best Loss: 0.000000978 in Epoch 1924
Epoch 1956
Epoch 1956, Loss: 0.000032489, Improvement: -0.000022340, Best Loss: 0.000000978 in Epoch 1924
Epoch 1957
Epoch 1957, Loss: 0.000010924, Improvement: -0.000021565, Best Loss: 0.000000978 in Epoch 1924
Epoch 1958
Epoch 1958, Loss: 0.000004904, Improvement: -0.000006020, Best Loss: 0.000000978 in Epoch 1924
Epoch 1959
Epoch 1959, Loss: 0.000003239, Improvement: -0.000001664, Best Loss: 0.000000978 in Epoch 1924
Epoch 1960
Epoch 1960, Loss: 0.000002090, Improvement: -0.000001149, Best Loss: 0.000000978 in Epoch 1924
Epoch 1961
Epoch 1961, Loss: 0.000002362, Improvement: 0.000000271, Best Loss: 0.000000978 in Epoch 1924
Epoch 1962
Epoch 1962, Loss: 0.000001993, Improvement: -0.000000368, Best Loss: 0.000000978 in Epoch 1924
Epoch 1963
Epoch 1963, Loss: 0.000001673, Improvement: -0.000000320, Best Loss: 0.000000978 in Epoch 1924
Epoch 1964
Epoch 1964, Loss: 0.000001516, Improvement: -0.000000157, Best Loss: 0.000000978 in Epoch 1924
Epoch 1965
A best model at epoch 1965 has been saved with training error 0.000000976.
A best model at epoch 1965 has been saved with training error 0.000000938.
Epoch 1965, Loss: 0.000001412, Improvement: -0.000000104, Best Loss: 0.000000938 in Epoch 1965
Epoch 1966
Epoch 1966, Loss: 0.000001318, Improvement: -0.000000094, Best Loss: 0.000000938 in Epoch 1965
Epoch 1967
A best model at epoch 1967 has been saved with training error 0.000000908.
Epoch 1967, Loss: 0.000001384, Improvement: 0.000000066, Best Loss: 0.000000908 in Epoch 1967
Epoch 1968
Epoch 1968, Loss: 0.000001306, Improvement: -0.000000078, Best Loss: 0.000000908 in Epoch 1967
Epoch 1969
A best model at epoch 1969 has been saved with training error 0.000000841.
Epoch 1969, Loss: 0.000001273, Improvement: -0.000000032, Best Loss: 0.000000841 in Epoch 1969
Epoch 1970
Epoch 1970, Loss: 0.000001306, Improvement: 0.000000033, Best Loss: 0.000000841 in Epoch 1969
Epoch 1971
Epoch 1971, Loss: 0.000001417, Improvement: 0.000000111, Best Loss: 0.000000841 in Epoch 1969
Epoch 1972
Epoch 1972, Loss: 0.000001476, Improvement: 0.000000059, Best Loss: 0.000000841 in Epoch 1969
Epoch 1973
Epoch 1973, Loss: 0.000001351, Improvement: -0.000000126, Best Loss: 0.000000841 in Epoch 1969
Epoch 1974
Epoch 1974, Loss: 0.000001779, Improvement: 0.000000429, Best Loss: 0.000000841 in Epoch 1969
Epoch 1975
Epoch 1975, Loss: 0.000002000, Improvement: 0.000000220, Best Loss: 0.000000841 in Epoch 1969
Epoch 1976
Epoch 1976, Loss: 0.000002473, Improvement: 0.000000473, Best Loss: 0.000000841 in Epoch 1969
Epoch 1977
Epoch 1977, Loss: 0.000004652, Improvement: 0.000002179, Best Loss: 0.000000841 in Epoch 1969
Epoch 1978
Epoch 1978, Loss: 0.000004233, Improvement: -0.000000420, Best Loss: 0.000000841 in Epoch 1969
Epoch 1979
Epoch 1979, Loss: 0.000002927, Improvement: -0.000001306, Best Loss: 0.000000841 in Epoch 1969
Epoch 1980
Epoch 1980, Loss: 0.000002081, Improvement: -0.000000846, Best Loss: 0.000000841 in Epoch 1969
Epoch 1981
Epoch 1981, Loss: 0.000002274, Improvement: 0.000000193, Best Loss: 0.000000841 in Epoch 1969
Epoch 1982
Epoch 1982, Loss: 0.000002651, Improvement: 0.000000377, Best Loss: 0.000000841 in Epoch 1969
Epoch 1983
Epoch 1983, Loss: 0.000004183, Improvement: 0.000001532, Best Loss: 0.000000841 in Epoch 1969
Epoch 1984
Epoch 1984, Loss: 0.000010065, Improvement: 0.000005882, Best Loss: 0.000000841 in Epoch 1969
Epoch 1985
Epoch 1985, Loss: 0.000005763, Improvement: -0.000004303, Best Loss: 0.000000841 in Epoch 1969
Epoch 1986
Epoch 1986, Loss: 0.000010102, Improvement: 0.000004339, Best Loss: 0.000000841 in Epoch 1969
Epoch 1987
Epoch 1987, Loss: 0.000010568, Improvement: 0.000000466, Best Loss: 0.000000841 in Epoch 1969
Epoch 1988
Epoch 1988, Loss: 0.000007493, Improvement: -0.000003075, Best Loss: 0.000000841 in Epoch 1969
Epoch 1989
Epoch 1989, Loss: 0.000005450, Improvement: -0.000002042, Best Loss: 0.000000841 in Epoch 1969
Epoch 1990
Epoch 1990, Loss: 0.000006498, Improvement: 0.000001048, Best Loss: 0.000000841 in Epoch 1969
Epoch 1991
Epoch 1991, Loss: 0.000003313, Improvement: -0.000003185, Best Loss: 0.000000841 in Epoch 1969
Epoch 1992
Epoch 1992, Loss: 0.000003580, Improvement: 0.000000267, Best Loss: 0.000000841 in Epoch 1969
Epoch 1993
Epoch 1993, Loss: 0.000004849, Improvement: 0.000001269, Best Loss: 0.000000841 in Epoch 1969
Epoch 1994
Epoch 1994, Loss: 0.000011577, Improvement: 0.000006728, Best Loss: 0.000000841 in Epoch 1969
Epoch 1995
Epoch 1995, Loss: 0.000028727, Improvement: 0.000017151, Best Loss: 0.000000841 in Epoch 1969
Epoch 1996
Epoch 1996, Loss: 0.000020054, Improvement: -0.000008673, Best Loss: 0.000000841 in Epoch 1969
Epoch 1997
Epoch 1997, Loss: 0.000013145, Improvement: -0.000006909, Best Loss: 0.000000841 in Epoch 1969
Epoch 1998
Epoch 1998, Loss: 0.000013111, Improvement: -0.000000034, Best Loss: 0.000000841 in Epoch 1969
Epoch 1999
Epoch 1999, Loss: 0.000005595, Improvement: -0.000007516, Best Loss: 0.000000841 in Epoch 1969
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.000003224, Improvement: -0.000002370, Best Loss: 0.000000841 in Epoch 1969
Epoch 2001
Epoch 2001, Loss: 0.000002501, Improvement: -0.000000723, Best Loss: 0.000000841 in Epoch 1969
Epoch 2002
Epoch 2002, Loss: 0.000002356, Improvement: -0.000000145, Best Loss: 0.000000841 in Epoch 1969
Epoch 2003
Epoch 2003, Loss: 0.000002441, Improvement: 0.000000084, Best Loss: 0.000000841 in Epoch 1969
Epoch 2004
Epoch 2004, Loss: 0.000001865, Improvement: -0.000000576, Best Loss: 0.000000841 in Epoch 1969
Epoch 2005
Epoch 2005, Loss: 0.000001753, Improvement: -0.000000112, Best Loss: 0.000000841 in Epoch 1969
Epoch 2006
Epoch 2006, Loss: 0.000002234, Improvement: 0.000000481, Best Loss: 0.000000841 in Epoch 1969
Epoch 2007
Epoch 2007, Loss: 0.000004862, Improvement: 0.000002628, Best Loss: 0.000000841 in Epoch 1969
Epoch 2008
Epoch 2008, Loss: 0.000006517, Improvement: 0.000001655, Best Loss: 0.000000841 in Epoch 1969
Epoch 2009
Epoch 2009, Loss: 0.000005526, Improvement: -0.000000991, Best Loss: 0.000000841 in Epoch 1969
Epoch 2010
Epoch 2010, Loss: 0.000005280, Improvement: -0.000000245, Best Loss: 0.000000841 in Epoch 1969
Epoch 2011
Epoch 2011, Loss: 0.000005176, Improvement: -0.000000105, Best Loss: 0.000000841 in Epoch 1969
Epoch 2012
Epoch 2012, Loss: 0.000004418, Improvement: -0.000000757, Best Loss: 0.000000841 in Epoch 1969
Epoch 2013
Epoch 2013, Loss: 0.000006309, Improvement: 0.000001891, Best Loss: 0.000000841 in Epoch 1969
Epoch 2014
Epoch 2014, Loss: 0.000007489, Improvement: 0.000001181, Best Loss: 0.000000841 in Epoch 1969
Epoch 2015
Epoch 2015, Loss: 0.000005411, Improvement: -0.000002078, Best Loss: 0.000000841 in Epoch 1969
Epoch 2016
Epoch 2016, Loss: 0.000002960, Improvement: -0.000002451, Best Loss: 0.000000841 in Epoch 1969
Epoch 2017
Epoch 2017, Loss: 0.000003259, Improvement: 0.000000298, Best Loss: 0.000000841 in Epoch 1969
Epoch 2018
Epoch 2018, Loss: 0.000008647, Improvement: 0.000005388, Best Loss: 0.000000841 in Epoch 1969
Epoch 2019
Epoch 2019, Loss: 0.000009644, Improvement: 0.000000997, Best Loss: 0.000000841 in Epoch 1969
Epoch 2020
Epoch 2020, Loss: 0.000013356, Improvement: 0.000003712, Best Loss: 0.000000841 in Epoch 1969
Epoch 2021
Epoch 2021, Loss: 0.000015373, Improvement: 0.000002017, Best Loss: 0.000000841 in Epoch 1969
Epoch 2022
Epoch 2022, Loss: 0.000010595, Improvement: -0.000004779, Best Loss: 0.000000841 in Epoch 1969
Epoch 2023
Epoch 2023, Loss: 0.000009219, Improvement: -0.000001376, Best Loss: 0.000000841 in Epoch 1969
Epoch 2024
Epoch 2024, Loss: 0.000013420, Improvement: 0.000004201, Best Loss: 0.000000841 in Epoch 1969
Epoch 2025
Epoch 2025, Loss: 0.000011084, Improvement: -0.000002336, Best Loss: 0.000000841 in Epoch 1969
Epoch 2026
Epoch 2026, Loss: 0.000020937, Improvement: 0.000009853, Best Loss: 0.000000841 in Epoch 1969
Epoch 2027
Epoch 2027, Loss: 0.000024282, Improvement: 0.000003345, Best Loss: 0.000000841 in Epoch 1969
Epoch 2028
Epoch 2028, Loss: 0.000018554, Improvement: -0.000005728, Best Loss: 0.000000841 in Epoch 1969
Epoch 2029
Epoch 2029, Loss: 0.000016599, Improvement: -0.000001955, Best Loss: 0.000000841 in Epoch 1969
Epoch 2030
Epoch 2030, Loss: 0.000015766, Improvement: -0.000000833, Best Loss: 0.000000841 in Epoch 1969
Epoch 2031
Epoch 2031, Loss: 0.000014441, Improvement: -0.000001325, Best Loss: 0.000000841 in Epoch 1969
Epoch 2032
Epoch 2032, Loss: 0.000014603, Improvement: 0.000000162, Best Loss: 0.000000841 in Epoch 1969
Epoch 2033
Epoch 2033, Loss: 0.000008564, Improvement: -0.000006039, Best Loss: 0.000000841 in Epoch 1969
Epoch 2034
Epoch 2034, Loss: 0.000005805, Improvement: -0.000002759, Best Loss: 0.000000841 in Epoch 1969
Epoch 2035
Epoch 2035, Loss: 0.000011660, Improvement: 0.000005855, Best Loss: 0.000000841 in Epoch 1969
Epoch 2036
Epoch 2036, Loss: 0.000020211, Improvement: 0.000008551, Best Loss: 0.000000841 in Epoch 1969
Epoch 2037
Epoch 2037, Loss: 0.000013812, Improvement: -0.000006399, Best Loss: 0.000000841 in Epoch 1969
Epoch 2038
Epoch 2038, Loss: 0.000022980, Improvement: 0.000009168, Best Loss: 0.000000841 in Epoch 1969
Epoch 2039
Epoch 2039, Loss: 0.000015515, Improvement: -0.000007465, Best Loss: 0.000000841 in Epoch 1969
Epoch 2040
Epoch 2040, Loss: 0.000009376, Improvement: -0.000006139, Best Loss: 0.000000841 in Epoch 1969
Epoch 2041
Epoch 2041, Loss: 0.000006935, Improvement: -0.000002441, Best Loss: 0.000000841 in Epoch 1969
Epoch 2042
Epoch 2042, Loss: 0.000008942, Improvement: 0.000002007, Best Loss: 0.000000841 in Epoch 1969
Epoch 2043
Epoch 2043, Loss: 0.000008990, Improvement: 0.000000048, Best Loss: 0.000000841 in Epoch 1969
Epoch 2044
Epoch 2044, Loss: 0.000014275, Improvement: 0.000005285, Best Loss: 0.000000841 in Epoch 1969
Epoch 2045
Epoch 2045, Loss: 0.000016549, Improvement: 0.000002275, Best Loss: 0.000000841 in Epoch 1969
Epoch 2046
Epoch 2046, Loss: 0.000015628, Improvement: -0.000000921, Best Loss: 0.000000841 in Epoch 1969
Epoch 2047
Epoch 2047, Loss: 0.000018883, Improvement: 0.000003255, Best Loss: 0.000000841 in Epoch 1969
Epoch 2048
Epoch 2048, Loss: 0.000012384, Improvement: -0.000006499, Best Loss: 0.000000841 in Epoch 1969
Epoch 2049
Epoch 2049, Loss: 0.000004797, Improvement: -0.000007587, Best Loss: 0.000000841 in Epoch 1969
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.000007486, Improvement: 0.000002688, Best Loss: 0.000000841 in Epoch 1969
Epoch 2051
Epoch 2051, Loss: 0.000009243, Improvement: 0.000001757, Best Loss: 0.000000841 in Epoch 1969
Epoch 2052
Epoch 2052, Loss: 0.000008322, Improvement: -0.000000921, Best Loss: 0.000000841 in Epoch 1969
Epoch 2053
Epoch 2053, Loss: 0.000012079, Improvement: 0.000003757, Best Loss: 0.000000841 in Epoch 1969
Epoch 2054
Epoch 2054, Loss: 0.000020854, Improvement: 0.000008775, Best Loss: 0.000000841 in Epoch 1969
Epoch 2055
Epoch 2055, Loss: 0.000015752, Improvement: -0.000005103, Best Loss: 0.000000841 in Epoch 1969
Epoch 2056
Epoch 2056, Loss: 0.000009897, Improvement: -0.000005855, Best Loss: 0.000000841 in Epoch 1969
Epoch 2057
Epoch 2057, Loss: 0.000006621, Improvement: -0.000003276, Best Loss: 0.000000841 in Epoch 1969
Epoch 2058
Epoch 2058, Loss: 0.000003848, Improvement: -0.000002773, Best Loss: 0.000000841 in Epoch 1969
Epoch 2059
Epoch 2059, Loss: 0.000002759, Improvement: -0.000001090, Best Loss: 0.000000841 in Epoch 1969
Epoch 2060
Epoch 2060, Loss: 0.000001912, Improvement: -0.000000847, Best Loss: 0.000000841 in Epoch 1969
Epoch 2061
Epoch 2061, Loss: 0.000002002, Improvement: 0.000000090, Best Loss: 0.000000841 in Epoch 1969
Epoch 2062
Epoch 2062, Loss: 0.000002491, Improvement: 0.000000489, Best Loss: 0.000000841 in Epoch 1969
Epoch 2063
Epoch 2063, Loss: 0.000003191, Improvement: 0.000000700, Best Loss: 0.000000841 in Epoch 1969
Epoch 2064
Epoch 2064, Loss: 0.000003170, Improvement: -0.000000020, Best Loss: 0.000000841 in Epoch 1969
Epoch 2065
Epoch 2065, Loss: 0.000004734, Improvement: 0.000001564, Best Loss: 0.000000841 in Epoch 1969
Epoch 2066
Epoch 2066, Loss: 0.000009709, Improvement: 0.000004975, Best Loss: 0.000000841 in Epoch 1969
Epoch 2067
Epoch 2067, Loss: 0.000006045, Improvement: -0.000003664, Best Loss: 0.000000841 in Epoch 1969
Epoch 2068
Epoch 2068, Loss: 0.000005037, Improvement: -0.000001008, Best Loss: 0.000000841 in Epoch 1969
Epoch 2069
Epoch 2069, Loss: 0.000007781, Improvement: 0.000002744, Best Loss: 0.000000841 in Epoch 1969
Epoch 2070
Epoch 2070, Loss: 0.000013129, Improvement: 0.000005348, Best Loss: 0.000000841 in Epoch 1969
Epoch 2071
Epoch 2071, Loss: 0.000017396, Improvement: 0.000004267, Best Loss: 0.000000841 in Epoch 1969
Epoch 2072
Epoch 2072, Loss: 0.000037713, Improvement: 0.000020317, Best Loss: 0.000000841 in Epoch 1969
Epoch 2073
Epoch 2073, Loss: 0.000025374, Improvement: -0.000012339, Best Loss: 0.000000841 in Epoch 1969
Epoch 2074
Epoch 2074, Loss: 0.000007875, Improvement: -0.000017499, Best Loss: 0.000000841 in Epoch 1969
Epoch 2075
Epoch 2075, Loss: 0.000004905, Improvement: -0.000002970, Best Loss: 0.000000841 in Epoch 1969
Epoch 2076
Epoch 2076, Loss: 0.000003437, Improvement: -0.000001468, Best Loss: 0.000000841 in Epoch 1969
Epoch 2077
Epoch 2077, Loss: 0.000002818, Improvement: -0.000000618, Best Loss: 0.000000841 in Epoch 1969
Epoch 2078
Epoch 2078, Loss: 0.000001946, Improvement: -0.000000872, Best Loss: 0.000000841 in Epoch 1969
Epoch 2079
Epoch 2079, Loss: 0.000001933, Improvement: -0.000000013, Best Loss: 0.000000841 in Epoch 1969
Epoch 2080
Epoch 2080, Loss: 0.000001596, Improvement: -0.000000338, Best Loss: 0.000000841 in Epoch 1969
Epoch 2081
Epoch 2081, Loss: 0.000001754, Improvement: 0.000000158, Best Loss: 0.000000841 in Epoch 1969
Epoch 2082
Epoch 2082, Loss: 0.000001773, Improvement: 0.000000019, Best Loss: 0.000000841 in Epoch 1969
Epoch 2083
Epoch 2083, Loss: 0.000005493, Improvement: 0.000003720, Best Loss: 0.000000841 in Epoch 1969
Epoch 2084
Epoch 2084, Loss: 0.000008685, Improvement: 0.000003193, Best Loss: 0.000000841 in Epoch 1969
Epoch 2085
Epoch 2085, Loss: 0.000013078, Improvement: 0.000004393, Best Loss: 0.000000841 in Epoch 1969
Epoch 2086
Epoch 2086, Loss: 0.000008996, Improvement: -0.000004082, Best Loss: 0.000000841 in Epoch 1969
Epoch 2087
Epoch 2087, Loss: 0.000008066, Improvement: -0.000000930, Best Loss: 0.000000841 in Epoch 1969
Epoch 2088
Epoch 2088, Loss: 0.000006560, Improvement: -0.000001505, Best Loss: 0.000000841 in Epoch 1969
Epoch 2089
Epoch 2089, Loss: 0.000005140, Improvement: -0.000001420, Best Loss: 0.000000841 in Epoch 1969
Epoch 2090
Epoch 2090, Loss: 0.000003176, Improvement: -0.000001964, Best Loss: 0.000000841 in Epoch 1969
Epoch 2091
Epoch 2091, Loss: 0.000002981, Improvement: -0.000000196, Best Loss: 0.000000841 in Epoch 1969
Epoch 2092
Epoch 2092, Loss: 0.000004384, Improvement: 0.000001403, Best Loss: 0.000000841 in Epoch 1969
Epoch 2093
Epoch 2093, Loss: 0.000005223, Improvement: 0.000000840, Best Loss: 0.000000841 in Epoch 1969
Epoch 2094
Epoch 2094, Loss: 0.000013038, Improvement: 0.000007815, Best Loss: 0.000000841 in Epoch 1969
Epoch 2095
Epoch 2095, Loss: 0.000012780, Improvement: -0.000000258, Best Loss: 0.000000841 in Epoch 1969
Epoch 2096
Epoch 2096, Loss: 0.000040870, Improvement: 0.000028090, Best Loss: 0.000000841 in Epoch 1969
Epoch 2097
Epoch 2097, Loss: 0.000025193, Improvement: -0.000015678, Best Loss: 0.000000841 in Epoch 1969
Epoch 2098
Epoch 2098, Loss: 0.000017034, Improvement: -0.000008159, Best Loss: 0.000000841 in Epoch 1969
Epoch 2099
Epoch 2099, Loss: 0.000007622, Improvement: -0.000009412, Best Loss: 0.000000841 in Epoch 1969
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.000006499, Improvement: -0.000001123, Best Loss: 0.000000841 in Epoch 1969
Epoch 2101
Epoch 2101, Loss: 0.000005143, Improvement: -0.000001356, Best Loss: 0.000000841 in Epoch 1969
Epoch 2102
Epoch 2102, Loss: 0.000005867, Improvement: 0.000000724, Best Loss: 0.000000841 in Epoch 1969
Epoch 2103
Epoch 2103, Loss: 0.000003936, Improvement: -0.000001931, Best Loss: 0.000000841 in Epoch 1969
Epoch 2104
Epoch 2104, Loss: 0.000003132, Improvement: -0.000000804, Best Loss: 0.000000841 in Epoch 1969
Epoch 2105
Epoch 2105, Loss: 0.000001881, Improvement: -0.000001251, Best Loss: 0.000000841 in Epoch 1969
Epoch 2106
Epoch 2106, Loss: 0.000001756, Improvement: -0.000000125, Best Loss: 0.000000841 in Epoch 1969
Epoch 2107
Epoch 2107, Loss: 0.000001516, Improvement: -0.000000240, Best Loss: 0.000000841 in Epoch 1969
Epoch 2108
Epoch 2108, Loss: 0.000001660, Improvement: 0.000000144, Best Loss: 0.000000841 in Epoch 1969
Epoch 2109
Epoch 2109, Loss: 0.000001651, Improvement: -0.000000009, Best Loss: 0.000000841 in Epoch 1969
Epoch 2110
Epoch 2110, Loss: 0.000001497, Improvement: -0.000000154, Best Loss: 0.000000841 in Epoch 1969
Epoch 2111
Epoch 2111, Loss: 0.000001299, Improvement: -0.000000198, Best Loss: 0.000000841 in Epoch 1969
Epoch 2112
Epoch 2112, Loss: 0.000001311, Improvement: 0.000000012, Best Loss: 0.000000841 in Epoch 1969
Epoch 2113
Epoch 2113, Loss: 0.000001253, Improvement: -0.000000057, Best Loss: 0.000000841 in Epoch 1969
Epoch 2114
A best model at epoch 2114 has been saved with training error 0.000000822.
Epoch 2114, Loss: 0.000001184, Improvement: -0.000000069, Best Loss: 0.000000822 in Epoch 2114
Epoch 2115
Epoch 2115, Loss: 0.000001118, Improvement: -0.000000066, Best Loss: 0.000000822 in Epoch 2114
Epoch 2116
Epoch 2116, Loss: 0.000001146, Improvement: 0.000000029, Best Loss: 0.000000822 in Epoch 2114
Epoch 2117
Epoch 2117, Loss: 0.000001188, Improvement: 0.000000042, Best Loss: 0.000000822 in Epoch 2114
Epoch 2118
Epoch 2118, Loss: 0.000002035, Improvement: 0.000000847, Best Loss: 0.000000822 in Epoch 2114
Epoch 2119
Epoch 2119, Loss: 0.000002617, Improvement: 0.000000582, Best Loss: 0.000000822 in Epoch 2114
Epoch 2120
Epoch 2120, Loss: 0.000004515, Improvement: 0.000001897, Best Loss: 0.000000822 in Epoch 2114
Epoch 2121
Epoch 2121, Loss: 0.000004091, Improvement: -0.000000423, Best Loss: 0.000000822 in Epoch 2114
Epoch 2122
Epoch 2122, Loss: 0.000005208, Improvement: 0.000001116, Best Loss: 0.000000822 in Epoch 2114
Epoch 2123
Epoch 2123, Loss: 0.000003177, Improvement: -0.000002030, Best Loss: 0.000000822 in Epoch 2114
Epoch 2124
Epoch 2124, Loss: 0.000002479, Improvement: -0.000000699, Best Loss: 0.000000822 in Epoch 2114
Epoch 2125
Epoch 2125, Loss: 0.000004328, Improvement: 0.000001849, Best Loss: 0.000000822 in Epoch 2114
Epoch 2126
Epoch 2126, Loss: 0.000005895, Improvement: 0.000001567, Best Loss: 0.000000822 in Epoch 2114
Epoch 2127
Epoch 2127, Loss: 0.000004064, Improvement: -0.000001832, Best Loss: 0.000000822 in Epoch 2114
Epoch 2128
Epoch 2128, Loss: 0.000011398, Improvement: 0.000007334, Best Loss: 0.000000822 in Epoch 2114
Epoch 2129
Epoch 2129, Loss: 0.000015606, Improvement: 0.000004209, Best Loss: 0.000000822 in Epoch 2114
Epoch 2130
Epoch 2130, Loss: 0.000025587, Improvement: 0.000009980, Best Loss: 0.000000822 in Epoch 2114
Epoch 2131
Epoch 2131, Loss: 0.000029762, Improvement: 0.000004176, Best Loss: 0.000000822 in Epoch 2114
Epoch 2132
Epoch 2132, Loss: 0.000013251, Improvement: -0.000016512, Best Loss: 0.000000822 in Epoch 2114
Epoch 2133
Epoch 2133, Loss: 0.000005617, Improvement: -0.000007634, Best Loss: 0.000000822 in Epoch 2114
Epoch 2134
Epoch 2134, Loss: 0.000004393, Improvement: -0.000001223, Best Loss: 0.000000822 in Epoch 2114
Epoch 2135
Epoch 2135, Loss: 0.000004060, Improvement: -0.000000334, Best Loss: 0.000000822 in Epoch 2114
Epoch 2136
Epoch 2136, Loss: 0.000010655, Improvement: 0.000006595, Best Loss: 0.000000822 in Epoch 2114
Epoch 2137
Epoch 2137, Loss: 0.000008978, Improvement: -0.000001677, Best Loss: 0.000000822 in Epoch 2114
Epoch 2138
Epoch 2138, Loss: 0.000007091, Improvement: -0.000001887, Best Loss: 0.000000822 in Epoch 2114
Epoch 2139
Epoch 2139, Loss: 0.000007747, Improvement: 0.000000655, Best Loss: 0.000000822 in Epoch 2114
Epoch 2140
Epoch 2140, Loss: 0.000005739, Improvement: -0.000002008, Best Loss: 0.000000822 in Epoch 2114
Epoch 2141
Epoch 2141, Loss: 0.000008433, Improvement: 0.000002694, Best Loss: 0.000000822 in Epoch 2114
Epoch 2142
Epoch 2142, Loss: 0.000011058, Improvement: 0.000002625, Best Loss: 0.000000822 in Epoch 2114
Epoch 2143
Epoch 2143, Loss: 0.000020210, Improvement: 0.000009152, Best Loss: 0.000000822 in Epoch 2114
Epoch 2144
Epoch 2144, Loss: 0.000034244, Improvement: 0.000014034, Best Loss: 0.000000822 in Epoch 2114
Epoch 2145
Epoch 2145, Loss: 0.000019199, Improvement: -0.000015044, Best Loss: 0.000000822 in Epoch 2114
Epoch 2146
Epoch 2146, Loss: 0.000023874, Improvement: 0.000004675, Best Loss: 0.000000822 in Epoch 2114
Epoch 2147
Epoch 2147, Loss: 0.000012294, Improvement: -0.000011580, Best Loss: 0.000000822 in Epoch 2114
Epoch 2148
Epoch 2148, Loss: 0.000004677, Improvement: -0.000007617, Best Loss: 0.000000822 in Epoch 2114
Epoch 2149
Epoch 2149, Loss: 0.000002070, Improvement: -0.000002607, Best Loss: 0.000000822 in Epoch 2114
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.000001469, Improvement: -0.000000601, Best Loss: 0.000000822 in Epoch 2114
Epoch 2151
Epoch 2151, Loss: 0.000001343, Improvement: -0.000000125, Best Loss: 0.000000822 in Epoch 2114
Epoch 2152
Epoch 2152, Loss: 0.000001378, Improvement: 0.000000035, Best Loss: 0.000000822 in Epoch 2114
Epoch 2153
Epoch 2153, Loss: 0.000001873, Improvement: 0.000000494, Best Loss: 0.000000822 in Epoch 2114
Epoch 2154
Epoch 2154, Loss: 0.000001611, Improvement: -0.000000261, Best Loss: 0.000000822 in Epoch 2114
Epoch 2155
Epoch 2155, Loss: 0.000001428, Improvement: -0.000000183, Best Loss: 0.000000822 in Epoch 2114
Epoch 2156
Epoch 2156, Loss: 0.000001185, Improvement: -0.000000243, Best Loss: 0.000000822 in Epoch 2114
Epoch 2157
Epoch 2157, Loss: 0.000001182, Improvement: -0.000000003, Best Loss: 0.000000822 in Epoch 2114
Epoch 2158
Epoch 2158, Loss: 0.000001276, Improvement: 0.000000095, Best Loss: 0.000000822 in Epoch 2114
Epoch 2159
Epoch 2159, Loss: 0.000001453, Improvement: 0.000000176, Best Loss: 0.000000822 in Epoch 2114
Epoch 2160
Epoch 2160, Loss: 0.000001818, Improvement: 0.000000365, Best Loss: 0.000000822 in Epoch 2114
Epoch 2161
Epoch 2161, Loss: 0.000003865, Improvement: 0.000002047, Best Loss: 0.000000822 in Epoch 2114
Epoch 2162
Epoch 2162, Loss: 0.000004282, Improvement: 0.000000417, Best Loss: 0.000000822 in Epoch 2114
Epoch 2163
Epoch 2163, Loss: 0.000005439, Improvement: 0.000001157, Best Loss: 0.000000822 in Epoch 2114
Epoch 2164
Epoch 2164, Loss: 0.000004506, Improvement: -0.000000933, Best Loss: 0.000000822 in Epoch 2114
Epoch 2165
Epoch 2165, Loss: 0.000004129, Improvement: -0.000000378, Best Loss: 0.000000822 in Epoch 2114
Epoch 2166
Epoch 2166, Loss: 0.000004869, Improvement: 0.000000740, Best Loss: 0.000000822 in Epoch 2114
Epoch 2167
Epoch 2167, Loss: 0.000005719, Improvement: 0.000000849, Best Loss: 0.000000822 in Epoch 2114
Epoch 2168
Epoch 2168, Loss: 0.000005611, Improvement: -0.000000108, Best Loss: 0.000000822 in Epoch 2114
Epoch 2169
Epoch 2169, Loss: 0.000012798, Improvement: 0.000007187, Best Loss: 0.000000822 in Epoch 2114
Epoch 2170
Epoch 2170, Loss: 0.000025160, Improvement: 0.000012362, Best Loss: 0.000000822 in Epoch 2114
Epoch 2171
Epoch 2171, Loss: 0.000018258, Improvement: -0.000006902, Best Loss: 0.000000822 in Epoch 2114
Epoch 2172
Epoch 2172, Loss: 0.000020501, Improvement: 0.000002243, Best Loss: 0.000000822 in Epoch 2114
Epoch 2173
Epoch 2173, Loss: 0.000008414, Improvement: -0.000012087, Best Loss: 0.000000822 in Epoch 2114
Epoch 2174
Epoch 2174, Loss: 0.000004659, Improvement: -0.000003755, Best Loss: 0.000000822 in Epoch 2114
Epoch 2175
Epoch 2175, Loss: 0.000007415, Improvement: 0.000002756, Best Loss: 0.000000822 in Epoch 2114
Epoch 2176
Epoch 2176, Loss: 0.000008419, Improvement: 0.000001005, Best Loss: 0.000000822 in Epoch 2114
Epoch 2177
Epoch 2177, Loss: 0.000007416, Improvement: -0.000001004, Best Loss: 0.000000822 in Epoch 2114
Epoch 2178
Epoch 2178, Loss: 0.000016801, Improvement: 0.000009386, Best Loss: 0.000000822 in Epoch 2114
Epoch 2179
Epoch 2179, Loss: 0.000044074, Improvement: 0.000027273, Best Loss: 0.000000822 in Epoch 2114
Epoch 2180
Epoch 2180, Loss: 0.000021385, Improvement: -0.000022689, Best Loss: 0.000000822 in Epoch 2114
Epoch 2181
Epoch 2181, Loss: 0.000013380, Improvement: -0.000008005, Best Loss: 0.000000822 in Epoch 2114
Epoch 2182
Epoch 2182, Loss: 0.000007205, Improvement: -0.000006176, Best Loss: 0.000000822 in Epoch 2114
Epoch 2183
Epoch 2183, Loss: 0.000003639, Improvement: -0.000003565, Best Loss: 0.000000822 in Epoch 2114
Epoch 2184
Epoch 2184, Loss: 0.000002046, Improvement: -0.000001593, Best Loss: 0.000000822 in Epoch 2114
Epoch 2185
Epoch 2185, Loss: 0.000001489, Improvement: -0.000000557, Best Loss: 0.000000822 in Epoch 2114
Epoch 2186
Epoch 2186, Loss: 0.000001605, Improvement: 0.000000117, Best Loss: 0.000000822 in Epoch 2114
Epoch 2187
Epoch 2187, Loss: 0.000001588, Improvement: -0.000000017, Best Loss: 0.000000822 in Epoch 2114
Epoch 2188
Epoch 2188, Loss: 0.000001348, Improvement: -0.000000240, Best Loss: 0.000000822 in Epoch 2114
Epoch 2189
Epoch 2189, Loss: 0.000001222, Improvement: -0.000000126, Best Loss: 0.000000822 in Epoch 2114
Epoch 2190
A best model at epoch 2190 has been saved with training error 0.000000819.
Epoch 2190, Loss: 0.000001169, Improvement: -0.000000052, Best Loss: 0.000000819 in Epoch 2190
Epoch 2191
Epoch 2191, Loss: 0.000001308, Improvement: 0.000000139, Best Loss: 0.000000819 in Epoch 2190
Epoch 2192
Epoch 2192, Loss: 0.000001828, Improvement: 0.000000520, Best Loss: 0.000000819 in Epoch 2190
Epoch 2193
Epoch 2193, Loss: 0.000003129, Improvement: 0.000001301, Best Loss: 0.000000819 in Epoch 2190
Epoch 2194
Epoch 2194, Loss: 0.000002363, Improvement: -0.000000766, Best Loss: 0.000000819 in Epoch 2190
Epoch 2195
Epoch 2195, Loss: 0.000001628, Improvement: -0.000000735, Best Loss: 0.000000819 in Epoch 2190
Epoch 2196
Epoch 2196, Loss: 0.000001626, Improvement: -0.000000002, Best Loss: 0.000000819 in Epoch 2190
Epoch 2197
Epoch 2197, Loss: 0.000001588, Improvement: -0.000000038, Best Loss: 0.000000819 in Epoch 2190
Epoch 2198
Epoch 2198, Loss: 0.000001645, Improvement: 0.000000057, Best Loss: 0.000000819 in Epoch 2190
Epoch 2199
Epoch 2199, Loss: 0.000001537, Improvement: -0.000000108, Best Loss: 0.000000819 in Epoch 2190
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.000001859, Improvement: 0.000000323, Best Loss: 0.000000819 in Epoch 2190
Epoch 2201
Epoch 2201, Loss: 0.000002371, Improvement: 0.000000512, Best Loss: 0.000000819 in Epoch 2190
Epoch 2202
Epoch 2202, Loss: 0.000004510, Improvement: 0.000002139, Best Loss: 0.000000819 in Epoch 2190
Epoch 2203
Epoch 2203, Loss: 0.000005644, Improvement: 0.000001134, Best Loss: 0.000000819 in Epoch 2190
Epoch 2204
Epoch 2204, Loss: 0.000007007, Improvement: 0.000001363, Best Loss: 0.000000819 in Epoch 2190
Epoch 2205
Epoch 2205, Loss: 0.000004448, Improvement: -0.000002560, Best Loss: 0.000000819 in Epoch 2190
Epoch 2206
Epoch 2206, Loss: 0.000003185, Improvement: -0.000001263, Best Loss: 0.000000819 in Epoch 2190
Epoch 2207
Epoch 2207, Loss: 0.000003514, Improvement: 0.000000329, Best Loss: 0.000000819 in Epoch 2190
Epoch 2208
Epoch 2208, Loss: 0.000004674, Improvement: 0.000001161, Best Loss: 0.000000819 in Epoch 2190
Epoch 2209
Epoch 2209, Loss: 0.000002963, Improvement: -0.000001712, Best Loss: 0.000000819 in Epoch 2190
Epoch 2210
Epoch 2210, Loss: 0.000003747, Improvement: 0.000000785, Best Loss: 0.000000819 in Epoch 2190
Epoch 2211
Epoch 2211, Loss: 0.000002231, Improvement: -0.000001516, Best Loss: 0.000000819 in Epoch 2190
Epoch 2212
Epoch 2212, Loss: 0.000002458, Improvement: 0.000000226, Best Loss: 0.000000819 in Epoch 2190
Epoch 2213
Epoch 2213, Loss: 0.000004263, Improvement: 0.000001806, Best Loss: 0.000000819 in Epoch 2190
Epoch 2214
Epoch 2214, Loss: 0.000015646, Improvement: 0.000011383, Best Loss: 0.000000819 in Epoch 2190
Epoch 2215
Epoch 2215, Loss: 0.000026558, Improvement: 0.000010912, Best Loss: 0.000000819 in Epoch 2190
Epoch 2216
Epoch 2216, Loss: 0.000015518, Improvement: -0.000011040, Best Loss: 0.000000819 in Epoch 2190
Epoch 2217
Epoch 2217, Loss: 0.000014468, Improvement: -0.000001050, Best Loss: 0.000000819 in Epoch 2190
Epoch 2218
Epoch 2218, Loss: 0.000016684, Improvement: 0.000002216, Best Loss: 0.000000819 in Epoch 2190
Epoch 2219
Epoch 2219, Loss: 0.000008831, Improvement: -0.000007853, Best Loss: 0.000000819 in Epoch 2190
Epoch 2220
Epoch 2220, Loss: 0.000006917, Improvement: -0.000001913, Best Loss: 0.000000819 in Epoch 2190
Epoch 2221
Epoch 2221, Loss: 0.000003719, Improvement: -0.000003199, Best Loss: 0.000000819 in Epoch 2190
Epoch 2222
Epoch 2222, Loss: 0.000003195, Improvement: -0.000000523, Best Loss: 0.000000819 in Epoch 2190
Epoch 2223
Epoch 2223, Loss: 0.000002555, Improvement: -0.000000641, Best Loss: 0.000000819 in Epoch 2190
Epoch 2224
Epoch 2224, Loss: 0.000002782, Improvement: 0.000000227, Best Loss: 0.000000819 in Epoch 2190
Epoch 2225
Epoch 2225, Loss: 0.000002329, Improvement: -0.000000453, Best Loss: 0.000000819 in Epoch 2190
Epoch 2226
Epoch 2226, Loss: 0.000002649, Improvement: 0.000000320, Best Loss: 0.000000819 in Epoch 2190
Epoch 2227
Epoch 2227, Loss: 0.000004714, Improvement: 0.000002065, Best Loss: 0.000000819 in Epoch 2190
Epoch 2228
Epoch 2228, Loss: 0.000003221, Improvement: -0.000001493, Best Loss: 0.000000819 in Epoch 2190
Epoch 2229
Epoch 2229, Loss: 0.000002171, Improvement: -0.000001050, Best Loss: 0.000000819 in Epoch 2190
Epoch 2230
Epoch 2230, Loss: 0.000003244, Improvement: 0.000001073, Best Loss: 0.000000819 in Epoch 2190
Epoch 2231
Epoch 2231, Loss: 0.000003392, Improvement: 0.000000147, Best Loss: 0.000000819 in Epoch 2190
Epoch 2232
Epoch 2232, Loss: 0.000006703, Improvement: 0.000003312, Best Loss: 0.000000819 in Epoch 2190
Epoch 2233
Epoch 2233, Loss: 0.000008429, Improvement: 0.000001726, Best Loss: 0.000000819 in Epoch 2190
Epoch 2234
Epoch 2234, Loss: 0.000008783, Improvement: 0.000000354, Best Loss: 0.000000819 in Epoch 2190
Epoch 2235
Epoch 2235, Loss: 0.000007751, Improvement: -0.000001033, Best Loss: 0.000000819 in Epoch 2190
Epoch 2236
Epoch 2236, Loss: 0.000006860, Improvement: -0.000000890, Best Loss: 0.000000819 in Epoch 2190
Epoch 2237
Epoch 2237, Loss: 0.000006628, Improvement: -0.000000232, Best Loss: 0.000000819 in Epoch 2190
Epoch 2238
Epoch 2238, Loss: 0.000009841, Improvement: 0.000003213, Best Loss: 0.000000819 in Epoch 2190
Epoch 2239
Epoch 2239, Loss: 0.000005473, Improvement: -0.000004368, Best Loss: 0.000000819 in Epoch 2190
Epoch 2240
Epoch 2240, Loss: 0.000010393, Improvement: 0.000004920, Best Loss: 0.000000819 in Epoch 2190
Epoch 2241
Epoch 2241, Loss: 0.000019644, Improvement: 0.000009251, Best Loss: 0.000000819 in Epoch 2190
Epoch 2242
Epoch 2242, Loss: 0.000014889, Improvement: -0.000004755, Best Loss: 0.000000819 in Epoch 2190
Epoch 2243
Epoch 2243, Loss: 0.000007667, Improvement: -0.000007222, Best Loss: 0.000000819 in Epoch 2190
Epoch 2244
Epoch 2244, Loss: 0.000013172, Improvement: 0.000005506, Best Loss: 0.000000819 in Epoch 2190
Epoch 2245
Epoch 2245, Loss: 0.000022377, Improvement: 0.000009205, Best Loss: 0.000000819 in Epoch 2190
Epoch 2246
Epoch 2246, Loss: 0.000034056, Improvement: 0.000011679, Best Loss: 0.000000819 in Epoch 2190
Epoch 2247
Epoch 2247, Loss: 0.000031737, Improvement: -0.000002319, Best Loss: 0.000000819 in Epoch 2190
Epoch 2248
Epoch 2248, Loss: 0.000019010, Improvement: -0.000012728, Best Loss: 0.000000819 in Epoch 2190
Epoch 2249
Epoch 2249, Loss: 0.000006746, Improvement: -0.000012264, Best Loss: 0.000000819 in Epoch 2190
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.000003163, Improvement: -0.000003583, Best Loss: 0.000000819 in Epoch 2190
Epoch 2251
Epoch 2251, Loss: 0.000003037, Improvement: -0.000000126, Best Loss: 0.000000819 in Epoch 2190
Epoch 2252
Epoch 2252, Loss: 0.000001703, Improvement: -0.000001334, Best Loss: 0.000000819 in Epoch 2190
Epoch 2253
Epoch 2253, Loss: 0.000001891, Improvement: 0.000000188, Best Loss: 0.000000819 in Epoch 2190
Epoch 2254
Epoch 2254, Loss: 0.000002562, Improvement: 0.000000671, Best Loss: 0.000000819 in Epoch 2190
Epoch 2255
Epoch 2255, Loss: 0.000002102, Improvement: -0.000000460, Best Loss: 0.000000819 in Epoch 2190
Epoch 2256
Epoch 2256, Loss: 0.000001991, Improvement: -0.000000111, Best Loss: 0.000000819 in Epoch 2190
Epoch 2257
Epoch 2257, Loss: 0.000003227, Improvement: 0.000001236, Best Loss: 0.000000819 in Epoch 2190
Epoch 2258
Epoch 2258, Loss: 0.000002103, Improvement: -0.000001124, Best Loss: 0.000000819 in Epoch 2190
Epoch 2259
Epoch 2259, Loss: 0.000001513, Improvement: -0.000000590, Best Loss: 0.000000819 in Epoch 2190
Epoch 2260
Epoch 2260, Loss: 0.000001729, Improvement: 0.000000216, Best Loss: 0.000000819 in Epoch 2190
Epoch 2261
Epoch 2261, Loss: 0.000003184, Improvement: 0.000001455, Best Loss: 0.000000819 in Epoch 2190
Epoch 2262
Epoch 2262, Loss: 0.000003592, Improvement: 0.000000409, Best Loss: 0.000000819 in Epoch 2190
Epoch 2263
Epoch 2263, Loss: 0.000002414, Improvement: -0.000001178, Best Loss: 0.000000819 in Epoch 2190
Epoch 2264
Epoch 2264, Loss: 0.000001906, Improvement: -0.000000508, Best Loss: 0.000000819 in Epoch 2190
Epoch 2265
Epoch 2265, Loss: 0.000001350, Improvement: -0.000000556, Best Loss: 0.000000819 in Epoch 2190
Epoch 2266
Epoch 2266, Loss: 0.000001193, Improvement: -0.000000158, Best Loss: 0.000000819 in Epoch 2190
Epoch 2267
A best model at epoch 2267 has been saved with training error 0.000000812.
Epoch 2267, Loss: 0.000001262, Improvement: 0.000000069, Best Loss: 0.000000812 in Epoch 2267
Epoch 2268
A best model at epoch 2268 has been saved with training error 0.000000782.
Epoch 2268, Loss: 0.000001184, Improvement: -0.000000078, Best Loss: 0.000000782 in Epoch 2268
Epoch 2269
A best model at epoch 2269 has been saved with training error 0.000000764.
Epoch 2269, Loss: 0.000001255, Improvement: 0.000000071, Best Loss: 0.000000764 in Epoch 2269
Epoch 2270
Epoch 2270, Loss: 0.000004140, Improvement: 0.000002885, Best Loss: 0.000000764 in Epoch 2269
Epoch 2271
Epoch 2271, Loss: 0.000013815, Improvement: 0.000009675, Best Loss: 0.000000764 in Epoch 2269
Epoch 2272
Epoch 2272, Loss: 0.000018467, Improvement: 0.000004652, Best Loss: 0.000000764 in Epoch 2269
Epoch 2273
Epoch 2273, Loss: 0.000012227, Improvement: -0.000006240, Best Loss: 0.000000764 in Epoch 2269
Epoch 2274
Epoch 2274, Loss: 0.000026557, Improvement: 0.000014330, Best Loss: 0.000000764 in Epoch 2269
Epoch 2275
Epoch 2275, Loss: 0.000018270, Improvement: -0.000008287, Best Loss: 0.000000764 in Epoch 2269
Epoch 2276
Epoch 2276, Loss: 0.000023603, Improvement: 0.000005333, Best Loss: 0.000000764 in Epoch 2269
Epoch 2277
Epoch 2277, Loss: 0.000016641, Improvement: -0.000006962, Best Loss: 0.000000764 in Epoch 2269
Epoch 2278
Epoch 2278, Loss: 0.000012206, Improvement: -0.000004435, Best Loss: 0.000000764 in Epoch 2269
Epoch 2279
Epoch 2279, Loss: 0.000004951, Improvement: -0.000007255, Best Loss: 0.000000764 in Epoch 2269
Epoch 2280
Epoch 2280, Loss: 0.000005495, Improvement: 0.000000544, Best Loss: 0.000000764 in Epoch 2269
Epoch 2281
Epoch 2281, Loss: 0.000003933, Improvement: -0.000001561, Best Loss: 0.000000764 in Epoch 2269
Epoch 2282
Epoch 2282, Loss: 0.000003086, Improvement: -0.000000848, Best Loss: 0.000000764 in Epoch 2269
Epoch 2283
Epoch 2283, Loss: 0.000002613, Improvement: -0.000000472, Best Loss: 0.000000764 in Epoch 2269
Epoch 2284
Epoch 2284, Loss: 0.000002330, Improvement: -0.000000283, Best Loss: 0.000000764 in Epoch 2269
Epoch 2285
Epoch 2285, Loss: 0.000002287, Improvement: -0.000000043, Best Loss: 0.000000764 in Epoch 2269
Epoch 2286
Epoch 2286, Loss: 0.000003007, Improvement: 0.000000720, Best Loss: 0.000000764 in Epoch 2269
Epoch 2287
Epoch 2287, Loss: 0.000001973, Improvement: -0.000001034, Best Loss: 0.000000764 in Epoch 2269
Epoch 2288
Epoch 2288, Loss: 0.000001675, Improvement: -0.000000298, Best Loss: 0.000000764 in Epoch 2269
Epoch 2289
Epoch 2289, Loss: 0.000001967, Improvement: 0.000000291, Best Loss: 0.000000764 in Epoch 2269
Epoch 2290
Epoch 2290, Loss: 0.000001603, Improvement: -0.000000364, Best Loss: 0.000000764 in Epoch 2269
Epoch 2291
Epoch 2291, Loss: 0.000001845, Improvement: 0.000000242, Best Loss: 0.000000764 in Epoch 2269
Epoch 2292
Epoch 2292, Loss: 0.000002415, Improvement: 0.000000571, Best Loss: 0.000000764 in Epoch 2269
Epoch 2293
Epoch 2293, Loss: 0.000003120, Improvement: 0.000000705, Best Loss: 0.000000764 in Epoch 2269
Epoch 2294
Epoch 2294, Loss: 0.000003985, Improvement: 0.000000865, Best Loss: 0.000000764 in Epoch 2269
Epoch 2295
Epoch 2295, Loss: 0.000003247, Improvement: -0.000000738, Best Loss: 0.000000764 in Epoch 2269
Epoch 2296
Epoch 2296, Loss: 0.000003607, Improvement: 0.000000360, Best Loss: 0.000000764 in Epoch 2269
Epoch 2297
Epoch 2297, Loss: 0.000008153, Improvement: 0.000004546, Best Loss: 0.000000764 in Epoch 2269
Epoch 2298
Epoch 2298, Loss: 0.000020984, Improvement: 0.000012832, Best Loss: 0.000000764 in Epoch 2269
Epoch 2299
Epoch 2299, Loss: 0.000010352, Improvement: -0.000010633, Best Loss: 0.000000764 in Epoch 2269
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.000014562, Improvement: 0.000004211, Best Loss: 0.000000764 in Epoch 2269
Epoch 2301
Epoch 2301, Loss: 0.000012371, Improvement: -0.000002192, Best Loss: 0.000000764 in Epoch 2269
Epoch 2302
Epoch 2302, Loss: 0.000006904, Improvement: -0.000005467, Best Loss: 0.000000764 in Epoch 2269
Epoch 2303
Epoch 2303, Loss: 0.000006434, Improvement: -0.000000470, Best Loss: 0.000000764 in Epoch 2269
Epoch 2304
Epoch 2304, Loss: 0.000006167, Improvement: -0.000000268, Best Loss: 0.000000764 in Epoch 2269
Epoch 2305
Epoch 2305, Loss: 0.000005642, Improvement: -0.000000525, Best Loss: 0.000000764 in Epoch 2269
Epoch 2306
Epoch 2306, Loss: 0.000005385, Improvement: -0.000000257, Best Loss: 0.000000764 in Epoch 2269
Epoch 2307
Epoch 2307, Loss: 0.000006656, Improvement: 0.000001271, Best Loss: 0.000000764 in Epoch 2269
Epoch 2308
Epoch 2308, Loss: 0.000011056, Improvement: 0.000004401, Best Loss: 0.000000764 in Epoch 2269
Epoch 2309
Epoch 2309, Loss: 0.000006875, Improvement: -0.000004181, Best Loss: 0.000000764 in Epoch 2269
Epoch 2310
Epoch 2310, Loss: 0.000003679, Improvement: -0.000003196, Best Loss: 0.000000764 in Epoch 2269
Epoch 2311
Epoch 2311, Loss: 0.000002718, Improvement: -0.000000960, Best Loss: 0.000000764 in Epoch 2269
Epoch 2312
Epoch 2312, Loss: 0.000002029, Improvement: -0.000000690, Best Loss: 0.000000764 in Epoch 2269
Epoch 2313
Epoch 2313, Loss: 0.000002434, Improvement: 0.000000405, Best Loss: 0.000000764 in Epoch 2269
Epoch 2314
Epoch 2314, Loss: 0.000004443, Improvement: 0.000002009, Best Loss: 0.000000764 in Epoch 2269
Epoch 2315
Epoch 2315, Loss: 0.000007873, Improvement: 0.000003430, Best Loss: 0.000000764 in Epoch 2269
Epoch 2316
Epoch 2316, Loss: 0.000020427, Improvement: 0.000012554, Best Loss: 0.000000764 in Epoch 2269
Epoch 2317
Epoch 2317, Loss: 0.000009254, Improvement: -0.000011173, Best Loss: 0.000000764 in Epoch 2269
Epoch 2318
Epoch 2318, Loss: 0.000006485, Improvement: -0.000002769, Best Loss: 0.000000764 in Epoch 2269
Epoch 2319
Epoch 2319, Loss: 0.000012973, Improvement: 0.000006489, Best Loss: 0.000000764 in Epoch 2269
Epoch 2320
Epoch 2320, Loss: 0.000027080, Improvement: 0.000014106, Best Loss: 0.000000764 in Epoch 2269
Epoch 2321
Epoch 2321, Loss: 0.000009297, Improvement: -0.000017783, Best Loss: 0.000000764 in Epoch 2269
Epoch 2322
Epoch 2322, Loss: 0.000003890, Improvement: -0.000005407, Best Loss: 0.000000764 in Epoch 2269
Epoch 2323
Epoch 2323, Loss: 0.000003638, Improvement: -0.000000252, Best Loss: 0.000000764 in Epoch 2269
Epoch 2324
Epoch 2324, Loss: 0.000005630, Improvement: 0.000001992, Best Loss: 0.000000764 in Epoch 2269
Epoch 2325
Epoch 2325, Loss: 0.000005755, Improvement: 0.000000125, Best Loss: 0.000000764 in Epoch 2269
Epoch 2326
Epoch 2326, Loss: 0.000003855, Improvement: -0.000001900, Best Loss: 0.000000764 in Epoch 2269
Epoch 2327
Epoch 2327, Loss: 0.000002282, Improvement: -0.000001573, Best Loss: 0.000000764 in Epoch 2269
Epoch 2328
Epoch 2328, Loss: 0.000003496, Improvement: 0.000001214, Best Loss: 0.000000764 in Epoch 2269
Epoch 2329
Epoch 2329, Loss: 0.000003610, Improvement: 0.000000114, Best Loss: 0.000000764 in Epoch 2269
Epoch 2330
Epoch 2330, Loss: 0.000008969, Improvement: 0.000005360, Best Loss: 0.000000764 in Epoch 2269
Epoch 2331
Epoch 2331, Loss: 0.000007360, Improvement: -0.000001610, Best Loss: 0.000000764 in Epoch 2269
Epoch 2332
Epoch 2332, Loss: 0.000003437, Improvement: -0.000003922, Best Loss: 0.000000764 in Epoch 2269
Epoch 2333
Epoch 2333, Loss: 0.000004427, Improvement: 0.000000990, Best Loss: 0.000000764 in Epoch 2269
Epoch 2334
Epoch 2334, Loss: 0.000003429, Improvement: -0.000000998, Best Loss: 0.000000764 in Epoch 2269
Epoch 2335
Epoch 2335, Loss: 0.000002363, Improvement: -0.000001067, Best Loss: 0.000000764 in Epoch 2269
Epoch 2336
Epoch 2336, Loss: 0.000002222, Improvement: -0.000000140, Best Loss: 0.000000764 in Epoch 2269
Epoch 2337
Epoch 2337, Loss: 0.000003009, Improvement: 0.000000787, Best Loss: 0.000000764 in Epoch 2269
Epoch 2338
Epoch 2338, Loss: 0.000002711, Improvement: -0.000000298, Best Loss: 0.000000764 in Epoch 2269
Epoch 2339
Epoch 2339, Loss: 0.000002605, Improvement: -0.000000106, Best Loss: 0.000000764 in Epoch 2269
Epoch 2340
Epoch 2340, Loss: 0.000001606, Improvement: -0.000000999, Best Loss: 0.000000764 in Epoch 2269
Epoch 2341
Epoch 2341, Loss: 0.000003252, Improvement: 0.000001646, Best Loss: 0.000000764 in Epoch 2269
Epoch 2342
Epoch 2342, Loss: 0.000004089, Improvement: 0.000000836, Best Loss: 0.000000764 in Epoch 2269
Epoch 2343
Epoch 2343, Loss: 0.000004259, Improvement: 0.000000171, Best Loss: 0.000000764 in Epoch 2269
Epoch 2344
Epoch 2344, Loss: 0.000009675, Improvement: 0.000005416, Best Loss: 0.000000764 in Epoch 2269
Epoch 2345
Epoch 2345, Loss: 0.000016632, Improvement: 0.000006957, Best Loss: 0.000000764 in Epoch 2269
Epoch 2346
Epoch 2346, Loss: 0.000016363, Improvement: -0.000000269, Best Loss: 0.000000764 in Epoch 2269
Epoch 2347
Epoch 2347, Loss: 0.000016326, Improvement: -0.000000037, Best Loss: 0.000000764 in Epoch 2269
Epoch 2348
Epoch 2348, Loss: 0.000009233, Improvement: -0.000007093, Best Loss: 0.000000764 in Epoch 2269
Epoch 2349
Epoch 2349, Loss: 0.000007035, Improvement: -0.000002198, Best Loss: 0.000000764 in Epoch 2269
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.000005728, Improvement: -0.000001307, Best Loss: 0.000000764 in Epoch 2269
Epoch 2351
Epoch 2351, Loss: 0.000004887, Improvement: -0.000000841, Best Loss: 0.000000764 in Epoch 2269
Epoch 2352
Epoch 2352, Loss: 0.000003223, Improvement: -0.000001663, Best Loss: 0.000000764 in Epoch 2269
Epoch 2353
Epoch 2353, Loss: 0.000002349, Improvement: -0.000000875, Best Loss: 0.000000764 in Epoch 2269
Epoch 2354
Epoch 2354, Loss: 0.000001581, Improvement: -0.000000768, Best Loss: 0.000000764 in Epoch 2269
Epoch 2355
Epoch 2355, Loss: 0.000001443, Improvement: -0.000000138, Best Loss: 0.000000764 in Epoch 2269
Epoch 2356
Epoch 2356, Loss: 0.000001679, Improvement: 0.000000236, Best Loss: 0.000000764 in Epoch 2269
Epoch 2357
Epoch 2357, Loss: 0.000001892, Improvement: 0.000000213, Best Loss: 0.000000764 in Epoch 2269
Epoch 2358
Epoch 2358, Loss: 0.000001683, Improvement: -0.000000209, Best Loss: 0.000000764 in Epoch 2269
Epoch 2359
Epoch 2359, Loss: 0.000001735, Improvement: 0.000000052, Best Loss: 0.000000764 in Epoch 2269
Epoch 2360
Epoch 2360, Loss: 0.000001548, Improvement: -0.000000187, Best Loss: 0.000000764 in Epoch 2269
Epoch 2361
Epoch 2361, Loss: 0.000001280, Improvement: -0.000000269, Best Loss: 0.000000764 in Epoch 2269
Epoch 2362
Epoch 2362, Loss: 0.000001620, Improvement: 0.000000340, Best Loss: 0.000000764 in Epoch 2269
Epoch 2363
Epoch 2363, Loss: 0.000001519, Improvement: -0.000000101, Best Loss: 0.000000764 in Epoch 2269
Epoch 2364
Epoch 2364, Loss: 0.000001548, Improvement: 0.000000030, Best Loss: 0.000000764 in Epoch 2269
Epoch 2365
Epoch 2365, Loss: 0.000002121, Improvement: 0.000000572, Best Loss: 0.000000764 in Epoch 2269
Epoch 2366
Epoch 2366, Loss: 0.000002866, Improvement: 0.000000746, Best Loss: 0.000000764 in Epoch 2269
Epoch 2367
Epoch 2367, Loss: 0.000002395, Improvement: -0.000000471, Best Loss: 0.000000764 in Epoch 2269
Epoch 2368
Epoch 2368, Loss: 0.000007702, Improvement: 0.000005307, Best Loss: 0.000000764 in Epoch 2269
Epoch 2369
Epoch 2369, Loss: 0.000012616, Improvement: 0.000004913, Best Loss: 0.000000764 in Epoch 2269
Epoch 2370
Epoch 2370, Loss: 0.000017019, Improvement: 0.000004403, Best Loss: 0.000000764 in Epoch 2269
Epoch 2371
Epoch 2371, Loss: 0.000035164, Improvement: 0.000018145, Best Loss: 0.000000764 in Epoch 2269
Epoch 2372
Epoch 2372, Loss: 0.000042033, Improvement: 0.000006869, Best Loss: 0.000000764 in Epoch 2269
Epoch 2373
Epoch 2373, Loss: 0.000027652, Improvement: -0.000014381, Best Loss: 0.000000764 in Epoch 2269
Epoch 2374
Epoch 2374, Loss: 0.000006579, Improvement: -0.000021073, Best Loss: 0.000000764 in Epoch 2269
Epoch 2375
Epoch 2375, Loss: 0.000002661, Improvement: -0.000003918, Best Loss: 0.000000764 in Epoch 2269
Epoch 2376
Epoch 2376, Loss: 0.000001954, Improvement: -0.000000707, Best Loss: 0.000000764 in Epoch 2269
Epoch 2377
Epoch 2377, Loss: 0.000001341, Improvement: -0.000000613, Best Loss: 0.000000764 in Epoch 2269
Epoch 2378
Epoch 2378, Loss: 0.000001323, Improvement: -0.000000018, Best Loss: 0.000000764 in Epoch 2269
Epoch 2379
Epoch 2379, Loss: 0.000001329, Improvement: 0.000000006, Best Loss: 0.000000764 in Epoch 2269
Epoch 2380
Epoch 2380, Loss: 0.000001195, Improvement: -0.000000134, Best Loss: 0.000000764 in Epoch 2269
Epoch 2381
Epoch 2381, Loss: 0.000001076, Improvement: -0.000000119, Best Loss: 0.000000764 in Epoch 2269
Epoch 2382
A best model at epoch 2382 has been saved with training error 0.000000754.
A best model at epoch 2382 has been saved with training error 0.000000733.
Epoch 2382, Loss: 0.000001011, Improvement: -0.000000065, Best Loss: 0.000000733 in Epoch 2382
Epoch 2383
Epoch 2383, Loss: 0.000001082, Improvement: 0.000000071, Best Loss: 0.000000733 in Epoch 2382
Epoch 2384
Epoch 2384, Loss: 0.000001014, Improvement: -0.000000068, Best Loss: 0.000000733 in Epoch 2382
Epoch 2385
A best model at epoch 2385 has been saved with training error 0.000000684.
Epoch 2385, Loss: 0.000000987, Improvement: -0.000000028, Best Loss: 0.000000684 in Epoch 2385
Epoch 2386
Epoch 2386, Loss: 0.000000998, Improvement: 0.000000012, Best Loss: 0.000000684 in Epoch 2385
Epoch 2387
A best model at epoch 2387 has been saved with training error 0.000000679.
Epoch 2387, Loss: 0.000000999, Improvement: 0.000000001, Best Loss: 0.000000679 in Epoch 2387
Epoch 2388
Epoch 2388, Loss: 0.000000977, Improvement: -0.000000023, Best Loss: 0.000000679 in Epoch 2387
Epoch 2389
Epoch 2389, Loss: 0.000000988, Improvement: 0.000000011, Best Loss: 0.000000679 in Epoch 2387
Epoch 2390
Epoch 2390, Loss: 0.000001050, Improvement: 0.000000062, Best Loss: 0.000000679 in Epoch 2387
Epoch 2391
Epoch 2391, Loss: 0.000001222, Improvement: 0.000000172, Best Loss: 0.000000679 in Epoch 2387
Epoch 2392
Epoch 2392, Loss: 0.000001325, Improvement: 0.000000103, Best Loss: 0.000000679 in Epoch 2387
Epoch 2393
Epoch 2393, Loss: 0.000001192, Improvement: -0.000000132, Best Loss: 0.000000679 in Epoch 2387
Epoch 2394
Epoch 2394, Loss: 0.000001184, Improvement: -0.000000009, Best Loss: 0.000000679 in Epoch 2387
Epoch 2395
Epoch 2395, Loss: 0.000001062, Improvement: -0.000000121, Best Loss: 0.000000679 in Epoch 2387
Epoch 2396
Epoch 2396, Loss: 0.000001083, Improvement: 0.000000020, Best Loss: 0.000000679 in Epoch 2387
Epoch 2397
Epoch 2397, Loss: 0.000002392, Improvement: 0.000001309, Best Loss: 0.000000679 in Epoch 2387
Epoch 2398
Epoch 2398, Loss: 0.000003655, Improvement: 0.000001263, Best Loss: 0.000000679 in Epoch 2387
Epoch 2399
Epoch 2399, Loss: 0.000003121, Improvement: -0.000000534, Best Loss: 0.000000679 in Epoch 2387
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.000006348, Improvement: 0.000003227, Best Loss: 0.000000679 in Epoch 2387
Epoch 2401
Epoch 2401, Loss: 0.000022211, Improvement: 0.000015863, Best Loss: 0.000000679 in Epoch 2387
Epoch 2402
Epoch 2402, Loss: 0.000014627, Improvement: -0.000007584, Best Loss: 0.000000679 in Epoch 2387
Epoch 2403
Epoch 2403, Loss: 0.000018508, Improvement: 0.000003881, Best Loss: 0.000000679 in Epoch 2387
Epoch 2404
Epoch 2404, Loss: 0.000011411, Improvement: -0.000007097, Best Loss: 0.000000679 in Epoch 2387
Epoch 2405
Epoch 2405, Loss: 0.000008032, Improvement: -0.000003379, Best Loss: 0.000000679 in Epoch 2387
Epoch 2406
Epoch 2406, Loss: 0.000013947, Improvement: 0.000005915, Best Loss: 0.000000679 in Epoch 2387
Epoch 2407
Epoch 2407, Loss: 0.000009065, Improvement: -0.000004882, Best Loss: 0.000000679 in Epoch 2387
Epoch 2408
Epoch 2408, Loss: 0.000005868, Improvement: -0.000003197, Best Loss: 0.000000679 in Epoch 2387
Epoch 2409
Epoch 2409, Loss: 0.000007424, Improvement: 0.000001556, Best Loss: 0.000000679 in Epoch 2387
Epoch 2410
Epoch 2410, Loss: 0.000006663, Improvement: -0.000000761, Best Loss: 0.000000679 in Epoch 2387
Epoch 2411
Epoch 2411, Loss: 0.000004311, Improvement: -0.000002352, Best Loss: 0.000000679 in Epoch 2387
Epoch 2412
Epoch 2412, Loss: 0.000002828, Improvement: -0.000001483, Best Loss: 0.000000679 in Epoch 2387
Epoch 2413
Epoch 2413, Loss: 0.000001753, Improvement: -0.000001075, Best Loss: 0.000000679 in Epoch 2387
Epoch 2414
Epoch 2414, Loss: 0.000001507, Improvement: -0.000000245, Best Loss: 0.000000679 in Epoch 2387
Epoch 2415
Epoch 2415, Loss: 0.000001639, Improvement: 0.000000132, Best Loss: 0.000000679 in Epoch 2387
Epoch 2416
Epoch 2416, Loss: 0.000004635, Improvement: 0.000002996, Best Loss: 0.000000679 in Epoch 2387
Epoch 2417
Epoch 2417, Loss: 0.000002482, Improvement: -0.000002152, Best Loss: 0.000000679 in Epoch 2387
Epoch 2418
Epoch 2418, Loss: 0.000006671, Improvement: 0.000004189, Best Loss: 0.000000679 in Epoch 2387
Epoch 2419
Epoch 2419, Loss: 0.000007416, Improvement: 0.000000746, Best Loss: 0.000000679 in Epoch 2387
Epoch 2420
Epoch 2420, Loss: 0.000013849, Improvement: 0.000006433, Best Loss: 0.000000679 in Epoch 2387
Epoch 2421
Epoch 2421, Loss: 0.000018649, Improvement: 0.000004800, Best Loss: 0.000000679 in Epoch 2387
Epoch 2422
Epoch 2422, Loss: 0.000016789, Improvement: -0.000001861, Best Loss: 0.000000679 in Epoch 2387
Epoch 2423
Epoch 2423, Loss: 0.000008568, Improvement: -0.000008221, Best Loss: 0.000000679 in Epoch 2387
Epoch 2424
Epoch 2424, Loss: 0.000003139, Improvement: -0.000005429, Best Loss: 0.000000679 in Epoch 2387
Epoch 2425
Epoch 2425, Loss: 0.000002507, Improvement: -0.000000632, Best Loss: 0.000000679 in Epoch 2387
Epoch 2426
Epoch 2426, Loss: 0.000002064, Improvement: -0.000000442, Best Loss: 0.000000679 in Epoch 2387
Epoch 2427
Epoch 2427, Loss: 0.000003554, Improvement: 0.000001490, Best Loss: 0.000000679 in Epoch 2387
Epoch 2428
Epoch 2428, Loss: 0.000004936, Improvement: 0.000001382, Best Loss: 0.000000679 in Epoch 2387
Epoch 2429
Epoch 2429, Loss: 0.000002165, Improvement: -0.000002771, Best Loss: 0.000000679 in Epoch 2387
Epoch 2430
Epoch 2430, Loss: 0.000001922, Improvement: -0.000000244, Best Loss: 0.000000679 in Epoch 2387
Epoch 2431
Epoch 2431, Loss: 0.000001549, Improvement: -0.000000373, Best Loss: 0.000000679 in Epoch 2387
Epoch 2432
Epoch 2432, Loss: 0.000001526, Improvement: -0.000000023, Best Loss: 0.000000679 in Epoch 2387
Epoch 2433
Epoch 2433, Loss: 0.000001794, Improvement: 0.000000268, Best Loss: 0.000000679 in Epoch 2387
Epoch 2434
Epoch 2434, Loss: 0.000001641, Improvement: -0.000000153, Best Loss: 0.000000679 in Epoch 2387
Epoch 2435
Epoch 2435, Loss: 0.000001926, Improvement: 0.000000285, Best Loss: 0.000000679 in Epoch 2387
Epoch 2436
Epoch 2436, Loss: 0.000002665, Improvement: 0.000000739, Best Loss: 0.000000679 in Epoch 2387
Epoch 2437
Epoch 2437, Loss: 0.000003498, Improvement: 0.000000833, Best Loss: 0.000000679 in Epoch 2387
Epoch 2438
Epoch 2438, Loss: 0.000006007, Improvement: 0.000002509, Best Loss: 0.000000679 in Epoch 2387
Epoch 2439
Epoch 2439, Loss: 0.000013666, Improvement: 0.000007659, Best Loss: 0.000000679 in Epoch 2387
Epoch 2440
Epoch 2440, Loss: 0.000017749, Improvement: 0.000004083, Best Loss: 0.000000679 in Epoch 2387
Epoch 2441
Epoch 2441, Loss: 0.000007749, Improvement: -0.000010000, Best Loss: 0.000000679 in Epoch 2387
Epoch 2442
Epoch 2442, Loss: 0.000005199, Improvement: -0.000002550, Best Loss: 0.000000679 in Epoch 2387
Epoch 2443
Epoch 2443, Loss: 0.000005974, Improvement: 0.000000775, Best Loss: 0.000000679 in Epoch 2387
Epoch 2444
Epoch 2444, Loss: 0.000004220, Improvement: -0.000001754, Best Loss: 0.000000679 in Epoch 2387
Epoch 2445
Epoch 2445, Loss: 0.000005458, Improvement: 0.000001238, Best Loss: 0.000000679 in Epoch 2387
Epoch 2446
Epoch 2446, Loss: 0.000008571, Improvement: 0.000003113, Best Loss: 0.000000679 in Epoch 2387
Epoch 2447
Epoch 2447, Loss: 0.000005640, Improvement: -0.000002931, Best Loss: 0.000000679 in Epoch 2387
Epoch 2448
Epoch 2448, Loss: 0.000013510, Improvement: 0.000007870, Best Loss: 0.000000679 in Epoch 2387
Epoch 2449
Epoch 2449, Loss: 0.000015654, Improvement: 0.000002144, Best Loss: 0.000000679 in Epoch 2387
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.000010609, Improvement: -0.000005045, Best Loss: 0.000000679 in Epoch 2387
Epoch 2451
Epoch 2451, Loss: 0.000009908, Improvement: -0.000000702, Best Loss: 0.000000679 in Epoch 2387
Epoch 2452
Epoch 2452, Loss: 0.000016720, Improvement: 0.000006812, Best Loss: 0.000000679 in Epoch 2387
Epoch 2453
Epoch 2453, Loss: 0.000011686, Improvement: -0.000005034, Best Loss: 0.000000679 in Epoch 2387
Epoch 2454
Epoch 2454, Loss: 0.000007237, Improvement: -0.000004449, Best Loss: 0.000000679 in Epoch 2387
Epoch 2455
Epoch 2455, Loss: 0.000004368, Improvement: -0.000002869, Best Loss: 0.000000679 in Epoch 2387
Epoch 2456
Epoch 2456, Loss: 0.000005408, Improvement: 0.000001041, Best Loss: 0.000000679 in Epoch 2387
Epoch 2457
Epoch 2457, Loss: 0.000004690, Improvement: -0.000000718, Best Loss: 0.000000679 in Epoch 2387
Epoch 2458
Epoch 2458, Loss: 0.000006775, Improvement: 0.000002085, Best Loss: 0.000000679 in Epoch 2387
Epoch 2459
Epoch 2459, Loss: 0.000005942, Improvement: -0.000000833, Best Loss: 0.000000679 in Epoch 2387
Epoch 2460
Epoch 2460, Loss: 0.000006069, Improvement: 0.000000127, Best Loss: 0.000000679 in Epoch 2387
Epoch 2461
Epoch 2461, Loss: 0.000003269, Improvement: -0.000002799, Best Loss: 0.000000679 in Epoch 2387
Epoch 2462
Epoch 2462, Loss: 0.000003152, Improvement: -0.000000117, Best Loss: 0.000000679 in Epoch 2387
Epoch 2463
Epoch 2463, Loss: 0.000001983, Improvement: -0.000001169, Best Loss: 0.000000679 in Epoch 2387
Epoch 2464
Epoch 2464, Loss: 0.000003154, Improvement: 0.000001171, Best Loss: 0.000000679 in Epoch 2387
Epoch 2465
Epoch 2465, Loss: 0.000007388, Improvement: 0.000004234, Best Loss: 0.000000679 in Epoch 2387
Epoch 2466
Epoch 2466, Loss: 0.000021116, Improvement: 0.000013729, Best Loss: 0.000000679 in Epoch 2387
Epoch 2467
Epoch 2467, Loss: 0.000021994, Improvement: 0.000000878, Best Loss: 0.000000679 in Epoch 2387
Epoch 2468
Epoch 2468, Loss: 0.000016080, Improvement: -0.000005914, Best Loss: 0.000000679 in Epoch 2387
Epoch 2469
Epoch 2469, Loss: 0.000007715, Improvement: -0.000008365, Best Loss: 0.000000679 in Epoch 2387
Epoch 2470
Epoch 2470, Loss: 0.000005015, Improvement: -0.000002700, Best Loss: 0.000000679 in Epoch 2387
Epoch 2471
Epoch 2471, Loss: 0.000003499, Improvement: -0.000001516, Best Loss: 0.000000679 in Epoch 2387
Epoch 2472
Epoch 2472, Loss: 0.000002647, Improvement: -0.000000853, Best Loss: 0.000000679 in Epoch 2387
Epoch 2473
Epoch 2473, Loss: 0.000002171, Improvement: -0.000000476, Best Loss: 0.000000679 in Epoch 2387
Epoch 2474
Epoch 2474, Loss: 0.000005979, Improvement: 0.000003808, Best Loss: 0.000000679 in Epoch 2387
Epoch 2475
Epoch 2475, Loss: 0.000009272, Improvement: 0.000003293, Best Loss: 0.000000679 in Epoch 2387
Epoch 2476
Epoch 2476, Loss: 0.000018680, Improvement: 0.000009408, Best Loss: 0.000000679 in Epoch 2387
Epoch 2477
Epoch 2477, Loss: 0.000020420, Improvement: 0.000001740, Best Loss: 0.000000679 in Epoch 2387
Epoch 2478
Epoch 2478, Loss: 0.000008496, Improvement: -0.000011924, Best Loss: 0.000000679 in Epoch 2387
Epoch 2479
Epoch 2479, Loss: 0.000003793, Improvement: -0.000004702, Best Loss: 0.000000679 in Epoch 2387
Epoch 2480
Epoch 2480, Loss: 0.000003047, Improvement: -0.000000746, Best Loss: 0.000000679 in Epoch 2387
Epoch 2481
Epoch 2481, Loss: 0.000001934, Improvement: -0.000001113, Best Loss: 0.000000679 in Epoch 2387
Epoch 2482
Epoch 2482, Loss: 0.000006561, Improvement: 0.000004627, Best Loss: 0.000000679 in Epoch 2387
Epoch 2483
Epoch 2483, Loss: 0.000013177, Improvement: 0.000006616, Best Loss: 0.000000679 in Epoch 2387
Epoch 2484
Epoch 2484, Loss: 0.000004952, Improvement: -0.000008225, Best Loss: 0.000000679 in Epoch 2387
Epoch 2485
Epoch 2485, Loss: 0.000005196, Improvement: 0.000000244, Best Loss: 0.000000679 in Epoch 2387
Epoch 2486
Epoch 2486, Loss: 0.000004825, Improvement: -0.000000371, Best Loss: 0.000000679 in Epoch 2387
Epoch 2487
Epoch 2487, Loss: 0.000003817, Improvement: -0.000001009, Best Loss: 0.000000679 in Epoch 2387
Epoch 2488
Epoch 2488, Loss: 0.000004804, Improvement: 0.000000988, Best Loss: 0.000000679 in Epoch 2387
Epoch 2489
Epoch 2489, Loss: 0.000002864, Improvement: -0.000001940, Best Loss: 0.000000679 in Epoch 2387
Epoch 2490
Epoch 2490, Loss: 0.000002080, Improvement: -0.000000784, Best Loss: 0.000000679 in Epoch 2387
Epoch 2491
Epoch 2491, Loss: 0.000001913, Improvement: -0.000000167, Best Loss: 0.000000679 in Epoch 2387
Epoch 2492
Epoch 2492, Loss: 0.000002950, Improvement: 0.000001037, Best Loss: 0.000000679 in Epoch 2387
Epoch 2493
Epoch 2493, Loss: 0.000009870, Improvement: 0.000006920, Best Loss: 0.000000679 in Epoch 2387
Epoch 2494
Epoch 2494, Loss: 0.000010258, Improvement: 0.000000388, Best Loss: 0.000000679 in Epoch 2387
Epoch 2495
Epoch 2495, Loss: 0.000008283, Improvement: -0.000001975, Best Loss: 0.000000679 in Epoch 2387
Epoch 2496
Epoch 2496, Loss: 0.000009627, Improvement: 0.000001345, Best Loss: 0.000000679 in Epoch 2387
Epoch 2497
Epoch 2497, Loss: 0.000008289, Improvement: -0.000001338, Best Loss: 0.000000679 in Epoch 2387
Epoch 2498
Epoch 2498, Loss: 0.000007057, Improvement: -0.000001232, Best Loss: 0.000000679 in Epoch 2387
Epoch 2499
Epoch 2499, Loss: 0.000002622, Improvement: -0.000004435, Best Loss: 0.000000679 in Epoch 2387
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.000004412, Improvement: 0.000001790, Best Loss: 0.000000679 in Epoch 2387
Epoch 2501
Epoch 2501, Loss: 0.000005219, Improvement: 0.000000806, Best Loss: 0.000000679 in Epoch 2387
Epoch 2502
Epoch 2502, Loss: 0.000005412, Improvement: 0.000000193, Best Loss: 0.000000679 in Epoch 2387
Epoch 2503
Epoch 2503, Loss: 0.000004122, Improvement: -0.000001290, Best Loss: 0.000000679 in Epoch 2387
Epoch 2504
Epoch 2504, Loss: 0.000003852, Improvement: -0.000000271, Best Loss: 0.000000679 in Epoch 2387
Epoch 2505
Epoch 2505, Loss: 0.000003619, Improvement: -0.000000233, Best Loss: 0.000000679 in Epoch 2387
Epoch 2506
Epoch 2506, Loss: 0.000006684, Improvement: 0.000003065, Best Loss: 0.000000679 in Epoch 2387
Epoch 2507
Epoch 2507, Loss: 0.000009024, Improvement: 0.000002341, Best Loss: 0.000000679 in Epoch 2387
Epoch 2508
Epoch 2508, Loss: 0.000014623, Improvement: 0.000005598, Best Loss: 0.000000679 in Epoch 2387
Epoch 2509
Epoch 2509, Loss: 0.000005615, Improvement: -0.000009007, Best Loss: 0.000000679 in Epoch 2387
Epoch 2510
Epoch 2510, Loss: 0.000009947, Improvement: 0.000004331, Best Loss: 0.000000679 in Epoch 2387
Epoch 2511
Epoch 2511, Loss: 0.000022702, Improvement: 0.000012755, Best Loss: 0.000000679 in Epoch 2387
Epoch 2512
Epoch 2512, Loss: 0.000019996, Improvement: -0.000002706, Best Loss: 0.000000679 in Epoch 2387
Epoch 2513
Epoch 2513, Loss: 0.000014566, Improvement: -0.000005430, Best Loss: 0.000000679 in Epoch 2387
Epoch 2514
Epoch 2514, Loss: 0.000010252, Improvement: -0.000004314, Best Loss: 0.000000679 in Epoch 2387
Epoch 2515
Epoch 2515, Loss: 0.000007060, Improvement: -0.000003192, Best Loss: 0.000000679 in Epoch 2387
Epoch 2516
Epoch 2516, Loss: 0.000008885, Improvement: 0.000001824, Best Loss: 0.000000679 in Epoch 2387
Epoch 2517
Epoch 2517, Loss: 0.000007381, Improvement: -0.000001504, Best Loss: 0.000000679 in Epoch 2387
Epoch 2518
Epoch 2518, Loss: 0.000011193, Improvement: 0.000003812, Best Loss: 0.000000679 in Epoch 2387
Epoch 2519
Epoch 2519, Loss: 0.000008768, Improvement: -0.000002425, Best Loss: 0.000000679 in Epoch 2387
Epoch 2520
Epoch 2520, Loss: 0.000009013, Improvement: 0.000000245, Best Loss: 0.000000679 in Epoch 2387
Epoch 2521
Epoch 2521, Loss: 0.000009295, Improvement: 0.000000282, Best Loss: 0.000000679 in Epoch 2387
Epoch 2522
Epoch 2522, Loss: 0.000011958, Improvement: 0.000002663, Best Loss: 0.000000679 in Epoch 2387
Epoch 2523
Epoch 2523, Loss: 0.000011159, Improvement: -0.000000799, Best Loss: 0.000000679 in Epoch 2387
Epoch 2524
Epoch 2524, Loss: 0.000005078, Improvement: -0.000006081, Best Loss: 0.000000679 in Epoch 2387
Epoch 2525
Epoch 2525, Loss: 0.000003615, Improvement: -0.000001463, Best Loss: 0.000000679 in Epoch 2387
Epoch 2526
Epoch 2526, Loss: 0.000002315, Improvement: -0.000001300, Best Loss: 0.000000679 in Epoch 2387
Epoch 2527
Epoch 2527, Loss: 0.000003550, Improvement: 0.000001235, Best Loss: 0.000000679 in Epoch 2387
Epoch 2528
Epoch 2528, Loss: 0.000002185, Improvement: -0.000001365, Best Loss: 0.000000679 in Epoch 2387
Epoch 2529
Epoch 2529, Loss: 0.000002765, Improvement: 0.000000580, Best Loss: 0.000000679 in Epoch 2387
Epoch 2530
Epoch 2530, Loss: 0.000003114, Improvement: 0.000000350, Best Loss: 0.000000679 in Epoch 2387
Epoch 2531
Epoch 2531, Loss: 0.000005030, Improvement: 0.000001916, Best Loss: 0.000000679 in Epoch 2387
Epoch 2532
Epoch 2532, Loss: 0.000020003, Improvement: 0.000014972, Best Loss: 0.000000679 in Epoch 2387
Epoch 2533
Epoch 2533, Loss: 0.000035407, Improvement: 0.000015405, Best Loss: 0.000000679 in Epoch 2387
Epoch 2534
Epoch 2534, Loss: 0.000015427, Improvement: -0.000019981, Best Loss: 0.000000679 in Epoch 2387
Epoch 2535
Epoch 2535, Loss: 0.000009760, Improvement: -0.000005667, Best Loss: 0.000000679 in Epoch 2387
Epoch 2536
Epoch 2536, Loss: 0.000003792, Improvement: -0.000005968, Best Loss: 0.000000679 in Epoch 2387
Epoch 2537
Epoch 2537, Loss: 0.000002154, Improvement: -0.000001638, Best Loss: 0.000000679 in Epoch 2387
Epoch 2538
Epoch 2538, Loss: 0.000001523, Improvement: -0.000000631, Best Loss: 0.000000679 in Epoch 2387
Epoch 2539
Epoch 2539, Loss: 0.000001806, Improvement: 0.000000283, Best Loss: 0.000000679 in Epoch 2387
Epoch 2540
Epoch 2540, Loss: 0.000004331, Improvement: 0.000002526, Best Loss: 0.000000679 in Epoch 2387
Epoch 2541
Epoch 2541, Loss: 0.000008584, Improvement: 0.000004252, Best Loss: 0.000000679 in Epoch 2387
Epoch 2542
Epoch 2542, Loss: 0.000008782, Improvement: 0.000000198, Best Loss: 0.000000679 in Epoch 2387
Epoch 2543
Epoch 2543, Loss: 0.000008026, Improvement: -0.000000756, Best Loss: 0.000000679 in Epoch 2387
Epoch 2544
Epoch 2544, Loss: 0.000005980, Improvement: -0.000002046, Best Loss: 0.000000679 in Epoch 2387
Epoch 2545
Epoch 2545, Loss: 0.000002621, Improvement: -0.000003359, Best Loss: 0.000000679 in Epoch 2387
Epoch 2546
Epoch 2546, Loss: 0.000002000, Improvement: -0.000000621, Best Loss: 0.000000679 in Epoch 2387
Epoch 2547
Epoch 2547, Loss: 0.000002141, Improvement: 0.000000141, Best Loss: 0.000000679 in Epoch 2387
Epoch 2548
Epoch 2548, Loss: 0.000002075, Improvement: -0.000000066, Best Loss: 0.000000679 in Epoch 2387
Epoch 2549
Epoch 2549, Loss: 0.000001500, Improvement: -0.000000575, Best Loss: 0.000000679 in Epoch 2387
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.000001257, Improvement: -0.000000243, Best Loss: 0.000000679 in Epoch 2387
Epoch 2551
Epoch 2551, Loss: 0.000001827, Improvement: 0.000000570, Best Loss: 0.000000679 in Epoch 2387
Epoch 2552
Epoch 2552, Loss: 0.000004351, Improvement: 0.000002524, Best Loss: 0.000000679 in Epoch 2387
Epoch 2553
Epoch 2553, Loss: 0.000005149, Improvement: 0.000000797, Best Loss: 0.000000679 in Epoch 2387
Epoch 2554
Epoch 2554, Loss: 0.000010291, Improvement: 0.000005142, Best Loss: 0.000000679 in Epoch 2387
Epoch 2555
Epoch 2555, Loss: 0.000008986, Improvement: -0.000001305, Best Loss: 0.000000679 in Epoch 2387
Epoch 2556
Epoch 2556, Loss: 0.000012177, Improvement: 0.000003191, Best Loss: 0.000000679 in Epoch 2387
Epoch 2557
Epoch 2557, Loss: 0.000010277, Improvement: -0.000001900, Best Loss: 0.000000679 in Epoch 2387
Epoch 2558
Epoch 2558, Loss: 0.000007490, Improvement: -0.000002787, Best Loss: 0.000000679 in Epoch 2387
Epoch 2559
Epoch 2559, Loss: 0.000004742, Improvement: -0.000002748, Best Loss: 0.000000679 in Epoch 2387
Epoch 2560
Epoch 2560, Loss: 0.000005604, Improvement: 0.000000862, Best Loss: 0.000000679 in Epoch 2387
Epoch 2561
Epoch 2561, Loss: 0.000004803, Improvement: -0.000000801, Best Loss: 0.000000679 in Epoch 2387
Epoch 2562
Epoch 2562, Loss: 0.000015380, Improvement: 0.000010578, Best Loss: 0.000000679 in Epoch 2387
Epoch 2563
Epoch 2563, Loss: 0.000021449, Improvement: 0.000006069, Best Loss: 0.000000679 in Epoch 2387
Epoch 2564
Epoch 2564, Loss: 0.000013986, Improvement: -0.000007463, Best Loss: 0.000000679 in Epoch 2387
Epoch 2565
Epoch 2565, Loss: 0.000012672, Improvement: -0.000001314, Best Loss: 0.000000679 in Epoch 2387
Epoch 2566
Epoch 2566, Loss: 0.000007760, Improvement: -0.000004912, Best Loss: 0.000000679 in Epoch 2387
Epoch 2567
Epoch 2567, Loss: 0.000019337, Improvement: 0.000011577, Best Loss: 0.000000679 in Epoch 2387
Epoch 2568
Epoch 2568, Loss: 0.000007703, Improvement: -0.000011634, Best Loss: 0.000000679 in Epoch 2387
Epoch 2569
Epoch 2569, Loss: 0.000004895, Improvement: -0.000002807, Best Loss: 0.000000679 in Epoch 2387
Epoch 2570
Epoch 2570, Loss: 0.000002421, Improvement: -0.000002474, Best Loss: 0.000000679 in Epoch 2387
Epoch 2571
Epoch 2571, Loss: 0.000002135, Improvement: -0.000000287, Best Loss: 0.000000679 in Epoch 2387
Epoch 2572
Epoch 2572, Loss: 0.000002751, Improvement: 0.000000617, Best Loss: 0.000000679 in Epoch 2387
Epoch 2573
Epoch 2573, Loss: 0.000003150, Improvement: 0.000000399, Best Loss: 0.000000679 in Epoch 2387
Epoch 2574
Epoch 2574, Loss: 0.000001630, Improvement: -0.000001521, Best Loss: 0.000000679 in Epoch 2387
Epoch 2575
Epoch 2575, Loss: 0.000001440, Improvement: -0.000000189, Best Loss: 0.000000679 in Epoch 2387
Epoch 2576
Epoch 2576, Loss: 0.000001156, Improvement: -0.000000284, Best Loss: 0.000000679 in Epoch 2387
Epoch 2577
A best model at epoch 2577 has been saved with training error 0.000000661.
Epoch 2577, Loss: 0.000001065, Improvement: -0.000000091, Best Loss: 0.000000661 in Epoch 2577
Epoch 2578
Epoch 2578, Loss: 0.000001009, Improvement: -0.000000056, Best Loss: 0.000000661 in Epoch 2577
Epoch 2579
Epoch 2579, Loss: 0.000000908, Improvement: -0.000000101, Best Loss: 0.000000661 in Epoch 2577
Epoch 2580
A best model at epoch 2580 has been saved with training error 0.000000645.
Epoch 2580, Loss: 0.000000907, Improvement: -0.000000001, Best Loss: 0.000000645 in Epoch 2580
Epoch 2581
A best model at epoch 2581 has been saved with training error 0.000000640.
Epoch 2581, Loss: 0.000000885, Improvement: -0.000000022, Best Loss: 0.000000640 in Epoch 2581
Epoch 2582
A best model at epoch 2582 has been saved with training error 0.000000602.
Epoch 2582, Loss: 0.000000890, Improvement: 0.000000005, Best Loss: 0.000000602 in Epoch 2582
Epoch 2583
Epoch 2583, Loss: 0.000001112, Improvement: 0.000000222, Best Loss: 0.000000602 in Epoch 2582
Epoch 2584
Epoch 2584, Loss: 0.000001307, Improvement: 0.000000194, Best Loss: 0.000000602 in Epoch 2582
Epoch 2585
Epoch 2585, Loss: 0.000001013, Improvement: -0.000000294, Best Loss: 0.000000602 in Epoch 2582
Epoch 2586
Epoch 2586, Loss: 0.000001146, Improvement: 0.000000133, Best Loss: 0.000000602 in Epoch 2582
Epoch 2587
Epoch 2587, Loss: 0.000001014, Improvement: -0.000000132, Best Loss: 0.000000602 in Epoch 2582
Epoch 2588
Epoch 2588, Loss: 0.000001068, Improvement: 0.000000054, Best Loss: 0.000000602 in Epoch 2582
Epoch 2589
Epoch 2589, Loss: 0.000001067, Improvement: -0.000000001, Best Loss: 0.000000602 in Epoch 2582
Epoch 2590
Epoch 2590, Loss: 0.000001272, Improvement: 0.000000205, Best Loss: 0.000000602 in Epoch 2582
Epoch 2591
Epoch 2591, Loss: 0.000002193, Improvement: 0.000000921, Best Loss: 0.000000602 in Epoch 2582
Epoch 2592
Epoch 2592, Loss: 0.000004576, Improvement: 0.000002383, Best Loss: 0.000000602 in Epoch 2582
Epoch 2593
Epoch 2593, Loss: 0.000003514, Improvement: -0.000001063, Best Loss: 0.000000602 in Epoch 2582
Epoch 2594
Epoch 2594, Loss: 0.000007212, Improvement: 0.000003698, Best Loss: 0.000000602 in Epoch 2582
Epoch 2595
Epoch 2595, Loss: 0.000007834, Improvement: 0.000000622, Best Loss: 0.000000602 in Epoch 2582
Epoch 2596
Epoch 2596, Loss: 0.000008291, Improvement: 0.000000457, Best Loss: 0.000000602 in Epoch 2582
Epoch 2597
Epoch 2597, Loss: 0.000011252, Improvement: 0.000002961, Best Loss: 0.000000602 in Epoch 2582
Epoch 2598
Epoch 2598, Loss: 0.000013561, Improvement: 0.000002309, Best Loss: 0.000000602 in Epoch 2582
Epoch 2599
Epoch 2599, Loss: 0.000026009, Improvement: 0.000012448, Best Loss: 0.000000602 in Epoch 2582
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.000013585, Improvement: -0.000012423, Best Loss: 0.000000602 in Epoch 2582
Epoch 2601
Epoch 2601, Loss: 0.000005232, Improvement: -0.000008354, Best Loss: 0.000000602 in Epoch 2582
Epoch 2602
Epoch 2602, Loss: 0.000003365, Improvement: -0.000001867, Best Loss: 0.000000602 in Epoch 2582
Epoch 2603
Epoch 2603, Loss: 0.000002313, Improvement: -0.000001052, Best Loss: 0.000000602 in Epoch 2582
Epoch 2604
Epoch 2604, Loss: 0.000001695, Improvement: -0.000000618, Best Loss: 0.000000602 in Epoch 2582
Epoch 2605
Epoch 2605, Loss: 0.000002200, Improvement: 0.000000505, Best Loss: 0.000000602 in Epoch 2582
Epoch 2606
Epoch 2606, Loss: 0.000004935, Improvement: 0.000002735, Best Loss: 0.000000602 in Epoch 2582
Epoch 2607
Epoch 2607, Loss: 0.000005138, Improvement: 0.000000202, Best Loss: 0.000000602 in Epoch 2582
Epoch 2608
Epoch 2608, Loss: 0.000003004, Improvement: -0.000002134, Best Loss: 0.000000602 in Epoch 2582
Epoch 2609
Epoch 2609, Loss: 0.000003228, Improvement: 0.000000224, Best Loss: 0.000000602 in Epoch 2582
Epoch 2610
Epoch 2610, Loss: 0.000002052, Improvement: -0.000001176, Best Loss: 0.000000602 in Epoch 2582
Epoch 2611
Epoch 2611, Loss: 0.000001219, Improvement: -0.000000833, Best Loss: 0.000000602 in Epoch 2582
Epoch 2612
Epoch 2612, Loss: 0.000001155, Improvement: -0.000000064, Best Loss: 0.000000602 in Epoch 2582
Epoch 2613
Epoch 2613, Loss: 0.000001380, Improvement: 0.000000225, Best Loss: 0.000000602 in Epoch 2582
Epoch 2614
Epoch 2614, Loss: 0.000002416, Improvement: 0.000001036, Best Loss: 0.000000602 in Epoch 2582
Epoch 2615
Epoch 2615, Loss: 0.000004397, Improvement: 0.000001981, Best Loss: 0.000000602 in Epoch 2582
Epoch 2616
Epoch 2616, Loss: 0.000005429, Improvement: 0.000001032, Best Loss: 0.000000602 in Epoch 2582
Epoch 2617
Epoch 2617, Loss: 0.000004683, Improvement: -0.000000746, Best Loss: 0.000000602 in Epoch 2582
Epoch 2618
Epoch 2618, Loss: 0.000006058, Improvement: 0.000001375, Best Loss: 0.000000602 in Epoch 2582
Epoch 2619
Epoch 2619, Loss: 0.000005038, Improvement: -0.000001020, Best Loss: 0.000000602 in Epoch 2582
Epoch 2620
Epoch 2620, Loss: 0.000005145, Improvement: 0.000000107, Best Loss: 0.000000602 in Epoch 2582
Epoch 2621
Epoch 2621, Loss: 0.000009050, Improvement: 0.000003906, Best Loss: 0.000000602 in Epoch 2582
Epoch 2622
Epoch 2622, Loss: 0.000007554, Improvement: -0.000001496, Best Loss: 0.000000602 in Epoch 2582
Epoch 2623
Epoch 2623, Loss: 0.000004480, Improvement: -0.000003074, Best Loss: 0.000000602 in Epoch 2582
Epoch 2624
Epoch 2624, Loss: 0.000004426, Improvement: -0.000000054, Best Loss: 0.000000602 in Epoch 2582
Epoch 2625
Epoch 2625, Loss: 0.000008475, Improvement: 0.000004049, Best Loss: 0.000000602 in Epoch 2582
Epoch 2626
Epoch 2626, Loss: 0.000007173, Improvement: -0.000001302, Best Loss: 0.000000602 in Epoch 2582
Epoch 2627
Epoch 2627, Loss: 0.000009760, Improvement: 0.000002587, Best Loss: 0.000000602 in Epoch 2582
Epoch 2628
Epoch 2628, Loss: 0.000016812, Improvement: 0.000007052, Best Loss: 0.000000602 in Epoch 2582
Epoch 2629
Epoch 2629, Loss: 0.000023153, Improvement: 0.000006341, Best Loss: 0.000000602 in Epoch 2582
Epoch 2630
Epoch 2630, Loss: 0.000017505, Improvement: -0.000005648, Best Loss: 0.000000602 in Epoch 2582
Epoch 2631
Epoch 2631, Loss: 0.000005725, Improvement: -0.000011780, Best Loss: 0.000000602 in Epoch 2582
Epoch 2632
Epoch 2632, Loss: 0.000003002, Improvement: -0.000002722, Best Loss: 0.000000602 in Epoch 2582
Epoch 2633
Epoch 2633, Loss: 0.000004632, Improvement: 0.000001629, Best Loss: 0.000000602 in Epoch 2582
Epoch 2634
Epoch 2634, Loss: 0.000004016, Improvement: -0.000000616, Best Loss: 0.000000602 in Epoch 2582
Epoch 2635
Epoch 2635, Loss: 0.000003722, Improvement: -0.000000294, Best Loss: 0.000000602 in Epoch 2582
Epoch 2636
Epoch 2636, Loss: 0.000002168, Improvement: -0.000001554, Best Loss: 0.000000602 in Epoch 2582
Epoch 2637
Epoch 2637, Loss: 0.000001270, Improvement: -0.000000898, Best Loss: 0.000000602 in Epoch 2582
Epoch 2638
Epoch 2638, Loss: 0.000001082, Improvement: -0.000000188, Best Loss: 0.000000602 in Epoch 2582
Epoch 2639
Epoch 2639, Loss: 0.000001119, Improvement: 0.000000038, Best Loss: 0.000000602 in Epoch 2582
Epoch 2640
Epoch 2640, Loss: 0.000001199, Improvement: 0.000000079, Best Loss: 0.000000602 in Epoch 2582
Epoch 2641
Epoch 2641, Loss: 0.000001510, Improvement: 0.000000311, Best Loss: 0.000000602 in Epoch 2582
Epoch 2642
Epoch 2642, Loss: 0.000001182, Improvement: -0.000000328, Best Loss: 0.000000602 in Epoch 2582
Epoch 2643
Epoch 2643, Loss: 0.000001039, Improvement: -0.000000143, Best Loss: 0.000000602 in Epoch 2582
Epoch 2644
Epoch 2644, Loss: 0.000001285, Improvement: 0.000000245, Best Loss: 0.000000602 in Epoch 2582
Epoch 2645
Epoch 2645, Loss: 0.000001591, Improvement: 0.000000306, Best Loss: 0.000000602 in Epoch 2582
Epoch 2646
Epoch 2646, Loss: 0.000001431, Improvement: -0.000000161, Best Loss: 0.000000602 in Epoch 2582
Epoch 2647
Epoch 2647, Loss: 0.000002188, Improvement: 0.000000757, Best Loss: 0.000000602 in Epoch 2582
Epoch 2648
Epoch 2648, Loss: 0.000004598, Improvement: 0.000002410, Best Loss: 0.000000602 in Epoch 2582
Epoch 2649
Epoch 2649, Loss: 0.000002958, Improvement: -0.000001640, Best Loss: 0.000000602 in Epoch 2582
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.000002678, Improvement: -0.000000280, Best Loss: 0.000000602 in Epoch 2582
Epoch 2651
Epoch 2651, Loss: 0.000009256, Improvement: 0.000006578, Best Loss: 0.000000602 in Epoch 2582
Epoch 2652
Epoch 2652, Loss: 0.000009948, Improvement: 0.000000692, Best Loss: 0.000000602 in Epoch 2582
Epoch 2653
Epoch 2653, Loss: 0.000006722, Improvement: -0.000003226, Best Loss: 0.000000602 in Epoch 2582
Epoch 2654
Epoch 2654, Loss: 0.000010648, Improvement: 0.000003926, Best Loss: 0.000000602 in Epoch 2582
Epoch 2655
Epoch 2655, Loss: 0.000017513, Improvement: 0.000006865, Best Loss: 0.000000602 in Epoch 2582
Epoch 2656
Epoch 2656, Loss: 0.000013388, Improvement: -0.000004125, Best Loss: 0.000000602 in Epoch 2582
Epoch 2657
Epoch 2657, Loss: 0.000004310, Improvement: -0.000009077, Best Loss: 0.000000602 in Epoch 2582
Epoch 2658
Epoch 2658, Loss: 0.000004423, Improvement: 0.000000112, Best Loss: 0.000000602 in Epoch 2582
Epoch 2659
Epoch 2659, Loss: 0.000005215, Improvement: 0.000000792, Best Loss: 0.000000602 in Epoch 2582
Epoch 2660
Epoch 2660, Loss: 0.000003328, Improvement: -0.000001887, Best Loss: 0.000000602 in Epoch 2582
Epoch 2661
Epoch 2661, Loss: 0.000002669, Improvement: -0.000000659, Best Loss: 0.000000602 in Epoch 2582
Epoch 2662
Epoch 2662, Loss: 0.000002747, Improvement: 0.000000078, Best Loss: 0.000000602 in Epoch 2582
Epoch 2663
Epoch 2663, Loss: 0.000001840, Improvement: -0.000000907, Best Loss: 0.000000602 in Epoch 2582
Epoch 2664
Epoch 2664, Loss: 0.000001491, Improvement: -0.000000349, Best Loss: 0.000000602 in Epoch 2582
Epoch 2665
Epoch 2665, Loss: 0.000002014, Improvement: 0.000000523, Best Loss: 0.000000602 in Epoch 2582
Epoch 2666
Epoch 2666, Loss: 0.000001613, Improvement: -0.000000401, Best Loss: 0.000000602 in Epoch 2582
Epoch 2667
Epoch 2667, Loss: 0.000002363, Improvement: 0.000000750, Best Loss: 0.000000602 in Epoch 2582
Epoch 2668
Epoch 2668, Loss: 0.000005528, Improvement: 0.000003165, Best Loss: 0.000000602 in Epoch 2582
Epoch 2669
Epoch 2669, Loss: 0.000012258, Improvement: 0.000006730, Best Loss: 0.000000602 in Epoch 2582
Epoch 2670
Epoch 2670, Loss: 0.000010991, Improvement: -0.000001267, Best Loss: 0.000000602 in Epoch 2582
Epoch 2671
Epoch 2671, Loss: 0.000007962, Improvement: -0.000003029, Best Loss: 0.000000602 in Epoch 2582
Epoch 2672
Epoch 2672, Loss: 0.000010076, Improvement: 0.000002114, Best Loss: 0.000000602 in Epoch 2582
Epoch 2673
Epoch 2673, Loss: 0.000011181, Improvement: 0.000001106, Best Loss: 0.000000602 in Epoch 2582
Epoch 2674
Epoch 2674, Loss: 0.000012069, Improvement: 0.000000887, Best Loss: 0.000000602 in Epoch 2582
Epoch 2675
Epoch 2675, Loss: 0.000007673, Improvement: -0.000004395, Best Loss: 0.000000602 in Epoch 2582
Epoch 2676
Epoch 2676, Loss: 0.000003652, Improvement: -0.000004021, Best Loss: 0.000000602 in Epoch 2582
Epoch 2677
Epoch 2677, Loss: 0.000004695, Improvement: 0.000001043, Best Loss: 0.000000602 in Epoch 2582
Epoch 2678
Epoch 2678, Loss: 0.000004778, Improvement: 0.000000083, Best Loss: 0.000000602 in Epoch 2582
Epoch 2679
Epoch 2679, Loss: 0.000007954, Improvement: 0.000003176, Best Loss: 0.000000602 in Epoch 2582
Epoch 2680
Epoch 2680, Loss: 0.000019161, Improvement: 0.000011207, Best Loss: 0.000000602 in Epoch 2582
Epoch 2681
Epoch 2681, Loss: 0.000027177, Improvement: 0.000008016, Best Loss: 0.000000602 in Epoch 2582
Epoch 2682
Epoch 2682, Loss: 0.000032921, Improvement: 0.000005743, Best Loss: 0.000000602 in Epoch 2582
Epoch 2683
Epoch 2683, Loss: 0.000009555, Improvement: -0.000023366, Best Loss: 0.000000602 in Epoch 2582
Epoch 2684
Epoch 2684, Loss: 0.000006282, Improvement: -0.000003272, Best Loss: 0.000000602 in Epoch 2582
Epoch 2685
Epoch 2685, Loss: 0.000004504, Improvement: -0.000001779, Best Loss: 0.000000602 in Epoch 2582
Epoch 2686
Epoch 2686, Loss: 0.000001907, Improvement: -0.000002596, Best Loss: 0.000000602 in Epoch 2582
Epoch 2687
Epoch 2687, Loss: 0.000001404, Improvement: -0.000000503, Best Loss: 0.000000602 in Epoch 2582
Epoch 2688
Epoch 2688, Loss: 0.000001348, Improvement: -0.000000055, Best Loss: 0.000000602 in Epoch 2582
Epoch 2689
Epoch 2689, Loss: 0.000001162, Improvement: -0.000000186, Best Loss: 0.000000602 in Epoch 2582
Epoch 2690
Epoch 2690, Loss: 0.000001307, Improvement: 0.000000145, Best Loss: 0.000000602 in Epoch 2582
Epoch 2691
Epoch 2691, Loss: 0.000001407, Improvement: 0.000000100, Best Loss: 0.000000602 in Epoch 2582
Epoch 2692
Epoch 2692, Loss: 0.000001573, Improvement: 0.000000167, Best Loss: 0.000000602 in Epoch 2582
Epoch 2693
Epoch 2693, Loss: 0.000000975, Improvement: -0.000000598, Best Loss: 0.000000602 in Epoch 2582
Epoch 2694
Epoch 2694, Loss: 0.000000884, Improvement: -0.000000092, Best Loss: 0.000000602 in Epoch 2582
Epoch 2695
Epoch 2695, Loss: 0.000000934, Improvement: 0.000000051, Best Loss: 0.000000602 in Epoch 2582
Epoch 2696
Epoch 2696, Loss: 0.000000916, Improvement: -0.000000019, Best Loss: 0.000000602 in Epoch 2582
Epoch 2697
Epoch 2697, Loss: 0.000000938, Improvement: 0.000000022, Best Loss: 0.000000602 in Epoch 2582
Epoch 2698
Epoch 2698, Loss: 0.000000952, Improvement: 0.000000014, Best Loss: 0.000000602 in Epoch 2582
Epoch 2699
Epoch 2699, Loss: 0.000000987, Improvement: 0.000000035, Best Loss: 0.000000602 in Epoch 2582
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.000000930, Improvement: -0.000000057, Best Loss: 0.000000602 in Epoch 2582
Epoch 2701
Epoch 2701, Loss: 0.000001166, Improvement: 0.000000236, Best Loss: 0.000000602 in Epoch 2582
Epoch 2702
Epoch 2702, Loss: 0.000001287, Improvement: 0.000000121, Best Loss: 0.000000602 in Epoch 2582
Epoch 2703
Epoch 2703, Loss: 0.000001441, Improvement: 0.000000154, Best Loss: 0.000000602 in Epoch 2582
Epoch 2704
Epoch 2704, Loss: 0.000001972, Improvement: 0.000000531, Best Loss: 0.000000602 in Epoch 2582
Epoch 2705
Epoch 2705, Loss: 0.000003794, Improvement: 0.000001822, Best Loss: 0.000000602 in Epoch 2582
Epoch 2706
Epoch 2706, Loss: 0.000004727, Improvement: 0.000000933, Best Loss: 0.000000602 in Epoch 2582
Epoch 2707
Epoch 2707, Loss: 0.000003843, Improvement: -0.000000883, Best Loss: 0.000000602 in Epoch 2582
Epoch 2708
Epoch 2708, Loss: 0.000006616, Improvement: 0.000002773, Best Loss: 0.000000602 in Epoch 2582
Epoch 2709
Epoch 2709, Loss: 0.000010266, Improvement: 0.000003650, Best Loss: 0.000000602 in Epoch 2582
Epoch 2710
Epoch 2710, Loss: 0.000010798, Improvement: 0.000000531, Best Loss: 0.000000602 in Epoch 2582
Epoch 2711
Epoch 2711, Loss: 0.000009229, Improvement: -0.000001569, Best Loss: 0.000000602 in Epoch 2582
Epoch 2712
Epoch 2712, Loss: 0.000005726, Improvement: -0.000003503, Best Loss: 0.000000602 in Epoch 2582
Epoch 2713
Epoch 2713, Loss: 0.000003589, Improvement: -0.000002137, Best Loss: 0.000000602 in Epoch 2582
Epoch 2714
Epoch 2714, Loss: 0.000002966, Improvement: -0.000000622, Best Loss: 0.000000602 in Epoch 2582
Epoch 2715
Epoch 2715, Loss: 0.000003604, Improvement: 0.000000638, Best Loss: 0.000000602 in Epoch 2582
Epoch 2716
Epoch 2716, Loss: 0.000003723, Improvement: 0.000000119, Best Loss: 0.000000602 in Epoch 2582
Epoch 2717
Epoch 2717, Loss: 0.000003496, Improvement: -0.000000228, Best Loss: 0.000000602 in Epoch 2582
Epoch 2718
Epoch 2718, Loss: 0.000007947, Improvement: 0.000004451, Best Loss: 0.000000602 in Epoch 2582
Epoch 2719
Epoch 2719, Loss: 0.000016201, Improvement: 0.000008254, Best Loss: 0.000000602 in Epoch 2582
Epoch 2720
Epoch 2720, Loss: 0.000019658, Improvement: 0.000003457, Best Loss: 0.000000602 in Epoch 2582
Epoch 2721
Epoch 2721, Loss: 0.000011818, Improvement: -0.000007841, Best Loss: 0.000000602 in Epoch 2582
Epoch 2722
Epoch 2722, Loss: 0.000008941, Improvement: -0.000002877, Best Loss: 0.000000602 in Epoch 2582
Epoch 2723
Epoch 2723, Loss: 0.000003543, Improvement: -0.000005398, Best Loss: 0.000000602 in Epoch 2582
Epoch 2724
Epoch 2724, Loss: 0.000002838, Improvement: -0.000000704, Best Loss: 0.000000602 in Epoch 2582
Epoch 2725
Epoch 2725, Loss: 0.000002769, Improvement: -0.000000070, Best Loss: 0.000000602 in Epoch 2582
Epoch 2726
Epoch 2726, Loss: 0.000005595, Improvement: 0.000002826, Best Loss: 0.000000602 in Epoch 2582
Epoch 2727
Epoch 2727, Loss: 0.000004259, Improvement: -0.000001336, Best Loss: 0.000000602 in Epoch 2582
Epoch 2728
Epoch 2728, Loss: 0.000010958, Improvement: 0.000006699, Best Loss: 0.000000602 in Epoch 2582
Epoch 2729
Epoch 2729, Loss: 0.000015253, Improvement: 0.000004295, Best Loss: 0.000000602 in Epoch 2582
Epoch 2730
Epoch 2730, Loss: 0.000006756, Improvement: -0.000008498, Best Loss: 0.000000602 in Epoch 2582
Epoch 2731
Epoch 2731, Loss: 0.000006709, Improvement: -0.000000047, Best Loss: 0.000000602 in Epoch 2582
Epoch 2732
Epoch 2732, Loss: 0.000002426, Improvement: -0.000004282, Best Loss: 0.000000602 in Epoch 2582
Epoch 2733
Epoch 2733, Loss: 0.000001760, Improvement: -0.000000667, Best Loss: 0.000000602 in Epoch 2582
Epoch 2734
Epoch 2734, Loss: 0.000001251, Improvement: -0.000000509, Best Loss: 0.000000602 in Epoch 2582
Epoch 2735
Epoch 2735, Loss: 0.000001276, Improvement: 0.000000025, Best Loss: 0.000000602 in Epoch 2582
Epoch 2736
Epoch 2736, Loss: 0.000001416, Improvement: 0.000000140, Best Loss: 0.000000602 in Epoch 2582
Epoch 2737
Epoch 2737, Loss: 0.000001391, Improvement: -0.000000025, Best Loss: 0.000000602 in Epoch 2582
Epoch 2738
Epoch 2738, Loss: 0.000001357, Improvement: -0.000000034, Best Loss: 0.000000602 in Epoch 2582
Epoch 2739
Epoch 2739, Loss: 0.000001053, Improvement: -0.000000304, Best Loss: 0.000000602 in Epoch 2582
Epoch 2740
Epoch 2740, Loss: 0.000001027, Improvement: -0.000000026, Best Loss: 0.000000602 in Epoch 2582
Epoch 2741
Epoch 2741, Loss: 0.000000980, Improvement: -0.000000047, Best Loss: 0.000000602 in Epoch 2582
Epoch 2742
Epoch 2742, Loss: 0.000001035, Improvement: 0.000000055, Best Loss: 0.000000602 in Epoch 2582
Epoch 2743
Epoch 2743, Loss: 0.000001433, Improvement: 0.000000399, Best Loss: 0.000000602 in Epoch 2582
Epoch 2744
Epoch 2744, Loss: 0.000001707, Improvement: 0.000000274, Best Loss: 0.000000602 in Epoch 2582
Epoch 2745
Epoch 2745, Loss: 0.000001888, Improvement: 0.000000181, Best Loss: 0.000000602 in Epoch 2582
Epoch 2746
Epoch 2746, Loss: 0.000002628, Improvement: 0.000000740, Best Loss: 0.000000602 in Epoch 2582
Epoch 2747
Epoch 2747, Loss: 0.000002160, Improvement: -0.000000468, Best Loss: 0.000000602 in Epoch 2582
Epoch 2748
Epoch 2748, Loss: 0.000003829, Improvement: 0.000001669, Best Loss: 0.000000602 in Epoch 2582
Epoch 2749
Epoch 2749, Loss: 0.000010178, Improvement: 0.000006348, Best Loss: 0.000000602 in Epoch 2582
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.000012902, Improvement: 0.000002724, Best Loss: 0.000000602 in Epoch 2582
Epoch 2751
Epoch 2751, Loss: 0.000009742, Improvement: -0.000003160, Best Loss: 0.000000602 in Epoch 2582
Epoch 2752
Epoch 2752, Loss: 0.000005827, Improvement: -0.000003915, Best Loss: 0.000000602 in Epoch 2582
Epoch 2753
Epoch 2753, Loss: 0.000003981, Improvement: -0.000001847, Best Loss: 0.000000602 in Epoch 2582
Epoch 2754
Epoch 2754, Loss: 0.000004402, Improvement: 0.000000422, Best Loss: 0.000000602 in Epoch 2582
Epoch 2755
Epoch 2755, Loss: 0.000008349, Improvement: 0.000003947, Best Loss: 0.000000602 in Epoch 2582
Epoch 2756
Epoch 2756, Loss: 0.000009064, Improvement: 0.000000715, Best Loss: 0.000000602 in Epoch 2582
Epoch 2757
Epoch 2757, Loss: 0.000018636, Improvement: 0.000009572, Best Loss: 0.000000602 in Epoch 2582
Epoch 2758
Epoch 2758, Loss: 0.000014621, Improvement: -0.000004015, Best Loss: 0.000000602 in Epoch 2582
Epoch 2759
Epoch 2759, Loss: 0.000013584, Improvement: -0.000001036, Best Loss: 0.000000602 in Epoch 2582
Epoch 2760
Epoch 2760, Loss: 0.000011223, Improvement: -0.000002361, Best Loss: 0.000000602 in Epoch 2582
Epoch 2761
Epoch 2761, Loss: 0.000013809, Improvement: 0.000002586, Best Loss: 0.000000602 in Epoch 2582
Epoch 2762
Epoch 2762, Loss: 0.000004225, Improvement: -0.000009585, Best Loss: 0.000000602 in Epoch 2582
Epoch 2763
Epoch 2763, Loss: 0.000003217, Improvement: -0.000001008, Best Loss: 0.000000602 in Epoch 2582
Epoch 2764
Epoch 2764, Loss: 0.000002147, Improvement: -0.000001069, Best Loss: 0.000000602 in Epoch 2582
Epoch 2765
Epoch 2765, Loss: 0.000001371, Improvement: -0.000000776, Best Loss: 0.000000602 in Epoch 2582
Epoch 2766
Epoch 2766, Loss: 0.000001125, Improvement: -0.000000246, Best Loss: 0.000000602 in Epoch 2582
Epoch 2767
Epoch 2767, Loss: 0.000001123, Improvement: -0.000000002, Best Loss: 0.000000602 in Epoch 2582
Epoch 2768
Epoch 2768, Loss: 0.000001267, Improvement: 0.000000144, Best Loss: 0.000000602 in Epoch 2582
Epoch 2769
Epoch 2769, Loss: 0.000001058, Improvement: -0.000000210, Best Loss: 0.000000602 in Epoch 2582
Epoch 2770
Epoch 2770, Loss: 0.000000977, Improvement: -0.000000081, Best Loss: 0.000000602 in Epoch 2582
Epoch 2771
Epoch 2771, Loss: 0.000001381, Improvement: 0.000000404, Best Loss: 0.000000602 in Epoch 2582
Epoch 2772
Epoch 2772, Loss: 0.000002162, Improvement: 0.000000781, Best Loss: 0.000000602 in Epoch 2582
Epoch 2773
Epoch 2773, Loss: 0.000004256, Improvement: 0.000002094, Best Loss: 0.000000602 in Epoch 2582
Epoch 2774
Epoch 2774, Loss: 0.000007531, Improvement: 0.000003274, Best Loss: 0.000000602 in Epoch 2582
Epoch 2775
Epoch 2775, Loss: 0.000008082, Improvement: 0.000000551, Best Loss: 0.000000602 in Epoch 2582
Epoch 2776
Epoch 2776, Loss: 0.000008357, Improvement: 0.000000276, Best Loss: 0.000000602 in Epoch 2582
Epoch 2777
Epoch 2777, Loss: 0.000011382, Improvement: 0.000003025, Best Loss: 0.000000602 in Epoch 2582
Epoch 2778
Epoch 2778, Loss: 0.000011091, Improvement: -0.000000291, Best Loss: 0.000000602 in Epoch 2582
Epoch 2779
Epoch 2779, Loss: 0.000014178, Improvement: 0.000003088, Best Loss: 0.000000602 in Epoch 2582
Epoch 2780
Epoch 2780, Loss: 0.000021095, Improvement: 0.000006916, Best Loss: 0.000000602 in Epoch 2582
Epoch 2781
Epoch 2781, Loss: 0.000014334, Improvement: -0.000006760, Best Loss: 0.000000602 in Epoch 2582
Epoch 2782
Epoch 2782, Loss: 0.000010429, Improvement: -0.000003905, Best Loss: 0.000000602 in Epoch 2582
Epoch 2783
Epoch 2783, Loss: 0.000005836, Improvement: -0.000004593, Best Loss: 0.000000602 in Epoch 2582
Epoch 2784
Epoch 2784, Loss: 0.000004828, Improvement: -0.000001008, Best Loss: 0.000000602 in Epoch 2582
Epoch 2785
Epoch 2785, Loss: 0.000002391, Improvement: -0.000002437, Best Loss: 0.000000602 in Epoch 2582
Epoch 2786
Epoch 2786, Loss: 0.000003176, Improvement: 0.000000785, Best Loss: 0.000000602 in Epoch 2582
Epoch 2787
Epoch 2787, Loss: 0.000003694, Improvement: 0.000000517, Best Loss: 0.000000602 in Epoch 2582
Epoch 2788
Epoch 2788, Loss: 0.000005066, Improvement: 0.000001373, Best Loss: 0.000000602 in Epoch 2582
Epoch 2789
Epoch 2789, Loss: 0.000003505, Improvement: -0.000001561, Best Loss: 0.000000602 in Epoch 2582
Epoch 2790
Epoch 2790, Loss: 0.000003677, Improvement: 0.000000172, Best Loss: 0.000000602 in Epoch 2582
Epoch 2791
Epoch 2791, Loss: 0.000005330, Improvement: 0.000001653, Best Loss: 0.000000602 in Epoch 2582
Epoch 2792
Epoch 2792, Loss: 0.000005763, Improvement: 0.000000433, Best Loss: 0.000000602 in Epoch 2582
Epoch 2793
Epoch 2793, Loss: 0.000004040, Improvement: -0.000001723, Best Loss: 0.000000602 in Epoch 2582
Epoch 2794
Epoch 2794, Loss: 0.000005428, Improvement: 0.000001388, Best Loss: 0.000000602 in Epoch 2582
Epoch 2795
Epoch 2795, Loss: 0.000002582, Improvement: -0.000002846, Best Loss: 0.000000602 in Epoch 2582
Epoch 2796
Epoch 2796, Loss: 0.000003473, Improvement: 0.000000890, Best Loss: 0.000000602 in Epoch 2582
Epoch 2797
Epoch 2797, Loss: 0.000003153, Improvement: -0.000000319, Best Loss: 0.000000602 in Epoch 2582
Epoch 2798
Epoch 2798, Loss: 0.000007996, Improvement: 0.000004843, Best Loss: 0.000000602 in Epoch 2582
Epoch 2799
Epoch 2799, Loss: 0.000004838, Improvement: -0.000003159, Best Loss: 0.000000602 in Epoch 2582
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.000004596, Improvement: -0.000000242, Best Loss: 0.000000602 in Epoch 2582
Epoch 2801
Epoch 2801, Loss: 0.000008414, Improvement: 0.000003818, Best Loss: 0.000000602 in Epoch 2582
Epoch 2802
Epoch 2802, Loss: 0.000016775, Improvement: 0.000008361, Best Loss: 0.000000602 in Epoch 2582
Epoch 2803
Epoch 2803, Loss: 0.000009849, Improvement: -0.000006926, Best Loss: 0.000000602 in Epoch 2582
Epoch 2804
Epoch 2804, Loss: 0.000004717, Improvement: -0.000005132, Best Loss: 0.000000602 in Epoch 2582
Epoch 2805
Epoch 2805, Loss: 0.000002922, Improvement: -0.000001795, Best Loss: 0.000000602 in Epoch 2582
Epoch 2806
Epoch 2806, Loss: 0.000002326, Improvement: -0.000000596, Best Loss: 0.000000602 in Epoch 2582
Epoch 2807
Epoch 2807, Loss: 0.000002084, Improvement: -0.000000242, Best Loss: 0.000000602 in Epoch 2582
Epoch 2808
Epoch 2808, Loss: 0.000002172, Improvement: 0.000000088, Best Loss: 0.000000602 in Epoch 2582
Epoch 2809
Epoch 2809, Loss: 0.000001556, Improvement: -0.000000617, Best Loss: 0.000000602 in Epoch 2582
Epoch 2810
Epoch 2810, Loss: 0.000001305, Improvement: -0.000000250, Best Loss: 0.000000602 in Epoch 2582
Epoch 2811
Epoch 2811, Loss: 0.000001983, Improvement: 0.000000678, Best Loss: 0.000000602 in Epoch 2582
Epoch 2812
Epoch 2812, Loss: 0.000004079, Improvement: 0.000002096, Best Loss: 0.000000602 in Epoch 2582
Epoch 2813
Epoch 2813, Loss: 0.000010760, Improvement: 0.000006681, Best Loss: 0.000000602 in Epoch 2582
Epoch 2814
Epoch 2814, Loss: 0.000011617, Improvement: 0.000000857, Best Loss: 0.000000602 in Epoch 2582
Epoch 2815
Epoch 2815, Loss: 0.000021306, Improvement: 0.000009688, Best Loss: 0.000000602 in Epoch 2582
Epoch 2816
Epoch 2816, Loss: 0.000051441, Improvement: 0.000030135, Best Loss: 0.000000602 in Epoch 2582
Epoch 2817
Epoch 2817, Loss: 0.000013607, Improvement: -0.000037834, Best Loss: 0.000000602 in Epoch 2582
Epoch 2818
Epoch 2818, Loss: 0.000006053, Improvement: -0.000007554, Best Loss: 0.000000602 in Epoch 2582
Epoch 2819
Epoch 2819, Loss: 0.000003790, Improvement: -0.000002263, Best Loss: 0.000000602 in Epoch 2582
Epoch 2820
Epoch 2820, Loss: 0.000002372, Improvement: -0.000001417, Best Loss: 0.000000602 in Epoch 2582
Epoch 2821
Epoch 2821, Loss: 0.000001370, Improvement: -0.000001003, Best Loss: 0.000000602 in Epoch 2582
Epoch 2822
Epoch 2822, Loss: 0.000001088, Improvement: -0.000000282, Best Loss: 0.000000602 in Epoch 2582
Epoch 2823
Epoch 2823, Loss: 0.000000922, Improvement: -0.000000165, Best Loss: 0.000000602 in Epoch 2582
Epoch 2824
Epoch 2824, Loss: 0.000000924, Improvement: 0.000000002, Best Loss: 0.000000602 in Epoch 2582
Epoch 2825
Epoch 2825, Loss: 0.000000901, Improvement: -0.000000023, Best Loss: 0.000000602 in Epoch 2582
Epoch 2826
Epoch 2826, Loss: 0.000000832, Improvement: -0.000000070, Best Loss: 0.000000602 in Epoch 2582
Epoch 2827
Epoch 2827, Loss: 0.000000844, Improvement: 0.000000012, Best Loss: 0.000000602 in Epoch 2582
Epoch 2828
Epoch 2828, Loss: 0.000000854, Improvement: 0.000000010, Best Loss: 0.000000602 in Epoch 2582
Epoch 2829
Epoch 2829, Loss: 0.000000825, Improvement: -0.000000029, Best Loss: 0.000000602 in Epoch 2582
Epoch 2830
A best model at epoch 2830 has been saved with training error 0.000000570.
Epoch 2830, Loss: 0.000000784, Improvement: -0.000000041, Best Loss: 0.000000570 in Epoch 2830
Epoch 2831
A best model at epoch 2831 has been saved with training error 0.000000550.
Epoch 2831, Loss: 0.000000788, Improvement: 0.000000004, Best Loss: 0.000000550 in Epoch 2831
Epoch 2832
A best model at epoch 2832 has been saved with training error 0.000000504.
Epoch 2832, Loss: 0.000000817, Improvement: 0.000000030, Best Loss: 0.000000504 in Epoch 2832
Epoch 2833
Epoch 2833, Loss: 0.000000898, Improvement: 0.000000081, Best Loss: 0.000000504 in Epoch 2832
Epoch 2834
Epoch 2834, Loss: 0.000000854, Improvement: -0.000000044, Best Loss: 0.000000504 in Epoch 2832
Epoch 2835
Epoch 2835, Loss: 0.000000776, Improvement: -0.000000078, Best Loss: 0.000000504 in Epoch 2832
Epoch 2836
Epoch 2836, Loss: 0.000000752, Improvement: -0.000000024, Best Loss: 0.000000504 in Epoch 2832
Epoch 2837
Epoch 2837, Loss: 0.000000732, Improvement: -0.000000020, Best Loss: 0.000000504 in Epoch 2832
Epoch 2838
Epoch 2838, Loss: 0.000000778, Improvement: 0.000000046, Best Loss: 0.000000504 in Epoch 2832
Epoch 2839
Epoch 2839, Loss: 0.000000759, Improvement: -0.000000019, Best Loss: 0.000000504 in Epoch 2832
Epoch 2840
Epoch 2840, Loss: 0.000000754, Improvement: -0.000000004, Best Loss: 0.000000504 in Epoch 2832
Epoch 2841
Epoch 2841, Loss: 0.000000733, Improvement: -0.000000021, Best Loss: 0.000000504 in Epoch 2832
Epoch 2842
Epoch 2842, Loss: 0.000000773, Improvement: 0.000000039, Best Loss: 0.000000504 in Epoch 2832
Epoch 2843
Epoch 2843, Loss: 0.000000851, Improvement: 0.000000078, Best Loss: 0.000000504 in Epoch 2832
Epoch 2844
Epoch 2844, Loss: 0.000001074, Improvement: 0.000000223, Best Loss: 0.000000504 in Epoch 2832
Epoch 2845
Epoch 2845, Loss: 0.000001277, Improvement: 0.000000204, Best Loss: 0.000000504 in Epoch 2832
Epoch 2846
Epoch 2846, Loss: 0.000001386, Improvement: 0.000000108, Best Loss: 0.000000504 in Epoch 2832
Epoch 2847
Epoch 2847, Loss: 0.000001249, Improvement: -0.000000137, Best Loss: 0.000000504 in Epoch 2832
Epoch 2848
Epoch 2848, Loss: 0.000001239, Improvement: -0.000000010, Best Loss: 0.000000504 in Epoch 2832
Epoch 2849
Epoch 2849, Loss: 0.000000987, Improvement: -0.000000252, Best Loss: 0.000000504 in Epoch 2832
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.000001793, Improvement: 0.000000806, Best Loss: 0.000000504 in Epoch 2832
Epoch 2851
Epoch 2851, Loss: 0.000002228, Improvement: 0.000000435, Best Loss: 0.000000504 in Epoch 2832
Epoch 2852
Epoch 2852, Loss: 0.000004988, Improvement: 0.000002760, Best Loss: 0.000000504 in Epoch 2832
Epoch 2853
Epoch 2853, Loss: 0.000002607, Improvement: -0.000002381, Best Loss: 0.000000504 in Epoch 2832
Epoch 2854
Epoch 2854, Loss: 0.000001710, Improvement: -0.000000897, Best Loss: 0.000000504 in Epoch 2832
Epoch 2855
Epoch 2855, Loss: 0.000001688, Improvement: -0.000000023, Best Loss: 0.000000504 in Epoch 2832
Epoch 2856
Epoch 2856, Loss: 0.000002209, Improvement: 0.000000521, Best Loss: 0.000000504 in Epoch 2832
Epoch 2857
Epoch 2857, Loss: 0.000002454, Improvement: 0.000000246, Best Loss: 0.000000504 in Epoch 2832
Epoch 2858
Epoch 2858, Loss: 0.000004980, Improvement: 0.000002525, Best Loss: 0.000000504 in Epoch 2832
Epoch 2859
Epoch 2859, Loss: 0.000005610, Improvement: 0.000000630, Best Loss: 0.000000504 in Epoch 2832
Epoch 2860
Epoch 2860, Loss: 0.000005570, Improvement: -0.000000040, Best Loss: 0.000000504 in Epoch 2832
Epoch 2861
Epoch 2861, Loss: 0.000005556, Improvement: -0.000000014, Best Loss: 0.000000504 in Epoch 2832
Epoch 2862
Epoch 2862, Loss: 0.000005756, Improvement: 0.000000201, Best Loss: 0.000000504 in Epoch 2832
Epoch 2863
Epoch 2863, Loss: 0.000004293, Improvement: -0.000001464, Best Loss: 0.000000504 in Epoch 2832
Epoch 2864
Epoch 2864, Loss: 0.000005683, Improvement: 0.000001390, Best Loss: 0.000000504 in Epoch 2832
Epoch 2865
Epoch 2865, Loss: 0.000014263, Improvement: 0.000008580, Best Loss: 0.000000504 in Epoch 2832
Epoch 2866
Epoch 2866, Loss: 0.000007259, Improvement: -0.000007004, Best Loss: 0.000000504 in Epoch 2832
Epoch 2867
Epoch 2867, Loss: 0.000003945, Improvement: -0.000003314, Best Loss: 0.000000504 in Epoch 2832
Epoch 2868
Epoch 2868, Loss: 0.000001959, Improvement: -0.000001987, Best Loss: 0.000000504 in Epoch 2832
Epoch 2869
Epoch 2869, Loss: 0.000001255, Improvement: -0.000000704, Best Loss: 0.000000504 in Epoch 2832
Epoch 2870
Epoch 2870, Loss: 0.000001286, Improvement: 0.000000031, Best Loss: 0.000000504 in Epoch 2832
Epoch 2871
Epoch 2871, Loss: 0.000002562, Improvement: 0.000001276, Best Loss: 0.000000504 in Epoch 2832
Epoch 2872
Epoch 2872, Loss: 0.000002542, Improvement: -0.000000020, Best Loss: 0.000000504 in Epoch 2832
Epoch 2873
Epoch 2873, Loss: 0.000003615, Improvement: 0.000001074, Best Loss: 0.000000504 in Epoch 2832
Epoch 2874
Epoch 2874, Loss: 0.000007291, Improvement: 0.000003676, Best Loss: 0.000000504 in Epoch 2832
Epoch 2875
Epoch 2875, Loss: 0.000018514, Improvement: 0.000011223, Best Loss: 0.000000504 in Epoch 2832
Epoch 2876
Epoch 2876, Loss: 0.000020546, Improvement: 0.000002032, Best Loss: 0.000000504 in Epoch 2832
Epoch 2877
Epoch 2877, Loss: 0.000015489, Improvement: -0.000005058, Best Loss: 0.000000504 in Epoch 2832
Epoch 2878
Epoch 2878, Loss: 0.000006148, Improvement: -0.000009341, Best Loss: 0.000000504 in Epoch 2832
Epoch 2879
Epoch 2879, Loss: 0.000002736, Improvement: -0.000003412, Best Loss: 0.000000504 in Epoch 2832
Epoch 2880
Epoch 2880, Loss: 0.000001814, Improvement: -0.000000922, Best Loss: 0.000000504 in Epoch 2832
Epoch 2881
Epoch 2881, Loss: 0.000002316, Improvement: 0.000000502, Best Loss: 0.000000504 in Epoch 2832
Epoch 2882
Epoch 2882, Loss: 0.000001969, Improvement: -0.000000347, Best Loss: 0.000000504 in Epoch 2832
Epoch 2883
Epoch 2883, Loss: 0.000001632, Improvement: -0.000000337, Best Loss: 0.000000504 in Epoch 2832
Epoch 2884
Epoch 2884, Loss: 0.000001504, Improvement: -0.000000128, Best Loss: 0.000000504 in Epoch 2832
Epoch 2885
Epoch 2885, Loss: 0.000001860, Improvement: 0.000000356, Best Loss: 0.000000504 in Epoch 2832
Epoch 2886
Epoch 2886, Loss: 0.000001307, Improvement: -0.000000553, Best Loss: 0.000000504 in Epoch 2832
Epoch 2887
Epoch 2887, Loss: 0.000001149, Improvement: -0.000000158, Best Loss: 0.000000504 in Epoch 2832
Epoch 2888
Epoch 2888, Loss: 0.000000894, Improvement: -0.000000255, Best Loss: 0.000000504 in Epoch 2832
Epoch 2889
Epoch 2889, Loss: 0.000001502, Improvement: 0.000000608, Best Loss: 0.000000504 in Epoch 2832
Epoch 2890
Epoch 2890, Loss: 0.000001220, Improvement: -0.000000282, Best Loss: 0.000000504 in Epoch 2832
Epoch 2891
Epoch 2891, Loss: 0.000001481, Improvement: 0.000000260, Best Loss: 0.000000504 in Epoch 2832
Epoch 2892
Epoch 2892, Loss: 0.000001263, Improvement: -0.000000217, Best Loss: 0.000000504 in Epoch 2832
Epoch 2893
Epoch 2893, Loss: 0.000001158, Improvement: -0.000000105, Best Loss: 0.000000504 in Epoch 2832
Epoch 2894
Epoch 2894, Loss: 0.000001311, Improvement: 0.000000153, Best Loss: 0.000000504 in Epoch 2832
Epoch 2895
Epoch 2895, Loss: 0.000001210, Improvement: -0.000000101, Best Loss: 0.000000504 in Epoch 2832
Epoch 2896
Epoch 2896, Loss: 0.000001803, Improvement: 0.000000592, Best Loss: 0.000000504 in Epoch 2832
Epoch 2897
Epoch 2897, Loss: 0.000003619, Improvement: 0.000001816, Best Loss: 0.000000504 in Epoch 2832
Epoch 2898
Epoch 2898, Loss: 0.000003678, Improvement: 0.000000059, Best Loss: 0.000000504 in Epoch 2832
Epoch 2899
Epoch 2899, Loss: 0.000007905, Improvement: 0.000004227, Best Loss: 0.000000504 in Epoch 2832
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.000008114, Improvement: 0.000000208, Best Loss: 0.000000504 in Epoch 2832
Epoch 2901
Epoch 2901, Loss: 0.000008641, Improvement: 0.000000528, Best Loss: 0.000000504 in Epoch 2832
Epoch 2902
Epoch 2902, Loss: 0.000008941, Improvement: 0.000000300, Best Loss: 0.000000504 in Epoch 2832
Epoch 2903
Epoch 2903, Loss: 0.000016586, Improvement: 0.000007644, Best Loss: 0.000000504 in Epoch 2832
Epoch 2904
Epoch 2904, Loss: 0.000012210, Improvement: -0.000004375, Best Loss: 0.000000504 in Epoch 2832
Epoch 2905
Epoch 2905, Loss: 0.000009462, Improvement: -0.000002748, Best Loss: 0.000000504 in Epoch 2832
Epoch 2906
Epoch 2906, Loss: 0.000007807, Improvement: -0.000001655, Best Loss: 0.000000504 in Epoch 2832
Epoch 2907
Epoch 2907, Loss: 0.000004787, Improvement: -0.000003020, Best Loss: 0.000000504 in Epoch 2832
Epoch 2908
Epoch 2908, Loss: 0.000004081, Improvement: -0.000000706, Best Loss: 0.000000504 in Epoch 2832
Epoch 2909
Epoch 2909, Loss: 0.000009946, Improvement: 0.000005864, Best Loss: 0.000000504 in Epoch 2832
Epoch 2910
Epoch 2910, Loss: 0.000008550, Improvement: -0.000001396, Best Loss: 0.000000504 in Epoch 2832
Epoch 2911
Epoch 2911, Loss: 0.000005229, Improvement: -0.000003321, Best Loss: 0.000000504 in Epoch 2832
Epoch 2912
Epoch 2912, Loss: 0.000002814, Improvement: -0.000002415, Best Loss: 0.000000504 in Epoch 2832
Epoch 2913
Epoch 2913, Loss: 0.000002435, Improvement: -0.000000379, Best Loss: 0.000000504 in Epoch 2832
Epoch 2914
Epoch 2914, Loss: 0.000002981, Improvement: 0.000000546, Best Loss: 0.000000504 in Epoch 2832
Epoch 2915
Epoch 2915, Loss: 0.000002708, Improvement: -0.000000273, Best Loss: 0.000000504 in Epoch 2832
Epoch 2916
Epoch 2916, Loss: 0.000002298, Improvement: -0.000000410, Best Loss: 0.000000504 in Epoch 2832
Epoch 2917
Epoch 2917, Loss: 0.000003132, Improvement: 0.000000834, Best Loss: 0.000000504 in Epoch 2832
Epoch 2918
Epoch 2918, Loss: 0.000004342, Improvement: 0.000001210, Best Loss: 0.000000504 in Epoch 2832
Epoch 2919
Epoch 2919, Loss: 0.000005400, Improvement: 0.000001058, Best Loss: 0.000000504 in Epoch 2832
Epoch 2920
Epoch 2920, Loss: 0.000013327, Improvement: 0.000007927, Best Loss: 0.000000504 in Epoch 2832
Epoch 2921
Epoch 2921, Loss: 0.000008857, Improvement: -0.000004470, Best Loss: 0.000000504 in Epoch 2832
Epoch 2922
Epoch 2922, Loss: 0.000013591, Improvement: 0.000004735, Best Loss: 0.000000504 in Epoch 2832
Epoch 2923
Epoch 2923, Loss: 0.000006126, Improvement: -0.000007465, Best Loss: 0.000000504 in Epoch 2832
Epoch 2924
Epoch 2924, Loss: 0.000003806, Improvement: -0.000002320, Best Loss: 0.000000504 in Epoch 2832
Epoch 2925
Epoch 2925, Loss: 0.000002986, Improvement: -0.000000820, Best Loss: 0.000000504 in Epoch 2832
Epoch 2926
Epoch 2926, Loss: 0.000001900, Improvement: -0.000001086, Best Loss: 0.000000504 in Epoch 2832
Epoch 2927
Epoch 2927, Loss: 0.000001350, Improvement: -0.000000551, Best Loss: 0.000000504 in Epoch 2832
Epoch 2928
Epoch 2928, Loss: 0.000001252, Improvement: -0.000000097, Best Loss: 0.000000504 in Epoch 2832
Epoch 2929
Epoch 2929, Loss: 0.000001143, Improvement: -0.000000110, Best Loss: 0.000000504 in Epoch 2832
Epoch 2930
Epoch 2930, Loss: 0.000002319, Improvement: 0.000001176, Best Loss: 0.000000504 in Epoch 2832
Epoch 2931
Epoch 2931, Loss: 0.000003408, Improvement: 0.000001089, Best Loss: 0.000000504 in Epoch 2832
Epoch 2932
Epoch 2932, Loss: 0.000002840, Improvement: -0.000000568, Best Loss: 0.000000504 in Epoch 2832
Epoch 2933
Epoch 2933, Loss: 0.000002758, Improvement: -0.000000082, Best Loss: 0.000000504 in Epoch 2832
Epoch 2934
Epoch 2934, Loss: 0.000006051, Improvement: 0.000003292, Best Loss: 0.000000504 in Epoch 2832
Epoch 2935
Epoch 2935, Loss: 0.000005617, Improvement: -0.000000434, Best Loss: 0.000000504 in Epoch 2832
Epoch 2936
Epoch 2936, Loss: 0.000018976, Improvement: 0.000013358, Best Loss: 0.000000504 in Epoch 2832
Epoch 2937
Epoch 2937, Loss: 0.000016589, Improvement: -0.000002387, Best Loss: 0.000000504 in Epoch 2832
Epoch 2938
Epoch 2938, Loss: 0.000008569, Improvement: -0.000008021, Best Loss: 0.000000504 in Epoch 2832
Epoch 2939
Epoch 2939, Loss: 0.000004636, Improvement: -0.000003932, Best Loss: 0.000000504 in Epoch 2832
Epoch 2940
Epoch 2940, Loss: 0.000002975, Improvement: -0.000001661, Best Loss: 0.000000504 in Epoch 2832
Epoch 2941
Epoch 2941, Loss: 0.000002229, Improvement: -0.000000746, Best Loss: 0.000000504 in Epoch 2832
Epoch 2942
Epoch 2942, Loss: 0.000004110, Improvement: 0.000001881, Best Loss: 0.000000504 in Epoch 2832
Epoch 2943
Epoch 2943, Loss: 0.000003925, Improvement: -0.000000185, Best Loss: 0.000000504 in Epoch 2832
Epoch 2944
Epoch 2944, Loss: 0.000002567, Improvement: -0.000001358, Best Loss: 0.000000504 in Epoch 2832
Epoch 2945
Epoch 2945, Loss: 0.000002636, Improvement: 0.000000069, Best Loss: 0.000000504 in Epoch 2832
Epoch 2946
Epoch 2946, Loss: 0.000002673, Improvement: 0.000000037, Best Loss: 0.000000504 in Epoch 2832
Epoch 2947
Epoch 2947, Loss: 0.000001586, Improvement: -0.000001087, Best Loss: 0.000000504 in Epoch 2832
Epoch 2948
Epoch 2948, Loss: 0.000001309, Improvement: -0.000000277, Best Loss: 0.000000504 in Epoch 2832
Epoch 2949
Epoch 2949, Loss: 0.000001458, Improvement: 0.000000149, Best Loss: 0.000000504 in Epoch 2832
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.000007045, Improvement: 0.000005587, Best Loss: 0.000000504 in Epoch 2832
Epoch 2951
Epoch 2951, Loss: 0.000005548, Improvement: -0.000001497, Best Loss: 0.000000504 in Epoch 2832
Epoch 2952
Epoch 2952, Loss: 0.000004434, Improvement: -0.000001113, Best Loss: 0.000000504 in Epoch 2832
Epoch 2953
Epoch 2953, Loss: 0.000005081, Improvement: 0.000000647, Best Loss: 0.000000504 in Epoch 2832
Epoch 2954
Epoch 2954, Loss: 0.000008244, Improvement: 0.000003163, Best Loss: 0.000000504 in Epoch 2832
Epoch 2955
Epoch 2955, Loss: 0.000013317, Improvement: 0.000005073, Best Loss: 0.000000504 in Epoch 2832
Epoch 2956
Epoch 2956, Loss: 0.000009521, Improvement: -0.000003796, Best Loss: 0.000000504 in Epoch 2832
Epoch 2957
Epoch 2957, Loss: 0.000006576, Improvement: -0.000002945, Best Loss: 0.000000504 in Epoch 2832
Epoch 2958
Epoch 2958, Loss: 0.000005614, Improvement: -0.000000962, Best Loss: 0.000000504 in Epoch 2832
Epoch 2959
Epoch 2959, Loss: 0.000008366, Improvement: 0.000002753, Best Loss: 0.000000504 in Epoch 2832
Epoch 2960
Epoch 2960, Loss: 0.000006950, Improvement: -0.000001417, Best Loss: 0.000000504 in Epoch 2832
Epoch 2961
Epoch 2961, Loss: 0.000009081, Improvement: 0.000002131, Best Loss: 0.000000504 in Epoch 2832
Epoch 2962
Epoch 2962, Loss: 0.000013252, Improvement: 0.000004171, Best Loss: 0.000000504 in Epoch 2832
Epoch 2963
Epoch 2963, Loss: 0.000004767, Improvement: -0.000008485, Best Loss: 0.000000504 in Epoch 2832
Epoch 2964
Epoch 2964, Loss: 0.000003028, Improvement: -0.000001739, Best Loss: 0.000000504 in Epoch 2832
Epoch 2965
Epoch 2965, Loss: 0.000003143, Improvement: 0.000000115, Best Loss: 0.000000504 in Epoch 2832
Epoch 2966
Epoch 2966, Loss: 0.000003109, Improvement: -0.000000035, Best Loss: 0.000000504 in Epoch 2832
Epoch 2967
Epoch 2967, Loss: 0.000002853, Improvement: -0.000000256, Best Loss: 0.000000504 in Epoch 2832
Epoch 2968
Epoch 2968, Loss: 0.000002130, Improvement: -0.000000723, Best Loss: 0.000000504 in Epoch 2832
Epoch 2969
Epoch 2969, Loss: 0.000003149, Improvement: 0.000001020, Best Loss: 0.000000504 in Epoch 2832
Epoch 2970
Epoch 2970, Loss: 0.000006743, Improvement: 0.000003593, Best Loss: 0.000000504 in Epoch 2832
Epoch 2971
Epoch 2971, Loss: 0.000005810, Improvement: -0.000000933, Best Loss: 0.000000504 in Epoch 2832
Epoch 2972
Epoch 2972, Loss: 0.000014023, Improvement: 0.000008213, Best Loss: 0.000000504 in Epoch 2832
Epoch 2973
Epoch 2973, Loss: 0.000006715, Improvement: -0.000007308, Best Loss: 0.000000504 in Epoch 2832
Epoch 2974
Epoch 2974, Loss: 0.000003966, Improvement: -0.000002750, Best Loss: 0.000000504 in Epoch 2832
Epoch 2975
Epoch 2975, Loss: 0.000002610, Improvement: -0.000001356, Best Loss: 0.000000504 in Epoch 2832
Epoch 2976
Epoch 2976, Loss: 0.000002505, Improvement: -0.000000105, Best Loss: 0.000000504 in Epoch 2832
Epoch 2977
Epoch 2977, Loss: 0.000005827, Improvement: 0.000003322, Best Loss: 0.000000504 in Epoch 2832
Epoch 2978
Epoch 2978, Loss: 0.000011283, Improvement: 0.000005455, Best Loss: 0.000000504 in Epoch 2832
Epoch 2979
Epoch 2979, Loss: 0.000007043, Improvement: -0.000004240, Best Loss: 0.000000504 in Epoch 2832
Epoch 2980
Epoch 2980, Loss: 0.000008870, Improvement: 0.000001827, Best Loss: 0.000000504 in Epoch 2832
Epoch 2981
Epoch 2981, Loss: 0.000018855, Improvement: 0.000009986, Best Loss: 0.000000504 in Epoch 2832
Epoch 2982
Epoch 2982, Loss: 0.000008509, Improvement: -0.000010346, Best Loss: 0.000000504 in Epoch 2832
Epoch 2983
Epoch 2983, Loss: 0.000005109, Improvement: -0.000003399, Best Loss: 0.000000504 in Epoch 2832
Epoch 2984
Epoch 2984, Loss: 0.000009670, Improvement: 0.000004561, Best Loss: 0.000000504 in Epoch 2832
Epoch 2985
Epoch 2985, Loss: 0.000010997, Improvement: 0.000001327, Best Loss: 0.000000504 in Epoch 2832
Epoch 2986
Epoch 2986, Loss: 0.000008831, Improvement: -0.000002166, Best Loss: 0.000000504 in Epoch 2832
Epoch 2987
Epoch 2987, Loss: 0.000003160, Improvement: -0.000005671, Best Loss: 0.000000504 in Epoch 2832
Epoch 2988
Epoch 2988, Loss: 0.000002031, Improvement: -0.000001129, Best Loss: 0.000000504 in Epoch 2832
Epoch 2989
Epoch 2989, Loss: 0.000001140, Improvement: -0.000000891, Best Loss: 0.000000504 in Epoch 2832
Epoch 2990
Epoch 2990, Loss: 0.000000914, Improvement: -0.000000227, Best Loss: 0.000000504 in Epoch 2832
Epoch 2991
Epoch 2991, Loss: 0.000000792, Improvement: -0.000000122, Best Loss: 0.000000504 in Epoch 2832
Epoch 2992
Epoch 2992, Loss: 0.000000776, Improvement: -0.000000016, Best Loss: 0.000000504 in Epoch 2832
Epoch 2993
Epoch 2993, Loss: 0.000000797, Improvement: 0.000000021, Best Loss: 0.000000504 in Epoch 2832
Epoch 2994
Epoch 2994, Loss: 0.000001034, Improvement: 0.000000237, Best Loss: 0.000000504 in Epoch 2832
Epoch 2995
Epoch 2995, Loss: 0.000000899, Improvement: -0.000000135, Best Loss: 0.000000504 in Epoch 2832
Epoch 2996
Epoch 2996, Loss: 0.000000923, Improvement: 0.000000024, Best Loss: 0.000000504 in Epoch 2832
Epoch 2997
Epoch 2997, Loss: 0.000001092, Improvement: 0.000000168, Best Loss: 0.000000504 in Epoch 2832
Epoch 2998
Epoch 2998, Loss: 0.000001001, Improvement: -0.000000091, Best Loss: 0.000000504 in Epoch 2832
Epoch 2999
Epoch 2999, Loss: 0.000001022, Improvement: 0.000000022, Best Loss: 0.000000504 in Epoch 2832
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.000002719, Improvement: 0.000001697, Best Loss: 0.000000504 in Epoch 2832
