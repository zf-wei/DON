The dimension of y_tensor is torch.Size([5000, 2]).
The dimension of y_expanded is torch.Size([500, 5000, 2]) after expanding.
The dimensions of the initial conditions are: (500, 50)
The dimensions of the solutions are: (500, 100, 50)
The dimension of u_tensor is torch.Size([500, 50]).
The dimension of u_expanded is torch.Size([500, 5000, 50]) after expanding.
torch.Size([500, 5000, 1])
The loaded solution dataset has dimension (500, 100, 50),
	 while the arranged linearized dataset has dimension (500, 5000).
The dimension of s_tensor is torch.Size([500, 5000]).
The dimension of s_expanded is torch.Size([500, 5000, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.016071063.
A best model at epoch 1 has been saved with training error 0.014331384.
A best model at epoch 1 has been saved with training error 0.012770327.
A best model at epoch 1 has been saved with training error 0.010833593.
A best model at epoch 1 has been saved with training error 0.010447389.
A best model at epoch 1 has been saved with training error 0.009867419.
A best model at epoch 1 has been saved with training error 0.008084899.
A best model at epoch 1 has been saved with training error 0.007943470.
A best model at epoch 1 has been saved with training error 0.007738974.
Epoch 1, Loss: 0.015376271, Improvement: 0.015376271, Best Loss: 0.007738974 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.006400247.
Epoch 2, Loss: 0.010073776, Improvement: -0.005302495, Best Loss: 0.006400247 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.006244446.
A best model at epoch 3 has been saved with training error 0.005441160.
Epoch 3, Loss: 0.009163054, Improvement: -0.000910722, Best Loss: 0.005441160 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.004051328.
Epoch 4, Loss: 0.008672572, Improvement: -0.000490483, Best Loss: 0.004051328 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.003625237.
Epoch 5, Loss: 0.008082889, Improvement: -0.000589682, Best Loss: 0.003625237 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.006693817, Improvement: -0.001389072, Best Loss: 0.003625237 in Epoch 5
Epoch 7
A best model at epoch 7 has been saved with training error 0.003574595.
A best model at epoch 7 has been saved with training error 0.003551781.
A best model at epoch 7 has been saved with training error 0.003390253.
Epoch 7, Loss: 0.006452642, Improvement: -0.000241175, Best Loss: 0.003390253 in Epoch 7
Epoch 8
A best model at epoch 8 has been saved with training error 0.003333880.
Epoch 8, Loss: 0.005557681, Improvement: -0.000894961, Best Loss: 0.003333880 in Epoch 8
Epoch 9
A best model at epoch 9 has been saved with training error 0.003251990.
A best model at epoch 9 has been saved with training error 0.002778881.
Epoch 9, Loss: 0.004488067, Improvement: -0.001069614, Best Loss: 0.002778881 in Epoch 9
Epoch 10
A best model at epoch 10 has been saved with training error 0.002380599.
Epoch 10, Loss: 0.004113374, Improvement: -0.000374692, Best Loss: 0.002380599 in Epoch 10
Epoch 11
Epoch 11, Loss: 0.003906764, Improvement: -0.000206610, Best Loss: 0.002380599 in Epoch 10
Epoch 12
A best model at epoch 12 has been saved with training error 0.002229131.
Epoch 12, Loss: 0.003772584, Improvement: -0.000134180, Best Loss: 0.002229131 in Epoch 12
Epoch 13
A best model at epoch 13 has been saved with training error 0.001909957.
Epoch 13, Loss: 0.003681628, Improvement: -0.000090956, Best Loss: 0.001909957 in Epoch 13
Epoch 14
Epoch 14, Loss: 0.003631539, Improvement: -0.000050089, Best Loss: 0.001909957 in Epoch 13
Epoch 15
Epoch 15, Loss: 0.003556569, Improvement: -0.000074971, Best Loss: 0.001909957 in Epoch 13
Epoch 16
Epoch 16, Loss: 0.003513605, Improvement: -0.000042964, Best Loss: 0.001909957 in Epoch 13
Epoch 17
Epoch 17, Loss: 0.003401683, Improvement: -0.000111922, Best Loss: 0.001909957 in Epoch 13
Epoch 18
Epoch 18, Loss: 0.003398200, Improvement: -0.000003483, Best Loss: 0.001909957 in Epoch 13
Epoch 19
Epoch 19, Loss: 0.003340763, Improvement: -0.000057437, Best Loss: 0.001909957 in Epoch 13
Epoch 20
Epoch 20, Loss: 0.003328000, Improvement: -0.000012763, Best Loss: 0.001909957 in Epoch 13
Epoch 21
Epoch 21, Loss: 0.003230269, Improvement: -0.000097731, Best Loss: 0.001909957 in Epoch 13
Epoch 22
Epoch 22, Loss: 0.003104524, Improvement: -0.000125745, Best Loss: 0.001909957 in Epoch 13
Epoch 23
Epoch 23, Loss: 0.003051541, Improvement: -0.000052983, Best Loss: 0.001909957 in Epoch 13
Epoch 24
Epoch 24, Loss: 0.002971513, Improvement: -0.000080028, Best Loss: 0.001909957 in Epoch 13
Epoch 25
Epoch 25, Loss: 0.002885188, Improvement: -0.000086324, Best Loss: 0.001909957 in Epoch 13
Epoch 26
Epoch 26, Loss: 0.002796040, Improvement: -0.000089148, Best Loss: 0.001909957 in Epoch 13
Epoch 27
Epoch 27, Loss: 0.002747682, Improvement: -0.000048358, Best Loss: 0.001909957 in Epoch 13
Epoch 28
A best model at epoch 28 has been saved with training error 0.001783947.
Epoch 28, Loss: 0.002719362, Improvement: -0.000028320, Best Loss: 0.001783947 in Epoch 28
Epoch 29
Epoch 29, Loss: 0.003202077, Improvement: 0.000482714, Best Loss: 0.001783947 in Epoch 28
Epoch 30
A best model at epoch 30 has been saved with training error 0.001647339.
Epoch 30, Loss: 0.002801465, Improvement: -0.000400612, Best Loss: 0.001647339 in Epoch 30
Epoch 31
Epoch 31, Loss: 0.002448356, Improvement: -0.000353109, Best Loss: 0.001647339 in Epoch 30
Epoch 32
Epoch 32, Loss: 0.002582477, Improvement: 0.000134121, Best Loss: 0.001647339 in Epoch 30
Epoch 33
Epoch 33, Loss: 0.002652757, Improvement: 0.000070280, Best Loss: 0.001647339 in Epoch 30
Epoch 34
A best model at epoch 34 has been saved with training error 0.001575475.
Epoch 34, Loss: 0.002233301, Improvement: -0.000419456, Best Loss: 0.001575475 in Epoch 34
Epoch 35
A best model at epoch 35 has been saved with training error 0.001549673.
A best model at epoch 35 has been saved with training error 0.001525425.
A best model at epoch 35 has been saved with training error 0.001219272.
Epoch 35, Loss: 0.002038882, Improvement: -0.000194419, Best Loss: 0.001219272 in Epoch 35
Epoch 36
Epoch 36, Loss: 0.002026062, Improvement: -0.000012820, Best Loss: 0.001219272 in Epoch 35
Epoch 37
Epoch 37, Loss: 0.002189915, Improvement: 0.000163853, Best Loss: 0.001219272 in Epoch 35
Epoch 38
Epoch 38, Loss: 0.002146341, Improvement: -0.000043574, Best Loss: 0.001219272 in Epoch 35
Epoch 39
Epoch 39, Loss: 0.002127901, Improvement: -0.000018440, Best Loss: 0.001219272 in Epoch 35
Epoch 40
Epoch 40, Loss: 0.001917895, Improvement: -0.000210006, Best Loss: 0.001219272 in Epoch 35
Epoch 41
Epoch 41, Loss: 0.001745227, Improvement: -0.000172667, Best Loss: 0.001219272 in Epoch 35
Epoch 42
Epoch 42, Loss: 0.002617255, Improvement: 0.000872028, Best Loss: 0.001219272 in Epoch 35
Epoch 43
Epoch 43, Loss: 0.001926451, Improvement: -0.000690805, Best Loss: 0.001219272 in Epoch 35
Epoch 44
Epoch 44, Loss: 0.001599635, Improvement: -0.000326816, Best Loss: 0.001219272 in Epoch 35
Epoch 45
A best model at epoch 45 has been saved with training error 0.001165543.
A best model at epoch 45 has been saved with training error 0.001149847.
A best model at epoch 45 has been saved with training error 0.001146241.
Epoch 45, Loss: 0.001500845, Improvement: -0.000098789, Best Loss: 0.001146241 in Epoch 45
Epoch 46
A best model at epoch 46 has been saved with training error 0.001092187.
A best model at epoch 46 has been saved with training error 0.001013591.
Epoch 46, Loss: 0.001858183, Improvement: 0.000357337, Best Loss: 0.001013591 in Epoch 46
Epoch 47
Epoch 47, Loss: 0.001695714, Improvement: -0.000162468, Best Loss: 0.001013591 in Epoch 46
Epoch 48
Epoch 48, Loss: 0.001452876, Improvement: -0.000242839, Best Loss: 0.001013591 in Epoch 46
Epoch 49
A best model at epoch 49 has been saved with training error 0.000951975.
Epoch 49, Loss: 0.001380847, Improvement: -0.000072029, Best Loss: 0.000951975 in Epoch 49
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.001503141, Improvement: 0.000122294, Best Loss: 0.000951975 in Epoch 49
Epoch 51
Epoch 51, Loss: 0.001853208, Improvement: 0.000350067, Best Loss: 0.000951975 in Epoch 49
Epoch 52
A best model at epoch 52 has been saved with training error 0.000934140.
Epoch 52, Loss: 0.001377222, Improvement: -0.000475986, Best Loss: 0.000934140 in Epoch 52
Epoch 53
A best model at epoch 53 has been saved with training error 0.000822544.
Epoch 53, Loss: 0.001344328, Improvement: -0.000032894, Best Loss: 0.000822544 in Epoch 53
Epoch 54
A best model at epoch 54 has been saved with training error 0.000760442.
Epoch 54, Loss: 0.001564368, Improvement: 0.000220040, Best Loss: 0.000760442 in Epoch 54
Epoch 55
Epoch 55, Loss: 0.002323658, Improvement: 0.000759290, Best Loss: 0.000760442 in Epoch 54
Epoch 56
Epoch 56, Loss: 0.001848427, Improvement: -0.000475232, Best Loss: 0.000760442 in Epoch 54
Epoch 57
Epoch 57, Loss: 0.001572606, Improvement: -0.000275821, Best Loss: 0.000760442 in Epoch 54
Epoch 58
Epoch 58, Loss: 0.001316477, Improvement: -0.000256129, Best Loss: 0.000760442 in Epoch 54
Epoch 59
Epoch 59, Loss: 0.001196494, Improvement: -0.000119983, Best Loss: 0.000760442 in Epoch 54
Epoch 60
A best model at epoch 60 has been saved with training error 0.000737380.
Epoch 60, Loss: 0.001171061, Improvement: -0.000025433, Best Loss: 0.000737380 in Epoch 60
Epoch 61
A best model at epoch 61 has been saved with training error 0.000642600.
Epoch 61, Loss: 0.001116613, Improvement: -0.000054449, Best Loss: 0.000642600 in Epoch 61
Epoch 62
Epoch 62, Loss: 0.001055358, Improvement: -0.000061255, Best Loss: 0.000642600 in Epoch 61
Epoch 63
Epoch 63, Loss: 0.001046446, Improvement: -0.000008911, Best Loss: 0.000642600 in Epoch 61
Epoch 64
Epoch 64, Loss: 0.001124958, Improvement: 0.000078512, Best Loss: 0.000642600 in Epoch 61
Epoch 65
Epoch 65, Loss: 0.001021340, Improvement: -0.000103618, Best Loss: 0.000642600 in Epoch 61
Epoch 66
Epoch 66, Loss: 0.001250511, Improvement: 0.000229170, Best Loss: 0.000642600 in Epoch 61
Epoch 67
Epoch 67, Loss: 0.001350456, Improvement: 0.000099946, Best Loss: 0.000642600 in Epoch 61
Epoch 68
Epoch 68, Loss: 0.001156824, Improvement: -0.000193632, Best Loss: 0.000642600 in Epoch 61
Epoch 69
Epoch 69, Loss: 0.001091340, Improvement: -0.000065484, Best Loss: 0.000642600 in Epoch 61
Epoch 70
Epoch 70, Loss: 0.001003471, Improvement: -0.000087869, Best Loss: 0.000642600 in Epoch 61
Epoch 71
Epoch 71, Loss: 0.000916084, Improvement: -0.000087387, Best Loss: 0.000642600 in Epoch 61
Epoch 72
A best model at epoch 72 has been saved with training error 0.000637955.
A best model at epoch 72 has been saved with training error 0.000619772.
Epoch 72, Loss: 0.000855534, Improvement: -0.000060550, Best Loss: 0.000619772 in Epoch 72
Epoch 73
A best model at epoch 73 has been saved with training error 0.000591361.
A best model at epoch 73 has been saved with training error 0.000526445.
Epoch 73, Loss: 0.000817388, Improvement: -0.000038146, Best Loss: 0.000526445 in Epoch 73
Epoch 74
Epoch 74, Loss: 0.000850700, Improvement: 0.000033312, Best Loss: 0.000526445 in Epoch 73
Epoch 75
Epoch 75, Loss: 0.001112632, Improvement: 0.000261931, Best Loss: 0.000526445 in Epoch 73
Epoch 76
Epoch 76, Loss: 0.000992768, Improvement: -0.000119863, Best Loss: 0.000526445 in Epoch 73
Epoch 77
Epoch 77, Loss: 0.000935854, Improvement: -0.000056915, Best Loss: 0.000526445 in Epoch 73
Epoch 78
Epoch 78, Loss: 0.001101083, Improvement: 0.000165230, Best Loss: 0.000526445 in Epoch 73
Epoch 79
Epoch 79, Loss: 0.000962205, Improvement: -0.000138878, Best Loss: 0.000526445 in Epoch 73
Epoch 80
A best model at epoch 80 has been saved with training error 0.000380107.
Epoch 80, Loss: 0.000763326, Improvement: -0.000198878, Best Loss: 0.000380107 in Epoch 80
Epoch 81
Epoch 81, Loss: 0.000734934, Improvement: -0.000028393, Best Loss: 0.000380107 in Epoch 80
Epoch 82
Epoch 82, Loss: 0.000793041, Improvement: 0.000058107, Best Loss: 0.000380107 in Epoch 80
Epoch 83
Epoch 83, Loss: 0.000901328, Improvement: 0.000108288, Best Loss: 0.000380107 in Epoch 80
Epoch 84
A best model at epoch 84 has been saved with training error 0.000350819.
Epoch 84, Loss: 0.000778280, Improvement: -0.000123048, Best Loss: 0.000350819 in Epoch 84
Epoch 85
Epoch 85, Loss: 0.000691743, Improvement: -0.000086537, Best Loss: 0.000350819 in Epoch 84
Epoch 86
Epoch 86, Loss: 0.000671863, Improvement: -0.000019880, Best Loss: 0.000350819 in Epoch 84
Epoch 87
Epoch 87, Loss: 0.000699002, Improvement: 0.000027139, Best Loss: 0.000350819 in Epoch 84
Epoch 88
Epoch 88, Loss: 0.000748811, Improvement: 0.000049808, Best Loss: 0.000350819 in Epoch 84
Epoch 89
Epoch 89, Loss: 0.000616999, Improvement: -0.000131812, Best Loss: 0.000350819 in Epoch 84
Epoch 90
Epoch 90, Loss: 0.000590037, Improvement: -0.000026962, Best Loss: 0.000350819 in Epoch 84
Epoch 91
Epoch 91, Loss: 0.001116875, Improvement: 0.000526838, Best Loss: 0.000350819 in Epoch 84
Epoch 92
Epoch 92, Loss: 0.001009423, Improvement: -0.000107451, Best Loss: 0.000350819 in Epoch 84
Epoch 93
Epoch 93, Loss: 0.000695693, Improvement: -0.000313730, Best Loss: 0.000350819 in Epoch 84
Epoch 94
Epoch 94, Loss: 0.000631047, Improvement: -0.000064646, Best Loss: 0.000350819 in Epoch 84
Epoch 95
Epoch 95, Loss: 0.000584482, Improvement: -0.000046566, Best Loss: 0.000350819 in Epoch 84
Epoch 96
A best model at epoch 96 has been saved with training error 0.000348915.
Epoch 96, Loss: 0.000546119, Improvement: -0.000038363, Best Loss: 0.000348915 in Epoch 96
Epoch 97
Epoch 97, Loss: 0.000583305, Improvement: 0.000037186, Best Loss: 0.000348915 in Epoch 96
Epoch 98
Epoch 98, Loss: 0.000597958, Improvement: 0.000014653, Best Loss: 0.000348915 in Epoch 96
Epoch 99
Epoch 99, Loss: 0.001205823, Improvement: 0.000607865, Best Loss: 0.000348915 in Epoch 96
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.000748707, Improvement: -0.000457116, Best Loss: 0.000348915 in Epoch 96
Epoch 101
Epoch 101, Loss: 0.000632140, Improvement: -0.000116566, Best Loss: 0.000348915 in Epoch 96
Epoch 102
Epoch 102, Loss: 0.000513384, Improvement: -0.000118756, Best Loss: 0.000348915 in Epoch 96
Epoch 103
A best model at epoch 103 has been saved with training error 0.000280214.
Epoch 103, Loss: 0.000461127, Improvement: -0.000052258, Best Loss: 0.000280214 in Epoch 103
Epoch 104
Epoch 104, Loss: 0.000469408, Improvement: 0.000008282, Best Loss: 0.000280214 in Epoch 103
Epoch 105
Epoch 105, Loss: 0.000738043, Improvement: 0.000268634, Best Loss: 0.000280214 in Epoch 103
Epoch 106
Epoch 106, Loss: 0.001106282, Improvement: 0.000368239, Best Loss: 0.000280214 in Epoch 103
Epoch 107
Epoch 107, Loss: 0.000600276, Improvement: -0.000506006, Best Loss: 0.000280214 in Epoch 103
Epoch 108
Epoch 108, Loss: 0.000501835, Improvement: -0.000098441, Best Loss: 0.000280214 in Epoch 103
Epoch 109
Epoch 109, Loss: 0.000475280, Improvement: -0.000026555, Best Loss: 0.000280214 in Epoch 103
Epoch 110
Epoch 110, Loss: 0.000446887, Improvement: -0.000028393, Best Loss: 0.000280214 in Epoch 103
Epoch 111
Epoch 111, Loss: 0.000724201, Improvement: 0.000277314, Best Loss: 0.000280214 in Epoch 103
Epoch 112
Epoch 112, Loss: 0.001516565, Improvement: 0.000792364, Best Loss: 0.000280214 in Epoch 103
Epoch 113
Epoch 113, Loss: 0.000903951, Improvement: -0.000612614, Best Loss: 0.000280214 in Epoch 103
Epoch 114
Epoch 114, Loss: 0.000633379, Improvement: -0.000270572, Best Loss: 0.000280214 in Epoch 103
Epoch 115
Epoch 115, Loss: 0.000481488, Improvement: -0.000151891, Best Loss: 0.000280214 in Epoch 103
Epoch 116
Epoch 116, Loss: 0.000415756, Improvement: -0.000065732, Best Loss: 0.000280214 in Epoch 103
Epoch 117
Epoch 117, Loss: 0.000398280, Improvement: -0.000017476, Best Loss: 0.000280214 in Epoch 103
Epoch 118
A best model at epoch 118 has been saved with training error 0.000273756.
A best model at epoch 118 has been saved with training error 0.000270674.
A best model at epoch 118 has been saved with training error 0.000234557.
Epoch 118, Loss: 0.000375392, Improvement: -0.000022887, Best Loss: 0.000234557 in Epoch 118
Epoch 119
Epoch 119, Loss: 0.000387013, Improvement: 0.000011620, Best Loss: 0.000234557 in Epoch 118
Epoch 120
Epoch 120, Loss: 0.000359056, Improvement: -0.000027957, Best Loss: 0.000234557 in Epoch 118
Epoch 121
Epoch 121, Loss: 0.000357230, Improvement: -0.000001826, Best Loss: 0.000234557 in Epoch 118
Epoch 122
Epoch 122, Loss: 0.000345338, Improvement: -0.000011892, Best Loss: 0.000234557 in Epoch 118
Epoch 123
Epoch 123, Loss: 0.000501788, Improvement: 0.000156450, Best Loss: 0.000234557 in Epoch 118
Epoch 124
Epoch 124, Loss: 0.000477665, Improvement: -0.000024123, Best Loss: 0.000234557 in Epoch 118
Epoch 125
Epoch 125, Loss: 0.000553623, Improvement: 0.000075958, Best Loss: 0.000234557 in Epoch 118
Epoch 126
Epoch 126, Loss: 0.000470509, Improvement: -0.000083114, Best Loss: 0.000234557 in Epoch 118
Epoch 127
Epoch 127, Loss: 0.000407128, Improvement: -0.000063381, Best Loss: 0.000234557 in Epoch 118
Epoch 128
A best model at epoch 128 has been saved with training error 0.000231949.
Epoch 128, Loss: 0.000362906, Improvement: -0.000044222, Best Loss: 0.000231949 in Epoch 128
Epoch 129
A best model at epoch 129 has been saved with training error 0.000231415.
A best model at epoch 129 has been saved with training error 0.000208021.
A best model at epoch 129 has been saved with training error 0.000181927.
Epoch 129, Loss: 0.000319011, Improvement: -0.000043895, Best Loss: 0.000181927 in Epoch 129
Epoch 130
Epoch 130, Loss: 0.000421656, Improvement: 0.000102646, Best Loss: 0.000181927 in Epoch 129
Epoch 131
Epoch 131, Loss: 0.001047527, Improvement: 0.000625870, Best Loss: 0.000181927 in Epoch 129
Epoch 132
Epoch 132, Loss: 0.001317530, Improvement: 0.000270003, Best Loss: 0.000181927 in Epoch 129
Epoch 133
Epoch 133, Loss: 0.001041705, Improvement: -0.000275825, Best Loss: 0.000181927 in Epoch 129
Epoch 134
Epoch 134, Loss: 0.000615108, Improvement: -0.000426597, Best Loss: 0.000181927 in Epoch 129
Epoch 135
Epoch 135, Loss: 0.000388586, Improvement: -0.000226522, Best Loss: 0.000181927 in Epoch 129
Epoch 136
Epoch 136, Loss: 0.000292787, Improvement: -0.000095799, Best Loss: 0.000181927 in Epoch 129
Epoch 137
Epoch 137, Loss: 0.000269519, Improvement: -0.000023268, Best Loss: 0.000181927 in Epoch 129
Epoch 138
Epoch 138, Loss: 0.000249096, Improvement: -0.000020423, Best Loss: 0.000181927 in Epoch 129
Epoch 139
Epoch 139, Loss: 0.000251807, Improvement: 0.000002711, Best Loss: 0.000181927 in Epoch 129
Epoch 140
Epoch 140, Loss: 0.000292067, Improvement: 0.000040261, Best Loss: 0.000181927 in Epoch 129
Epoch 141
Epoch 141, Loss: 0.000287579, Improvement: -0.000004489, Best Loss: 0.000181927 in Epoch 129
Epoch 142
A best model at epoch 142 has been saved with training error 0.000177831.
A best model at epoch 142 has been saved with training error 0.000166382.
Epoch 142, Loss: 0.000250729, Improvement: -0.000036850, Best Loss: 0.000166382 in Epoch 142
Epoch 143
Epoch 143, Loss: 0.000220505, Improvement: -0.000030225, Best Loss: 0.000166382 in Epoch 142
Epoch 144
Epoch 144, Loss: 0.000336843, Improvement: 0.000116338, Best Loss: 0.000166382 in Epoch 142
Epoch 145
Epoch 145, Loss: 0.000576093, Improvement: 0.000239251, Best Loss: 0.000166382 in Epoch 142
Epoch 146
Epoch 146, Loss: 0.000447918, Improvement: -0.000128176, Best Loss: 0.000166382 in Epoch 142
Epoch 147
Epoch 147, Loss: 0.000297175, Improvement: -0.000150742, Best Loss: 0.000166382 in Epoch 142
Epoch 148
A best model at epoch 148 has been saved with training error 0.000164782.
Epoch 148, Loss: 0.000303809, Improvement: 0.000006634, Best Loss: 0.000164782 in Epoch 148
Epoch 149
Epoch 149, Loss: 0.000297236, Improvement: -0.000006573, Best Loss: 0.000164782 in Epoch 148
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.000273047, Improvement: -0.000024189, Best Loss: 0.000164782 in Epoch 148
Epoch 151
A best model at epoch 151 has been saved with training error 0.000156811.
Epoch 151, Loss: 0.000277694, Improvement: 0.000004646, Best Loss: 0.000156811 in Epoch 151
Epoch 152
Epoch 152, Loss: 0.000365686, Improvement: 0.000087992, Best Loss: 0.000156811 in Epoch 151
Epoch 153
Epoch 153, Loss: 0.000264375, Improvement: -0.000101311, Best Loss: 0.000156811 in Epoch 151
Epoch 154
Epoch 154, Loss: 0.000296710, Improvement: 0.000032336, Best Loss: 0.000156811 in Epoch 151
Epoch 155
Epoch 155, Loss: 0.000312906, Improvement: 0.000016195, Best Loss: 0.000156811 in Epoch 151
Epoch 156
Epoch 156, Loss: 0.000330813, Improvement: 0.000017907, Best Loss: 0.000156811 in Epoch 151
Epoch 157
Epoch 157, Loss: 0.000283300, Improvement: -0.000047513, Best Loss: 0.000156811 in Epoch 151
Epoch 158
Epoch 158, Loss: 0.000340209, Improvement: 0.000056910, Best Loss: 0.000156811 in Epoch 151
Epoch 159
Epoch 159, Loss: 0.000274944, Improvement: -0.000065266, Best Loss: 0.000156811 in Epoch 151
Epoch 160
Epoch 160, Loss: 0.000294096, Improvement: 0.000019153, Best Loss: 0.000156811 in Epoch 151
Epoch 161
Epoch 161, Loss: 0.000626969, Improvement: 0.000332873, Best Loss: 0.000156811 in Epoch 151
Epoch 162
Epoch 162, Loss: 0.000431546, Improvement: -0.000195423, Best Loss: 0.000156811 in Epoch 151
Epoch 163
Epoch 163, Loss: 0.000278193, Improvement: -0.000153353, Best Loss: 0.000156811 in Epoch 151
Epoch 164
A best model at epoch 164 has been saved with training error 0.000154283.
A best model at epoch 164 has been saved with training error 0.000153350.
Epoch 164, Loss: 0.000213722, Improvement: -0.000064471, Best Loss: 0.000153350 in Epoch 164
Epoch 165
A best model at epoch 165 has been saved with training error 0.000135869.
A best model at epoch 165 has been saved with training error 0.000112034.
Epoch 165, Loss: 0.000191927, Improvement: -0.000021795, Best Loss: 0.000112034 in Epoch 165
Epoch 166
Epoch 166, Loss: 0.000187672, Improvement: -0.000004255, Best Loss: 0.000112034 in Epoch 165
Epoch 167
Epoch 167, Loss: 0.000192949, Improvement: 0.000005278, Best Loss: 0.000112034 in Epoch 165
Epoch 168
Epoch 168, Loss: 0.000286309, Improvement: 0.000093360, Best Loss: 0.000112034 in Epoch 165
Epoch 169
Epoch 169, Loss: 0.000330381, Improvement: 0.000044072, Best Loss: 0.000112034 in Epoch 165
Epoch 170
Epoch 170, Loss: 0.000256397, Improvement: -0.000073984, Best Loss: 0.000112034 in Epoch 165
Epoch 171
Epoch 171, Loss: 0.000260444, Improvement: 0.000004047, Best Loss: 0.000112034 in Epoch 165
Epoch 172
Epoch 172, Loss: 0.000213164, Improvement: -0.000047280, Best Loss: 0.000112034 in Epoch 165
Epoch 173
Epoch 173, Loss: 0.000258651, Improvement: 0.000045487, Best Loss: 0.000112034 in Epoch 165
Epoch 174
Epoch 174, Loss: 0.001265578, Improvement: 0.001006927, Best Loss: 0.000112034 in Epoch 165
Epoch 175
Epoch 175, Loss: 0.000637495, Improvement: -0.000628083, Best Loss: 0.000112034 in Epoch 165
Epoch 176
Epoch 176, Loss: 0.000313509, Improvement: -0.000323986, Best Loss: 0.000112034 in Epoch 165
Epoch 177
Epoch 177, Loss: 0.000223310, Improvement: -0.000090200, Best Loss: 0.000112034 in Epoch 165
Epoch 178
Epoch 178, Loss: 0.000190422, Improvement: -0.000032888, Best Loss: 0.000112034 in Epoch 165
Epoch 179
Epoch 179, Loss: 0.000166252, Improvement: -0.000024170, Best Loss: 0.000112034 in Epoch 165
Epoch 180
Epoch 180, Loss: 0.000156693, Improvement: -0.000009559, Best Loss: 0.000112034 in Epoch 165
Epoch 181
A best model at epoch 181 has been saved with training error 0.000106989.
Epoch 181, Loss: 0.000152627, Improvement: -0.000004066, Best Loss: 0.000106989 in Epoch 181
Epoch 182
Epoch 182, Loss: 0.000150722, Improvement: -0.000001906, Best Loss: 0.000106989 in Epoch 181
Epoch 183
Epoch 183, Loss: 0.000150274, Improvement: -0.000000448, Best Loss: 0.000106989 in Epoch 181
Epoch 184
A best model at epoch 184 has been saved with training error 0.000101466.
Epoch 184, Loss: 0.000145368, Improvement: -0.000004906, Best Loss: 0.000101466 in Epoch 184
Epoch 185
Epoch 185, Loss: 0.000143002, Improvement: -0.000002365, Best Loss: 0.000101466 in Epoch 184
Epoch 186
Epoch 186, Loss: 0.000139261, Improvement: -0.000003741, Best Loss: 0.000101466 in Epoch 184
Epoch 187
A best model at epoch 187 has been saved with training error 0.000100803.
A best model at epoch 187 has been saved with training error 0.000099035.
Epoch 187, Loss: 0.000134609, Improvement: -0.000004652, Best Loss: 0.000099035 in Epoch 187
Epoch 188
Epoch 188, Loss: 0.000179548, Improvement: 0.000044939, Best Loss: 0.000099035 in Epoch 187
Epoch 189
Epoch 189, Loss: 0.000272753, Improvement: 0.000093205, Best Loss: 0.000099035 in Epoch 187
Epoch 190
Epoch 190, Loss: 0.000223576, Improvement: -0.000049177, Best Loss: 0.000099035 in Epoch 187
Epoch 191
Epoch 191, Loss: 0.000161540, Improvement: -0.000062036, Best Loss: 0.000099035 in Epoch 187
Epoch 192
Epoch 192, Loss: 0.000139980, Improvement: -0.000021560, Best Loss: 0.000099035 in Epoch 187
Epoch 193
A best model at epoch 193 has been saved with training error 0.000086500.
Epoch 193, Loss: 0.000142681, Improvement: 0.000002701, Best Loss: 0.000086500 in Epoch 193
Epoch 194
Epoch 194, Loss: 0.000129587, Improvement: -0.000013094, Best Loss: 0.000086500 in Epoch 193
Epoch 195
Epoch 195, Loss: 0.000129232, Improvement: -0.000000355, Best Loss: 0.000086500 in Epoch 193
Epoch 196
Epoch 196, Loss: 0.000204203, Improvement: 0.000074971, Best Loss: 0.000086500 in Epoch 193
Epoch 197
Epoch 197, Loss: 0.000161322, Improvement: -0.000042881, Best Loss: 0.000086500 in Epoch 193
Epoch 198
Epoch 198, Loss: 0.000135185, Improvement: -0.000026137, Best Loss: 0.000086500 in Epoch 193
Epoch 199
Epoch 199, Loss: 0.000134942, Improvement: -0.000000243, Best Loss: 0.000086500 in Epoch 193
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.000133239, Improvement: -0.000001702, Best Loss: 0.000086500 in Epoch 193
Epoch 201
Epoch 201, Loss: 0.000125243, Improvement: -0.000007996, Best Loss: 0.000086500 in Epoch 193
Epoch 202
Epoch 202, Loss: 0.000125124, Improvement: -0.000000119, Best Loss: 0.000086500 in Epoch 193
Epoch 203
Epoch 203, Loss: 0.000148373, Improvement: 0.000023250, Best Loss: 0.000086500 in Epoch 193
Epoch 204
Epoch 204, Loss: 0.000295267, Improvement: 0.000146893, Best Loss: 0.000086500 in Epoch 193
Epoch 205
Epoch 205, Loss: 0.000209317, Improvement: -0.000085949, Best Loss: 0.000086500 in Epoch 193
Epoch 206
Epoch 206, Loss: 0.000201672, Improvement: -0.000007645, Best Loss: 0.000086500 in Epoch 193
Epoch 207
Epoch 207, Loss: 0.000165604, Improvement: -0.000036068, Best Loss: 0.000086500 in Epoch 193
Epoch 208
Epoch 208, Loss: 0.000282672, Improvement: 0.000117067, Best Loss: 0.000086500 in Epoch 193
Epoch 209
Epoch 209, Loss: 0.000362801, Improvement: 0.000080130, Best Loss: 0.000086500 in Epoch 193
Epoch 210
Epoch 210, Loss: 0.000323679, Improvement: -0.000039122, Best Loss: 0.000086500 in Epoch 193
Epoch 211
Epoch 211, Loss: 0.000330505, Improvement: 0.000006826, Best Loss: 0.000086500 in Epoch 193
Epoch 212
Epoch 212, Loss: 0.000226832, Improvement: -0.000103673, Best Loss: 0.000086500 in Epoch 193
Epoch 213
Epoch 213, Loss: 0.000151313, Improvement: -0.000075519, Best Loss: 0.000086500 in Epoch 193
Epoch 214
Epoch 214, Loss: 0.000227882, Improvement: 0.000076569, Best Loss: 0.000086500 in Epoch 193
Epoch 215
Epoch 215, Loss: 0.000297566, Improvement: 0.000069684, Best Loss: 0.000086500 in Epoch 193
Epoch 216
Epoch 216, Loss: 0.000317396, Improvement: 0.000019830, Best Loss: 0.000086500 in Epoch 193
Epoch 217
Epoch 217, Loss: 0.000226923, Improvement: -0.000090473, Best Loss: 0.000086500 in Epoch 193
Epoch 218
Epoch 218, Loss: 0.000298150, Improvement: 0.000071227, Best Loss: 0.000086500 in Epoch 193
Epoch 219
Epoch 219, Loss: 0.000217487, Improvement: -0.000080664, Best Loss: 0.000086500 in Epoch 193
Epoch 220
Epoch 220, Loss: 0.000128212, Improvement: -0.000089275, Best Loss: 0.000086500 in Epoch 193
Epoch 221
Epoch 221, Loss: 0.000130318, Improvement: 0.000002106, Best Loss: 0.000086500 in Epoch 193
Epoch 222
Epoch 222, Loss: 0.000146300, Improvement: 0.000015982, Best Loss: 0.000086500 in Epoch 193
Epoch 223
Epoch 223, Loss: 0.000190129, Improvement: 0.000043829, Best Loss: 0.000086500 in Epoch 193
Epoch 224
Epoch 224, Loss: 0.000230606, Improvement: 0.000040477, Best Loss: 0.000086500 in Epoch 193
Epoch 225
Epoch 225, Loss: 0.000371803, Improvement: 0.000141198, Best Loss: 0.000086500 in Epoch 193
Epoch 226
Epoch 226, Loss: 0.000407221, Improvement: 0.000035418, Best Loss: 0.000086500 in Epoch 193
Epoch 227
Epoch 227, Loss: 0.000264158, Improvement: -0.000143063, Best Loss: 0.000086500 in Epoch 193
Epoch 228
Epoch 228, Loss: 0.000208790, Improvement: -0.000055368, Best Loss: 0.000086500 in Epoch 193
Epoch 229
Epoch 229, Loss: 0.000250688, Improvement: 0.000041897, Best Loss: 0.000086500 in Epoch 193
Epoch 230
Epoch 230, Loss: 0.000307301, Improvement: 0.000056613, Best Loss: 0.000086500 in Epoch 193
Epoch 231
Epoch 231, Loss: 0.000302793, Improvement: -0.000004508, Best Loss: 0.000086500 in Epoch 193
Epoch 232
Epoch 232, Loss: 0.000138919, Improvement: -0.000163873, Best Loss: 0.000086500 in Epoch 193
Epoch 233
Epoch 233, Loss: 0.000119940, Improvement: -0.000018979, Best Loss: 0.000086500 in Epoch 193
Epoch 234
A best model at epoch 234 has been saved with training error 0.000070530.
Epoch 234, Loss: 0.000112340, Improvement: -0.000007601, Best Loss: 0.000070530 in Epoch 234
Epoch 235
Epoch 235, Loss: 0.000107769, Improvement: -0.000004571, Best Loss: 0.000070530 in Epoch 234
Epoch 236
Epoch 236, Loss: 0.000103875, Improvement: -0.000003895, Best Loss: 0.000070530 in Epoch 234
Epoch 237
A best model at epoch 237 has been saved with training error 0.000069132.
Epoch 237, Loss: 0.000104419, Improvement: 0.000000544, Best Loss: 0.000069132 in Epoch 237
Epoch 238
Epoch 238, Loss: 0.000106485, Improvement: 0.000002066, Best Loss: 0.000069132 in Epoch 237
Epoch 239
Epoch 239, Loss: 0.000103237, Improvement: -0.000003249, Best Loss: 0.000069132 in Epoch 237
Epoch 240
Epoch 240, Loss: 0.000119028, Improvement: 0.000015791, Best Loss: 0.000069132 in Epoch 237
Epoch 241
Epoch 241, Loss: 0.000196687, Improvement: 0.000077659, Best Loss: 0.000069132 in Epoch 237
Epoch 242
Epoch 242, Loss: 0.000237440, Improvement: 0.000040753, Best Loss: 0.000069132 in Epoch 237
Epoch 243
Epoch 243, Loss: 0.000183038, Improvement: -0.000054402, Best Loss: 0.000069132 in Epoch 237
Epoch 244
Epoch 244, Loss: 0.000137944, Improvement: -0.000045094, Best Loss: 0.000069132 in Epoch 237
Epoch 245
Epoch 245, Loss: 0.000147478, Improvement: 0.000009533, Best Loss: 0.000069132 in Epoch 237
Epoch 246
Epoch 246, Loss: 0.000114838, Improvement: -0.000032639, Best Loss: 0.000069132 in Epoch 237
Epoch 247
Epoch 247, Loss: 0.000179580, Improvement: 0.000064741, Best Loss: 0.000069132 in Epoch 237
Epoch 248
Epoch 248, Loss: 0.000217296, Improvement: 0.000037716, Best Loss: 0.000069132 in Epoch 237
Epoch 249
Epoch 249, Loss: 0.000138753, Improvement: -0.000078543, Best Loss: 0.000069132 in Epoch 237
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.000110680, Improvement: -0.000028073, Best Loss: 0.000069132 in Epoch 237
Epoch 251
A best model at epoch 251 has been saved with training error 0.000067316.
Epoch 251, Loss: 0.000095804, Improvement: -0.000014875, Best Loss: 0.000067316 in Epoch 251
Epoch 252
A best model at epoch 252 has been saved with training error 0.000060439.
Epoch 252, Loss: 0.000088096, Improvement: -0.000007708, Best Loss: 0.000060439 in Epoch 252
Epoch 253
Epoch 253, Loss: 0.000115432, Improvement: 0.000027335, Best Loss: 0.000060439 in Epoch 252
Epoch 254
Epoch 254, Loss: 0.000125712, Improvement: 0.000010280, Best Loss: 0.000060439 in Epoch 252
Epoch 255
Epoch 255, Loss: 0.000104399, Improvement: -0.000021313, Best Loss: 0.000060439 in Epoch 252
Epoch 256
Epoch 256, Loss: 0.000098674, Improvement: -0.000005725, Best Loss: 0.000060439 in Epoch 252
Epoch 257
Epoch 257, Loss: 0.000090707, Improvement: -0.000007967, Best Loss: 0.000060439 in Epoch 252
Epoch 258
Epoch 258, Loss: 0.000105698, Improvement: 0.000014991, Best Loss: 0.000060439 in Epoch 252
Epoch 259
Epoch 259, Loss: 0.000178470, Improvement: 0.000072772, Best Loss: 0.000060439 in Epoch 252
Epoch 260
Epoch 260, Loss: 0.000216817, Improvement: 0.000038347, Best Loss: 0.000060439 in Epoch 252
Epoch 261
Epoch 261, Loss: 0.000208011, Improvement: -0.000008806, Best Loss: 0.000060439 in Epoch 252
Epoch 262
Epoch 262, Loss: 0.000397556, Improvement: 0.000189545, Best Loss: 0.000060439 in Epoch 252
Epoch 263
Epoch 263, Loss: 0.000227467, Improvement: -0.000170089, Best Loss: 0.000060439 in Epoch 252
Epoch 264
Epoch 264, Loss: 0.000165734, Improvement: -0.000061733, Best Loss: 0.000060439 in Epoch 252
Epoch 265
Epoch 265, Loss: 0.000107525, Improvement: -0.000058209, Best Loss: 0.000060439 in Epoch 252
Epoch 266
Epoch 266, Loss: 0.000090038, Improvement: -0.000017487, Best Loss: 0.000060439 in Epoch 252
Epoch 267
Epoch 267, Loss: 0.000083097, Improvement: -0.000006941, Best Loss: 0.000060439 in Epoch 252
Epoch 268
Epoch 268, Loss: 0.000080255, Improvement: -0.000002842, Best Loss: 0.000060439 in Epoch 252
Epoch 269
A best model at epoch 269 has been saved with training error 0.000057885.
A best model at epoch 269 has been saved with training error 0.000052724.
Epoch 269, Loss: 0.000078089, Improvement: -0.000002165, Best Loss: 0.000052724 in Epoch 269
Epoch 270
Epoch 270, Loss: 0.000077067, Improvement: -0.000001023, Best Loss: 0.000052724 in Epoch 269
Epoch 271
Epoch 271, Loss: 0.000092960, Improvement: 0.000015893, Best Loss: 0.000052724 in Epoch 269
Epoch 272
Epoch 272, Loss: 0.000091251, Improvement: -0.000001709, Best Loss: 0.000052724 in Epoch 269
Epoch 273
Epoch 273, Loss: 0.000164967, Improvement: 0.000073716, Best Loss: 0.000052724 in Epoch 269
Epoch 274
Epoch 274, Loss: 0.000199456, Improvement: 0.000034489, Best Loss: 0.000052724 in Epoch 269
Epoch 275
Epoch 275, Loss: 0.000118080, Improvement: -0.000081376, Best Loss: 0.000052724 in Epoch 269
Epoch 276
Epoch 276, Loss: 0.000093689, Improvement: -0.000024391, Best Loss: 0.000052724 in Epoch 269
Epoch 277
Epoch 277, Loss: 0.000083918, Improvement: -0.000009771, Best Loss: 0.000052724 in Epoch 269
Epoch 278
Epoch 278, Loss: 0.000075520, Improvement: -0.000008397, Best Loss: 0.000052724 in Epoch 269
Epoch 279
Epoch 279, Loss: 0.000080506, Improvement: 0.000004986, Best Loss: 0.000052724 in Epoch 269
Epoch 280
A best model at epoch 280 has been saved with training error 0.000052553.
Epoch 280, Loss: 0.000096750, Improvement: 0.000016244, Best Loss: 0.000052553 in Epoch 280
Epoch 281
Epoch 281, Loss: 0.000307815, Improvement: 0.000211065, Best Loss: 0.000052553 in Epoch 280
Epoch 282
Epoch 282, Loss: 0.000311866, Improvement: 0.000004050, Best Loss: 0.000052553 in Epoch 280
Epoch 283
Epoch 283, Loss: 0.000285948, Improvement: -0.000025918, Best Loss: 0.000052553 in Epoch 280
Epoch 284
Epoch 284, Loss: 0.000200704, Improvement: -0.000085244, Best Loss: 0.000052553 in Epoch 280
Epoch 285
Epoch 285, Loss: 0.000146413, Improvement: -0.000054291, Best Loss: 0.000052553 in Epoch 280
Epoch 286
Epoch 286, Loss: 0.000099166, Improvement: -0.000047247, Best Loss: 0.000052553 in Epoch 280
Epoch 287
Epoch 287, Loss: 0.000088668, Improvement: -0.000010498, Best Loss: 0.000052553 in Epoch 280
Epoch 288
Epoch 288, Loss: 0.000082620, Improvement: -0.000006048, Best Loss: 0.000052553 in Epoch 280
Epoch 289
Epoch 289, Loss: 0.000080758, Improvement: -0.000001862, Best Loss: 0.000052553 in Epoch 280
Epoch 290
A best model at epoch 290 has been saved with training error 0.000050531.
Epoch 290, Loss: 0.000079924, Improvement: -0.000000834, Best Loss: 0.000050531 in Epoch 290
Epoch 291
Epoch 291, Loss: 0.000106709, Improvement: 0.000026785, Best Loss: 0.000050531 in Epoch 290
Epoch 292
Epoch 292, Loss: 0.000108032, Improvement: 0.000001324, Best Loss: 0.000050531 in Epoch 290
Epoch 293
Epoch 293, Loss: 0.000105010, Improvement: -0.000003022, Best Loss: 0.000050531 in Epoch 290
Epoch 294
Epoch 294, Loss: 0.000077502, Improvement: -0.000027508, Best Loss: 0.000050531 in Epoch 290
Epoch 295
Epoch 295, Loss: 0.000081905, Improvement: 0.000004403, Best Loss: 0.000050531 in Epoch 290
Epoch 296
Epoch 296, Loss: 0.000108880, Improvement: 0.000026976, Best Loss: 0.000050531 in Epoch 290
Epoch 297
Epoch 297, Loss: 0.000087165, Improvement: -0.000021715, Best Loss: 0.000050531 in Epoch 290
Epoch 298
Epoch 298, Loss: 0.000076119, Improvement: -0.000011046, Best Loss: 0.000050531 in Epoch 290
Epoch 299
Epoch 299, Loss: 0.000085124, Improvement: 0.000009005, Best Loss: 0.000050531 in Epoch 290
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.000082212, Improvement: -0.000002912, Best Loss: 0.000050531 in Epoch 290
Epoch 301
Epoch 301, Loss: 0.000068370, Improvement: -0.000013842, Best Loss: 0.000050531 in Epoch 290
Epoch 302
Epoch 302, Loss: 0.000079505, Improvement: 0.000011135, Best Loss: 0.000050531 in Epoch 290
Epoch 303
A best model at epoch 303 has been saved with training error 0.000048281.
Epoch 303, Loss: 0.000079733, Improvement: 0.000000228, Best Loss: 0.000048281 in Epoch 303
Epoch 304
Epoch 304, Loss: 0.000140673, Improvement: 0.000060940, Best Loss: 0.000048281 in Epoch 303
Epoch 305
Epoch 305, Loss: 0.000239494, Improvement: 0.000098821, Best Loss: 0.000048281 in Epoch 303
Epoch 306
Epoch 306, Loss: 0.000175325, Improvement: -0.000064169, Best Loss: 0.000048281 in Epoch 303
Epoch 307
Epoch 307, Loss: 0.000118220, Improvement: -0.000057104, Best Loss: 0.000048281 in Epoch 303
Epoch 308
Epoch 308, Loss: 0.000194910, Improvement: 0.000076689, Best Loss: 0.000048281 in Epoch 303
Epoch 309
Epoch 309, Loss: 0.000245671, Improvement: 0.000050762, Best Loss: 0.000048281 in Epoch 303
Epoch 310
Epoch 310, Loss: 0.000129149, Improvement: -0.000116522, Best Loss: 0.000048281 in Epoch 303
Epoch 311
Epoch 311, Loss: 0.000124479, Improvement: -0.000004670, Best Loss: 0.000048281 in Epoch 303
Epoch 312
Epoch 312, Loss: 0.000222592, Improvement: 0.000098113, Best Loss: 0.000048281 in Epoch 303
Epoch 313
Epoch 313, Loss: 0.000193498, Improvement: -0.000029094, Best Loss: 0.000048281 in Epoch 303
Epoch 314
Epoch 314, Loss: 0.000137694, Improvement: -0.000055804, Best Loss: 0.000048281 in Epoch 303
Epoch 315
Epoch 315, Loss: 0.000110147, Improvement: -0.000027547, Best Loss: 0.000048281 in Epoch 303
Epoch 316
Epoch 316, Loss: 0.000087458, Improvement: -0.000022689, Best Loss: 0.000048281 in Epoch 303
Epoch 317
Epoch 317, Loss: 0.000084397, Improvement: -0.000003061, Best Loss: 0.000048281 in Epoch 303
Epoch 318
Epoch 318, Loss: 0.000090755, Improvement: 0.000006357, Best Loss: 0.000048281 in Epoch 303
Epoch 319
Epoch 319, Loss: 0.000097436, Improvement: 0.000006682, Best Loss: 0.000048281 in Epoch 303
Epoch 320
Epoch 320, Loss: 0.000104755, Improvement: 0.000007318, Best Loss: 0.000048281 in Epoch 303
Epoch 321
Epoch 321, Loss: 0.000188498, Improvement: 0.000083743, Best Loss: 0.000048281 in Epoch 303
Epoch 322
Epoch 322, Loss: 0.000212100, Improvement: 0.000023602, Best Loss: 0.000048281 in Epoch 303
Epoch 323
Epoch 323, Loss: 0.000162933, Improvement: -0.000049167, Best Loss: 0.000048281 in Epoch 303
Epoch 324
Epoch 324, Loss: 0.000091778, Improvement: -0.000071154, Best Loss: 0.000048281 in Epoch 303
Epoch 325
Epoch 325, Loss: 0.000076414, Improvement: -0.000015365, Best Loss: 0.000048281 in Epoch 303
Epoch 326
Epoch 326, Loss: 0.000071874, Improvement: -0.000004539, Best Loss: 0.000048281 in Epoch 303
Epoch 327
A best model at epoch 327 has been saved with training error 0.000042005.
A best model at epoch 327 has been saved with training error 0.000041399.
Epoch 327, Loss: 0.000064562, Improvement: -0.000007313, Best Loss: 0.000041399 in Epoch 327
Epoch 328
Epoch 328, Loss: 0.000082807, Improvement: 0.000018245, Best Loss: 0.000041399 in Epoch 327
Epoch 329
Epoch 329, Loss: 0.000079904, Improvement: -0.000002902, Best Loss: 0.000041399 in Epoch 327
Epoch 330
Epoch 330, Loss: 0.000095573, Improvement: 0.000015668, Best Loss: 0.000041399 in Epoch 327
Epoch 331
Epoch 331, Loss: 0.000083365, Improvement: -0.000012208, Best Loss: 0.000041399 in Epoch 327
Epoch 332
Epoch 332, Loss: 0.000066602, Improvement: -0.000016762, Best Loss: 0.000041399 in Epoch 327
Epoch 333
Epoch 333, Loss: 0.000072423, Improvement: 0.000005821, Best Loss: 0.000041399 in Epoch 327
Epoch 334
Epoch 334, Loss: 0.000095316, Improvement: 0.000022893, Best Loss: 0.000041399 in Epoch 327
Epoch 335
Epoch 335, Loss: 0.000094826, Improvement: -0.000000489, Best Loss: 0.000041399 in Epoch 327
Epoch 336
Epoch 336, Loss: 0.000329347, Improvement: 0.000234520, Best Loss: 0.000041399 in Epoch 327
Epoch 337
Epoch 337, Loss: 0.000301381, Improvement: -0.000027965, Best Loss: 0.000041399 in Epoch 327
Epoch 338
Epoch 338, Loss: 0.000220613, Improvement: -0.000080768, Best Loss: 0.000041399 in Epoch 327
Epoch 339
Epoch 339, Loss: 0.000289877, Improvement: 0.000069264, Best Loss: 0.000041399 in Epoch 327
Epoch 340
Epoch 340, Loss: 0.000220825, Improvement: -0.000069052, Best Loss: 0.000041399 in Epoch 327
Epoch 341
Epoch 341, Loss: 0.000119382, Improvement: -0.000101444, Best Loss: 0.000041399 in Epoch 327
Epoch 342
Epoch 342, Loss: 0.000108171, Improvement: -0.000011210, Best Loss: 0.000041399 in Epoch 327
Epoch 343
Epoch 343, Loss: 0.000166744, Improvement: 0.000058573, Best Loss: 0.000041399 in Epoch 327
Epoch 344
Epoch 344, Loss: 0.000165438, Improvement: -0.000001306, Best Loss: 0.000041399 in Epoch 327
Epoch 345
Epoch 345, Loss: 0.000114861, Improvement: -0.000050577, Best Loss: 0.000041399 in Epoch 327
Epoch 346
Epoch 346, Loss: 0.000072981, Improvement: -0.000041880, Best Loss: 0.000041399 in Epoch 327
Epoch 347
Epoch 347, Loss: 0.000062497, Improvement: -0.000010484, Best Loss: 0.000041399 in Epoch 327
Epoch 348
Epoch 348, Loss: 0.000057067, Improvement: -0.000005430, Best Loss: 0.000041399 in Epoch 327
Epoch 349
A best model at epoch 349 has been saved with training error 0.000040367.
Epoch 349, Loss: 0.000053430, Improvement: -0.000003637, Best Loss: 0.000040367 in Epoch 349
Epoch 350
A best model at epoch 350 has been saved with training error 0.000036601.
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.000050909, Improvement: -0.000002521, Best Loss: 0.000036601 in Epoch 350
Epoch 351
A best model at epoch 351 has been saved with training error 0.000029230.
Epoch 351, Loss: 0.000052386, Improvement: 0.000001477, Best Loss: 0.000029230 in Epoch 351
Epoch 352
Epoch 352, Loss: 0.000076834, Improvement: 0.000024448, Best Loss: 0.000029230 in Epoch 351
Epoch 353
Epoch 353, Loss: 0.000144476, Improvement: 0.000067642, Best Loss: 0.000029230 in Epoch 351
Epoch 354
Epoch 354, Loss: 0.000176989, Improvement: 0.000032513, Best Loss: 0.000029230 in Epoch 351
Epoch 355
Epoch 355, Loss: 0.000113676, Improvement: -0.000063312, Best Loss: 0.000029230 in Epoch 351
Epoch 356
Epoch 356, Loss: 0.000094548, Improvement: -0.000019128, Best Loss: 0.000029230 in Epoch 351
Epoch 357
Epoch 357, Loss: 0.000131480, Improvement: 0.000036932, Best Loss: 0.000029230 in Epoch 351
Epoch 358
Epoch 358, Loss: 0.000133531, Improvement: 0.000002051, Best Loss: 0.000029230 in Epoch 351
Epoch 359
Epoch 359, Loss: 0.000076507, Improvement: -0.000057024, Best Loss: 0.000029230 in Epoch 351
Epoch 360
Epoch 360, Loss: 0.000054716, Improvement: -0.000021791, Best Loss: 0.000029230 in Epoch 351
Epoch 361
Epoch 361, Loss: 0.000052152, Improvement: -0.000002564, Best Loss: 0.000029230 in Epoch 351
Epoch 362
Epoch 362, Loss: 0.000056085, Improvement: 0.000003933, Best Loss: 0.000029230 in Epoch 351
Epoch 363
Epoch 363, Loss: 0.000062582, Improvement: 0.000006496, Best Loss: 0.000029230 in Epoch 351
Epoch 364
Epoch 364, Loss: 0.000059799, Improvement: -0.000002782, Best Loss: 0.000029230 in Epoch 351
Epoch 365
Epoch 365, Loss: 0.000069541, Improvement: 0.000009742, Best Loss: 0.000029230 in Epoch 351
Epoch 366
Epoch 366, Loss: 0.000058434, Improvement: -0.000011107, Best Loss: 0.000029230 in Epoch 351
Epoch 367
Epoch 367, Loss: 0.000051284, Improvement: -0.000007151, Best Loss: 0.000029230 in Epoch 351
Epoch 368
Epoch 368, Loss: 0.000118643, Improvement: 0.000067359, Best Loss: 0.000029230 in Epoch 351
Epoch 369
Epoch 369, Loss: 0.000157739, Improvement: 0.000039096, Best Loss: 0.000029230 in Epoch 351
Epoch 370
Epoch 370, Loss: 0.000355279, Improvement: 0.000197540, Best Loss: 0.000029230 in Epoch 351
Epoch 371
Epoch 371, Loss: 0.000184781, Improvement: -0.000170498, Best Loss: 0.000029230 in Epoch 351
Epoch 372
Epoch 372, Loss: 0.000083414, Improvement: -0.000101367, Best Loss: 0.000029230 in Epoch 351
Epoch 373
Epoch 373, Loss: 0.000058357, Improvement: -0.000025057, Best Loss: 0.000029230 in Epoch 351
Epoch 374
Epoch 374, Loss: 0.000049398, Improvement: -0.000008959, Best Loss: 0.000029230 in Epoch 351
Epoch 375
Epoch 375, Loss: 0.000048179, Improvement: -0.000001219, Best Loss: 0.000029230 in Epoch 351
Epoch 376
Epoch 376, Loss: 0.000045075, Improvement: -0.000003104, Best Loss: 0.000029230 in Epoch 351
Epoch 377
Epoch 377, Loss: 0.000068893, Improvement: 0.000023819, Best Loss: 0.000029230 in Epoch 351
Epoch 378
Epoch 378, Loss: 0.000147506, Improvement: 0.000078613, Best Loss: 0.000029230 in Epoch 351
Epoch 379
Epoch 379, Loss: 0.000079548, Improvement: -0.000067959, Best Loss: 0.000029230 in Epoch 351
Epoch 380
Epoch 380, Loss: 0.000053703, Improvement: -0.000025845, Best Loss: 0.000029230 in Epoch 351
Epoch 381
Epoch 381, Loss: 0.000054201, Improvement: 0.000000498, Best Loss: 0.000029230 in Epoch 351
Epoch 382
Epoch 382, Loss: 0.000060670, Improvement: 0.000006469, Best Loss: 0.000029230 in Epoch 351
Epoch 383
Epoch 383, Loss: 0.000088039, Improvement: 0.000027369, Best Loss: 0.000029230 in Epoch 351
Epoch 384
Epoch 384, Loss: 0.000089068, Improvement: 0.000001029, Best Loss: 0.000029230 in Epoch 351
Epoch 385
Epoch 385, Loss: 0.000066845, Improvement: -0.000022223, Best Loss: 0.000029230 in Epoch 351
Epoch 386
Epoch 386, Loss: 0.000054152, Improvement: -0.000012693, Best Loss: 0.000029230 in Epoch 351
Epoch 387
A best model at epoch 387 has been saved with training error 0.000027231.
Epoch 387, Loss: 0.000051565, Improvement: -0.000002588, Best Loss: 0.000027231 in Epoch 387
Epoch 388
Epoch 388, Loss: 0.000098132, Improvement: 0.000046568, Best Loss: 0.000027231 in Epoch 387
Epoch 389
Epoch 389, Loss: 0.000096082, Improvement: -0.000002050, Best Loss: 0.000027231 in Epoch 387
Epoch 390
Epoch 390, Loss: 0.000094664, Improvement: -0.000001419, Best Loss: 0.000027231 in Epoch 387
Epoch 391
Epoch 391, Loss: 0.000063260, Improvement: -0.000031403, Best Loss: 0.000027231 in Epoch 387
Epoch 392
Epoch 392, Loss: 0.000045911, Improvement: -0.000017350, Best Loss: 0.000027231 in Epoch 387
Epoch 393
Epoch 393, Loss: 0.000041330, Improvement: -0.000004581, Best Loss: 0.000027231 in Epoch 387
Epoch 394
Epoch 394, Loss: 0.000037207, Improvement: -0.000004123, Best Loss: 0.000027231 in Epoch 387
Epoch 395
Epoch 395, Loss: 0.000063384, Improvement: 0.000026177, Best Loss: 0.000027231 in Epoch 387
Epoch 396
Epoch 396, Loss: 0.000088403, Improvement: 0.000025019, Best Loss: 0.000027231 in Epoch 387
Epoch 397
Epoch 397, Loss: 0.000060708, Improvement: -0.000027695, Best Loss: 0.000027231 in Epoch 387
Epoch 398
Epoch 398, Loss: 0.000077023, Improvement: 0.000016315, Best Loss: 0.000027231 in Epoch 387
Epoch 399
Epoch 399, Loss: 0.000108215, Improvement: 0.000031192, Best Loss: 0.000027231 in Epoch 387
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.000081427, Improvement: -0.000026787, Best Loss: 0.000027231 in Epoch 387
Epoch 401
Epoch 401, Loss: 0.000156973, Improvement: 0.000075546, Best Loss: 0.000027231 in Epoch 387
Epoch 402
Epoch 402, Loss: 0.000143273, Improvement: -0.000013700, Best Loss: 0.000027231 in Epoch 387
Epoch 403
Epoch 403, Loss: 0.000116348, Improvement: -0.000026925, Best Loss: 0.000027231 in Epoch 387
Epoch 404
Epoch 404, Loss: 0.000136641, Improvement: 0.000020293, Best Loss: 0.000027231 in Epoch 387
Epoch 405
Epoch 405, Loss: 0.000091851, Improvement: -0.000044790, Best Loss: 0.000027231 in Epoch 387
Epoch 406
Epoch 406, Loss: 0.000089444, Improvement: -0.000002407, Best Loss: 0.000027231 in Epoch 387
Epoch 407
Epoch 407, Loss: 0.000091437, Improvement: 0.000001993, Best Loss: 0.000027231 in Epoch 387
Epoch 408
Epoch 408, Loss: 0.000125472, Improvement: 0.000034035, Best Loss: 0.000027231 in Epoch 387
Epoch 409
Epoch 409, Loss: 0.000083583, Improvement: -0.000041889, Best Loss: 0.000027231 in Epoch 387
Epoch 410
Epoch 410, Loss: 0.000071121, Improvement: -0.000012462, Best Loss: 0.000027231 in Epoch 387
Epoch 411
Epoch 411, Loss: 0.000073036, Improvement: 0.000001915, Best Loss: 0.000027231 in Epoch 387
Epoch 412
Epoch 412, Loss: 0.000132294, Improvement: 0.000059257, Best Loss: 0.000027231 in Epoch 387
Epoch 413
Epoch 413, Loss: 0.000225928, Improvement: 0.000093635, Best Loss: 0.000027231 in Epoch 387
Epoch 414
Epoch 414, Loss: 0.000238989, Improvement: 0.000013061, Best Loss: 0.000027231 in Epoch 387
Epoch 415
Epoch 415, Loss: 0.000102554, Improvement: -0.000136435, Best Loss: 0.000027231 in Epoch 387
Epoch 416
Epoch 416, Loss: 0.000063691, Improvement: -0.000038863, Best Loss: 0.000027231 in Epoch 387
Epoch 417
Epoch 417, Loss: 0.000053968, Improvement: -0.000009723, Best Loss: 0.000027231 in Epoch 387
Epoch 418
Epoch 418, Loss: 0.000045631, Improvement: -0.000008337, Best Loss: 0.000027231 in Epoch 387
Epoch 419
A best model at epoch 419 has been saved with training error 0.000022042.
Epoch 419, Loss: 0.000043697, Improvement: -0.000001934, Best Loss: 0.000022042 in Epoch 419
Epoch 420
A best model at epoch 420 has been saved with training error 0.000021776.
Epoch 420, Loss: 0.000034116, Improvement: -0.000009582, Best Loss: 0.000021776 in Epoch 420
Epoch 421
Epoch 421, Loss: 0.000050784, Improvement: 0.000016668, Best Loss: 0.000021776 in Epoch 420
Epoch 422
Epoch 422, Loss: 0.000040442, Improvement: -0.000010342, Best Loss: 0.000021776 in Epoch 420
Epoch 423
Epoch 423, Loss: 0.000082292, Improvement: 0.000041850, Best Loss: 0.000021776 in Epoch 420
Epoch 424
Epoch 424, Loss: 0.000094419, Improvement: 0.000012127, Best Loss: 0.000021776 in Epoch 420
Epoch 425
Epoch 425, Loss: 0.000089822, Improvement: -0.000004597, Best Loss: 0.000021776 in Epoch 420
Epoch 426
Epoch 426, Loss: 0.000065877, Improvement: -0.000023945, Best Loss: 0.000021776 in Epoch 420
Epoch 427
Epoch 427, Loss: 0.000057649, Improvement: -0.000008228, Best Loss: 0.000021776 in Epoch 420
Epoch 428
Epoch 428, Loss: 0.000050025, Improvement: -0.000007623, Best Loss: 0.000021776 in Epoch 420
Epoch 429
Epoch 429, Loss: 0.000047565, Improvement: -0.000002461, Best Loss: 0.000021776 in Epoch 420
Epoch 430
A best model at epoch 430 has been saved with training error 0.000021610.
Epoch 430, Loss: 0.000034457, Improvement: -0.000013108, Best Loss: 0.000021610 in Epoch 430
Epoch 431
Epoch 431, Loss: 0.000036452, Improvement: 0.000001995, Best Loss: 0.000021610 in Epoch 430
Epoch 432
Epoch 432, Loss: 0.000044439, Improvement: 0.000007987, Best Loss: 0.000021610 in Epoch 430
Epoch 433
Epoch 433, Loss: 0.000049938, Improvement: 0.000005499, Best Loss: 0.000021610 in Epoch 430
Epoch 434
Epoch 434, Loss: 0.000039625, Improvement: -0.000010313, Best Loss: 0.000021610 in Epoch 430
Epoch 435
Epoch 435, Loss: 0.000044112, Improvement: 0.000004488, Best Loss: 0.000021610 in Epoch 430
Epoch 436
Epoch 436, Loss: 0.000053523, Improvement: 0.000009411, Best Loss: 0.000021610 in Epoch 430
Epoch 437
Epoch 437, Loss: 0.000045833, Improvement: -0.000007690, Best Loss: 0.000021610 in Epoch 430
Epoch 438
Epoch 438, Loss: 0.000122879, Improvement: 0.000077046, Best Loss: 0.000021610 in Epoch 430
Epoch 439
Epoch 439, Loss: 0.000174904, Improvement: 0.000052026, Best Loss: 0.000021610 in Epoch 430
Epoch 440
Epoch 440, Loss: 0.000140122, Improvement: -0.000034783, Best Loss: 0.000021610 in Epoch 430
Epoch 441
Epoch 441, Loss: 0.000106801, Improvement: -0.000033320, Best Loss: 0.000021610 in Epoch 430
Epoch 442
Epoch 442, Loss: 0.000076293, Improvement: -0.000030508, Best Loss: 0.000021610 in Epoch 430
Epoch 443
Epoch 443, Loss: 0.000075687, Improvement: -0.000000606, Best Loss: 0.000021610 in Epoch 430
Epoch 444
Epoch 444, Loss: 0.000079124, Improvement: 0.000003437, Best Loss: 0.000021610 in Epoch 430
Epoch 445
Epoch 445, Loss: 0.000150134, Improvement: 0.000071010, Best Loss: 0.000021610 in Epoch 430
Epoch 446
Epoch 446, Loss: 0.000165744, Improvement: 0.000015611, Best Loss: 0.000021610 in Epoch 430
Epoch 447
Epoch 447, Loss: 0.000109924, Improvement: -0.000055820, Best Loss: 0.000021610 in Epoch 430
Epoch 448
Epoch 448, Loss: 0.000075001, Improvement: -0.000034923, Best Loss: 0.000021610 in Epoch 430
Epoch 449
Epoch 449, Loss: 0.000055857, Improvement: -0.000019144, Best Loss: 0.000021610 in Epoch 430
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.000040416, Improvement: -0.000015441, Best Loss: 0.000021610 in Epoch 430
Epoch 451
Epoch 451, Loss: 0.000052350, Improvement: 0.000011934, Best Loss: 0.000021610 in Epoch 430
Epoch 452
Epoch 452, Loss: 0.000048780, Improvement: -0.000003569, Best Loss: 0.000021610 in Epoch 430
Epoch 453
A best model at epoch 453 has been saved with training error 0.000020563.
Epoch 453, Loss: 0.000031085, Improvement: -0.000017695, Best Loss: 0.000020563 in Epoch 453
Epoch 454
Epoch 454, Loss: 0.000028611, Improvement: -0.000002474, Best Loss: 0.000020563 in Epoch 453
Epoch 455
Epoch 455, Loss: 0.000029540, Improvement: 0.000000929, Best Loss: 0.000020563 in Epoch 453
Epoch 456
Epoch 456, Loss: 0.000034740, Improvement: 0.000005200, Best Loss: 0.000020563 in Epoch 453
Epoch 457
Epoch 457, Loss: 0.000066824, Improvement: 0.000032084, Best Loss: 0.000020563 in Epoch 453
Epoch 458
Epoch 458, Loss: 0.000101789, Improvement: 0.000034964, Best Loss: 0.000020563 in Epoch 453
Epoch 459
Epoch 459, Loss: 0.000073086, Improvement: -0.000028702, Best Loss: 0.000020563 in Epoch 453
Epoch 460
Epoch 460, Loss: 0.000057955, Improvement: -0.000015131, Best Loss: 0.000020563 in Epoch 453
Epoch 461
Epoch 461, Loss: 0.000043890, Improvement: -0.000014065, Best Loss: 0.000020563 in Epoch 453
Epoch 462
Epoch 462, Loss: 0.000035751, Improvement: -0.000008139, Best Loss: 0.000020563 in Epoch 453
Epoch 463
Epoch 463, Loss: 0.000028737, Improvement: -0.000007014, Best Loss: 0.000020563 in Epoch 453
Epoch 464
Epoch 464, Loss: 0.000028218, Improvement: -0.000000518, Best Loss: 0.000020563 in Epoch 453
Epoch 465
Epoch 465, Loss: 0.000029723, Improvement: 0.000001505, Best Loss: 0.000020563 in Epoch 453
Epoch 466
A best model at epoch 466 has been saved with training error 0.000020090.
Epoch 466, Loss: 0.000029757, Improvement: 0.000000034, Best Loss: 0.000020090 in Epoch 466
Epoch 467
Epoch 467, Loss: 0.000048884, Improvement: 0.000019128, Best Loss: 0.000020090 in Epoch 466
Epoch 468
Epoch 468, Loss: 0.000059292, Improvement: 0.000010408, Best Loss: 0.000020090 in Epoch 466
Epoch 469
Epoch 469, Loss: 0.000040385, Improvement: -0.000018907, Best Loss: 0.000020090 in Epoch 466
Epoch 470
Epoch 470, Loss: 0.000043590, Improvement: 0.000003204, Best Loss: 0.000020090 in Epoch 466
Epoch 471
Epoch 471, Loss: 0.000068580, Improvement: 0.000024990, Best Loss: 0.000020090 in Epoch 466
Epoch 472
Epoch 472, Loss: 0.000076856, Improvement: 0.000008276, Best Loss: 0.000020090 in Epoch 466
Epoch 473
Epoch 473, Loss: 0.000051476, Improvement: -0.000025380, Best Loss: 0.000020090 in Epoch 466
Epoch 474
Epoch 474, Loss: 0.000052178, Improvement: 0.000000702, Best Loss: 0.000020090 in Epoch 466
Epoch 475
Epoch 475, Loss: 0.000066423, Improvement: 0.000014245, Best Loss: 0.000020090 in Epoch 466
Epoch 476
Epoch 476, Loss: 0.000070884, Improvement: 0.000004462, Best Loss: 0.000020090 in Epoch 466
Epoch 477
Epoch 477, Loss: 0.000037643, Improvement: -0.000033241, Best Loss: 0.000020090 in Epoch 466
Epoch 478
A best model at epoch 478 has been saved with training error 0.000019402.
Epoch 478, Loss: 0.000033124, Improvement: -0.000004519, Best Loss: 0.000019402 in Epoch 478
Epoch 479
A best model at epoch 479 has been saved with training error 0.000018103.
Epoch 479, Loss: 0.000041127, Improvement: 0.000008003, Best Loss: 0.000018103 in Epoch 479
Epoch 480
Epoch 480, Loss: 0.000035895, Improvement: -0.000005232, Best Loss: 0.000018103 in Epoch 479
Epoch 481
Epoch 481, Loss: 0.000036425, Improvement: 0.000000531, Best Loss: 0.000018103 in Epoch 479
Epoch 482
Epoch 482, Loss: 0.000102715, Improvement: 0.000066290, Best Loss: 0.000018103 in Epoch 479
Epoch 483
Epoch 483, Loss: 0.000125910, Improvement: 0.000023195, Best Loss: 0.000018103 in Epoch 479
Epoch 484
Epoch 484, Loss: 0.000116350, Improvement: -0.000009560, Best Loss: 0.000018103 in Epoch 479
Epoch 485
Epoch 485, Loss: 0.000049527, Improvement: -0.000066822, Best Loss: 0.000018103 in Epoch 479
Epoch 486
Epoch 486, Loss: 0.000041734, Improvement: -0.000007794, Best Loss: 0.000018103 in Epoch 479
Epoch 487
Epoch 487, Loss: 0.000032922, Improvement: -0.000008812, Best Loss: 0.000018103 in Epoch 479
Epoch 488
Epoch 488, Loss: 0.000029345, Improvement: -0.000003577, Best Loss: 0.000018103 in Epoch 479
Epoch 489
Epoch 489, Loss: 0.000028078, Improvement: -0.000001267, Best Loss: 0.000018103 in Epoch 479
Epoch 490
Epoch 490, Loss: 0.000028374, Improvement: 0.000000297, Best Loss: 0.000018103 in Epoch 479
Epoch 491
Epoch 491, Loss: 0.000027122, Improvement: -0.000001252, Best Loss: 0.000018103 in Epoch 479
Epoch 492
Epoch 492, Loss: 0.000034680, Improvement: 0.000007558, Best Loss: 0.000018103 in Epoch 479
Epoch 493
Epoch 493, Loss: 0.000047295, Improvement: 0.000012615, Best Loss: 0.000018103 in Epoch 479
Epoch 494
Epoch 494, Loss: 0.000033542, Improvement: -0.000013753, Best Loss: 0.000018103 in Epoch 479
Epoch 495
Epoch 495, Loss: 0.000039754, Improvement: 0.000006211, Best Loss: 0.000018103 in Epoch 479
Epoch 496
Epoch 496, Loss: 0.000032872, Improvement: -0.000006882, Best Loss: 0.000018103 in Epoch 479
Epoch 497
Epoch 497, Loss: 0.000039511, Improvement: 0.000006639, Best Loss: 0.000018103 in Epoch 479
Epoch 498
Epoch 498, Loss: 0.000056158, Improvement: 0.000016647, Best Loss: 0.000018103 in Epoch 479
Epoch 499
Epoch 499, Loss: 0.000048946, Improvement: -0.000007212, Best Loss: 0.000018103 in Epoch 479
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.000132725, Improvement: 0.000083779, Best Loss: 0.000018103 in Epoch 479
Epoch 501
Epoch 501, Loss: 0.000119529, Improvement: -0.000013196, Best Loss: 0.000018103 in Epoch 479
Epoch 502
Epoch 502, Loss: 0.000080457, Improvement: -0.000039072, Best Loss: 0.000018103 in Epoch 479
Epoch 503
Epoch 503, Loss: 0.000043008, Improvement: -0.000037450, Best Loss: 0.000018103 in Epoch 479
Epoch 504
Epoch 504, Loss: 0.000032758, Improvement: -0.000010250, Best Loss: 0.000018103 in Epoch 479
Epoch 505
Epoch 505, Loss: 0.000056272, Improvement: 0.000023514, Best Loss: 0.000018103 in Epoch 479
Epoch 506
Epoch 506, Loss: 0.000051391, Improvement: -0.000004881, Best Loss: 0.000018103 in Epoch 479
Epoch 507
Epoch 507, Loss: 0.000036705, Improvement: -0.000014686, Best Loss: 0.000018103 in Epoch 479
Epoch 508
Epoch 508, Loss: 0.000054114, Improvement: 0.000017409, Best Loss: 0.000018103 in Epoch 479
Epoch 509
Epoch 509, Loss: 0.000056959, Improvement: 0.000002846, Best Loss: 0.000018103 in Epoch 479
Epoch 510
Epoch 510, Loss: 0.000124103, Improvement: 0.000067144, Best Loss: 0.000018103 in Epoch 479
Epoch 511
Epoch 511, Loss: 0.000126616, Improvement: 0.000002512, Best Loss: 0.000018103 in Epoch 479
Epoch 512
Epoch 512, Loss: 0.000060093, Improvement: -0.000066523, Best Loss: 0.000018103 in Epoch 479
Epoch 513
Epoch 513, Loss: 0.000049441, Improvement: -0.000010652, Best Loss: 0.000018103 in Epoch 479
Epoch 514
Epoch 514, Loss: 0.000074808, Improvement: 0.000025367, Best Loss: 0.000018103 in Epoch 479
Epoch 515
Epoch 515, Loss: 0.000066962, Improvement: -0.000007846, Best Loss: 0.000018103 in Epoch 479
Epoch 516
Epoch 516, Loss: 0.000041146, Improvement: -0.000025816, Best Loss: 0.000018103 in Epoch 479
Epoch 517
Epoch 517, Loss: 0.000028889, Improvement: -0.000012257, Best Loss: 0.000018103 in Epoch 479
Epoch 518
A best model at epoch 518 has been saved with training error 0.000017518.
Epoch 518, Loss: 0.000024509, Improvement: -0.000004380, Best Loss: 0.000017518 in Epoch 518
Epoch 519
Epoch 519, Loss: 0.000023585, Improvement: -0.000000924, Best Loss: 0.000017518 in Epoch 518
Epoch 520
Epoch 520, Loss: 0.000037852, Improvement: 0.000014267, Best Loss: 0.000017518 in Epoch 518
Epoch 521
Epoch 521, Loss: 0.000043588, Improvement: 0.000005737, Best Loss: 0.000017518 in Epoch 518
Epoch 522
Epoch 522, Loss: 0.000031702, Improvement: -0.000011887, Best Loss: 0.000017518 in Epoch 518
Epoch 523
Epoch 523, Loss: 0.000029158, Improvement: -0.000002544, Best Loss: 0.000017518 in Epoch 518
Epoch 524
Epoch 524, Loss: 0.000025439, Improvement: -0.000003719, Best Loss: 0.000017518 in Epoch 518
Epoch 525
A best model at epoch 525 has been saved with training error 0.000017262.
A best model at epoch 525 has been saved with training error 0.000014360.
Epoch 525, Loss: 0.000024238, Improvement: -0.000001202, Best Loss: 0.000014360 in Epoch 525
Epoch 526
Epoch 526, Loss: 0.000025598, Improvement: 0.000001360, Best Loss: 0.000014360 in Epoch 525
Epoch 527
Epoch 527, Loss: 0.000046771, Improvement: 0.000021173, Best Loss: 0.000014360 in Epoch 525
Epoch 528
Epoch 528, Loss: 0.000056594, Improvement: 0.000009823, Best Loss: 0.000014360 in Epoch 525
Epoch 529
Epoch 529, Loss: 0.000109681, Improvement: 0.000053087, Best Loss: 0.000014360 in Epoch 525
Epoch 530
Epoch 530, Loss: 0.000105056, Improvement: -0.000004625, Best Loss: 0.000014360 in Epoch 525
Epoch 531
Epoch 531, Loss: 0.000046250, Improvement: -0.000058806, Best Loss: 0.000014360 in Epoch 525
Epoch 532
Epoch 532, Loss: 0.000030185, Improvement: -0.000016065, Best Loss: 0.000014360 in Epoch 525
Epoch 533
Epoch 533, Loss: 0.000025775, Improvement: -0.000004410, Best Loss: 0.000014360 in Epoch 525
Epoch 534
Epoch 534, Loss: 0.000024847, Improvement: -0.000000928, Best Loss: 0.000014360 in Epoch 525
Epoch 535
Epoch 535, Loss: 0.000024795, Improvement: -0.000000052, Best Loss: 0.000014360 in Epoch 525
Epoch 536
Epoch 536, Loss: 0.000028898, Improvement: 0.000004103, Best Loss: 0.000014360 in Epoch 525
Epoch 537
Epoch 537, Loss: 0.000026933, Improvement: -0.000001965, Best Loss: 0.000014360 in Epoch 525
Epoch 538
Epoch 538, Loss: 0.000038011, Improvement: 0.000011078, Best Loss: 0.000014360 in Epoch 525
Epoch 539
Epoch 539, Loss: 0.000029206, Improvement: -0.000008805, Best Loss: 0.000014360 in Epoch 525
Epoch 540
Epoch 540, Loss: 0.000034256, Improvement: 0.000005050, Best Loss: 0.000014360 in Epoch 525
Epoch 541
Epoch 541, Loss: 0.000043041, Improvement: 0.000008786, Best Loss: 0.000014360 in Epoch 525
Epoch 542
Epoch 542, Loss: 0.000036161, Improvement: -0.000006880, Best Loss: 0.000014360 in Epoch 525
Epoch 543
Epoch 543, Loss: 0.000032567, Improvement: -0.000003594, Best Loss: 0.000014360 in Epoch 525
Epoch 544
Epoch 544, Loss: 0.000045665, Improvement: 0.000013098, Best Loss: 0.000014360 in Epoch 525
Epoch 545
Epoch 545, Loss: 0.000077981, Improvement: 0.000032316, Best Loss: 0.000014360 in Epoch 525
Epoch 546
Epoch 546, Loss: 0.000062746, Improvement: -0.000015235, Best Loss: 0.000014360 in Epoch 525
Epoch 547
Epoch 547, Loss: 0.000083257, Improvement: 0.000020511, Best Loss: 0.000014360 in Epoch 525
Epoch 548
Epoch 548, Loss: 0.000131437, Improvement: 0.000048180, Best Loss: 0.000014360 in Epoch 525
Epoch 549
Epoch 549, Loss: 0.000132959, Improvement: 0.000001522, Best Loss: 0.000014360 in Epoch 525
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.000074358, Improvement: -0.000058601, Best Loss: 0.000014360 in Epoch 525
Epoch 551
Epoch 551, Loss: 0.000055130, Improvement: -0.000019228, Best Loss: 0.000014360 in Epoch 525
Epoch 552
Epoch 552, Loss: 0.000045354, Improvement: -0.000009776, Best Loss: 0.000014360 in Epoch 525
Epoch 553
Epoch 553, Loss: 0.000066079, Improvement: 0.000020725, Best Loss: 0.000014360 in Epoch 525
Epoch 554
Epoch 554, Loss: 0.000036227, Improvement: -0.000029853, Best Loss: 0.000014360 in Epoch 525
Epoch 555
Epoch 555, Loss: 0.000028536, Improvement: -0.000007691, Best Loss: 0.000014360 in Epoch 525
Epoch 556
Epoch 556, Loss: 0.000025515, Improvement: -0.000003021, Best Loss: 0.000014360 in Epoch 525
Epoch 557
Epoch 557, Loss: 0.000025114, Improvement: -0.000000400, Best Loss: 0.000014360 in Epoch 525
Epoch 558
Epoch 558, Loss: 0.000031095, Improvement: 0.000005981, Best Loss: 0.000014360 in Epoch 525
Epoch 559
Epoch 559, Loss: 0.000037223, Improvement: 0.000006127, Best Loss: 0.000014360 in Epoch 525
Epoch 560
Epoch 560, Loss: 0.000038192, Improvement: 0.000000970, Best Loss: 0.000014360 in Epoch 525
Epoch 561
Epoch 561, Loss: 0.000037452, Improvement: -0.000000740, Best Loss: 0.000014360 in Epoch 525
Epoch 562
Epoch 562, Loss: 0.000040846, Improvement: 0.000003393, Best Loss: 0.000014360 in Epoch 525
Epoch 563
Epoch 563, Loss: 0.000059622, Improvement: 0.000018776, Best Loss: 0.000014360 in Epoch 525
Epoch 564
Epoch 564, Loss: 0.000067894, Improvement: 0.000008272, Best Loss: 0.000014360 in Epoch 525
Epoch 565
Epoch 565, Loss: 0.000111432, Improvement: 0.000043537, Best Loss: 0.000014360 in Epoch 525
Epoch 566
Epoch 566, Loss: 0.000064893, Improvement: -0.000046539, Best Loss: 0.000014360 in Epoch 525
Epoch 567
Epoch 567, Loss: 0.000048383, Improvement: -0.000016510, Best Loss: 0.000014360 in Epoch 525
Epoch 568
Epoch 568, Loss: 0.000055004, Improvement: 0.000006621, Best Loss: 0.000014360 in Epoch 525
Epoch 569
Epoch 569, Loss: 0.000044399, Improvement: -0.000010605, Best Loss: 0.000014360 in Epoch 525
Epoch 570
Epoch 570, Loss: 0.000038396, Improvement: -0.000006003, Best Loss: 0.000014360 in Epoch 525
Epoch 571
Epoch 571, Loss: 0.000068798, Improvement: 0.000030403, Best Loss: 0.000014360 in Epoch 525
Epoch 572
Epoch 572, Loss: 0.000056931, Improvement: -0.000011868, Best Loss: 0.000014360 in Epoch 525
Epoch 573
Epoch 573, Loss: 0.000076259, Improvement: 0.000019329, Best Loss: 0.000014360 in Epoch 525
Epoch 574
Epoch 574, Loss: 0.000179671, Improvement: 0.000103412, Best Loss: 0.000014360 in Epoch 525
Epoch 575
Epoch 575, Loss: 0.000080561, Improvement: -0.000099110, Best Loss: 0.000014360 in Epoch 525
Epoch 576
Epoch 576, Loss: 0.000055801, Improvement: -0.000024761, Best Loss: 0.000014360 in Epoch 525
Epoch 577
Epoch 577, Loss: 0.000029412, Improvement: -0.000026389, Best Loss: 0.000014360 in Epoch 525
Epoch 578
Epoch 578, Loss: 0.000022888, Improvement: -0.000006524, Best Loss: 0.000014360 in Epoch 525
Epoch 579
Epoch 579, Loss: 0.000022162, Improvement: -0.000000726, Best Loss: 0.000014360 in Epoch 525
Epoch 580
Epoch 580, Loss: 0.000027399, Improvement: 0.000005236, Best Loss: 0.000014360 in Epoch 525
Epoch 581
Epoch 581, Loss: 0.000039922, Improvement: 0.000012523, Best Loss: 0.000014360 in Epoch 525
Epoch 582
Epoch 582, Loss: 0.000081821, Improvement: 0.000041899, Best Loss: 0.000014360 in Epoch 525
Epoch 583
Epoch 583, Loss: 0.000074119, Improvement: -0.000007702, Best Loss: 0.000014360 in Epoch 525
Epoch 584
Epoch 584, Loss: 0.000045996, Improvement: -0.000028123, Best Loss: 0.000014360 in Epoch 525
Epoch 585
Epoch 585, Loss: 0.000040126, Improvement: -0.000005870, Best Loss: 0.000014360 in Epoch 525
Epoch 586
Epoch 586, Loss: 0.000035381, Improvement: -0.000004745, Best Loss: 0.000014360 in Epoch 525
Epoch 587
Epoch 587, Loss: 0.000028332, Improvement: -0.000007049, Best Loss: 0.000014360 in Epoch 525
Epoch 588
Epoch 588, Loss: 0.000033170, Improvement: 0.000004838, Best Loss: 0.000014360 in Epoch 525
Epoch 589
Epoch 589, Loss: 0.000028579, Improvement: -0.000004591, Best Loss: 0.000014360 in Epoch 525
Epoch 590
Epoch 590, Loss: 0.000027520, Improvement: -0.000001059, Best Loss: 0.000014360 in Epoch 525
Epoch 591
Epoch 591, Loss: 0.000034003, Improvement: 0.000006482, Best Loss: 0.000014360 in Epoch 525
Epoch 592
Epoch 592, Loss: 0.000029418, Improvement: -0.000004584, Best Loss: 0.000014360 in Epoch 525
Epoch 593
Epoch 593, Loss: 0.000042454, Improvement: 0.000013036, Best Loss: 0.000014360 in Epoch 525
Epoch 594
Epoch 594, Loss: 0.000037133, Improvement: -0.000005321, Best Loss: 0.000014360 in Epoch 525
Epoch 595
Epoch 595, Loss: 0.000033736, Improvement: -0.000003397, Best Loss: 0.000014360 in Epoch 525
Epoch 596
Epoch 596, Loss: 0.000029465, Improvement: -0.000004272, Best Loss: 0.000014360 in Epoch 525
Epoch 597
Epoch 597, Loss: 0.000041891, Improvement: 0.000012427, Best Loss: 0.000014360 in Epoch 525
Epoch 598
Epoch 598, Loss: 0.000029105, Improvement: -0.000012787, Best Loss: 0.000014360 in Epoch 525
Epoch 599
Epoch 599, Loss: 0.000024240, Improvement: -0.000004864, Best Loss: 0.000014360 in Epoch 525
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.000086453, Improvement: 0.000062212, Best Loss: 0.000014360 in Epoch 525
Epoch 601
Epoch 601, Loss: 0.000074821, Improvement: -0.000011631, Best Loss: 0.000014360 in Epoch 525
Epoch 602
Epoch 602, Loss: 0.000063500, Improvement: -0.000011321, Best Loss: 0.000014360 in Epoch 525
Epoch 603
Epoch 603, Loss: 0.000048044, Improvement: -0.000015456, Best Loss: 0.000014360 in Epoch 525
Epoch 604
Epoch 604, Loss: 0.000035795, Improvement: -0.000012249, Best Loss: 0.000014360 in Epoch 525
Epoch 605
Epoch 605, Loss: 0.000031728, Improvement: -0.000004067, Best Loss: 0.000014360 in Epoch 525
Epoch 606
Epoch 606, Loss: 0.000024420, Improvement: -0.000007308, Best Loss: 0.000014360 in Epoch 525
Epoch 607
Epoch 607, Loss: 0.000023528, Improvement: -0.000000892, Best Loss: 0.000014360 in Epoch 525
Epoch 608
Epoch 608, Loss: 0.000022872, Improvement: -0.000000656, Best Loss: 0.000014360 in Epoch 525
Epoch 609
Epoch 609, Loss: 0.000024568, Improvement: 0.000001696, Best Loss: 0.000014360 in Epoch 525
Epoch 610
Epoch 610, Loss: 0.000028949, Improvement: 0.000004381, Best Loss: 0.000014360 in Epoch 525
Epoch 611
Epoch 611, Loss: 0.000050605, Improvement: 0.000021656, Best Loss: 0.000014360 in Epoch 525
Epoch 612
Epoch 612, Loss: 0.000058981, Improvement: 0.000008376, Best Loss: 0.000014360 in Epoch 525
Epoch 613
Epoch 613, Loss: 0.000029529, Improvement: -0.000029451, Best Loss: 0.000014360 in Epoch 525
Epoch 614
Epoch 614, Loss: 0.000026998, Improvement: -0.000002532, Best Loss: 0.000014360 in Epoch 525
Epoch 615
Epoch 615, Loss: 0.000025405, Improvement: -0.000001593, Best Loss: 0.000014360 in Epoch 525
Epoch 616
Epoch 616, Loss: 0.000056332, Improvement: 0.000030927, Best Loss: 0.000014360 in Epoch 525
Epoch 617
Epoch 617, Loss: 0.000135503, Improvement: 0.000079171, Best Loss: 0.000014360 in Epoch 525
Epoch 618
Epoch 618, Loss: 0.000092008, Improvement: -0.000043495, Best Loss: 0.000014360 in Epoch 525
Epoch 619
Epoch 619, Loss: 0.000065507, Improvement: -0.000026501, Best Loss: 0.000014360 in Epoch 525
Epoch 620
Epoch 620, Loss: 0.000038822, Improvement: -0.000026685, Best Loss: 0.000014360 in Epoch 525
Epoch 621
Epoch 621, Loss: 0.000023591, Improvement: -0.000015231, Best Loss: 0.000014360 in Epoch 525
Epoch 622
Epoch 622, Loss: 0.000021066, Improvement: -0.000002525, Best Loss: 0.000014360 in Epoch 525
Epoch 623
A best model at epoch 623 has been saved with training error 0.000013884.
A best model at epoch 623 has been saved with training error 0.000013468.
Epoch 623, Loss: 0.000020311, Improvement: -0.000000755, Best Loss: 0.000013468 in Epoch 623
Epoch 624
Epoch 624, Loss: 0.000021034, Improvement: 0.000000724, Best Loss: 0.000013468 in Epoch 623
Epoch 625
Epoch 625, Loss: 0.000025372, Improvement: 0.000004338, Best Loss: 0.000013468 in Epoch 623
Epoch 626
Epoch 626, Loss: 0.000022195, Improvement: -0.000003177, Best Loss: 0.000013468 in Epoch 623
Epoch 627
Epoch 627, Loss: 0.000021418, Improvement: -0.000000777, Best Loss: 0.000013468 in Epoch 623
Epoch 628
A best model at epoch 628 has been saved with training error 0.000012573.
Epoch 628, Loss: 0.000021220, Improvement: -0.000000198, Best Loss: 0.000012573 in Epoch 628
Epoch 629
Epoch 629, Loss: 0.000032997, Improvement: 0.000011777, Best Loss: 0.000012573 in Epoch 628
Epoch 630
Epoch 630, Loss: 0.000046311, Improvement: 0.000013314, Best Loss: 0.000012573 in Epoch 628
Epoch 631
Epoch 631, Loss: 0.000033893, Improvement: -0.000012418, Best Loss: 0.000012573 in Epoch 628
Epoch 632
Epoch 632, Loss: 0.000031724, Improvement: -0.000002169, Best Loss: 0.000012573 in Epoch 628
Epoch 633
Epoch 633, Loss: 0.000028030, Improvement: -0.000003694, Best Loss: 0.000012573 in Epoch 628
Epoch 634
Epoch 634, Loss: 0.000022899, Improvement: -0.000005131, Best Loss: 0.000012573 in Epoch 628
Epoch 635
Epoch 635, Loss: 0.000020656, Improvement: -0.000002244, Best Loss: 0.000012573 in Epoch 628
Epoch 636
Epoch 636, Loss: 0.000019439, Improvement: -0.000001217, Best Loss: 0.000012573 in Epoch 628
Epoch 637
Epoch 637, Loss: 0.000021847, Improvement: 0.000002408, Best Loss: 0.000012573 in Epoch 628
Epoch 638
Epoch 638, Loss: 0.000028769, Improvement: 0.000006922, Best Loss: 0.000012573 in Epoch 628
Epoch 639
Epoch 639, Loss: 0.000054499, Improvement: 0.000025730, Best Loss: 0.000012573 in Epoch 628
Epoch 640
Epoch 640, Loss: 0.000062764, Improvement: 0.000008265, Best Loss: 0.000012573 in Epoch 628
Epoch 641
Epoch 641, Loss: 0.000041530, Improvement: -0.000021234, Best Loss: 0.000012573 in Epoch 628
Epoch 642
Epoch 642, Loss: 0.000052013, Improvement: 0.000010482, Best Loss: 0.000012573 in Epoch 628
Epoch 643
Epoch 643, Loss: 0.000048123, Improvement: -0.000003890, Best Loss: 0.000012573 in Epoch 628
Epoch 644
Epoch 644, Loss: 0.000039454, Improvement: -0.000008669, Best Loss: 0.000012573 in Epoch 628
Epoch 645
Epoch 645, Loss: 0.000049305, Improvement: 0.000009851, Best Loss: 0.000012573 in Epoch 628
Epoch 646
Epoch 646, Loss: 0.000045647, Improvement: -0.000003658, Best Loss: 0.000012573 in Epoch 628
Epoch 647
Epoch 647, Loss: 0.000035043, Improvement: -0.000010604, Best Loss: 0.000012573 in Epoch 628
Epoch 648
Epoch 648, Loss: 0.000028014, Improvement: -0.000007029, Best Loss: 0.000012573 in Epoch 628
Epoch 649
Epoch 649, Loss: 0.000028154, Improvement: 0.000000140, Best Loss: 0.000012573 in Epoch 628
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.000047607, Improvement: 0.000019453, Best Loss: 0.000012573 in Epoch 628
Epoch 651
Epoch 651, Loss: 0.000038677, Improvement: -0.000008930, Best Loss: 0.000012573 in Epoch 628
Epoch 652
Epoch 652, Loss: 0.000041903, Improvement: 0.000003226, Best Loss: 0.000012573 in Epoch 628
Epoch 653
Epoch 653, Loss: 0.000037912, Improvement: -0.000003991, Best Loss: 0.000012573 in Epoch 628
Epoch 654
Epoch 654, Loss: 0.000046069, Improvement: 0.000008157, Best Loss: 0.000012573 in Epoch 628
Epoch 655
Epoch 655, Loss: 0.000030205, Improvement: -0.000015864, Best Loss: 0.000012573 in Epoch 628
Epoch 656
Epoch 656, Loss: 0.000039756, Improvement: 0.000009550, Best Loss: 0.000012573 in Epoch 628
Epoch 657
Epoch 657, Loss: 0.000057947, Improvement: 0.000018191, Best Loss: 0.000012573 in Epoch 628
Epoch 658
Epoch 658, Loss: 0.000045501, Improvement: -0.000012446, Best Loss: 0.000012573 in Epoch 628
Epoch 659
Epoch 659, Loss: 0.000029310, Improvement: -0.000016192, Best Loss: 0.000012573 in Epoch 628
Epoch 660
Epoch 660, Loss: 0.000035086, Improvement: 0.000005777, Best Loss: 0.000012573 in Epoch 628
Epoch 661
Epoch 661, Loss: 0.000040289, Improvement: 0.000005202, Best Loss: 0.000012573 in Epoch 628
Epoch 662
Epoch 662, Loss: 0.000056864, Improvement: 0.000016575, Best Loss: 0.000012573 in Epoch 628
Epoch 663
Epoch 663, Loss: 0.000062236, Improvement: 0.000005372, Best Loss: 0.000012573 in Epoch 628
Epoch 664
Epoch 664, Loss: 0.000094538, Improvement: 0.000032302, Best Loss: 0.000012573 in Epoch 628
Epoch 665
Epoch 665, Loss: 0.000097345, Improvement: 0.000002807, Best Loss: 0.000012573 in Epoch 628
Epoch 666
Epoch 666, Loss: 0.000113388, Improvement: 0.000016042, Best Loss: 0.000012573 in Epoch 628
Epoch 667
Epoch 667, Loss: 0.000093719, Improvement: -0.000019669, Best Loss: 0.000012573 in Epoch 628
Epoch 668
Epoch 668, Loss: 0.000060630, Improvement: -0.000033088, Best Loss: 0.000012573 in Epoch 628
Epoch 669
Epoch 669, Loss: 0.000050173, Improvement: -0.000010457, Best Loss: 0.000012573 in Epoch 628
Epoch 670
Epoch 670, Loss: 0.000040718, Improvement: -0.000009455, Best Loss: 0.000012573 in Epoch 628
Epoch 671
Epoch 671, Loss: 0.000024782, Improvement: -0.000015937, Best Loss: 0.000012573 in Epoch 628
Epoch 672
Epoch 672, Loss: 0.000020237, Improvement: -0.000004544, Best Loss: 0.000012573 in Epoch 628
Epoch 673
A best model at epoch 673 has been saved with training error 0.000012215.
Epoch 673, Loss: 0.000018535, Improvement: -0.000001702, Best Loss: 0.000012215 in Epoch 673
Epoch 674
Epoch 674, Loss: 0.000018302, Improvement: -0.000000233, Best Loss: 0.000012215 in Epoch 673
Epoch 675
A best model at epoch 675 has been saved with training error 0.000011385.
Epoch 675, Loss: 0.000018256, Improvement: -0.000000046, Best Loss: 0.000011385 in Epoch 675
Epoch 676
Epoch 676, Loss: 0.000017277, Improvement: -0.000000979, Best Loss: 0.000011385 in Epoch 675
Epoch 677
Epoch 677, Loss: 0.000016713, Improvement: -0.000000565, Best Loss: 0.000011385 in Epoch 675
Epoch 678
A best model at epoch 678 has been saved with training error 0.000011197.
Epoch 678, Loss: 0.000016534, Improvement: -0.000000179, Best Loss: 0.000011197 in Epoch 678
Epoch 679
Epoch 679, Loss: 0.000017767, Improvement: 0.000001233, Best Loss: 0.000011197 in Epoch 678
Epoch 680
Epoch 680, Loss: 0.000022226, Improvement: 0.000004459, Best Loss: 0.000011197 in Epoch 678
Epoch 681
Epoch 681, Loss: 0.000038942, Improvement: 0.000016715, Best Loss: 0.000011197 in Epoch 678
Epoch 682
Epoch 682, Loss: 0.000029174, Improvement: -0.000009767, Best Loss: 0.000011197 in Epoch 678
Epoch 683
Epoch 683, Loss: 0.000037937, Improvement: 0.000008763, Best Loss: 0.000011197 in Epoch 678
Epoch 684
Epoch 684, Loss: 0.000045094, Improvement: 0.000007157, Best Loss: 0.000011197 in Epoch 678
Epoch 685
Epoch 685, Loss: 0.000070570, Improvement: 0.000025476, Best Loss: 0.000011197 in Epoch 678
Epoch 686
Epoch 686, Loss: 0.000041863, Improvement: -0.000028707, Best Loss: 0.000011197 in Epoch 678
Epoch 687
Epoch 687, Loss: 0.000042277, Improvement: 0.000000414, Best Loss: 0.000011197 in Epoch 678
Epoch 688
Epoch 688, Loss: 0.000046093, Improvement: 0.000003815, Best Loss: 0.000011197 in Epoch 678
Epoch 689
Epoch 689, Loss: 0.000026598, Improvement: -0.000019494, Best Loss: 0.000011197 in Epoch 678
Epoch 690
Epoch 690, Loss: 0.000025168, Improvement: -0.000001430, Best Loss: 0.000011197 in Epoch 678
Epoch 691
Epoch 691, Loss: 0.000024379, Improvement: -0.000000789, Best Loss: 0.000011197 in Epoch 678
Epoch 692
Epoch 692, Loss: 0.000021031, Improvement: -0.000003348, Best Loss: 0.000011197 in Epoch 678
Epoch 693
Epoch 693, Loss: 0.000022303, Improvement: 0.000001272, Best Loss: 0.000011197 in Epoch 678
Epoch 694
Epoch 694, Loss: 0.000021778, Improvement: -0.000000525, Best Loss: 0.000011197 in Epoch 678
Epoch 695
Epoch 695, Loss: 0.000050877, Improvement: 0.000029099, Best Loss: 0.000011197 in Epoch 678
Epoch 696
Epoch 696, Loss: 0.000082713, Improvement: 0.000031836, Best Loss: 0.000011197 in Epoch 678
Epoch 697
Epoch 697, Loss: 0.000119586, Improvement: 0.000036873, Best Loss: 0.000011197 in Epoch 678
Epoch 698
Epoch 698, Loss: 0.000059251, Improvement: -0.000060335, Best Loss: 0.000011197 in Epoch 678
Epoch 699
Epoch 699, Loss: 0.000040910, Improvement: -0.000018341, Best Loss: 0.000011197 in Epoch 678
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.000024408, Improvement: -0.000016503, Best Loss: 0.000011197 in Epoch 678
Epoch 701
Epoch 701, Loss: 0.000021325, Improvement: -0.000003083, Best Loss: 0.000011197 in Epoch 678
Epoch 702
A best model at epoch 702 has been saved with training error 0.000009264.
Epoch 702, Loss: 0.000017889, Improvement: -0.000003436, Best Loss: 0.000009264 in Epoch 702
Epoch 703
Epoch 703, Loss: 0.000017543, Improvement: -0.000000347, Best Loss: 0.000009264 in Epoch 702
Epoch 704
Epoch 704, Loss: 0.000017607, Improvement: 0.000000065, Best Loss: 0.000009264 in Epoch 702
Epoch 705
Epoch 705, Loss: 0.000021451, Improvement: 0.000003844, Best Loss: 0.000009264 in Epoch 702
Epoch 706
Epoch 706, Loss: 0.000023723, Improvement: 0.000002272, Best Loss: 0.000009264 in Epoch 702
Epoch 707
Epoch 707, Loss: 0.000021980, Improvement: -0.000001743, Best Loss: 0.000009264 in Epoch 702
Epoch 708
Epoch 708, Loss: 0.000018700, Improvement: -0.000003280, Best Loss: 0.000009264 in Epoch 702
Epoch 709
Epoch 709, Loss: 0.000016172, Improvement: -0.000002528, Best Loss: 0.000009264 in Epoch 702
Epoch 710
Epoch 710, Loss: 0.000015156, Improvement: -0.000001016, Best Loss: 0.000009264 in Epoch 702
Epoch 711
Epoch 711, Loss: 0.000015618, Improvement: 0.000000462, Best Loss: 0.000009264 in Epoch 702
Epoch 712
Epoch 712, Loss: 0.000016278, Improvement: 0.000000661, Best Loss: 0.000009264 in Epoch 702
Epoch 713
Epoch 713, Loss: 0.000025407, Improvement: 0.000009129, Best Loss: 0.000009264 in Epoch 702
Epoch 714
Epoch 714, Loss: 0.000041822, Improvement: 0.000016415, Best Loss: 0.000009264 in Epoch 702
Epoch 715
Epoch 715, Loss: 0.000034184, Improvement: -0.000007638, Best Loss: 0.000009264 in Epoch 702
Epoch 716
Epoch 716, Loss: 0.000028433, Improvement: -0.000005751, Best Loss: 0.000009264 in Epoch 702
Epoch 717
Epoch 717, Loss: 0.000023375, Improvement: -0.000005059, Best Loss: 0.000009264 in Epoch 702
Epoch 718
Epoch 718, Loss: 0.000020836, Improvement: -0.000002538, Best Loss: 0.000009264 in Epoch 702
Epoch 719
Epoch 719, Loss: 0.000017474, Improvement: -0.000003362, Best Loss: 0.000009264 in Epoch 702
Epoch 720
Epoch 720, Loss: 0.000017121, Improvement: -0.000000353, Best Loss: 0.000009264 in Epoch 702
Epoch 721
Epoch 721, Loss: 0.000018647, Improvement: 0.000001526, Best Loss: 0.000009264 in Epoch 702
Epoch 722
Epoch 722, Loss: 0.000023280, Improvement: 0.000004632, Best Loss: 0.000009264 in Epoch 702
Epoch 723
Epoch 723, Loss: 0.000022162, Improvement: -0.000001117, Best Loss: 0.000009264 in Epoch 702
Epoch 724
Epoch 724, Loss: 0.000018857, Improvement: -0.000003306, Best Loss: 0.000009264 in Epoch 702
Epoch 725
Epoch 725, Loss: 0.000033275, Improvement: 0.000014418, Best Loss: 0.000009264 in Epoch 702
Epoch 726
Epoch 726, Loss: 0.000022611, Improvement: -0.000010664, Best Loss: 0.000009264 in Epoch 702
Epoch 727
Epoch 727, Loss: 0.000017073, Improvement: -0.000005538, Best Loss: 0.000009264 in Epoch 702
Epoch 728
Epoch 728, Loss: 0.000020136, Improvement: 0.000003063, Best Loss: 0.000009264 in Epoch 702
Epoch 729
Epoch 729, Loss: 0.000025284, Improvement: 0.000005148, Best Loss: 0.000009264 in Epoch 702
Epoch 730
Epoch 730, Loss: 0.000029429, Improvement: 0.000004146, Best Loss: 0.000009264 in Epoch 702
Epoch 731
Epoch 731, Loss: 0.000038106, Improvement: 0.000008676, Best Loss: 0.000009264 in Epoch 702
Epoch 732
Epoch 732, Loss: 0.000043097, Improvement: 0.000004991, Best Loss: 0.000009264 in Epoch 702
Epoch 733
Epoch 733, Loss: 0.000066037, Improvement: 0.000022940, Best Loss: 0.000009264 in Epoch 702
Epoch 734
Epoch 734, Loss: 0.000034592, Improvement: -0.000031445, Best Loss: 0.000009264 in Epoch 702
Epoch 735
Epoch 735, Loss: 0.000027493, Improvement: -0.000007099, Best Loss: 0.000009264 in Epoch 702
Epoch 736
Epoch 736, Loss: 0.000047345, Improvement: 0.000019853, Best Loss: 0.000009264 in Epoch 702
Epoch 737
Epoch 737, Loss: 0.000090431, Improvement: 0.000043086, Best Loss: 0.000009264 in Epoch 702
Epoch 738
Epoch 738, Loss: 0.000123738, Improvement: 0.000033307, Best Loss: 0.000009264 in Epoch 702
Epoch 739
Epoch 739, Loss: 0.000186719, Improvement: 0.000062981, Best Loss: 0.000009264 in Epoch 702
Epoch 740
Epoch 740, Loss: 0.000111998, Improvement: -0.000074722, Best Loss: 0.000009264 in Epoch 702
Epoch 741
Epoch 741, Loss: 0.000064302, Improvement: -0.000047696, Best Loss: 0.000009264 in Epoch 702
Epoch 742
Epoch 742, Loss: 0.000042249, Improvement: -0.000022052, Best Loss: 0.000009264 in Epoch 702
Epoch 743
Epoch 743, Loss: 0.000024736, Improvement: -0.000017513, Best Loss: 0.000009264 in Epoch 702
Epoch 744
Epoch 744, Loss: 0.000020167, Improvement: -0.000004569, Best Loss: 0.000009264 in Epoch 702
Epoch 745
Epoch 745, Loss: 0.000021735, Improvement: 0.000001568, Best Loss: 0.000009264 in Epoch 702
Epoch 746
Epoch 746, Loss: 0.000019245, Improvement: -0.000002490, Best Loss: 0.000009264 in Epoch 702
Epoch 747
Epoch 747, Loss: 0.000017166, Improvement: -0.000002079, Best Loss: 0.000009264 in Epoch 702
Epoch 748
Epoch 748, Loss: 0.000019223, Improvement: 0.000002057, Best Loss: 0.000009264 in Epoch 702
Epoch 749
Epoch 749, Loss: 0.000019254, Improvement: 0.000000031, Best Loss: 0.000009264 in Epoch 702
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.000016948, Improvement: -0.000002306, Best Loss: 0.000009264 in Epoch 702
Epoch 751
Epoch 751, Loss: 0.000015145, Improvement: -0.000001803, Best Loss: 0.000009264 in Epoch 702
Epoch 752
Epoch 752, Loss: 0.000014691, Improvement: -0.000000454, Best Loss: 0.000009264 in Epoch 702
Epoch 753
Epoch 753, Loss: 0.000015029, Improvement: 0.000000338, Best Loss: 0.000009264 in Epoch 702
Epoch 754
Epoch 754, Loss: 0.000016671, Improvement: 0.000001642, Best Loss: 0.000009264 in Epoch 702
Epoch 755
Epoch 755, Loss: 0.000024550, Improvement: 0.000007879, Best Loss: 0.000009264 in Epoch 702
Epoch 756
Epoch 756, Loss: 0.000024134, Improvement: -0.000000416, Best Loss: 0.000009264 in Epoch 702
Epoch 757
Epoch 757, Loss: 0.000025821, Improvement: 0.000001687, Best Loss: 0.000009264 in Epoch 702
Epoch 758
Epoch 758, Loss: 0.000054946, Improvement: 0.000029125, Best Loss: 0.000009264 in Epoch 702
Epoch 759
Epoch 759, Loss: 0.000075696, Improvement: 0.000020750, Best Loss: 0.000009264 in Epoch 702
Epoch 760
Epoch 760, Loss: 0.000048267, Improvement: -0.000027430, Best Loss: 0.000009264 in Epoch 702
Epoch 761
Epoch 761, Loss: 0.000035687, Improvement: -0.000012579, Best Loss: 0.000009264 in Epoch 702
Epoch 762
Epoch 762, Loss: 0.000046414, Improvement: 0.000010726, Best Loss: 0.000009264 in Epoch 702
Epoch 763
Epoch 763, Loss: 0.000038369, Improvement: -0.000008044, Best Loss: 0.000009264 in Epoch 702
Epoch 764
Epoch 764, Loss: 0.000030356, Improvement: -0.000008014, Best Loss: 0.000009264 in Epoch 702
Epoch 765
Epoch 765, Loss: 0.000046702, Improvement: 0.000016346, Best Loss: 0.000009264 in Epoch 702
Epoch 766
Epoch 766, Loss: 0.000054219, Improvement: 0.000007517, Best Loss: 0.000009264 in Epoch 702
Epoch 767
Epoch 767, Loss: 0.000109833, Improvement: 0.000055614, Best Loss: 0.000009264 in Epoch 702
Epoch 768
Epoch 768, Loss: 0.000101734, Improvement: -0.000008099, Best Loss: 0.000009264 in Epoch 702
Epoch 769
Epoch 769, Loss: 0.000049765, Improvement: -0.000051969, Best Loss: 0.000009264 in Epoch 702
Epoch 770
Epoch 770, Loss: 0.000034018, Improvement: -0.000015747, Best Loss: 0.000009264 in Epoch 702
Epoch 771
Epoch 771, Loss: 0.000038662, Improvement: 0.000004644, Best Loss: 0.000009264 in Epoch 702
Epoch 772
Epoch 772, Loss: 0.000030297, Improvement: -0.000008365, Best Loss: 0.000009264 in Epoch 702
Epoch 773
Epoch 773, Loss: 0.000023625, Improvement: -0.000006672, Best Loss: 0.000009264 in Epoch 702
Epoch 774
Epoch 774, Loss: 0.000018374, Improvement: -0.000005251, Best Loss: 0.000009264 in Epoch 702
Epoch 775
Epoch 775, Loss: 0.000020430, Improvement: 0.000002056, Best Loss: 0.000009264 in Epoch 702
Epoch 776
Epoch 776, Loss: 0.000023935, Improvement: 0.000003505, Best Loss: 0.000009264 in Epoch 702
Epoch 777
Epoch 777, Loss: 0.000023443, Improvement: -0.000000491, Best Loss: 0.000009264 in Epoch 702
Epoch 778
Epoch 778, Loss: 0.000020777, Improvement: -0.000002667, Best Loss: 0.000009264 in Epoch 702
Epoch 779
Epoch 779, Loss: 0.000021358, Improvement: 0.000000581, Best Loss: 0.000009264 in Epoch 702
Epoch 780
Epoch 780, Loss: 0.000022497, Improvement: 0.000001139, Best Loss: 0.000009264 in Epoch 702
Epoch 781
Epoch 781, Loss: 0.000019034, Improvement: -0.000003463, Best Loss: 0.000009264 in Epoch 702
Epoch 782
Epoch 782, Loss: 0.000019172, Improvement: 0.000000138, Best Loss: 0.000009264 in Epoch 702
Epoch 783
Epoch 783, Loss: 0.000016012, Improvement: -0.000003160, Best Loss: 0.000009264 in Epoch 702
Epoch 784
A best model at epoch 784 has been saved with training error 0.000009011.
Epoch 784, Loss: 0.000015420, Improvement: -0.000000593, Best Loss: 0.000009011 in Epoch 784
Epoch 785
Epoch 785, Loss: 0.000014271, Improvement: -0.000001149, Best Loss: 0.000009011 in Epoch 784
Epoch 786
Epoch 786, Loss: 0.000014858, Improvement: 0.000000587, Best Loss: 0.000009011 in Epoch 784
Epoch 787
Epoch 787, Loss: 0.000026701, Improvement: 0.000011843, Best Loss: 0.000009011 in Epoch 784
Epoch 788
Epoch 788, Loss: 0.000033022, Improvement: 0.000006321, Best Loss: 0.000009011 in Epoch 784
Epoch 789
Epoch 789, Loss: 0.000041919, Improvement: 0.000008897, Best Loss: 0.000009011 in Epoch 784
Epoch 790
Epoch 790, Loss: 0.000105353, Improvement: 0.000063434, Best Loss: 0.000009011 in Epoch 784
Epoch 791
Epoch 791, Loss: 0.000056173, Improvement: -0.000049181, Best Loss: 0.000009011 in Epoch 784
Epoch 792
Epoch 792, Loss: 0.000097390, Improvement: 0.000041218, Best Loss: 0.000009011 in Epoch 784
Epoch 793
Epoch 793, Loss: 0.000075143, Improvement: -0.000022247, Best Loss: 0.000009011 in Epoch 784
Epoch 794
Epoch 794, Loss: 0.000055627, Improvement: -0.000019515, Best Loss: 0.000009011 in Epoch 784
Epoch 795
Epoch 795, Loss: 0.000038892, Improvement: -0.000016735, Best Loss: 0.000009011 in Epoch 784
Epoch 796
Epoch 796, Loss: 0.000023133, Improvement: -0.000015760, Best Loss: 0.000009011 in Epoch 784
Epoch 797
Epoch 797, Loss: 0.000016998, Improvement: -0.000006134, Best Loss: 0.000009011 in Epoch 784
Epoch 798
Epoch 798, Loss: 0.000015178, Improvement: -0.000001820, Best Loss: 0.000009011 in Epoch 784
Epoch 799
Epoch 799, Loss: 0.000015588, Improvement: 0.000000409, Best Loss: 0.000009011 in Epoch 784
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.000015762, Improvement: 0.000000174, Best Loss: 0.000009011 in Epoch 784
Epoch 801
Epoch 801, Loss: 0.000015744, Improvement: -0.000000018, Best Loss: 0.000009011 in Epoch 784
Epoch 802
Epoch 802, Loss: 0.000014327, Improvement: -0.000001417, Best Loss: 0.000009011 in Epoch 784
Epoch 803
Epoch 803, Loss: 0.000013933, Improvement: -0.000000394, Best Loss: 0.000009011 in Epoch 784
Epoch 804
Epoch 804, Loss: 0.000014585, Improvement: 0.000000652, Best Loss: 0.000009011 in Epoch 784
Epoch 805
Epoch 805, Loss: 0.000017199, Improvement: 0.000002613, Best Loss: 0.000009011 in Epoch 784
Epoch 806
Epoch 806, Loss: 0.000019804, Improvement: 0.000002606, Best Loss: 0.000009011 in Epoch 784
Epoch 807
Epoch 807, Loss: 0.000026601, Improvement: 0.000006797, Best Loss: 0.000009011 in Epoch 784
Epoch 808
Epoch 808, Loss: 0.000033756, Improvement: 0.000007155, Best Loss: 0.000009011 in Epoch 784
Epoch 809
Epoch 809, Loss: 0.000045051, Improvement: 0.000011295, Best Loss: 0.000009011 in Epoch 784
Epoch 810
Epoch 810, Loss: 0.000034201, Improvement: -0.000010850, Best Loss: 0.000009011 in Epoch 784
Epoch 811
Epoch 811, Loss: 0.000023939, Improvement: -0.000010262, Best Loss: 0.000009011 in Epoch 784
Epoch 812
Epoch 812, Loss: 0.000025617, Improvement: 0.000001678, Best Loss: 0.000009011 in Epoch 784
Epoch 813
Epoch 813, Loss: 0.000030308, Improvement: 0.000004691, Best Loss: 0.000009011 in Epoch 784
Epoch 814
Epoch 814, Loss: 0.000033628, Improvement: 0.000003320, Best Loss: 0.000009011 in Epoch 784
Epoch 815
Epoch 815, Loss: 0.000028534, Improvement: -0.000005094, Best Loss: 0.000009011 in Epoch 784
Epoch 816
Epoch 816, Loss: 0.000026117, Improvement: -0.000002417, Best Loss: 0.000009011 in Epoch 784
Epoch 817
Epoch 817, Loss: 0.000036170, Improvement: 0.000010053, Best Loss: 0.000009011 in Epoch 784
Epoch 818
Epoch 818, Loss: 0.000058262, Improvement: 0.000022092, Best Loss: 0.000009011 in Epoch 784
Epoch 819
Epoch 819, Loss: 0.000043125, Improvement: -0.000015138, Best Loss: 0.000009011 in Epoch 784
Epoch 820
Epoch 820, Loss: 0.000035313, Improvement: -0.000007812, Best Loss: 0.000009011 in Epoch 784
Epoch 821
Epoch 821, Loss: 0.000033892, Improvement: -0.000001421, Best Loss: 0.000009011 in Epoch 784
Epoch 822
Epoch 822, Loss: 0.000047157, Improvement: 0.000013266, Best Loss: 0.000009011 in Epoch 784
Epoch 823
Epoch 823, Loss: 0.000021404, Improvement: -0.000025753, Best Loss: 0.000009011 in Epoch 784
Epoch 824
Epoch 824, Loss: 0.000020177, Improvement: -0.000001227, Best Loss: 0.000009011 in Epoch 784
Epoch 825
Epoch 825, Loss: 0.000021565, Improvement: 0.000001388, Best Loss: 0.000009011 in Epoch 784
Epoch 826
Epoch 826, Loss: 0.000015215, Improvement: -0.000006350, Best Loss: 0.000009011 in Epoch 784
Epoch 827
Epoch 827, Loss: 0.000015393, Improvement: 0.000000178, Best Loss: 0.000009011 in Epoch 784
Epoch 828
Epoch 828, Loss: 0.000018051, Improvement: 0.000002658, Best Loss: 0.000009011 in Epoch 784
Epoch 829
Epoch 829, Loss: 0.000017922, Improvement: -0.000000129, Best Loss: 0.000009011 in Epoch 784
Epoch 830
Epoch 830, Loss: 0.000017237, Improvement: -0.000000685, Best Loss: 0.000009011 in Epoch 784
Epoch 831
Epoch 831, Loss: 0.000026347, Improvement: 0.000009110, Best Loss: 0.000009011 in Epoch 784
Epoch 832
Epoch 832, Loss: 0.000076203, Improvement: 0.000049856, Best Loss: 0.000009011 in Epoch 784
Epoch 833
Epoch 833, Loss: 0.000058840, Improvement: -0.000017363, Best Loss: 0.000009011 in Epoch 784
Epoch 834
Epoch 834, Loss: 0.000049397, Improvement: -0.000009444, Best Loss: 0.000009011 in Epoch 784
Epoch 835
Epoch 835, Loss: 0.000038721, Improvement: -0.000010676, Best Loss: 0.000009011 in Epoch 784
Epoch 836
Epoch 836, Loss: 0.000049245, Improvement: 0.000010524, Best Loss: 0.000009011 in Epoch 784
Epoch 837
Epoch 837, Loss: 0.000037340, Improvement: -0.000011904, Best Loss: 0.000009011 in Epoch 784
Epoch 838
Epoch 838, Loss: 0.000042295, Improvement: 0.000004955, Best Loss: 0.000009011 in Epoch 784
Epoch 839
Epoch 839, Loss: 0.000024072, Improvement: -0.000018223, Best Loss: 0.000009011 in Epoch 784
Epoch 840
Epoch 840, Loss: 0.000022419, Improvement: -0.000001653, Best Loss: 0.000009011 in Epoch 784
Epoch 841
Epoch 841, Loss: 0.000025011, Improvement: 0.000002592, Best Loss: 0.000009011 in Epoch 784
Epoch 842
Epoch 842, Loss: 0.000056655, Improvement: 0.000031643, Best Loss: 0.000009011 in Epoch 784
Epoch 843
Epoch 843, Loss: 0.000027300, Improvement: -0.000029355, Best Loss: 0.000009011 in Epoch 784
Epoch 844
Epoch 844, Loss: 0.000025108, Improvement: -0.000002191, Best Loss: 0.000009011 in Epoch 784
Epoch 845
Epoch 845, Loss: 0.000019787, Improvement: -0.000005321, Best Loss: 0.000009011 in Epoch 784
Epoch 846
Epoch 846, Loss: 0.000034181, Improvement: 0.000014394, Best Loss: 0.000009011 in Epoch 784
Epoch 847
Epoch 847, Loss: 0.000075753, Improvement: 0.000041572, Best Loss: 0.000009011 in Epoch 784
Epoch 848
Epoch 848, Loss: 0.000035987, Improvement: -0.000039766, Best Loss: 0.000009011 in Epoch 784
Epoch 849
Epoch 849, Loss: 0.000022643, Improvement: -0.000013344, Best Loss: 0.000009011 in Epoch 784
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.000015702, Improvement: -0.000006940, Best Loss: 0.000009011 in Epoch 784
Epoch 851
Epoch 851, Loss: 0.000012897, Improvement: -0.000002805, Best Loss: 0.000009011 in Epoch 784
Epoch 852
A best model at epoch 852 has been saved with training error 0.000007326.
Epoch 852, Loss: 0.000012734, Improvement: -0.000000163, Best Loss: 0.000007326 in Epoch 852
Epoch 853
Epoch 853, Loss: 0.000013968, Improvement: 0.000001234, Best Loss: 0.000007326 in Epoch 852
Epoch 854
Epoch 854, Loss: 0.000017317, Improvement: 0.000003349, Best Loss: 0.000007326 in Epoch 852
Epoch 855
Epoch 855, Loss: 0.000028372, Improvement: 0.000011055, Best Loss: 0.000007326 in Epoch 852
Epoch 856
Epoch 856, Loss: 0.000029118, Improvement: 0.000000745, Best Loss: 0.000007326 in Epoch 852
Epoch 857
Epoch 857, Loss: 0.000021007, Improvement: -0.000008111, Best Loss: 0.000007326 in Epoch 852
Epoch 858
Epoch 858, Loss: 0.000020731, Improvement: -0.000000276, Best Loss: 0.000007326 in Epoch 852
Epoch 859
Epoch 859, Loss: 0.000027100, Improvement: 0.000006369, Best Loss: 0.000007326 in Epoch 852
Epoch 860
Epoch 860, Loss: 0.000060654, Improvement: 0.000033554, Best Loss: 0.000007326 in Epoch 852
Epoch 861
Epoch 861, Loss: 0.000050424, Improvement: -0.000010230, Best Loss: 0.000007326 in Epoch 852
Epoch 862
Epoch 862, Loss: 0.000043710, Improvement: -0.000006714, Best Loss: 0.000007326 in Epoch 852
Epoch 863
Epoch 863, Loss: 0.000079940, Improvement: 0.000036230, Best Loss: 0.000007326 in Epoch 852
Epoch 864
Epoch 864, Loss: 0.000085755, Improvement: 0.000005814, Best Loss: 0.000007326 in Epoch 852
Epoch 865
Epoch 865, Loss: 0.000051735, Improvement: -0.000034019, Best Loss: 0.000007326 in Epoch 852
Epoch 866
Epoch 866, Loss: 0.000053503, Improvement: 0.000001768, Best Loss: 0.000007326 in Epoch 852
Epoch 867
Epoch 867, Loss: 0.000066433, Improvement: 0.000012930, Best Loss: 0.000007326 in Epoch 852
Epoch 868
Epoch 868, Loss: 0.000032932, Improvement: -0.000033502, Best Loss: 0.000007326 in Epoch 852
Epoch 869
Epoch 869, Loss: 0.000021594, Improvement: -0.000011338, Best Loss: 0.000007326 in Epoch 852
Epoch 870
Epoch 870, Loss: 0.000015111, Improvement: -0.000006483, Best Loss: 0.000007326 in Epoch 852
Epoch 871
Epoch 871, Loss: 0.000013810, Improvement: -0.000001300, Best Loss: 0.000007326 in Epoch 852
Epoch 872
Epoch 872, Loss: 0.000012497, Improvement: -0.000001314, Best Loss: 0.000007326 in Epoch 852
Epoch 873
Epoch 873, Loss: 0.000011934, Improvement: -0.000000563, Best Loss: 0.000007326 in Epoch 852
Epoch 874
Epoch 874, Loss: 0.000011565, Improvement: -0.000000368, Best Loss: 0.000007326 in Epoch 852
Epoch 875
A best model at epoch 875 has been saved with training error 0.000007165.
Epoch 875, Loss: 0.000011439, Improvement: -0.000000126, Best Loss: 0.000007165 in Epoch 875
Epoch 876
Epoch 876, Loss: 0.000011189, Improvement: -0.000000250, Best Loss: 0.000007165 in Epoch 875
Epoch 877
Epoch 877, Loss: 0.000011929, Improvement: 0.000000740, Best Loss: 0.000007165 in Epoch 875
Epoch 878
Epoch 878, Loss: 0.000014104, Improvement: 0.000002176, Best Loss: 0.000007165 in Epoch 875
Epoch 879
Epoch 879, Loss: 0.000014050, Improvement: -0.000000054, Best Loss: 0.000007165 in Epoch 875
Epoch 880
Epoch 880, Loss: 0.000014449, Improvement: 0.000000399, Best Loss: 0.000007165 in Epoch 875
Epoch 881
Epoch 881, Loss: 0.000019017, Improvement: 0.000004568, Best Loss: 0.000007165 in Epoch 875
Epoch 882
Epoch 882, Loss: 0.000033183, Improvement: 0.000014166, Best Loss: 0.000007165 in Epoch 875
Epoch 883
Epoch 883, Loss: 0.000024523, Improvement: -0.000008660, Best Loss: 0.000007165 in Epoch 875
Epoch 884
Epoch 884, Loss: 0.000021170, Improvement: -0.000003353, Best Loss: 0.000007165 in Epoch 875
Epoch 885
Epoch 885, Loss: 0.000020724, Improvement: -0.000000446, Best Loss: 0.000007165 in Epoch 875
Epoch 886
Epoch 886, Loss: 0.000017771, Improvement: -0.000002953, Best Loss: 0.000007165 in Epoch 875
Epoch 887
Epoch 887, Loss: 0.000012722, Improvement: -0.000005049, Best Loss: 0.000007165 in Epoch 875
Epoch 888
Epoch 888, Loss: 0.000013110, Improvement: 0.000000388, Best Loss: 0.000007165 in Epoch 875
Epoch 889
Epoch 889, Loss: 0.000012299, Improvement: -0.000000811, Best Loss: 0.000007165 in Epoch 875
Epoch 890
Epoch 890, Loss: 0.000012805, Improvement: 0.000000506, Best Loss: 0.000007165 in Epoch 875
Epoch 891
Epoch 891, Loss: 0.000016642, Improvement: 0.000003837, Best Loss: 0.000007165 in Epoch 875
Epoch 892
Epoch 892, Loss: 0.000028598, Improvement: 0.000011956, Best Loss: 0.000007165 in Epoch 875
Epoch 893
Epoch 893, Loss: 0.000033456, Improvement: 0.000004858, Best Loss: 0.000007165 in Epoch 875
Epoch 894
Epoch 894, Loss: 0.000058744, Improvement: 0.000025288, Best Loss: 0.000007165 in Epoch 875
Epoch 895
Epoch 895, Loss: 0.000033161, Improvement: -0.000025583, Best Loss: 0.000007165 in Epoch 875
Epoch 896
Epoch 896, Loss: 0.000025778, Improvement: -0.000007383, Best Loss: 0.000007165 in Epoch 875
Epoch 897
Epoch 897, Loss: 0.000027583, Improvement: 0.000001805, Best Loss: 0.000007165 in Epoch 875
Epoch 898
Epoch 898, Loss: 0.000024484, Improvement: -0.000003099, Best Loss: 0.000007165 in Epoch 875
Epoch 899
Epoch 899, Loss: 0.000023153, Improvement: -0.000001331, Best Loss: 0.000007165 in Epoch 875
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.000030917, Improvement: 0.000007764, Best Loss: 0.000007165 in Epoch 875
Epoch 901
Epoch 901, Loss: 0.000049106, Improvement: 0.000018189, Best Loss: 0.000007165 in Epoch 875
Epoch 902
Epoch 902, Loss: 0.000028588, Improvement: -0.000020519, Best Loss: 0.000007165 in Epoch 875
Epoch 903
Epoch 903, Loss: 0.000022576, Improvement: -0.000006012, Best Loss: 0.000007165 in Epoch 875
Epoch 904
Epoch 904, Loss: 0.000030611, Improvement: 0.000008035, Best Loss: 0.000007165 in Epoch 875
Epoch 905
Epoch 905, Loss: 0.000044735, Improvement: 0.000014123, Best Loss: 0.000007165 in Epoch 875
Epoch 906
Epoch 906, Loss: 0.000039363, Improvement: -0.000005372, Best Loss: 0.000007165 in Epoch 875
Epoch 907
Epoch 907, Loss: 0.000036788, Improvement: -0.000002575, Best Loss: 0.000007165 in Epoch 875
Epoch 908
Epoch 908, Loss: 0.000052647, Improvement: 0.000015859, Best Loss: 0.000007165 in Epoch 875
Epoch 909
Epoch 909, Loss: 0.000033409, Improvement: -0.000019239, Best Loss: 0.000007165 in Epoch 875
Epoch 910
Epoch 910, Loss: 0.000034848, Improvement: 0.000001439, Best Loss: 0.000007165 in Epoch 875
Epoch 911
Epoch 911, Loss: 0.000019251, Improvement: -0.000015597, Best Loss: 0.000007165 in Epoch 875
Epoch 912
Epoch 912, Loss: 0.000021504, Improvement: 0.000002253, Best Loss: 0.000007165 in Epoch 875
Epoch 913
Epoch 913, Loss: 0.000039766, Improvement: 0.000018262, Best Loss: 0.000007165 in Epoch 875
Epoch 914
Epoch 914, Loss: 0.000098558, Improvement: 0.000058792, Best Loss: 0.000007165 in Epoch 875
Epoch 915
Epoch 915, Loss: 0.000114601, Improvement: 0.000016043, Best Loss: 0.000007165 in Epoch 875
Epoch 916
Epoch 916, Loss: 0.000156396, Improvement: 0.000041795, Best Loss: 0.000007165 in Epoch 875
Epoch 917
Epoch 917, Loss: 0.000114575, Improvement: -0.000041821, Best Loss: 0.000007165 in Epoch 875
Epoch 918
Epoch 918, Loss: 0.000057895, Improvement: -0.000056680, Best Loss: 0.000007165 in Epoch 875
Epoch 919
Epoch 919, Loss: 0.000037348, Improvement: -0.000020546, Best Loss: 0.000007165 in Epoch 875
Epoch 920
Epoch 920, Loss: 0.000049078, Improvement: 0.000011729, Best Loss: 0.000007165 in Epoch 875
Epoch 921
Epoch 921, Loss: 0.000026947, Improvement: -0.000022130, Best Loss: 0.000007165 in Epoch 875
Epoch 922
Epoch 922, Loss: 0.000015575, Improvement: -0.000011372, Best Loss: 0.000007165 in Epoch 875
Epoch 923
Epoch 923, Loss: 0.000013033, Improvement: -0.000002542, Best Loss: 0.000007165 in Epoch 875
Epoch 924
Epoch 924, Loss: 0.000012337, Improvement: -0.000000696, Best Loss: 0.000007165 in Epoch 875
Epoch 925
Epoch 925, Loss: 0.000011472, Improvement: -0.000000866, Best Loss: 0.000007165 in Epoch 875
Epoch 926
Epoch 926, Loss: 0.000010922, Improvement: -0.000000549, Best Loss: 0.000007165 in Epoch 875
Epoch 927
Epoch 927, Loss: 0.000010569, Improvement: -0.000000354, Best Loss: 0.000007165 in Epoch 875
Epoch 928
Epoch 928, Loss: 0.000010659, Improvement: 0.000000090, Best Loss: 0.000007165 in Epoch 875
Epoch 929
Epoch 929, Loss: 0.000011375, Improvement: 0.000000716, Best Loss: 0.000007165 in Epoch 875
Epoch 930
Epoch 930, Loss: 0.000010834, Improvement: -0.000000541, Best Loss: 0.000007165 in Epoch 875
Epoch 931
A best model at epoch 931 has been saved with training error 0.000006928.
Epoch 931, Loss: 0.000009935, Improvement: -0.000000899, Best Loss: 0.000006928 in Epoch 931
Epoch 932
Epoch 932, Loss: 0.000009559, Improvement: -0.000000375, Best Loss: 0.000006928 in Epoch 931
Epoch 933
A best model at epoch 933 has been saved with training error 0.000006612.
Epoch 933, Loss: 0.000009484, Improvement: -0.000000076, Best Loss: 0.000006612 in Epoch 933
Epoch 934
Epoch 934, Loss: 0.000009642, Improvement: 0.000000158, Best Loss: 0.000006612 in Epoch 933
Epoch 935
Epoch 935, Loss: 0.000009547, Improvement: -0.000000095, Best Loss: 0.000006612 in Epoch 933
Epoch 936
Epoch 936, Loss: 0.000013898, Improvement: 0.000004351, Best Loss: 0.000006612 in Epoch 933
Epoch 937
Epoch 937, Loss: 0.000022380, Improvement: 0.000008482, Best Loss: 0.000006612 in Epoch 933
Epoch 938
Epoch 938, Loss: 0.000016157, Improvement: -0.000006224, Best Loss: 0.000006612 in Epoch 933
Epoch 939
Epoch 939, Loss: 0.000013352, Improvement: -0.000002804, Best Loss: 0.000006612 in Epoch 933
Epoch 940
Epoch 940, Loss: 0.000017503, Improvement: 0.000004151, Best Loss: 0.000006612 in Epoch 933
Epoch 941
Epoch 941, Loss: 0.000017961, Improvement: 0.000000458, Best Loss: 0.000006612 in Epoch 933
Epoch 942
Epoch 942, Loss: 0.000013932, Improvement: -0.000004029, Best Loss: 0.000006612 in Epoch 933
Epoch 943
Epoch 943, Loss: 0.000012065, Improvement: -0.000001867, Best Loss: 0.000006612 in Epoch 933
Epoch 944
Epoch 944, Loss: 0.000012710, Improvement: 0.000000646, Best Loss: 0.000006612 in Epoch 933
Epoch 945
Epoch 945, Loss: 0.000014713, Improvement: 0.000002002, Best Loss: 0.000006612 in Epoch 933
Epoch 946
A best model at epoch 946 has been saved with training error 0.000006450.
Epoch 946, Loss: 0.000013604, Improvement: -0.000001109, Best Loss: 0.000006450 in Epoch 946
Epoch 947
Epoch 947, Loss: 0.000013519, Improvement: -0.000000085, Best Loss: 0.000006450 in Epoch 946
Epoch 948
Epoch 948, Loss: 0.000015115, Improvement: 0.000001595, Best Loss: 0.000006450 in Epoch 946
Epoch 949
Epoch 949, Loss: 0.000033276, Improvement: 0.000018161, Best Loss: 0.000006450 in Epoch 946
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.000075642, Improvement: 0.000042366, Best Loss: 0.000006450 in Epoch 946
Epoch 951
Epoch 951, Loss: 0.000082503, Improvement: 0.000006861, Best Loss: 0.000006450 in Epoch 946
Epoch 952
Epoch 952, Loss: 0.000044299, Improvement: -0.000038204, Best Loss: 0.000006450 in Epoch 946
Epoch 953
Epoch 953, Loss: 0.000039068, Improvement: -0.000005231, Best Loss: 0.000006450 in Epoch 946
Epoch 954
Epoch 954, Loss: 0.000036216, Improvement: -0.000002852, Best Loss: 0.000006450 in Epoch 946
Epoch 955
Epoch 955, Loss: 0.000035536, Improvement: -0.000000680, Best Loss: 0.000006450 in Epoch 946
Epoch 956
Epoch 956, Loss: 0.000018116, Improvement: -0.000017421, Best Loss: 0.000006450 in Epoch 946
Epoch 957
Epoch 957, Loss: 0.000018401, Improvement: 0.000000286, Best Loss: 0.000006450 in Epoch 946
Epoch 958
Epoch 958, Loss: 0.000024524, Improvement: 0.000006123, Best Loss: 0.000006450 in Epoch 946
Epoch 959
Epoch 959, Loss: 0.000016610, Improvement: -0.000007914, Best Loss: 0.000006450 in Epoch 946
Epoch 960
Epoch 960, Loss: 0.000014782, Improvement: -0.000001828, Best Loss: 0.000006450 in Epoch 946
Epoch 961
Epoch 961, Loss: 0.000037747, Improvement: 0.000022965, Best Loss: 0.000006450 in Epoch 946
Epoch 962
Epoch 962, Loss: 0.000050623, Improvement: 0.000012876, Best Loss: 0.000006450 in Epoch 946
Epoch 963
Epoch 963, Loss: 0.000021007, Improvement: -0.000029616, Best Loss: 0.000006450 in Epoch 946
Epoch 964
Epoch 964, Loss: 0.000015994, Improvement: -0.000005013, Best Loss: 0.000006450 in Epoch 946
Epoch 965
Epoch 965, Loss: 0.000031026, Improvement: 0.000015032, Best Loss: 0.000006450 in Epoch 946
Epoch 966
Epoch 966, Loss: 0.000032413, Improvement: 0.000001387, Best Loss: 0.000006450 in Epoch 946
Epoch 967
Epoch 967, Loss: 0.000018936, Improvement: -0.000013477, Best Loss: 0.000006450 in Epoch 946
Epoch 968
Epoch 968, Loss: 0.000034620, Improvement: 0.000015684, Best Loss: 0.000006450 in Epoch 946
Epoch 969
Epoch 969, Loss: 0.000018122, Improvement: -0.000016498, Best Loss: 0.000006450 in Epoch 946
Epoch 970
Epoch 970, Loss: 0.000018710, Improvement: 0.000000588, Best Loss: 0.000006450 in Epoch 946
Epoch 971
Epoch 971, Loss: 0.000019070, Improvement: 0.000000361, Best Loss: 0.000006450 in Epoch 946
Epoch 972
Epoch 972, Loss: 0.000026218, Improvement: 0.000007148, Best Loss: 0.000006450 in Epoch 946
Epoch 973
Epoch 973, Loss: 0.000059364, Improvement: 0.000033146, Best Loss: 0.000006450 in Epoch 946
Epoch 974
Epoch 974, Loss: 0.000107401, Improvement: 0.000048037, Best Loss: 0.000006450 in Epoch 946
Epoch 975
Epoch 975, Loss: 0.000046125, Improvement: -0.000061277, Best Loss: 0.000006450 in Epoch 946
Epoch 976
Epoch 976, Loss: 0.000022770, Improvement: -0.000023355, Best Loss: 0.000006450 in Epoch 946
Epoch 977
Epoch 977, Loss: 0.000014428, Improvement: -0.000008341, Best Loss: 0.000006450 in Epoch 946
Epoch 978
Epoch 978, Loss: 0.000011375, Improvement: -0.000003054, Best Loss: 0.000006450 in Epoch 946
Epoch 979
Epoch 979, Loss: 0.000010047, Improvement: -0.000001328, Best Loss: 0.000006450 in Epoch 946
Epoch 980
Epoch 980, Loss: 0.000010495, Improvement: 0.000000448, Best Loss: 0.000006450 in Epoch 946
Epoch 981
Epoch 981, Loss: 0.000010100, Improvement: -0.000000395, Best Loss: 0.000006450 in Epoch 946
Epoch 982
Epoch 982, Loss: 0.000009562, Improvement: -0.000000538, Best Loss: 0.000006450 in Epoch 946
Epoch 983
A best model at epoch 983 has been saved with training error 0.000006317.
Epoch 983, Loss: 0.000009282, Improvement: -0.000000280, Best Loss: 0.000006317 in Epoch 983
Epoch 984
Epoch 984, Loss: 0.000008762, Improvement: -0.000000520, Best Loss: 0.000006317 in Epoch 983
Epoch 985
A best model at epoch 985 has been saved with training error 0.000006311.
Epoch 985, Loss: 0.000008646, Improvement: -0.000000117, Best Loss: 0.000006311 in Epoch 985
Epoch 986
A best model at epoch 986 has been saved with training error 0.000006269.
A best model at epoch 986 has been saved with training error 0.000005944.
Epoch 986, Loss: 0.000008563, Improvement: -0.000000083, Best Loss: 0.000005944 in Epoch 986
Epoch 987
A best model at epoch 987 has been saved with training error 0.000005500.
Epoch 987, Loss: 0.000008818, Improvement: 0.000000256, Best Loss: 0.000005500 in Epoch 987
Epoch 988
Epoch 988, Loss: 0.000008099, Improvement: -0.000000719, Best Loss: 0.000005500 in Epoch 987
Epoch 989
Epoch 989, Loss: 0.000008450, Improvement: 0.000000350, Best Loss: 0.000005500 in Epoch 987
Epoch 990
Epoch 990, Loss: 0.000011901, Improvement: 0.000003451, Best Loss: 0.000005500 in Epoch 987
Epoch 991
Epoch 991, Loss: 0.000011226, Improvement: -0.000000675, Best Loss: 0.000005500 in Epoch 987
Epoch 992
Epoch 992, Loss: 0.000011690, Improvement: 0.000000465, Best Loss: 0.000005500 in Epoch 987
Epoch 993
Epoch 993, Loss: 0.000013440, Improvement: 0.000001750, Best Loss: 0.000005500 in Epoch 987
Epoch 994
Epoch 994, Loss: 0.000013631, Improvement: 0.000000192, Best Loss: 0.000005500 in Epoch 987
Epoch 995
Epoch 995, Loss: 0.000019563, Improvement: 0.000005932, Best Loss: 0.000005500 in Epoch 987
Epoch 996
Epoch 996, Loss: 0.000033384, Improvement: 0.000013821, Best Loss: 0.000005500 in Epoch 987
Epoch 997
Epoch 997, Loss: 0.000075588, Improvement: 0.000042203, Best Loss: 0.000005500 in Epoch 987
Epoch 998
Epoch 998, Loss: 0.000036302, Improvement: -0.000039286, Best Loss: 0.000005500 in Epoch 987
Epoch 999
Epoch 999, Loss: 0.000028720, Improvement: -0.000007582, Best Loss: 0.000005500 in Epoch 987
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.000023439, Improvement: -0.000005281, Best Loss: 0.000005500 in Epoch 987
Epoch 1001
Epoch 1001, Loss: 0.000015188, Improvement: -0.000008251, Best Loss: 0.000005500 in Epoch 987
Epoch 1002
Epoch 1002, Loss: 0.000013215, Improvement: -0.000001973, Best Loss: 0.000005500 in Epoch 987
Epoch 1003
Epoch 1003, Loss: 0.000013174, Improvement: -0.000000040, Best Loss: 0.000005500 in Epoch 987
Epoch 1004
Epoch 1004, Loss: 0.000012056, Improvement: -0.000001118, Best Loss: 0.000005500 in Epoch 987
Epoch 1005
Epoch 1005, Loss: 0.000010050, Improvement: -0.000002006, Best Loss: 0.000005500 in Epoch 987
Epoch 1006
Epoch 1006, Loss: 0.000009186, Improvement: -0.000000864, Best Loss: 0.000005500 in Epoch 987
Epoch 1007
Epoch 1007, Loss: 0.000009570, Improvement: 0.000000383, Best Loss: 0.000005500 in Epoch 987
Epoch 1008
Epoch 1008, Loss: 0.000009446, Improvement: -0.000000123, Best Loss: 0.000005500 in Epoch 987
Epoch 1009
Epoch 1009, Loss: 0.000009977, Improvement: 0.000000530, Best Loss: 0.000005500 in Epoch 987
Epoch 1010
Epoch 1010, Loss: 0.000011772, Improvement: 0.000001795, Best Loss: 0.000005500 in Epoch 987
Epoch 1011
Epoch 1011, Loss: 0.000015683, Improvement: 0.000003911, Best Loss: 0.000005500 in Epoch 987
Epoch 1012
Epoch 1012, Loss: 0.000013328, Improvement: -0.000002354, Best Loss: 0.000005500 in Epoch 987
Epoch 1013
Epoch 1013, Loss: 0.000009759, Improvement: -0.000003570, Best Loss: 0.000005500 in Epoch 987
Epoch 1014
Epoch 1014, Loss: 0.000010994, Improvement: 0.000001235, Best Loss: 0.000005500 in Epoch 987
Epoch 1015
Epoch 1015, Loss: 0.000015439, Improvement: 0.000004445, Best Loss: 0.000005500 in Epoch 987
Epoch 1016
Epoch 1016, Loss: 0.000065899, Improvement: 0.000050460, Best Loss: 0.000005500 in Epoch 987
Epoch 1017
Epoch 1017, Loss: 0.000053739, Improvement: -0.000012160, Best Loss: 0.000005500 in Epoch 987
Epoch 1018
Epoch 1018, Loss: 0.000042710, Improvement: -0.000011029, Best Loss: 0.000005500 in Epoch 987
Epoch 1019
Epoch 1019, Loss: 0.000053100, Improvement: 0.000010390, Best Loss: 0.000005500 in Epoch 987
Epoch 1020
Epoch 1020, Loss: 0.000041983, Improvement: -0.000011117, Best Loss: 0.000005500 in Epoch 987
Epoch 1021
Epoch 1021, Loss: 0.000024331, Improvement: -0.000017651, Best Loss: 0.000005500 in Epoch 987
Epoch 1022
Epoch 1022, Loss: 0.000020080, Improvement: -0.000004251, Best Loss: 0.000005500 in Epoch 987
Epoch 1023
Epoch 1023, Loss: 0.000063620, Improvement: 0.000043540, Best Loss: 0.000005500 in Epoch 987
Epoch 1024
Epoch 1024, Loss: 0.000056564, Improvement: -0.000007056, Best Loss: 0.000005500 in Epoch 987
Epoch 1025
Epoch 1025, Loss: 0.000019717, Improvement: -0.000036848, Best Loss: 0.000005500 in Epoch 987
Epoch 1026
Epoch 1026, Loss: 0.000018534, Improvement: -0.000001183, Best Loss: 0.000005500 in Epoch 987
Epoch 1027
Epoch 1027, Loss: 0.000019034, Improvement: 0.000000501, Best Loss: 0.000005500 in Epoch 987
Epoch 1028
Epoch 1028, Loss: 0.000014320, Improvement: -0.000004714, Best Loss: 0.000005500 in Epoch 987
Epoch 1029
Epoch 1029, Loss: 0.000012367, Improvement: -0.000001953, Best Loss: 0.000005500 in Epoch 987
Epoch 1030
Epoch 1030, Loss: 0.000009825, Improvement: -0.000002542, Best Loss: 0.000005500 in Epoch 987
Epoch 1031
Epoch 1031, Loss: 0.000008830, Improvement: -0.000000995, Best Loss: 0.000005500 in Epoch 987
Epoch 1032
Epoch 1032, Loss: 0.000008267, Improvement: -0.000000563, Best Loss: 0.000005500 in Epoch 987
Epoch 1033
Epoch 1033, Loss: 0.000008324, Improvement: 0.000000057, Best Loss: 0.000005500 in Epoch 987
Epoch 1034
Epoch 1034, Loss: 0.000008698, Improvement: 0.000000374, Best Loss: 0.000005500 in Epoch 987
Epoch 1035
Epoch 1035, Loss: 0.000008283, Improvement: -0.000000415, Best Loss: 0.000005500 in Epoch 987
Epoch 1036
Epoch 1036, Loss: 0.000009862, Improvement: 0.000001579, Best Loss: 0.000005500 in Epoch 987
Epoch 1037
Epoch 1037, Loss: 0.000011351, Improvement: 0.000001489, Best Loss: 0.000005500 in Epoch 987
Epoch 1038
Epoch 1038, Loss: 0.000018102, Improvement: 0.000006752, Best Loss: 0.000005500 in Epoch 987
Epoch 1039
Epoch 1039, Loss: 0.000058475, Improvement: 0.000040373, Best Loss: 0.000005500 in Epoch 987
Epoch 1040
Epoch 1040, Loss: 0.000052309, Improvement: -0.000006166, Best Loss: 0.000005500 in Epoch 987
Epoch 1041
Epoch 1041, Loss: 0.000035153, Improvement: -0.000017157, Best Loss: 0.000005500 in Epoch 987
Epoch 1042
Epoch 1042, Loss: 0.000049601, Improvement: 0.000014448, Best Loss: 0.000005500 in Epoch 987
Epoch 1043
Epoch 1043, Loss: 0.000052189, Improvement: 0.000002588, Best Loss: 0.000005500 in Epoch 987
Epoch 1044
Epoch 1044, Loss: 0.000026546, Improvement: -0.000025644, Best Loss: 0.000005500 in Epoch 987
Epoch 1045
Epoch 1045, Loss: 0.000022578, Improvement: -0.000003967, Best Loss: 0.000005500 in Epoch 987
Epoch 1046
Epoch 1046, Loss: 0.000029848, Improvement: 0.000007270, Best Loss: 0.000005500 in Epoch 987
Epoch 1047
Epoch 1047, Loss: 0.000016493, Improvement: -0.000013356, Best Loss: 0.000005500 in Epoch 987
Epoch 1048
Epoch 1048, Loss: 0.000011315, Improvement: -0.000005177, Best Loss: 0.000005500 in Epoch 987
Epoch 1049
Epoch 1049, Loss: 0.000020200, Improvement: 0.000008884, Best Loss: 0.000005500 in Epoch 987
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.000013698, Improvement: -0.000006501, Best Loss: 0.000005500 in Epoch 987
Epoch 1051
Epoch 1051, Loss: 0.000013005, Improvement: -0.000000693, Best Loss: 0.000005500 in Epoch 987
Epoch 1052
Epoch 1052, Loss: 0.000011034, Improvement: -0.000001971, Best Loss: 0.000005500 in Epoch 987
Epoch 1053
Epoch 1053, Loss: 0.000014463, Improvement: 0.000003429, Best Loss: 0.000005500 in Epoch 987
Epoch 1054
Epoch 1054, Loss: 0.000014268, Improvement: -0.000000195, Best Loss: 0.000005500 in Epoch 987
Epoch 1055
Epoch 1055, Loss: 0.000036925, Improvement: 0.000022657, Best Loss: 0.000005500 in Epoch 987
Epoch 1056
Epoch 1056, Loss: 0.000056495, Improvement: 0.000019569, Best Loss: 0.000005500 in Epoch 987
Epoch 1057
Epoch 1057, Loss: 0.000063567, Improvement: 0.000007072, Best Loss: 0.000005500 in Epoch 987
Epoch 1058
Epoch 1058, Loss: 0.000062054, Improvement: -0.000001513, Best Loss: 0.000005500 in Epoch 987
Epoch 1059
Epoch 1059, Loss: 0.000053877, Improvement: -0.000008177, Best Loss: 0.000005500 in Epoch 987
Epoch 1060
Epoch 1060, Loss: 0.000044948, Improvement: -0.000008929, Best Loss: 0.000005500 in Epoch 987
Epoch 1061
Epoch 1061, Loss: 0.000035024, Improvement: -0.000009924, Best Loss: 0.000005500 in Epoch 987
Epoch 1062
Epoch 1062, Loss: 0.000018872, Improvement: -0.000016152, Best Loss: 0.000005500 in Epoch 987
Epoch 1063
Epoch 1063, Loss: 0.000015176, Improvement: -0.000003695, Best Loss: 0.000005500 in Epoch 987
Epoch 1064
Epoch 1064, Loss: 0.000011852, Improvement: -0.000003324, Best Loss: 0.000005500 in Epoch 987
Epoch 1065
Epoch 1065, Loss: 0.000012385, Improvement: 0.000000533, Best Loss: 0.000005500 in Epoch 987
Epoch 1066
Epoch 1066, Loss: 0.000017055, Improvement: 0.000004670, Best Loss: 0.000005500 in Epoch 987
Epoch 1067
Epoch 1067, Loss: 0.000030610, Improvement: 0.000013555, Best Loss: 0.000005500 in Epoch 987
Epoch 1068
Epoch 1068, Loss: 0.000030433, Improvement: -0.000000177, Best Loss: 0.000005500 in Epoch 987
Epoch 1069
Epoch 1069, Loss: 0.000017156, Improvement: -0.000013276, Best Loss: 0.000005500 in Epoch 987
Epoch 1070
Epoch 1070, Loss: 0.000012697, Improvement: -0.000004460, Best Loss: 0.000005500 in Epoch 987
Epoch 1071
Epoch 1071, Loss: 0.000010768, Improvement: -0.000001928, Best Loss: 0.000005500 in Epoch 987
Epoch 1072
Epoch 1072, Loss: 0.000015457, Improvement: 0.000004689, Best Loss: 0.000005500 in Epoch 987
Epoch 1073
Epoch 1073, Loss: 0.000023143, Improvement: 0.000007686, Best Loss: 0.000005500 in Epoch 987
Epoch 1074
Epoch 1074, Loss: 0.000021821, Improvement: -0.000001322, Best Loss: 0.000005500 in Epoch 987
Epoch 1075
Epoch 1075, Loss: 0.000059930, Improvement: 0.000038109, Best Loss: 0.000005500 in Epoch 987
Epoch 1076
Epoch 1076, Loss: 0.000054072, Improvement: -0.000005858, Best Loss: 0.000005500 in Epoch 987
Epoch 1077
Epoch 1077, Loss: 0.000025031, Improvement: -0.000029041, Best Loss: 0.000005500 in Epoch 987
Epoch 1078
Epoch 1078, Loss: 0.000014341, Improvement: -0.000010690, Best Loss: 0.000005500 in Epoch 987
Epoch 1079
Epoch 1079, Loss: 0.000011504, Improvement: -0.000002837, Best Loss: 0.000005500 in Epoch 987
Epoch 1080
Epoch 1080, Loss: 0.000010995, Improvement: -0.000000509, Best Loss: 0.000005500 in Epoch 987
Epoch 1081
Epoch 1081, Loss: 0.000008962, Improvement: -0.000002034, Best Loss: 0.000005500 in Epoch 987
Epoch 1082
Epoch 1082, Loss: 0.000011390, Improvement: 0.000002429, Best Loss: 0.000005500 in Epoch 987
Epoch 1083
Epoch 1083, Loss: 0.000014448, Improvement: 0.000003058, Best Loss: 0.000005500 in Epoch 987
Epoch 1084
Epoch 1084, Loss: 0.000036768, Improvement: 0.000022319, Best Loss: 0.000005500 in Epoch 987
Epoch 1085
Epoch 1085, Loss: 0.000025001, Improvement: -0.000011767, Best Loss: 0.000005500 in Epoch 987
Epoch 1086
Epoch 1086, Loss: 0.000015537, Improvement: -0.000009463, Best Loss: 0.000005500 in Epoch 987
Epoch 1087
Epoch 1087, Loss: 0.000017313, Improvement: 0.000001776, Best Loss: 0.000005500 in Epoch 987
Epoch 1088
Epoch 1088, Loss: 0.000011081, Improvement: -0.000006232, Best Loss: 0.000005500 in Epoch 987
Epoch 1089
Epoch 1089, Loss: 0.000012087, Improvement: 0.000001006, Best Loss: 0.000005500 in Epoch 987
Epoch 1090
Epoch 1090, Loss: 0.000017426, Improvement: 0.000005338, Best Loss: 0.000005500 in Epoch 987
Epoch 1091
Epoch 1091, Loss: 0.000028567, Improvement: 0.000011141, Best Loss: 0.000005500 in Epoch 987
Epoch 1092
Epoch 1092, Loss: 0.000030676, Improvement: 0.000002109, Best Loss: 0.000005500 in Epoch 987
Epoch 1093
Epoch 1093, Loss: 0.000026907, Improvement: -0.000003769, Best Loss: 0.000005500 in Epoch 987
Epoch 1094
Epoch 1094, Loss: 0.000025447, Improvement: -0.000001460, Best Loss: 0.000005500 in Epoch 987
Epoch 1095
Epoch 1095, Loss: 0.000032259, Improvement: 0.000006811, Best Loss: 0.000005500 in Epoch 987
Epoch 1096
Epoch 1096, Loss: 0.000033837, Improvement: 0.000001578, Best Loss: 0.000005500 in Epoch 987
Epoch 1097
Epoch 1097, Loss: 0.000040760, Improvement: 0.000006923, Best Loss: 0.000005500 in Epoch 987
Epoch 1098
Epoch 1098, Loss: 0.000082941, Improvement: 0.000042181, Best Loss: 0.000005500 in Epoch 987
Epoch 1099
Epoch 1099, Loss: 0.000066911, Improvement: -0.000016030, Best Loss: 0.000005500 in Epoch 987
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.000033168, Improvement: -0.000033744, Best Loss: 0.000005500 in Epoch 987
Epoch 1101
Epoch 1101, Loss: 0.000023976, Improvement: -0.000009191, Best Loss: 0.000005500 in Epoch 987
Epoch 1102
Epoch 1102, Loss: 0.000015317, Improvement: -0.000008660, Best Loss: 0.000005500 in Epoch 987
Epoch 1103
Epoch 1103, Loss: 0.000011876, Improvement: -0.000003440, Best Loss: 0.000005500 in Epoch 987
Epoch 1104
Epoch 1104, Loss: 0.000013160, Improvement: 0.000001283, Best Loss: 0.000005500 in Epoch 987
Epoch 1105
Epoch 1105, Loss: 0.000012124, Improvement: -0.000001035, Best Loss: 0.000005500 in Epoch 987
Epoch 1106
Epoch 1106, Loss: 0.000009056, Improvement: -0.000003068, Best Loss: 0.000005500 in Epoch 987
Epoch 1107
A best model at epoch 1107 has been saved with training error 0.000005303.
Epoch 1107, Loss: 0.000008051, Improvement: -0.000001006, Best Loss: 0.000005303 in Epoch 1107
Epoch 1108
A best model at epoch 1108 has been saved with training error 0.000004620.
Epoch 1108, Loss: 0.000007913, Improvement: -0.000000138, Best Loss: 0.000004620 in Epoch 1108
Epoch 1109
Epoch 1109, Loss: 0.000010148, Improvement: 0.000002235, Best Loss: 0.000004620 in Epoch 1108
Epoch 1110
Epoch 1110, Loss: 0.000030641, Improvement: 0.000020494, Best Loss: 0.000004620 in Epoch 1108
Epoch 1111
Epoch 1111, Loss: 0.000033670, Improvement: 0.000003029, Best Loss: 0.000004620 in Epoch 1108
Epoch 1112
Epoch 1112, Loss: 0.000025181, Improvement: -0.000008490, Best Loss: 0.000004620 in Epoch 1108
Epoch 1113
Epoch 1113, Loss: 0.000112088, Improvement: 0.000086907, Best Loss: 0.000004620 in Epoch 1108
Epoch 1114
Epoch 1114, Loss: 0.000067272, Improvement: -0.000044816, Best Loss: 0.000004620 in Epoch 1108
Epoch 1115
Epoch 1115, Loss: 0.000028243, Improvement: -0.000039029, Best Loss: 0.000004620 in Epoch 1108
Epoch 1116
Epoch 1116, Loss: 0.000016811, Improvement: -0.000011432, Best Loss: 0.000004620 in Epoch 1108
Epoch 1117
Epoch 1117, Loss: 0.000012267, Improvement: -0.000004545, Best Loss: 0.000004620 in Epoch 1108
Epoch 1118
Epoch 1118, Loss: 0.000012239, Improvement: -0.000000027, Best Loss: 0.000004620 in Epoch 1108
Epoch 1119
Epoch 1119, Loss: 0.000009312, Improvement: -0.000002927, Best Loss: 0.000004620 in Epoch 1108
Epoch 1120
Epoch 1120, Loss: 0.000009385, Improvement: 0.000000072, Best Loss: 0.000004620 in Epoch 1108
Epoch 1121
Epoch 1121, Loss: 0.000010955, Improvement: 0.000001571, Best Loss: 0.000004620 in Epoch 1108
Epoch 1122
Epoch 1122, Loss: 0.000008779, Improvement: -0.000002176, Best Loss: 0.000004620 in Epoch 1108
Epoch 1123
Epoch 1123, Loss: 0.000007848, Improvement: -0.000000931, Best Loss: 0.000004620 in Epoch 1108
Epoch 1124
Epoch 1124, Loss: 0.000006955, Improvement: -0.000000893, Best Loss: 0.000004620 in Epoch 1108
Epoch 1125
Epoch 1125, Loss: 0.000006725, Improvement: -0.000000230, Best Loss: 0.000004620 in Epoch 1108
Epoch 1126
Epoch 1126, Loss: 0.000007929, Improvement: 0.000001204, Best Loss: 0.000004620 in Epoch 1108
Epoch 1127
Epoch 1127, Loss: 0.000013299, Improvement: 0.000005370, Best Loss: 0.000004620 in Epoch 1108
Epoch 1128
Epoch 1128, Loss: 0.000017324, Improvement: 0.000004025, Best Loss: 0.000004620 in Epoch 1108
Epoch 1129
Epoch 1129, Loss: 0.000018231, Improvement: 0.000000907, Best Loss: 0.000004620 in Epoch 1108
Epoch 1130
Epoch 1130, Loss: 0.000019201, Improvement: 0.000000971, Best Loss: 0.000004620 in Epoch 1108
Epoch 1131
Epoch 1131, Loss: 0.000021549, Improvement: 0.000002348, Best Loss: 0.000004620 in Epoch 1108
Epoch 1132
Epoch 1132, Loss: 0.000024262, Improvement: 0.000002712, Best Loss: 0.000004620 in Epoch 1108
Epoch 1133
Epoch 1133, Loss: 0.000015227, Improvement: -0.000009035, Best Loss: 0.000004620 in Epoch 1108
Epoch 1134
Epoch 1134, Loss: 0.000008982, Improvement: -0.000006245, Best Loss: 0.000004620 in Epoch 1108
Epoch 1135
Epoch 1135, Loss: 0.000008482, Improvement: -0.000000501, Best Loss: 0.000004620 in Epoch 1108
Epoch 1136
Epoch 1136, Loss: 0.000013923, Improvement: 0.000005441, Best Loss: 0.000004620 in Epoch 1108
Epoch 1137
Epoch 1137, Loss: 0.000023342, Improvement: 0.000009419, Best Loss: 0.000004620 in Epoch 1108
Epoch 1138
Epoch 1138, Loss: 0.000061499, Improvement: 0.000038157, Best Loss: 0.000004620 in Epoch 1108
Epoch 1139
Epoch 1139, Loss: 0.000061211, Improvement: -0.000000288, Best Loss: 0.000004620 in Epoch 1108
Epoch 1140
Epoch 1140, Loss: 0.000034459, Improvement: -0.000026752, Best Loss: 0.000004620 in Epoch 1108
Epoch 1141
Epoch 1141, Loss: 0.000021293, Improvement: -0.000013166, Best Loss: 0.000004620 in Epoch 1108
Epoch 1142
Epoch 1142, Loss: 0.000021719, Improvement: 0.000000426, Best Loss: 0.000004620 in Epoch 1108
Epoch 1143
Epoch 1143, Loss: 0.000014967, Improvement: -0.000006752, Best Loss: 0.000004620 in Epoch 1108
Epoch 1144
Epoch 1144, Loss: 0.000009551, Improvement: -0.000005416, Best Loss: 0.000004620 in Epoch 1108
Epoch 1145
Epoch 1145, Loss: 0.000008344, Improvement: -0.000001207, Best Loss: 0.000004620 in Epoch 1108
Epoch 1146
Epoch 1146, Loss: 0.000007819, Improvement: -0.000000525, Best Loss: 0.000004620 in Epoch 1108
Epoch 1147
Epoch 1147, Loss: 0.000007677, Improvement: -0.000000143, Best Loss: 0.000004620 in Epoch 1108
Epoch 1148
Epoch 1148, Loss: 0.000010369, Improvement: 0.000002693, Best Loss: 0.000004620 in Epoch 1108
Epoch 1149
Epoch 1149, Loss: 0.000010309, Improvement: -0.000000061, Best Loss: 0.000004620 in Epoch 1108
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.000009492, Improvement: -0.000000817, Best Loss: 0.000004620 in Epoch 1108
Epoch 1151
Epoch 1151, Loss: 0.000011870, Improvement: 0.000002378, Best Loss: 0.000004620 in Epoch 1108
Epoch 1152
Epoch 1152, Loss: 0.000017026, Improvement: 0.000005156, Best Loss: 0.000004620 in Epoch 1108
Epoch 1153
Epoch 1153, Loss: 0.000015730, Improvement: -0.000001296, Best Loss: 0.000004620 in Epoch 1108
Epoch 1154
Epoch 1154, Loss: 0.000012732, Improvement: -0.000002997, Best Loss: 0.000004620 in Epoch 1108
Epoch 1155
Epoch 1155, Loss: 0.000016355, Improvement: 0.000003623, Best Loss: 0.000004620 in Epoch 1108
Epoch 1156
Epoch 1156, Loss: 0.000025254, Improvement: 0.000008899, Best Loss: 0.000004620 in Epoch 1108
Epoch 1157
Epoch 1157, Loss: 0.000045380, Improvement: 0.000020125, Best Loss: 0.000004620 in Epoch 1108
Epoch 1158
Epoch 1158, Loss: 0.000044173, Improvement: -0.000001207, Best Loss: 0.000004620 in Epoch 1108
Epoch 1159
Epoch 1159, Loss: 0.000062012, Improvement: 0.000017840, Best Loss: 0.000004620 in Epoch 1108
Epoch 1160
Epoch 1160, Loss: 0.000086573, Improvement: 0.000024560, Best Loss: 0.000004620 in Epoch 1108
Epoch 1161
Epoch 1161, Loss: 0.000035748, Improvement: -0.000050825, Best Loss: 0.000004620 in Epoch 1108
Epoch 1162
Epoch 1162, Loss: 0.000020107, Improvement: -0.000015640, Best Loss: 0.000004620 in Epoch 1108
Epoch 1163
Epoch 1163, Loss: 0.000012944, Improvement: -0.000007164, Best Loss: 0.000004620 in Epoch 1108
Epoch 1164
Epoch 1164, Loss: 0.000009532, Improvement: -0.000003411, Best Loss: 0.000004620 in Epoch 1108
Epoch 1165
Epoch 1165, Loss: 0.000010336, Improvement: 0.000000804, Best Loss: 0.000004620 in Epoch 1108
Epoch 1166
Epoch 1166, Loss: 0.000008468, Improvement: -0.000001868, Best Loss: 0.000004620 in Epoch 1108
Epoch 1167
Epoch 1167, Loss: 0.000008438, Improvement: -0.000000030, Best Loss: 0.000004620 in Epoch 1108
Epoch 1168
Epoch 1168, Loss: 0.000012556, Improvement: 0.000004118, Best Loss: 0.000004620 in Epoch 1108
Epoch 1169
Epoch 1169, Loss: 0.000031185, Improvement: 0.000018629, Best Loss: 0.000004620 in Epoch 1108
Epoch 1170
Epoch 1170, Loss: 0.000035486, Improvement: 0.000004301, Best Loss: 0.000004620 in Epoch 1108
Epoch 1171
Epoch 1171, Loss: 0.000052347, Improvement: 0.000016861, Best Loss: 0.000004620 in Epoch 1108
Epoch 1172
Epoch 1172, Loss: 0.000027854, Improvement: -0.000024493, Best Loss: 0.000004620 in Epoch 1108
Epoch 1173
Epoch 1173, Loss: 0.000019082, Improvement: -0.000008772, Best Loss: 0.000004620 in Epoch 1108
Epoch 1174
Epoch 1174, Loss: 0.000011615, Improvement: -0.000007467, Best Loss: 0.000004620 in Epoch 1108
Epoch 1175
Epoch 1175, Loss: 0.000009913, Improvement: -0.000001702, Best Loss: 0.000004620 in Epoch 1108
Epoch 1176
Epoch 1176, Loss: 0.000011745, Improvement: 0.000001832, Best Loss: 0.000004620 in Epoch 1108
Epoch 1177
Epoch 1177, Loss: 0.000012853, Improvement: 0.000001108, Best Loss: 0.000004620 in Epoch 1108
Epoch 1178
Epoch 1178, Loss: 0.000042015, Improvement: 0.000029162, Best Loss: 0.000004620 in Epoch 1108
Epoch 1179
Epoch 1179, Loss: 0.000024780, Improvement: -0.000017235, Best Loss: 0.000004620 in Epoch 1108
Epoch 1180
Epoch 1180, Loss: 0.000028384, Improvement: 0.000003605, Best Loss: 0.000004620 in Epoch 1108
Epoch 1181
Epoch 1181, Loss: 0.000055667, Improvement: 0.000027283, Best Loss: 0.000004620 in Epoch 1108
Epoch 1182
Epoch 1182, Loss: 0.000071797, Improvement: 0.000016129, Best Loss: 0.000004620 in Epoch 1108
Epoch 1183
Epoch 1183, Loss: 0.000073259, Improvement: 0.000001462, Best Loss: 0.000004620 in Epoch 1108
Epoch 1184
Epoch 1184, Loss: 0.000034740, Improvement: -0.000038519, Best Loss: 0.000004620 in Epoch 1108
Epoch 1185
Epoch 1185, Loss: 0.000016168, Improvement: -0.000018572, Best Loss: 0.000004620 in Epoch 1108
Epoch 1186
Epoch 1186, Loss: 0.000010296, Improvement: -0.000005872, Best Loss: 0.000004620 in Epoch 1108
Epoch 1187
Epoch 1187, Loss: 0.000008729, Improvement: -0.000001566, Best Loss: 0.000004620 in Epoch 1108
Epoch 1188
Epoch 1188, Loss: 0.000008545, Improvement: -0.000000184, Best Loss: 0.000004620 in Epoch 1108
Epoch 1189
Epoch 1189, Loss: 0.000008983, Improvement: 0.000000438, Best Loss: 0.000004620 in Epoch 1108
Epoch 1190
Epoch 1190, Loss: 0.000008446, Improvement: -0.000000538, Best Loss: 0.000004620 in Epoch 1108
Epoch 1191
Epoch 1191, Loss: 0.000007346, Improvement: -0.000001099, Best Loss: 0.000004620 in Epoch 1108
Epoch 1192
Epoch 1192, Loss: 0.000006643, Improvement: -0.000000704, Best Loss: 0.000004620 in Epoch 1108
Epoch 1193
A best model at epoch 1193 has been saved with training error 0.000004502.
Epoch 1193, Loss: 0.000006395, Improvement: -0.000000247, Best Loss: 0.000004502 in Epoch 1193
Epoch 1194
A best model at epoch 1194 has been saved with training error 0.000004497.
Epoch 1194, Loss: 0.000006315, Improvement: -0.000000080, Best Loss: 0.000004497 in Epoch 1194
Epoch 1195
A best model at epoch 1195 has been saved with training error 0.000004300.
Epoch 1195, Loss: 0.000006307, Improvement: -0.000000008, Best Loss: 0.000004300 in Epoch 1195
Epoch 1196
Epoch 1196, Loss: 0.000006401, Improvement: 0.000000094, Best Loss: 0.000004300 in Epoch 1195
Epoch 1197
Epoch 1197, Loss: 0.000008354, Improvement: 0.000001952, Best Loss: 0.000004300 in Epoch 1195
Epoch 1198
Epoch 1198, Loss: 0.000009272, Improvement: 0.000000919, Best Loss: 0.000004300 in Epoch 1195
Epoch 1199
Epoch 1199, Loss: 0.000011723, Improvement: 0.000002450, Best Loss: 0.000004300 in Epoch 1195
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.000011793, Improvement: 0.000000070, Best Loss: 0.000004300 in Epoch 1195
Epoch 1201
Epoch 1201, Loss: 0.000018069, Improvement: 0.000006276, Best Loss: 0.000004300 in Epoch 1195
Epoch 1202
Epoch 1202, Loss: 0.000042612, Improvement: 0.000024543, Best Loss: 0.000004300 in Epoch 1195
Epoch 1203
Epoch 1203, Loss: 0.000048711, Improvement: 0.000006099, Best Loss: 0.000004300 in Epoch 1195
Epoch 1204
Epoch 1204, Loss: 0.000018323, Improvement: -0.000030388, Best Loss: 0.000004300 in Epoch 1195
Epoch 1205
Epoch 1205, Loss: 0.000013215, Improvement: -0.000005107, Best Loss: 0.000004300 in Epoch 1195
Epoch 1206
Epoch 1206, Loss: 0.000017607, Improvement: 0.000004392, Best Loss: 0.000004300 in Epoch 1195
Epoch 1207
Epoch 1207, Loss: 0.000031375, Improvement: 0.000013768, Best Loss: 0.000004300 in Epoch 1195
Epoch 1208
Epoch 1208, Loss: 0.000031369, Improvement: -0.000000006, Best Loss: 0.000004300 in Epoch 1195
Epoch 1209
Epoch 1209, Loss: 0.000031306, Improvement: -0.000000062, Best Loss: 0.000004300 in Epoch 1195
Epoch 1210
Epoch 1210, Loss: 0.000040503, Improvement: 0.000009197, Best Loss: 0.000004300 in Epoch 1195
Epoch 1211
Epoch 1211, Loss: 0.000071346, Improvement: 0.000030842, Best Loss: 0.000004300 in Epoch 1195
Epoch 1212
Epoch 1212, Loss: 0.000089813, Improvement: 0.000018467, Best Loss: 0.000004300 in Epoch 1195
Epoch 1213
Epoch 1213, Loss: 0.000061299, Improvement: -0.000028514, Best Loss: 0.000004300 in Epoch 1195
Epoch 1214
Epoch 1214, Loss: 0.000027753, Improvement: -0.000033546, Best Loss: 0.000004300 in Epoch 1195
Epoch 1215
Epoch 1215, Loss: 0.000027518, Improvement: -0.000000235, Best Loss: 0.000004300 in Epoch 1195
Epoch 1216
Epoch 1216, Loss: 0.000016768, Improvement: -0.000010750, Best Loss: 0.000004300 in Epoch 1195
Epoch 1217
Epoch 1217, Loss: 0.000013952, Improvement: -0.000002816, Best Loss: 0.000004300 in Epoch 1195
Epoch 1218
Epoch 1218, Loss: 0.000019088, Improvement: 0.000005136, Best Loss: 0.000004300 in Epoch 1195
Epoch 1219
Epoch 1219, Loss: 0.000029762, Improvement: 0.000010674, Best Loss: 0.000004300 in Epoch 1195
Epoch 1220
Epoch 1220, Loss: 0.000017263, Improvement: -0.000012500, Best Loss: 0.000004300 in Epoch 1195
Epoch 1221
Epoch 1221, Loss: 0.000013523, Improvement: -0.000003739, Best Loss: 0.000004300 in Epoch 1195
Epoch 1222
Epoch 1222, Loss: 0.000011369, Improvement: -0.000002154, Best Loss: 0.000004300 in Epoch 1195
Epoch 1223
Epoch 1223, Loss: 0.000008738, Improvement: -0.000002631, Best Loss: 0.000004300 in Epoch 1195
Epoch 1224
Epoch 1224, Loss: 0.000007811, Improvement: -0.000000927, Best Loss: 0.000004300 in Epoch 1195
Epoch 1225
Epoch 1225, Loss: 0.000015809, Improvement: 0.000007997, Best Loss: 0.000004300 in Epoch 1195
Epoch 1226
Epoch 1226, Loss: 0.000015806, Improvement: -0.000000003, Best Loss: 0.000004300 in Epoch 1195
Epoch 1227
Epoch 1227, Loss: 0.000010461, Improvement: -0.000005345, Best Loss: 0.000004300 in Epoch 1195
Epoch 1228
Epoch 1228, Loss: 0.000009191, Improvement: -0.000001270, Best Loss: 0.000004300 in Epoch 1195
Epoch 1229
Epoch 1229, Loss: 0.000011312, Improvement: 0.000002121, Best Loss: 0.000004300 in Epoch 1195
Epoch 1230
Epoch 1230, Loss: 0.000009871, Improvement: -0.000001442, Best Loss: 0.000004300 in Epoch 1195
Epoch 1231
Epoch 1231, Loss: 0.000009331, Improvement: -0.000000539, Best Loss: 0.000004300 in Epoch 1195
Epoch 1232
Epoch 1232, Loss: 0.000017938, Improvement: 0.000008607, Best Loss: 0.000004300 in Epoch 1195
Epoch 1233
Epoch 1233, Loss: 0.000017234, Improvement: -0.000000704, Best Loss: 0.000004300 in Epoch 1195
Epoch 1234
Epoch 1234, Loss: 0.000026943, Improvement: 0.000009709, Best Loss: 0.000004300 in Epoch 1195
Epoch 1235
Epoch 1235, Loss: 0.000052649, Improvement: 0.000025706, Best Loss: 0.000004300 in Epoch 1195
Epoch 1236
Epoch 1236, Loss: 0.000057973, Improvement: 0.000005324, Best Loss: 0.000004300 in Epoch 1195
Epoch 1237
Epoch 1237, Loss: 0.000038825, Improvement: -0.000019148, Best Loss: 0.000004300 in Epoch 1195
Epoch 1238
Epoch 1238, Loss: 0.000026931, Improvement: -0.000011894, Best Loss: 0.000004300 in Epoch 1195
Epoch 1239
Epoch 1239, Loss: 0.000017189, Improvement: -0.000009743, Best Loss: 0.000004300 in Epoch 1195
Epoch 1240
Epoch 1240, Loss: 0.000011691, Improvement: -0.000005498, Best Loss: 0.000004300 in Epoch 1195
Epoch 1241
Epoch 1241, Loss: 0.000008460, Improvement: -0.000003231, Best Loss: 0.000004300 in Epoch 1195
Epoch 1242
Epoch 1242, Loss: 0.000007145, Improvement: -0.000001315, Best Loss: 0.000004300 in Epoch 1195
Epoch 1243
Epoch 1243, Loss: 0.000008107, Improvement: 0.000000962, Best Loss: 0.000004300 in Epoch 1195
Epoch 1244
Epoch 1244, Loss: 0.000010684, Improvement: 0.000002577, Best Loss: 0.000004300 in Epoch 1195
Epoch 1245
Epoch 1245, Loss: 0.000019687, Improvement: 0.000009002, Best Loss: 0.000004300 in Epoch 1195
Epoch 1246
Epoch 1246, Loss: 0.000026562, Improvement: 0.000006875, Best Loss: 0.000004300 in Epoch 1195
Epoch 1247
Epoch 1247, Loss: 0.000042029, Improvement: 0.000015467, Best Loss: 0.000004300 in Epoch 1195
Epoch 1248
Epoch 1248, Loss: 0.000043589, Improvement: 0.000001560, Best Loss: 0.000004300 in Epoch 1195
Epoch 1249
Epoch 1249, Loss: 0.000026998, Improvement: -0.000016591, Best Loss: 0.000004300 in Epoch 1195
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.000015347, Improvement: -0.000011651, Best Loss: 0.000004300 in Epoch 1195
Epoch 1251
Epoch 1251, Loss: 0.000011686, Improvement: -0.000003661, Best Loss: 0.000004300 in Epoch 1195
Epoch 1252
Epoch 1252, Loss: 0.000012632, Improvement: 0.000000946, Best Loss: 0.000004300 in Epoch 1195
Epoch 1253
Epoch 1253, Loss: 0.000048058, Improvement: 0.000035426, Best Loss: 0.000004300 in Epoch 1195
Epoch 1254
Epoch 1254, Loss: 0.000022968, Improvement: -0.000025090, Best Loss: 0.000004300 in Epoch 1195
Epoch 1255
Epoch 1255, Loss: 0.000014489, Improvement: -0.000008479, Best Loss: 0.000004300 in Epoch 1195
Epoch 1256
Epoch 1256, Loss: 0.000011626, Improvement: -0.000002863, Best Loss: 0.000004300 in Epoch 1195
Epoch 1257
Epoch 1257, Loss: 0.000018629, Improvement: 0.000007003, Best Loss: 0.000004300 in Epoch 1195
Epoch 1258
Epoch 1258, Loss: 0.000022798, Improvement: 0.000004168, Best Loss: 0.000004300 in Epoch 1195
Epoch 1259
Epoch 1259, Loss: 0.000021769, Improvement: -0.000001028, Best Loss: 0.000004300 in Epoch 1195
Epoch 1260
Epoch 1260, Loss: 0.000016132, Improvement: -0.000005638, Best Loss: 0.000004300 in Epoch 1195
Epoch 1261
Epoch 1261, Loss: 0.000019760, Improvement: 0.000003628, Best Loss: 0.000004300 in Epoch 1195
Epoch 1262
Epoch 1262, Loss: 0.000013761, Improvement: -0.000005999, Best Loss: 0.000004300 in Epoch 1195
Epoch 1263
Epoch 1263, Loss: 0.000011023, Improvement: -0.000002738, Best Loss: 0.000004300 in Epoch 1195
Epoch 1264
Epoch 1264, Loss: 0.000007818, Improvement: -0.000003205, Best Loss: 0.000004300 in Epoch 1195
Epoch 1265
Epoch 1265, Loss: 0.000006435, Improvement: -0.000001383, Best Loss: 0.000004300 in Epoch 1195
Epoch 1266
A best model at epoch 1266 has been saved with training error 0.000003565.
Epoch 1266, Loss: 0.000006010, Improvement: -0.000000425, Best Loss: 0.000003565 in Epoch 1266
Epoch 1267
Epoch 1267, Loss: 0.000006321, Improvement: 0.000000310, Best Loss: 0.000003565 in Epoch 1266
Epoch 1268
Epoch 1268, Loss: 0.000006950, Improvement: 0.000000630, Best Loss: 0.000003565 in Epoch 1266
Epoch 1269
Epoch 1269, Loss: 0.000010655, Improvement: 0.000003704, Best Loss: 0.000003565 in Epoch 1266
Epoch 1270
Epoch 1270, Loss: 0.000010870, Improvement: 0.000000216, Best Loss: 0.000003565 in Epoch 1266
Epoch 1271
Epoch 1271, Loss: 0.000022695, Improvement: 0.000011825, Best Loss: 0.000003565 in Epoch 1266
Epoch 1272
Epoch 1272, Loss: 0.000031871, Improvement: 0.000009176, Best Loss: 0.000003565 in Epoch 1266
Epoch 1273
Epoch 1273, Loss: 0.000015156, Improvement: -0.000016715, Best Loss: 0.000003565 in Epoch 1266
Epoch 1274
Epoch 1274, Loss: 0.000017421, Improvement: 0.000002265, Best Loss: 0.000003565 in Epoch 1266
Epoch 1275
Epoch 1275, Loss: 0.000012168, Improvement: -0.000005253, Best Loss: 0.000003565 in Epoch 1266
Epoch 1276
Epoch 1276, Loss: 0.000015999, Improvement: 0.000003831, Best Loss: 0.000003565 in Epoch 1266
Epoch 1277
Epoch 1277, Loss: 0.000024218, Improvement: 0.000008219, Best Loss: 0.000003565 in Epoch 1266
Epoch 1278
Epoch 1278, Loss: 0.000015699, Improvement: -0.000008519, Best Loss: 0.000003565 in Epoch 1266
Epoch 1279
Epoch 1279, Loss: 0.000019852, Improvement: 0.000004153, Best Loss: 0.000003565 in Epoch 1266
Epoch 1280
Epoch 1280, Loss: 0.000033643, Improvement: 0.000013791, Best Loss: 0.000003565 in Epoch 1266
Epoch 1281
Epoch 1281, Loss: 0.000031525, Improvement: -0.000002118, Best Loss: 0.000003565 in Epoch 1266
Epoch 1282
Epoch 1282, Loss: 0.000044299, Improvement: 0.000012774, Best Loss: 0.000003565 in Epoch 1266
Epoch 1283
Epoch 1283, Loss: 0.000028030, Improvement: -0.000016269, Best Loss: 0.000003565 in Epoch 1266
Epoch 1284
Epoch 1284, Loss: 0.000019724, Improvement: -0.000008306, Best Loss: 0.000003565 in Epoch 1266
Epoch 1285
Epoch 1285, Loss: 0.000015328, Improvement: -0.000004396, Best Loss: 0.000003565 in Epoch 1266
Epoch 1286
Epoch 1286, Loss: 0.000022316, Improvement: 0.000006989, Best Loss: 0.000003565 in Epoch 1266
Epoch 1287
Epoch 1287, Loss: 0.000050588, Improvement: 0.000028272, Best Loss: 0.000003565 in Epoch 1266
Epoch 1288
Epoch 1288, Loss: 0.000069138, Improvement: 0.000018550, Best Loss: 0.000003565 in Epoch 1266
Epoch 1289
Epoch 1289, Loss: 0.000048483, Improvement: -0.000020655, Best Loss: 0.000003565 in Epoch 1266
Epoch 1290
Epoch 1290, Loss: 0.000023861, Improvement: -0.000024622, Best Loss: 0.000003565 in Epoch 1266
Epoch 1291
Epoch 1291, Loss: 0.000027947, Improvement: 0.000004086, Best Loss: 0.000003565 in Epoch 1266
Epoch 1292
Epoch 1292, Loss: 0.000013751, Improvement: -0.000014195, Best Loss: 0.000003565 in Epoch 1266
Epoch 1293
Epoch 1293, Loss: 0.000017068, Improvement: 0.000003316, Best Loss: 0.000003565 in Epoch 1266
Epoch 1294
Epoch 1294, Loss: 0.000010360, Improvement: -0.000006708, Best Loss: 0.000003565 in Epoch 1266
Epoch 1295
Epoch 1295, Loss: 0.000009348, Improvement: -0.000001012, Best Loss: 0.000003565 in Epoch 1266
Epoch 1296
Epoch 1296, Loss: 0.000010197, Improvement: 0.000000850, Best Loss: 0.000003565 in Epoch 1266
Epoch 1297
Epoch 1297, Loss: 0.000009374, Improvement: -0.000000823, Best Loss: 0.000003565 in Epoch 1266
Epoch 1298
Epoch 1298, Loss: 0.000007969, Improvement: -0.000001405, Best Loss: 0.000003565 in Epoch 1266
Epoch 1299
Epoch 1299, Loss: 0.000009273, Improvement: 0.000001304, Best Loss: 0.000003565 in Epoch 1266
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.000010672, Improvement: 0.000001399, Best Loss: 0.000003565 in Epoch 1266
Epoch 1301
Epoch 1301, Loss: 0.000008973, Improvement: -0.000001700, Best Loss: 0.000003565 in Epoch 1266
Epoch 1302
Epoch 1302, Loss: 0.000008445, Improvement: -0.000000528, Best Loss: 0.000003565 in Epoch 1266
Epoch 1303
Epoch 1303, Loss: 0.000008806, Improvement: 0.000000361, Best Loss: 0.000003565 in Epoch 1266
Epoch 1304
Epoch 1304, Loss: 0.000008022, Improvement: -0.000000784, Best Loss: 0.000003565 in Epoch 1266
Epoch 1305
Epoch 1305, Loss: 0.000007377, Improvement: -0.000000645, Best Loss: 0.000003565 in Epoch 1266
Epoch 1306
Epoch 1306, Loss: 0.000006494, Improvement: -0.000000883, Best Loss: 0.000003565 in Epoch 1266
Epoch 1307
Epoch 1307, Loss: 0.000008701, Improvement: 0.000002207, Best Loss: 0.000003565 in Epoch 1266
Epoch 1308
Epoch 1308, Loss: 0.000010892, Improvement: 0.000002191, Best Loss: 0.000003565 in Epoch 1266
Epoch 1309
Epoch 1309, Loss: 0.000037820, Improvement: 0.000026928, Best Loss: 0.000003565 in Epoch 1266
Epoch 1310
Epoch 1310, Loss: 0.000040281, Improvement: 0.000002461, Best Loss: 0.000003565 in Epoch 1266
Epoch 1311
Epoch 1311, Loss: 0.000053494, Improvement: 0.000013212, Best Loss: 0.000003565 in Epoch 1266
Epoch 1312
Epoch 1312, Loss: 0.000054114, Improvement: 0.000000620, Best Loss: 0.000003565 in Epoch 1266
Epoch 1313
Epoch 1313, Loss: 0.000013331, Improvement: -0.000040784, Best Loss: 0.000003565 in Epoch 1266
Epoch 1314
Epoch 1314, Loss: 0.000010155, Improvement: -0.000003175, Best Loss: 0.000003565 in Epoch 1266
Epoch 1315
Epoch 1315, Loss: 0.000008644, Improvement: -0.000001511, Best Loss: 0.000003565 in Epoch 1266
Epoch 1316
Epoch 1316, Loss: 0.000010788, Improvement: 0.000002144, Best Loss: 0.000003565 in Epoch 1266
Epoch 1317
Epoch 1317, Loss: 0.000008505, Improvement: -0.000002283, Best Loss: 0.000003565 in Epoch 1266
Epoch 1318
Epoch 1318, Loss: 0.000006971, Improvement: -0.000001533, Best Loss: 0.000003565 in Epoch 1266
Epoch 1319
Epoch 1319, Loss: 0.000005558, Improvement: -0.000001413, Best Loss: 0.000003565 in Epoch 1266
Epoch 1320
Epoch 1320, Loss: 0.000005498, Improvement: -0.000000060, Best Loss: 0.000003565 in Epoch 1266
Epoch 1321
Epoch 1321, Loss: 0.000009493, Improvement: 0.000003995, Best Loss: 0.000003565 in Epoch 1266
Epoch 1322
Epoch 1322, Loss: 0.000007510, Improvement: -0.000001984, Best Loss: 0.000003565 in Epoch 1266
Epoch 1323
Epoch 1323, Loss: 0.000007690, Improvement: 0.000000181, Best Loss: 0.000003565 in Epoch 1266
Epoch 1324
Epoch 1324, Loss: 0.000005985, Improvement: -0.000001705, Best Loss: 0.000003565 in Epoch 1266
Epoch 1325
Epoch 1325, Loss: 0.000007027, Improvement: 0.000001042, Best Loss: 0.000003565 in Epoch 1266
Epoch 1326
Epoch 1326, Loss: 0.000014182, Improvement: 0.000007155, Best Loss: 0.000003565 in Epoch 1266
Epoch 1327
Epoch 1327, Loss: 0.000018350, Improvement: 0.000004168, Best Loss: 0.000003565 in Epoch 1266
Epoch 1328
Epoch 1328, Loss: 0.000017616, Improvement: -0.000000734, Best Loss: 0.000003565 in Epoch 1266
Epoch 1329
Epoch 1329, Loss: 0.000029188, Improvement: 0.000011572, Best Loss: 0.000003565 in Epoch 1266
Epoch 1330
Epoch 1330, Loss: 0.000030309, Improvement: 0.000001121, Best Loss: 0.000003565 in Epoch 1266
Epoch 1331
Epoch 1331, Loss: 0.000021910, Improvement: -0.000008399, Best Loss: 0.000003565 in Epoch 1266
Epoch 1332
Epoch 1332, Loss: 0.000024004, Improvement: 0.000002094, Best Loss: 0.000003565 in Epoch 1266
Epoch 1333
Epoch 1333, Loss: 0.000026574, Improvement: 0.000002570, Best Loss: 0.000003565 in Epoch 1266
Epoch 1334
Epoch 1334, Loss: 0.000017496, Improvement: -0.000009078, Best Loss: 0.000003565 in Epoch 1266
Epoch 1335
Epoch 1335, Loss: 0.000029630, Improvement: 0.000012134, Best Loss: 0.000003565 in Epoch 1266
Epoch 1336
Epoch 1336, Loss: 0.000015419, Improvement: -0.000014211, Best Loss: 0.000003565 in Epoch 1266
Epoch 1337
Epoch 1337, Loss: 0.000014441, Improvement: -0.000000977, Best Loss: 0.000003565 in Epoch 1266
Epoch 1338
Epoch 1338, Loss: 0.000011911, Improvement: -0.000002530, Best Loss: 0.000003565 in Epoch 1266
Epoch 1339
Epoch 1339, Loss: 0.000008830, Improvement: -0.000003081, Best Loss: 0.000003565 in Epoch 1266
Epoch 1340
Epoch 1340, Loss: 0.000016450, Improvement: 0.000007620, Best Loss: 0.000003565 in Epoch 1266
Epoch 1341
Epoch 1341, Loss: 0.000025195, Improvement: 0.000008745, Best Loss: 0.000003565 in Epoch 1266
Epoch 1342
Epoch 1342, Loss: 0.000058403, Improvement: 0.000033208, Best Loss: 0.000003565 in Epoch 1266
Epoch 1343
Epoch 1343, Loss: 0.000024572, Improvement: -0.000033831, Best Loss: 0.000003565 in Epoch 1266
Epoch 1344
Epoch 1344, Loss: 0.000018066, Improvement: -0.000006507, Best Loss: 0.000003565 in Epoch 1266
Epoch 1345
Epoch 1345, Loss: 0.000015496, Improvement: -0.000002570, Best Loss: 0.000003565 in Epoch 1266
Epoch 1346
Epoch 1346, Loss: 0.000026096, Improvement: 0.000010600, Best Loss: 0.000003565 in Epoch 1266
Epoch 1347
Epoch 1347, Loss: 0.000031150, Improvement: 0.000005054, Best Loss: 0.000003565 in Epoch 1266
Epoch 1348
Epoch 1348, Loss: 0.000027523, Improvement: -0.000003627, Best Loss: 0.000003565 in Epoch 1266
Epoch 1349
Epoch 1349, Loss: 0.000025190, Improvement: -0.000002333, Best Loss: 0.000003565 in Epoch 1266
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.000012952, Improvement: -0.000012238, Best Loss: 0.000003565 in Epoch 1266
Epoch 1351
Epoch 1351, Loss: 0.000011395, Improvement: -0.000001557, Best Loss: 0.000003565 in Epoch 1266
Epoch 1352
Epoch 1352, Loss: 0.000013116, Improvement: 0.000001721, Best Loss: 0.000003565 in Epoch 1266
Epoch 1353
Epoch 1353, Loss: 0.000022855, Improvement: 0.000009739, Best Loss: 0.000003565 in Epoch 1266
Epoch 1354
Epoch 1354, Loss: 0.000030170, Improvement: 0.000007315, Best Loss: 0.000003565 in Epoch 1266
Epoch 1355
Epoch 1355, Loss: 0.000020584, Improvement: -0.000009585, Best Loss: 0.000003565 in Epoch 1266
Epoch 1356
Epoch 1356, Loss: 0.000012596, Improvement: -0.000007988, Best Loss: 0.000003565 in Epoch 1266
Epoch 1357
Epoch 1357, Loss: 0.000011443, Improvement: -0.000001154, Best Loss: 0.000003565 in Epoch 1266
Epoch 1358
Epoch 1358, Loss: 0.000011982, Improvement: 0.000000539, Best Loss: 0.000003565 in Epoch 1266
Epoch 1359
Epoch 1359, Loss: 0.000010318, Improvement: -0.000001664, Best Loss: 0.000003565 in Epoch 1266
Epoch 1360
Epoch 1360, Loss: 0.000010250, Improvement: -0.000000068, Best Loss: 0.000003565 in Epoch 1266
Epoch 1361
Epoch 1361, Loss: 0.000008165, Improvement: -0.000002085, Best Loss: 0.000003565 in Epoch 1266
Epoch 1362
Epoch 1362, Loss: 0.000013404, Improvement: 0.000005239, Best Loss: 0.000003565 in Epoch 1266
Epoch 1363
Epoch 1363, Loss: 0.000024614, Improvement: 0.000011210, Best Loss: 0.000003565 in Epoch 1266
Epoch 1364
Epoch 1364, Loss: 0.000019783, Improvement: -0.000004831, Best Loss: 0.000003565 in Epoch 1266
Epoch 1365
Epoch 1365, Loss: 0.000027702, Improvement: 0.000007919, Best Loss: 0.000003565 in Epoch 1266
Epoch 1366
Epoch 1366, Loss: 0.000032546, Improvement: 0.000004843, Best Loss: 0.000003565 in Epoch 1266
Epoch 1367
Epoch 1367, Loss: 0.000018688, Improvement: -0.000013858, Best Loss: 0.000003565 in Epoch 1266
Epoch 1368
Epoch 1368, Loss: 0.000015591, Improvement: -0.000003096, Best Loss: 0.000003565 in Epoch 1266
Epoch 1369
Epoch 1369, Loss: 0.000031741, Improvement: 0.000016150, Best Loss: 0.000003565 in Epoch 1266
Epoch 1370
Epoch 1370, Loss: 0.000045809, Improvement: 0.000014067, Best Loss: 0.000003565 in Epoch 1266
Epoch 1371
Epoch 1371, Loss: 0.000036263, Improvement: -0.000009545, Best Loss: 0.000003565 in Epoch 1266
Epoch 1372
Epoch 1372, Loss: 0.000017289, Improvement: -0.000018974, Best Loss: 0.000003565 in Epoch 1266
Epoch 1373
Epoch 1373, Loss: 0.000013722, Improvement: -0.000003567, Best Loss: 0.000003565 in Epoch 1266
Epoch 1374
Epoch 1374, Loss: 0.000010026, Improvement: -0.000003696, Best Loss: 0.000003565 in Epoch 1266
Epoch 1375
Epoch 1375, Loss: 0.000020124, Improvement: 0.000010099, Best Loss: 0.000003565 in Epoch 1266
Epoch 1376
Epoch 1376, Loss: 0.000029920, Improvement: 0.000009796, Best Loss: 0.000003565 in Epoch 1266
Epoch 1377
Epoch 1377, Loss: 0.000032725, Improvement: 0.000002805, Best Loss: 0.000003565 in Epoch 1266
Epoch 1378
Epoch 1378, Loss: 0.000019883, Improvement: -0.000012843, Best Loss: 0.000003565 in Epoch 1266
Epoch 1379
Epoch 1379, Loss: 0.000017494, Improvement: -0.000002389, Best Loss: 0.000003565 in Epoch 1266
Epoch 1380
Epoch 1380, Loss: 0.000033130, Improvement: 0.000015637, Best Loss: 0.000003565 in Epoch 1266
Epoch 1381
Epoch 1381, Loss: 0.000041980, Improvement: 0.000008849, Best Loss: 0.000003565 in Epoch 1266
Epoch 1382
Epoch 1382, Loss: 0.000058774, Improvement: 0.000016795, Best Loss: 0.000003565 in Epoch 1266
Epoch 1383
Epoch 1383, Loss: 0.000027478, Improvement: -0.000031297, Best Loss: 0.000003565 in Epoch 1266
Epoch 1384
Epoch 1384, Loss: 0.000025381, Improvement: -0.000002097, Best Loss: 0.000003565 in Epoch 1266
Epoch 1385
Epoch 1385, Loss: 0.000023344, Improvement: -0.000002037, Best Loss: 0.000003565 in Epoch 1266
Epoch 1386
Epoch 1386, Loss: 0.000018521, Improvement: -0.000004823, Best Loss: 0.000003565 in Epoch 1266
Epoch 1387
Epoch 1387, Loss: 0.000011222, Improvement: -0.000007298, Best Loss: 0.000003565 in Epoch 1266
Epoch 1388
Epoch 1388, Loss: 0.000009283, Improvement: -0.000001939, Best Loss: 0.000003565 in Epoch 1266
Epoch 1389
Epoch 1389, Loss: 0.000016450, Improvement: 0.000007168, Best Loss: 0.000003565 in Epoch 1266
Epoch 1390
Epoch 1390, Loss: 0.000013281, Improvement: -0.000003170, Best Loss: 0.000003565 in Epoch 1266
Epoch 1391
Epoch 1391, Loss: 0.000016377, Improvement: 0.000003096, Best Loss: 0.000003565 in Epoch 1266
Epoch 1392
Epoch 1392, Loss: 0.000010827, Improvement: -0.000005550, Best Loss: 0.000003565 in Epoch 1266
Epoch 1393
Epoch 1393, Loss: 0.000011860, Improvement: 0.000001033, Best Loss: 0.000003565 in Epoch 1266
Epoch 1394
Epoch 1394, Loss: 0.000014577, Improvement: 0.000002718, Best Loss: 0.000003565 in Epoch 1266
Epoch 1395
Epoch 1395, Loss: 0.000013982, Improvement: -0.000000596, Best Loss: 0.000003565 in Epoch 1266
Epoch 1396
Epoch 1396, Loss: 0.000021223, Improvement: 0.000007241, Best Loss: 0.000003565 in Epoch 1266
Epoch 1397
Epoch 1397, Loss: 0.000015351, Improvement: -0.000005872, Best Loss: 0.000003565 in Epoch 1266
Epoch 1398
Epoch 1398, Loss: 0.000015361, Improvement: 0.000000010, Best Loss: 0.000003565 in Epoch 1266
Epoch 1399
Epoch 1399, Loss: 0.000012522, Improvement: -0.000002840, Best Loss: 0.000003565 in Epoch 1266
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.000022382, Improvement: 0.000009860, Best Loss: 0.000003565 in Epoch 1266
Epoch 1401
Epoch 1401, Loss: 0.000017420, Improvement: -0.000004962, Best Loss: 0.000003565 in Epoch 1266
Epoch 1402
Epoch 1402, Loss: 0.000018716, Improvement: 0.000001296, Best Loss: 0.000003565 in Epoch 1266
Epoch 1403
Epoch 1403, Loss: 0.000093004, Improvement: 0.000074288, Best Loss: 0.000003565 in Epoch 1266
Epoch 1404
Epoch 1404, Loss: 0.000084309, Improvement: -0.000008695, Best Loss: 0.000003565 in Epoch 1266
Epoch 1405
Epoch 1405, Loss: 0.000023082, Improvement: -0.000061227, Best Loss: 0.000003565 in Epoch 1266
Epoch 1406
Epoch 1406, Loss: 0.000009496, Improvement: -0.000013587, Best Loss: 0.000003565 in Epoch 1266
Epoch 1407
Epoch 1407, Loss: 0.000006357, Improvement: -0.000003139, Best Loss: 0.000003565 in Epoch 1266
Epoch 1408
Epoch 1408, Loss: 0.000005209, Improvement: -0.000001148, Best Loss: 0.000003565 in Epoch 1266
Epoch 1409
A best model at epoch 1409 has been saved with training error 0.000003495.
Epoch 1409, Loss: 0.000005092, Improvement: -0.000000116, Best Loss: 0.000003495 in Epoch 1409
Epoch 1410
Epoch 1410, Loss: 0.000004852, Improvement: -0.000000240, Best Loss: 0.000003495 in Epoch 1409
Epoch 1411
A best model at epoch 1411 has been saved with training error 0.000003089.
Epoch 1411, Loss: 0.000004769, Improvement: -0.000000083, Best Loss: 0.000003089 in Epoch 1411
Epoch 1412
Epoch 1412, Loss: 0.000004594, Improvement: -0.000000175, Best Loss: 0.000003089 in Epoch 1411
Epoch 1413
Epoch 1413, Loss: 0.000005101, Improvement: 0.000000507, Best Loss: 0.000003089 in Epoch 1411
Epoch 1414
Epoch 1414, Loss: 0.000005702, Improvement: 0.000000601, Best Loss: 0.000003089 in Epoch 1411
Epoch 1415
Epoch 1415, Loss: 0.000005169, Improvement: -0.000000533, Best Loss: 0.000003089 in Epoch 1411
Epoch 1416
Epoch 1416, Loss: 0.000004486, Improvement: -0.000000683, Best Loss: 0.000003089 in Epoch 1411
Epoch 1417
Epoch 1417, Loss: 0.000005152, Improvement: 0.000000666, Best Loss: 0.000003089 in Epoch 1411
Epoch 1418
Epoch 1418, Loss: 0.000004461, Improvement: -0.000000691, Best Loss: 0.000003089 in Epoch 1411
Epoch 1419
Epoch 1419, Loss: 0.000004554, Improvement: 0.000000093, Best Loss: 0.000003089 in Epoch 1411
Epoch 1420
Epoch 1420, Loss: 0.000004473, Improvement: -0.000000081, Best Loss: 0.000003089 in Epoch 1411
Epoch 1421
Epoch 1421, Loss: 0.000004488, Improvement: 0.000000015, Best Loss: 0.000003089 in Epoch 1411
Epoch 1422
Epoch 1422, Loss: 0.000004243, Improvement: -0.000000245, Best Loss: 0.000003089 in Epoch 1411
Epoch 1423
Epoch 1423, Loss: 0.000004664, Improvement: 0.000000421, Best Loss: 0.000003089 in Epoch 1411
Epoch 1424
Epoch 1424, Loss: 0.000004280, Improvement: -0.000000384, Best Loss: 0.000003089 in Epoch 1411
Epoch 1425
Epoch 1425, Loss: 0.000005216, Improvement: 0.000000936, Best Loss: 0.000003089 in Epoch 1411
Epoch 1426
Epoch 1426, Loss: 0.000004827, Improvement: -0.000000389, Best Loss: 0.000003089 in Epoch 1411
Epoch 1427
Epoch 1427, Loss: 0.000004667, Improvement: -0.000000160, Best Loss: 0.000003089 in Epoch 1411
Epoch 1428
Epoch 1428, Loss: 0.000007181, Improvement: 0.000002514, Best Loss: 0.000003089 in Epoch 1411
Epoch 1429
Epoch 1429, Loss: 0.000012014, Improvement: 0.000004832, Best Loss: 0.000003089 in Epoch 1411
Epoch 1430
Epoch 1430, Loss: 0.000009413, Improvement: -0.000002601, Best Loss: 0.000003089 in Epoch 1411
Epoch 1431
Epoch 1431, Loss: 0.000006910, Improvement: -0.000002503, Best Loss: 0.000003089 in Epoch 1411
Epoch 1432
Epoch 1432, Loss: 0.000005305, Improvement: -0.000001605, Best Loss: 0.000003089 in Epoch 1411
Epoch 1433
Epoch 1433, Loss: 0.000005292, Improvement: -0.000000013, Best Loss: 0.000003089 in Epoch 1411
Epoch 1434
Epoch 1434, Loss: 0.000004522, Improvement: -0.000000769, Best Loss: 0.000003089 in Epoch 1411
Epoch 1435
A best model at epoch 1435 has been saved with training error 0.000002881.
Epoch 1435, Loss: 0.000004169, Improvement: -0.000000354, Best Loss: 0.000002881 in Epoch 1435
Epoch 1436
A best model at epoch 1436 has been saved with training error 0.000002864.
Epoch 1436, Loss: 0.000003995, Improvement: -0.000000174, Best Loss: 0.000002864 in Epoch 1436
Epoch 1437
Epoch 1437, Loss: 0.000005519, Improvement: 0.000001524, Best Loss: 0.000002864 in Epoch 1436
Epoch 1438
Epoch 1438, Loss: 0.000009494, Improvement: 0.000003975, Best Loss: 0.000002864 in Epoch 1436
Epoch 1439
Epoch 1439, Loss: 0.000008518, Improvement: -0.000000976, Best Loss: 0.000002864 in Epoch 1436
Epoch 1440
Epoch 1440, Loss: 0.000007992, Improvement: -0.000000526, Best Loss: 0.000002864 in Epoch 1436
Epoch 1441
Epoch 1441, Loss: 0.000008475, Improvement: 0.000000483, Best Loss: 0.000002864 in Epoch 1436
Epoch 1442
Epoch 1442, Loss: 0.000011816, Improvement: 0.000003341, Best Loss: 0.000002864 in Epoch 1436
Epoch 1443
Epoch 1443, Loss: 0.000022113, Improvement: 0.000010297, Best Loss: 0.000002864 in Epoch 1436
Epoch 1444
Epoch 1444, Loss: 0.000016019, Improvement: -0.000006095, Best Loss: 0.000002864 in Epoch 1436
Epoch 1445
Epoch 1445, Loss: 0.000011264, Improvement: -0.000004755, Best Loss: 0.000002864 in Epoch 1436
Epoch 1446
Epoch 1446, Loss: 0.000015606, Improvement: 0.000004342, Best Loss: 0.000002864 in Epoch 1436
Epoch 1447
Epoch 1447, Loss: 0.000015382, Improvement: -0.000000224, Best Loss: 0.000002864 in Epoch 1436
Epoch 1448
Epoch 1448, Loss: 0.000015346, Improvement: -0.000000036, Best Loss: 0.000002864 in Epoch 1436
Epoch 1449
Epoch 1449, Loss: 0.000011606, Improvement: -0.000003739, Best Loss: 0.000002864 in Epoch 1436
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.000009815, Improvement: -0.000001791, Best Loss: 0.000002864 in Epoch 1436
Epoch 1451
Epoch 1451, Loss: 0.000009051, Improvement: -0.000000765, Best Loss: 0.000002864 in Epoch 1436
Epoch 1452
Epoch 1452, Loss: 0.000017402, Improvement: 0.000008351, Best Loss: 0.000002864 in Epoch 1436
Epoch 1453
Epoch 1453, Loss: 0.000039957, Improvement: 0.000022555, Best Loss: 0.000002864 in Epoch 1436
Epoch 1454
Epoch 1454, Loss: 0.000050112, Improvement: 0.000010155, Best Loss: 0.000002864 in Epoch 1436
Epoch 1455
Epoch 1455, Loss: 0.000056186, Improvement: 0.000006074, Best Loss: 0.000002864 in Epoch 1436
Epoch 1456
Epoch 1456, Loss: 0.000042890, Improvement: -0.000013295, Best Loss: 0.000002864 in Epoch 1436
Epoch 1457
Epoch 1457, Loss: 0.000026349, Improvement: -0.000016541, Best Loss: 0.000002864 in Epoch 1436
Epoch 1458
Epoch 1458, Loss: 0.000009620, Improvement: -0.000016729, Best Loss: 0.000002864 in Epoch 1436
Epoch 1459
Epoch 1459, Loss: 0.000005587, Improvement: -0.000004033, Best Loss: 0.000002864 in Epoch 1436
Epoch 1460
Epoch 1460, Loss: 0.000005025, Improvement: -0.000000563, Best Loss: 0.000002864 in Epoch 1436
Epoch 1461
Epoch 1461, Loss: 0.000009753, Improvement: 0.000004729, Best Loss: 0.000002864 in Epoch 1436
Epoch 1462
Epoch 1462, Loss: 0.000006775, Improvement: -0.000002978, Best Loss: 0.000002864 in Epoch 1436
Epoch 1463
Epoch 1463, Loss: 0.000009562, Improvement: 0.000002787, Best Loss: 0.000002864 in Epoch 1436
Epoch 1464
Epoch 1464, Loss: 0.000007626, Improvement: -0.000001936, Best Loss: 0.000002864 in Epoch 1436
Epoch 1465
Epoch 1465, Loss: 0.000011309, Improvement: 0.000003683, Best Loss: 0.000002864 in Epoch 1436
Epoch 1466
Epoch 1466, Loss: 0.000011496, Improvement: 0.000000187, Best Loss: 0.000002864 in Epoch 1436
Epoch 1467
Epoch 1467, Loss: 0.000006148, Improvement: -0.000005348, Best Loss: 0.000002864 in Epoch 1436
Epoch 1468
Epoch 1468, Loss: 0.000005905, Improvement: -0.000000243, Best Loss: 0.000002864 in Epoch 1436
Epoch 1469
Epoch 1469, Loss: 0.000010500, Improvement: 0.000004595, Best Loss: 0.000002864 in Epoch 1436
Epoch 1470
Epoch 1470, Loss: 0.000044649, Improvement: 0.000034149, Best Loss: 0.000002864 in Epoch 1436
Epoch 1471
Epoch 1471, Loss: 0.000046283, Improvement: 0.000001634, Best Loss: 0.000002864 in Epoch 1436
Epoch 1472
Epoch 1472, Loss: 0.000032337, Improvement: -0.000013945, Best Loss: 0.000002864 in Epoch 1436
Epoch 1473
Epoch 1473, Loss: 0.000014337, Improvement: -0.000018000, Best Loss: 0.000002864 in Epoch 1436
Epoch 1474
Epoch 1474, Loss: 0.000009455, Improvement: -0.000004883, Best Loss: 0.000002864 in Epoch 1436
Epoch 1475
Epoch 1475, Loss: 0.000013794, Improvement: 0.000004339, Best Loss: 0.000002864 in Epoch 1436
Epoch 1476
Epoch 1476, Loss: 0.000010176, Improvement: -0.000003618, Best Loss: 0.000002864 in Epoch 1436
Epoch 1477
Epoch 1477, Loss: 0.000009677, Improvement: -0.000000499, Best Loss: 0.000002864 in Epoch 1436
Epoch 1478
Epoch 1478, Loss: 0.000008416, Improvement: -0.000001261, Best Loss: 0.000002864 in Epoch 1436
Epoch 1479
Epoch 1479, Loss: 0.000061273, Improvement: 0.000052857, Best Loss: 0.000002864 in Epoch 1436
Epoch 1480
Epoch 1480, Loss: 0.000040397, Improvement: -0.000020876, Best Loss: 0.000002864 in Epoch 1436
Epoch 1481
Epoch 1481, Loss: 0.000026606, Improvement: -0.000013791, Best Loss: 0.000002864 in Epoch 1436
Epoch 1482
Epoch 1482, Loss: 0.000015969, Improvement: -0.000010636, Best Loss: 0.000002864 in Epoch 1436
Epoch 1483
Epoch 1483, Loss: 0.000010440, Improvement: -0.000005530, Best Loss: 0.000002864 in Epoch 1436
Epoch 1484
Epoch 1484, Loss: 0.000012581, Improvement: 0.000002141, Best Loss: 0.000002864 in Epoch 1436
Epoch 1485
Epoch 1485, Loss: 0.000006461, Improvement: -0.000006119, Best Loss: 0.000002864 in Epoch 1436
Epoch 1486
Epoch 1486, Loss: 0.000005001, Improvement: -0.000001460, Best Loss: 0.000002864 in Epoch 1436
Epoch 1487
Epoch 1487, Loss: 0.000004688, Improvement: -0.000000314, Best Loss: 0.000002864 in Epoch 1436
Epoch 1488
Epoch 1488, Loss: 0.000004239, Improvement: -0.000000449, Best Loss: 0.000002864 in Epoch 1436
Epoch 1489
A best model at epoch 1489 has been saved with training error 0.000002815.
Epoch 1489, Loss: 0.000004238, Improvement: -0.000000001, Best Loss: 0.000002815 in Epoch 1489
Epoch 1490
Epoch 1490, Loss: 0.000004476, Improvement: 0.000000239, Best Loss: 0.000002815 in Epoch 1489
Epoch 1491
Epoch 1491, Loss: 0.000004564, Improvement: 0.000000088, Best Loss: 0.000002815 in Epoch 1489
Epoch 1492
Epoch 1492, Loss: 0.000006363, Improvement: 0.000001799, Best Loss: 0.000002815 in Epoch 1489
Epoch 1493
Epoch 1493, Loss: 0.000008176, Improvement: 0.000001813, Best Loss: 0.000002815 in Epoch 1489
Epoch 1494
Epoch 1494, Loss: 0.000007226, Improvement: -0.000000950, Best Loss: 0.000002815 in Epoch 1489
Epoch 1495
Epoch 1495, Loss: 0.000005106, Improvement: -0.000002120, Best Loss: 0.000002815 in Epoch 1489
Epoch 1496
Epoch 1496, Loss: 0.000004966, Improvement: -0.000000141, Best Loss: 0.000002815 in Epoch 1489
Epoch 1497
A best model at epoch 1497 has been saved with training error 0.000002796.
Epoch 1497, Loss: 0.000003922, Improvement: -0.000001043, Best Loss: 0.000002796 in Epoch 1497
Epoch 1498
Epoch 1498, Loss: 0.000006748, Improvement: 0.000002825, Best Loss: 0.000002796 in Epoch 1497
Epoch 1499
Epoch 1499, Loss: 0.000011105, Improvement: 0.000004357, Best Loss: 0.000002796 in Epoch 1497
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.000011406, Improvement: 0.000000301, Best Loss: 0.000002796 in Epoch 1497
Epoch 1501
Epoch 1501, Loss: 0.000050130, Improvement: 0.000038724, Best Loss: 0.000002796 in Epoch 1497
Epoch 1502
Epoch 1502, Loss: 0.000067579, Improvement: 0.000017450, Best Loss: 0.000002796 in Epoch 1497
Epoch 1503
Epoch 1503, Loss: 0.000040863, Improvement: -0.000026717, Best Loss: 0.000002796 in Epoch 1497
Epoch 1504
Epoch 1504, Loss: 0.000017983, Improvement: -0.000022879, Best Loss: 0.000002796 in Epoch 1497
Epoch 1505
Epoch 1505, Loss: 0.000014828, Improvement: -0.000003156, Best Loss: 0.000002796 in Epoch 1497
Epoch 1506
Epoch 1506, Loss: 0.000007372, Improvement: -0.000007455, Best Loss: 0.000002796 in Epoch 1497
Epoch 1507
Epoch 1507, Loss: 0.000004918, Improvement: -0.000002454, Best Loss: 0.000002796 in Epoch 1497
Epoch 1508
Epoch 1508, Loss: 0.000004166, Improvement: -0.000000752, Best Loss: 0.000002796 in Epoch 1497
Epoch 1509
Epoch 1509, Loss: 0.000003759, Improvement: -0.000000407, Best Loss: 0.000002796 in Epoch 1497
Epoch 1510
A best model at epoch 1510 has been saved with training error 0.000002586.
Epoch 1510, Loss: 0.000003648, Improvement: -0.000000111, Best Loss: 0.000002586 in Epoch 1510
Epoch 1511
A best model at epoch 1511 has been saved with training error 0.000002581.
Epoch 1511, Loss: 0.000003794, Improvement: 0.000000146, Best Loss: 0.000002581 in Epoch 1511
Epoch 1512
Epoch 1512, Loss: 0.000003920, Improvement: 0.000000126, Best Loss: 0.000002581 in Epoch 1511
Epoch 1513
A best model at epoch 1513 has been saved with training error 0.000002480.
Epoch 1513, Loss: 0.000003561, Improvement: -0.000000359, Best Loss: 0.000002480 in Epoch 1513
Epoch 1514
A best model at epoch 1514 has been saved with training error 0.000002415.
Epoch 1514, Loss: 0.000003385, Improvement: -0.000000176, Best Loss: 0.000002415 in Epoch 1514
Epoch 1515
A best model at epoch 1515 has been saved with training error 0.000002331.
Epoch 1515, Loss: 0.000003252, Improvement: -0.000000132, Best Loss: 0.000002331 in Epoch 1515
Epoch 1516
A best model at epoch 1516 has been saved with training error 0.000002297.
Epoch 1516, Loss: 0.000003266, Improvement: 0.000000014, Best Loss: 0.000002297 in Epoch 1516
Epoch 1517
Epoch 1517, Loss: 0.000003785, Improvement: 0.000000519, Best Loss: 0.000002297 in Epoch 1516
Epoch 1518
Epoch 1518, Loss: 0.000004548, Improvement: 0.000000762, Best Loss: 0.000002297 in Epoch 1516
Epoch 1519
Epoch 1519, Loss: 0.000004994, Improvement: 0.000000446, Best Loss: 0.000002297 in Epoch 1516
Epoch 1520
Epoch 1520, Loss: 0.000004998, Improvement: 0.000000005, Best Loss: 0.000002297 in Epoch 1516
Epoch 1521
Epoch 1521, Loss: 0.000006980, Improvement: 0.000001982, Best Loss: 0.000002297 in Epoch 1516
Epoch 1522
Epoch 1522, Loss: 0.000004633, Improvement: -0.000002347, Best Loss: 0.000002297 in Epoch 1516
Epoch 1523
Epoch 1523, Loss: 0.000003971, Improvement: -0.000000662, Best Loss: 0.000002297 in Epoch 1516
Epoch 1524
Epoch 1524, Loss: 0.000004050, Improvement: 0.000000079, Best Loss: 0.000002297 in Epoch 1516
Epoch 1525
Epoch 1525, Loss: 0.000004262, Improvement: 0.000000212, Best Loss: 0.000002297 in Epoch 1516
Epoch 1526
Epoch 1526, Loss: 0.000004997, Improvement: 0.000000735, Best Loss: 0.000002297 in Epoch 1516
Epoch 1527
Epoch 1527, Loss: 0.000003911, Improvement: -0.000001086, Best Loss: 0.000002297 in Epoch 1516
Epoch 1528
Epoch 1528, Loss: 0.000006535, Improvement: 0.000002624, Best Loss: 0.000002297 in Epoch 1516
Epoch 1529
Epoch 1529, Loss: 0.000033022, Improvement: 0.000026488, Best Loss: 0.000002297 in Epoch 1516
Epoch 1530
Epoch 1530, Loss: 0.000047401, Improvement: 0.000014379, Best Loss: 0.000002297 in Epoch 1516
Epoch 1531
Epoch 1531, Loss: 0.000080572, Improvement: 0.000033171, Best Loss: 0.000002297 in Epoch 1516
Epoch 1532
Epoch 1532, Loss: 0.000068927, Improvement: -0.000011645, Best Loss: 0.000002297 in Epoch 1516
Epoch 1533
Epoch 1533, Loss: 0.000036595, Improvement: -0.000032332, Best Loss: 0.000002297 in Epoch 1516
Epoch 1534
Epoch 1534, Loss: 0.000024698, Improvement: -0.000011897, Best Loss: 0.000002297 in Epoch 1516
Epoch 1535
Epoch 1535, Loss: 0.000009870, Improvement: -0.000014828, Best Loss: 0.000002297 in Epoch 1516
Epoch 1536
Epoch 1536, Loss: 0.000006635, Improvement: -0.000003235, Best Loss: 0.000002297 in Epoch 1516
Epoch 1537
Epoch 1537, Loss: 0.000005838, Improvement: -0.000000797, Best Loss: 0.000002297 in Epoch 1516
Epoch 1538
Epoch 1538, Loss: 0.000004540, Improvement: -0.000001298, Best Loss: 0.000002297 in Epoch 1516
Epoch 1539
Epoch 1539, Loss: 0.000004041, Improvement: -0.000000499, Best Loss: 0.000002297 in Epoch 1516
Epoch 1540
Epoch 1540, Loss: 0.000003466, Improvement: -0.000000576, Best Loss: 0.000002297 in Epoch 1516
Epoch 1541
Epoch 1541, Loss: 0.000003350, Improvement: -0.000000116, Best Loss: 0.000002297 in Epoch 1516
Epoch 1542
Epoch 1542, Loss: 0.000003281, Improvement: -0.000000069, Best Loss: 0.000002297 in Epoch 1516
Epoch 1543
A best model at epoch 1543 has been saved with training error 0.000002290.
Epoch 1543, Loss: 0.000003420, Improvement: 0.000000139, Best Loss: 0.000002290 in Epoch 1543
Epoch 1544
Epoch 1544, Loss: 0.000003228, Improvement: -0.000000192, Best Loss: 0.000002290 in Epoch 1543
Epoch 1545
Epoch 1545, Loss: 0.000003214, Improvement: -0.000000014, Best Loss: 0.000002290 in Epoch 1543
Epoch 1546
Epoch 1546, Loss: 0.000003297, Improvement: 0.000000082, Best Loss: 0.000002290 in Epoch 1543
Epoch 1547
Epoch 1547, Loss: 0.000003028, Improvement: -0.000000268, Best Loss: 0.000002290 in Epoch 1543
Epoch 1548
A best model at epoch 1548 has been saved with training error 0.000002165.
Epoch 1548, Loss: 0.000003381, Improvement: 0.000000353, Best Loss: 0.000002165 in Epoch 1548
Epoch 1549
Epoch 1549, Loss: 0.000004956, Improvement: 0.000001575, Best Loss: 0.000002165 in Epoch 1548
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.000004017, Improvement: -0.000000939, Best Loss: 0.000002165 in Epoch 1548
Epoch 1551
Epoch 1551, Loss: 0.000004843, Improvement: 0.000000825, Best Loss: 0.000002165 in Epoch 1548
Epoch 1552
Epoch 1552, Loss: 0.000005819, Improvement: 0.000000976, Best Loss: 0.000002165 in Epoch 1548
Epoch 1553
Epoch 1553, Loss: 0.000004567, Improvement: -0.000001252, Best Loss: 0.000002165 in Epoch 1548
Epoch 1554
Epoch 1554, Loss: 0.000005598, Improvement: 0.000001032, Best Loss: 0.000002165 in Epoch 1548
Epoch 1555
Epoch 1555, Loss: 0.000004966, Improvement: -0.000000633, Best Loss: 0.000002165 in Epoch 1548
Epoch 1556
Epoch 1556, Loss: 0.000011473, Improvement: 0.000006507, Best Loss: 0.000002165 in Epoch 1548
Epoch 1557
Epoch 1557, Loss: 0.000016814, Improvement: 0.000005341, Best Loss: 0.000002165 in Epoch 1548
Epoch 1558
Epoch 1558, Loss: 0.000014773, Improvement: -0.000002041, Best Loss: 0.000002165 in Epoch 1548
Epoch 1559
Epoch 1559, Loss: 0.000011781, Improvement: -0.000002992, Best Loss: 0.000002165 in Epoch 1548
Epoch 1560
Epoch 1560, Loss: 0.000010106, Improvement: -0.000001675, Best Loss: 0.000002165 in Epoch 1548
Epoch 1561
Epoch 1561, Loss: 0.000009570, Improvement: -0.000000536, Best Loss: 0.000002165 in Epoch 1548
Epoch 1562
Epoch 1562, Loss: 0.000006174, Improvement: -0.000003396, Best Loss: 0.000002165 in Epoch 1548
Epoch 1563
Epoch 1563, Loss: 0.000005053, Improvement: -0.000001121, Best Loss: 0.000002165 in Epoch 1548
Epoch 1564
Epoch 1564, Loss: 0.000006356, Improvement: 0.000001303, Best Loss: 0.000002165 in Epoch 1548
Epoch 1565
Epoch 1565, Loss: 0.000005947, Improvement: -0.000000409, Best Loss: 0.000002165 in Epoch 1548
Epoch 1566
Epoch 1566, Loss: 0.000007382, Improvement: 0.000001434, Best Loss: 0.000002165 in Epoch 1548
Epoch 1567
Epoch 1567, Loss: 0.000008013, Improvement: 0.000000632, Best Loss: 0.000002165 in Epoch 1548
Epoch 1568
Epoch 1568, Loss: 0.000019636, Improvement: 0.000011623, Best Loss: 0.000002165 in Epoch 1548
Epoch 1569
Epoch 1569, Loss: 0.000037109, Improvement: 0.000017473, Best Loss: 0.000002165 in Epoch 1548
Epoch 1570
Epoch 1570, Loss: 0.000087666, Improvement: 0.000050557, Best Loss: 0.000002165 in Epoch 1548
Epoch 1571
Epoch 1571, Loss: 0.000038294, Improvement: -0.000049372, Best Loss: 0.000002165 in Epoch 1548
Epoch 1572
Epoch 1572, Loss: 0.000015234, Improvement: -0.000023061, Best Loss: 0.000002165 in Epoch 1548
Epoch 1573
Epoch 1573, Loss: 0.000013267, Improvement: -0.000001967, Best Loss: 0.000002165 in Epoch 1548
Epoch 1574
Epoch 1574, Loss: 0.000009028, Improvement: -0.000004239, Best Loss: 0.000002165 in Epoch 1548
Epoch 1575
Epoch 1575, Loss: 0.000006331, Improvement: -0.000002697, Best Loss: 0.000002165 in Epoch 1548
Epoch 1576
Epoch 1576, Loss: 0.000005733, Improvement: -0.000000598, Best Loss: 0.000002165 in Epoch 1548
Epoch 1577
Epoch 1577, Loss: 0.000005205, Improvement: -0.000000528, Best Loss: 0.000002165 in Epoch 1548
Epoch 1578
Epoch 1578, Loss: 0.000004136, Improvement: -0.000001069, Best Loss: 0.000002165 in Epoch 1548
Epoch 1579
Epoch 1579, Loss: 0.000003920, Improvement: -0.000000216, Best Loss: 0.000002165 in Epoch 1548
Epoch 1580
Epoch 1580, Loss: 0.000004006, Improvement: 0.000000086, Best Loss: 0.000002165 in Epoch 1548
Epoch 1581
Epoch 1581, Loss: 0.000003896, Improvement: -0.000000110, Best Loss: 0.000002165 in Epoch 1548
Epoch 1582
Epoch 1582, Loss: 0.000003503, Improvement: -0.000000393, Best Loss: 0.000002165 in Epoch 1548
Epoch 1583
Epoch 1583, Loss: 0.000003275, Improvement: -0.000000227, Best Loss: 0.000002165 in Epoch 1548
Epoch 1584
Epoch 1584, Loss: 0.000003259, Improvement: -0.000000016, Best Loss: 0.000002165 in Epoch 1548
Epoch 1585
Epoch 1585, Loss: 0.000003008, Improvement: -0.000000251, Best Loss: 0.000002165 in Epoch 1548
Epoch 1586
A best model at epoch 1586 has been saved with training error 0.000002147.
Epoch 1586, Loss: 0.000002875, Improvement: -0.000000133, Best Loss: 0.000002147 in Epoch 1586
Epoch 1587
Epoch 1587, Loss: 0.000002838, Improvement: -0.000000037, Best Loss: 0.000002147 in Epoch 1586
Epoch 1588
Epoch 1588, Loss: 0.000002743, Improvement: -0.000000095, Best Loss: 0.000002147 in Epoch 1586
Epoch 1589
Epoch 1589, Loss: 0.000003103, Improvement: 0.000000361, Best Loss: 0.000002147 in Epoch 1586
Epoch 1590
Epoch 1590, Loss: 0.000004136, Improvement: 0.000001033, Best Loss: 0.000002147 in Epoch 1586
Epoch 1591
Epoch 1591, Loss: 0.000004569, Improvement: 0.000000433, Best Loss: 0.000002147 in Epoch 1586
Epoch 1592
Epoch 1592, Loss: 0.000004935, Improvement: 0.000000366, Best Loss: 0.000002147 in Epoch 1586
Epoch 1593
Epoch 1593, Loss: 0.000005181, Improvement: 0.000000246, Best Loss: 0.000002147 in Epoch 1586
Epoch 1594
Epoch 1594, Loss: 0.000006062, Improvement: 0.000000881, Best Loss: 0.000002147 in Epoch 1586
Epoch 1595
Epoch 1595, Loss: 0.000006675, Improvement: 0.000000613, Best Loss: 0.000002147 in Epoch 1586
Epoch 1596
Epoch 1596, Loss: 0.000008822, Improvement: 0.000002147, Best Loss: 0.000002147 in Epoch 1586
Epoch 1597
Epoch 1597, Loss: 0.000006460, Improvement: -0.000002362, Best Loss: 0.000002147 in Epoch 1586
Epoch 1598
Epoch 1598, Loss: 0.000008320, Improvement: 0.000001859, Best Loss: 0.000002147 in Epoch 1586
Epoch 1599
Epoch 1599, Loss: 0.000009331, Improvement: 0.000001011, Best Loss: 0.000002147 in Epoch 1586
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.000006812, Improvement: -0.000002519, Best Loss: 0.000002147 in Epoch 1586
Epoch 1601
Epoch 1601, Loss: 0.000005543, Improvement: -0.000001269, Best Loss: 0.000002147 in Epoch 1586
Epoch 1602
Epoch 1602, Loss: 0.000007317, Improvement: 0.000001774, Best Loss: 0.000002147 in Epoch 1586
Epoch 1603
Epoch 1603, Loss: 0.000005140, Improvement: -0.000002176, Best Loss: 0.000002147 in Epoch 1586
Epoch 1604
Epoch 1604, Loss: 0.000007059, Improvement: 0.000001918, Best Loss: 0.000002147 in Epoch 1586
Epoch 1605
Epoch 1605, Loss: 0.000010302, Improvement: 0.000003244, Best Loss: 0.000002147 in Epoch 1586
Epoch 1606
Epoch 1606, Loss: 0.000008576, Improvement: -0.000001727, Best Loss: 0.000002147 in Epoch 1586
Epoch 1607
Epoch 1607, Loss: 0.000014742, Improvement: 0.000006167, Best Loss: 0.000002147 in Epoch 1586
Epoch 1608
Epoch 1608, Loss: 0.000006411, Improvement: -0.000008331, Best Loss: 0.000002147 in Epoch 1586
Epoch 1609
Epoch 1609, Loss: 0.000008209, Improvement: 0.000001798, Best Loss: 0.000002147 in Epoch 1586
Epoch 1610
Epoch 1610, Loss: 0.000013855, Improvement: 0.000005646, Best Loss: 0.000002147 in Epoch 1586
Epoch 1611
Epoch 1611, Loss: 0.000028044, Improvement: 0.000014189, Best Loss: 0.000002147 in Epoch 1586
Epoch 1612
Epoch 1612, Loss: 0.000038382, Improvement: 0.000010338, Best Loss: 0.000002147 in Epoch 1586
Epoch 1613
Epoch 1613, Loss: 0.000041875, Improvement: 0.000003493, Best Loss: 0.000002147 in Epoch 1586
Epoch 1614
Epoch 1614, Loss: 0.000037350, Improvement: -0.000004525, Best Loss: 0.000002147 in Epoch 1586
Epoch 1615
Epoch 1615, Loss: 0.000072220, Improvement: 0.000034871, Best Loss: 0.000002147 in Epoch 1586
Epoch 1616
Epoch 1616, Loss: 0.000022383, Improvement: -0.000049838, Best Loss: 0.000002147 in Epoch 1586
Epoch 1617
Epoch 1617, Loss: 0.000010156, Improvement: -0.000012226, Best Loss: 0.000002147 in Epoch 1586
Epoch 1618
Epoch 1618, Loss: 0.000005172, Improvement: -0.000004984, Best Loss: 0.000002147 in Epoch 1586
Epoch 1619
Epoch 1619, Loss: 0.000004146, Improvement: -0.000001027, Best Loss: 0.000002147 in Epoch 1586
Epoch 1620
Epoch 1620, Loss: 0.000003481, Improvement: -0.000000665, Best Loss: 0.000002147 in Epoch 1586
Epoch 1621
Epoch 1621, Loss: 0.000003995, Improvement: 0.000000514, Best Loss: 0.000002147 in Epoch 1586
Epoch 1622
Epoch 1622, Loss: 0.000004230, Improvement: 0.000000235, Best Loss: 0.000002147 in Epoch 1586
Epoch 1623
Epoch 1623, Loss: 0.000004095, Improvement: -0.000000135, Best Loss: 0.000002147 in Epoch 1586
Epoch 1624
Epoch 1624, Loss: 0.000004365, Improvement: 0.000000270, Best Loss: 0.000002147 in Epoch 1586
Epoch 1625
Epoch 1625, Loss: 0.000003832, Improvement: -0.000000533, Best Loss: 0.000002147 in Epoch 1586
Epoch 1626
Epoch 1626, Loss: 0.000003271, Improvement: -0.000000561, Best Loss: 0.000002147 in Epoch 1586
Epoch 1627
Epoch 1627, Loss: 0.000003055, Improvement: -0.000000215, Best Loss: 0.000002147 in Epoch 1586
Epoch 1628
Epoch 1628, Loss: 0.000003274, Improvement: 0.000000219, Best Loss: 0.000002147 in Epoch 1586
Epoch 1629
A best model at epoch 1629 has been saved with training error 0.000001972.
Epoch 1629, Loss: 0.000002911, Improvement: -0.000000363, Best Loss: 0.000001972 in Epoch 1629
Epoch 1630
Epoch 1630, Loss: 0.000002793, Improvement: -0.000000118, Best Loss: 0.000001972 in Epoch 1629
Epoch 1631
Epoch 1631, Loss: 0.000004249, Improvement: 0.000001456, Best Loss: 0.000001972 in Epoch 1629
Epoch 1632
Epoch 1632, Loss: 0.000007841, Improvement: 0.000003592, Best Loss: 0.000001972 in Epoch 1629
Epoch 1633
Epoch 1633, Loss: 0.000005660, Improvement: -0.000002181, Best Loss: 0.000001972 in Epoch 1629
Epoch 1634
Epoch 1634, Loss: 0.000004400, Improvement: -0.000001260, Best Loss: 0.000001972 in Epoch 1629
Epoch 1635
Epoch 1635, Loss: 0.000004950, Improvement: 0.000000550, Best Loss: 0.000001972 in Epoch 1629
Epoch 1636
Epoch 1636, Loss: 0.000008148, Improvement: 0.000003197, Best Loss: 0.000001972 in Epoch 1629
Epoch 1637
Epoch 1637, Loss: 0.000016388, Improvement: 0.000008240, Best Loss: 0.000001972 in Epoch 1629
Epoch 1638
Epoch 1638, Loss: 0.000013805, Improvement: -0.000002583, Best Loss: 0.000001972 in Epoch 1629
Epoch 1639
Epoch 1639, Loss: 0.000016554, Improvement: 0.000002749, Best Loss: 0.000001972 in Epoch 1629
Epoch 1640
Epoch 1640, Loss: 0.000014675, Improvement: -0.000001879, Best Loss: 0.000001972 in Epoch 1629
Epoch 1641
Epoch 1641, Loss: 0.000010040, Improvement: -0.000004635, Best Loss: 0.000001972 in Epoch 1629
Epoch 1642
Epoch 1642, Loss: 0.000011665, Improvement: 0.000001625, Best Loss: 0.000001972 in Epoch 1629
Epoch 1643
Epoch 1643, Loss: 0.000015888, Improvement: 0.000004223, Best Loss: 0.000001972 in Epoch 1629
Epoch 1644
Epoch 1644, Loss: 0.000020160, Improvement: 0.000004272, Best Loss: 0.000001972 in Epoch 1629
Epoch 1645
Epoch 1645, Loss: 0.000039553, Improvement: 0.000019393, Best Loss: 0.000001972 in Epoch 1629
Epoch 1646
Epoch 1646, Loss: 0.000036022, Improvement: -0.000003531, Best Loss: 0.000001972 in Epoch 1629
Epoch 1647
Epoch 1647, Loss: 0.000026212, Improvement: -0.000009810, Best Loss: 0.000001972 in Epoch 1629
Epoch 1648
Epoch 1648, Loss: 0.000038972, Improvement: 0.000012760, Best Loss: 0.000001972 in Epoch 1629
Epoch 1649
Epoch 1649, Loss: 0.000014678, Improvement: -0.000024293, Best Loss: 0.000001972 in Epoch 1629
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.000008036, Improvement: -0.000006642, Best Loss: 0.000001972 in Epoch 1629
Epoch 1651
Epoch 1651, Loss: 0.000005373, Improvement: -0.000002663, Best Loss: 0.000001972 in Epoch 1629
Epoch 1652
Epoch 1652, Loss: 0.000004078, Improvement: -0.000001294, Best Loss: 0.000001972 in Epoch 1629
Epoch 1653
Epoch 1653, Loss: 0.000003786, Improvement: -0.000000293, Best Loss: 0.000001972 in Epoch 1629
Epoch 1654
Epoch 1654, Loss: 0.000003994, Improvement: 0.000000208, Best Loss: 0.000001972 in Epoch 1629
Epoch 1655
Epoch 1655, Loss: 0.000003583, Improvement: -0.000000411, Best Loss: 0.000001972 in Epoch 1629
Epoch 1656
Epoch 1656, Loss: 0.000003236, Improvement: -0.000000347, Best Loss: 0.000001972 in Epoch 1629
Epoch 1657
Epoch 1657, Loss: 0.000003162, Improvement: -0.000000074, Best Loss: 0.000001972 in Epoch 1629
Epoch 1658
Epoch 1658, Loss: 0.000003454, Improvement: 0.000000293, Best Loss: 0.000001972 in Epoch 1629
Epoch 1659
Epoch 1659, Loss: 0.000003266, Improvement: -0.000000189, Best Loss: 0.000001972 in Epoch 1629
Epoch 1660
Epoch 1660, Loss: 0.000003058, Improvement: -0.000000207, Best Loss: 0.000001972 in Epoch 1629
Epoch 1661
Epoch 1661, Loss: 0.000002952, Improvement: -0.000000107, Best Loss: 0.000001972 in Epoch 1629
Epoch 1662
Epoch 1662, Loss: 0.000003843, Improvement: 0.000000892, Best Loss: 0.000001972 in Epoch 1629
Epoch 1663
Epoch 1663, Loss: 0.000006475, Improvement: 0.000002632, Best Loss: 0.000001972 in Epoch 1629
Epoch 1664
Epoch 1664, Loss: 0.000017656, Improvement: 0.000011181, Best Loss: 0.000001972 in Epoch 1629
Epoch 1665
Epoch 1665, Loss: 0.000010909, Improvement: -0.000006748, Best Loss: 0.000001972 in Epoch 1629
Epoch 1666
Epoch 1666, Loss: 0.000007823, Improvement: -0.000003086, Best Loss: 0.000001972 in Epoch 1629
Epoch 1667
Epoch 1667, Loss: 0.000011521, Improvement: 0.000003699, Best Loss: 0.000001972 in Epoch 1629
Epoch 1668
Epoch 1668, Loss: 0.000012540, Improvement: 0.000001018, Best Loss: 0.000001972 in Epoch 1629
Epoch 1669
Epoch 1669, Loss: 0.000012320, Improvement: -0.000000220, Best Loss: 0.000001972 in Epoch 1629
Epoch 1670
Epoch 1670, Loss: 0.000010578, Improvement: -0.000001742, Best Loss: 0.000001972 in Epoch 1629
Epoch 1671
Epoch 1671, Loss: 0.000015063, Improvement: 0.000004485, Best Loss: 0.000001972 in Epoch 1629
Epoch 1672
Epoch 1672, Loss: 0.000025641, Improvement: 0.000010578, Best Loss: 0.000001972 in Epoch 1629
Epoch 1673
Epoch 1673, Loss: 0.000041505, Improvement: 0.000015864, Best Loss: 0.000001972 in Epoch 1629
Epoch 1674
Epoch 1674, Loss: 0.000071196, Improvement: 0.000029691, Best Loss: 0.000001972 in Epoch 1629
Epoch 1675
Epoch 1675, Loss: 0.000026283, Improvement: -0.000044913, Best Loss: 0.000001972 in Epoch 1629
Epoch 1676
Epoch 1676, Loss: 0.000021298, Improvement: -0.000004985, Best Loss: 0.000001972 in Epoch 1629
Epoch 1677
Epoch 1677, Loss: 0.000016295, Improvement: -0.000005004, Best Loss: 0.000001972 in Epoch 1629
Epoch 1678
Epoch 1678, Loss: 0.000011318, Improvement: -0.000004977, Best Loss: 0.000001972 in Epoch 1629
Epoch 1679
Epoch 1679, Loss: 0.000013688, Improvement: 0.000002370, Best Loss: 0.000001972 in Epoch 1629
Epoch 1680
Epoch 1680, Loss: 0.000010190, Improvement: -0.000003498, Best Loss: 0.000001972 in Epoch 1629
Epoch 1681
Epoch 1681, Loss: 0.000010110, Improvement: -0.000000080, Best Loss: 0.000001972 in Epoch 1629
Epoch 1682
Epoch 1682, Loss: 0.000013593, Improvement: 0.000003483, Best Loss: 0.000001972 in Epoch 1629
Epoch 1683
Epoch 1683, Loss: 0.000037612, Improvement: 0.000024020, Best Loss: 0.000001972 in Epoch 1629
Epoch 1684
Epoch 1684, Loss: 0.000050448, Improvement: 0.000012836, Best Loss: 0.000001972 in Epoch 1629
Epoch 1685
Epoch 1685, Loss: 0.000027729, Improvement: -0.000022719, Best Loss: 0.000001972 in Epoch 1629
Epoch 1686
Epoch 1686, Loss: 0.000011691, Improvement: -0.000016038, Best Loss: 0.000001972 in Epoch 1629
Epoch 1687
Epoch 1687, Loss: 0.000006099, Improvement: -0.000005591, Best Loss: 0.000001972 in Epoch 1629
Epoch 1688
Epoch 1688, Loss: 0.000004921, Improvement: -0.000001179, Best Loss: 0.000001972 in Epoch 1629
Epoch 1689
Epoch 1689, Loss: 0.000005914, Improvement: 0.000000993, Best Loss: 0.000001972 in Epoch 1629
Epoch 1690
Epoch 1690, Loss: 0.000006091, Improvement: 0.000000177, Best Loss: 0.000001972 in Epoch 1629
Epoch 1691
Epoch 1691, Loss: 0.000008542, Improvement: 0.000002451, Best Loss: 0.000001972 in Epoch 1629
Epoch 1692
Epoch 1692, Loss: 0.000005967, Improvement: -0.000002575, Best Loss: 0.000001972 in Epoch 1629
Epoch 1693
Epoch 1693, Loss: 0.000004048, Improvement: -0.000001918, Best Loss: 0.000001972 in Epoch 1629
Epoch 1694
Epoch 1694, Loss: 0.000003918, Improvement: -0.000000130, Best Loss: 0.000001972 in Epoch 1629
Epoch 1695
Epoch 1695, Loss: 0.000003318, Improvement: -0.000000600, Best Loss: 0.000001972 in Epoch 1629
Epoch 1696
Epoch 1696, Loss: 0.000003049, Improvement: -0.000000269, Best Loss: 0.000001972 in Epoch 1629
Epoch 1697
Epoch 1697, Loss: 0.000003121, Improvement: 0.000000072, Best Loss: 0.000001972 in Epoch 1629
Epoch 1698
Epoch 1698, Loss: 0.000003128, Improvement: 0.000000007, Best Loss: 0.000001972 in Epoch 1629
Epoch 1699
Epoch 1699, Loss: 0.000002952, Improvement: -0.000000176, Best Loss: 0.000001972 in Epoch 1629
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.000002868, Improvement: -0.000000084, Best Loss: 0.000001972 in Epoch 1629
Epoch 1701
Epoch 1701, Loss: 0.000003438, Improvement: 0.000000570, Best Loss: 0.000001972 in Epoch 1629
Epoch 1702
Epoch 1702, Loss: 0.000003140, Improvement: -0.000000298, Best Loss: 0.000001972 in Epoch 1629
Epoch 1703
Epoch 1703, Loss: 0.000003427, Improvement: 0.000000288, Best Loss: 0.000001972 in Epoch 1629
Epoch 1704
Epoch 1704, Loss: 0.000003644, Improvement: 0.000000217, Best Loss: 0.000001972 in Epoch 1629
Epoch 1705
Epoch 1705, Loss: 0.000003018, Improvement: -0.000000626, Best Loss: 0.000001972 in Epoch 1629
Epoch 1706
Epoch 1706, Loss: 0.000002866, Improvement: -0.000000152, Best Loss: 0.000001972 in Epoch 1629
Epoch 1707
Epoch 1707, Loss: 0.000003521, Improvement: 0.000000655, Best Loss: 0.000001972 in Epoch 1629
Epoch 1708
Epoch 1708, Loss: 0.000003656, Improvement: 0.000000135, Best Loss: 0.000001972 in Epoch 1629
Epoch 1709
Epoch 1709, Loss: 0.000004833, Improvement: 0.000001177, Best Loss: 0.000001972 in Epoch 1629
Epoch 1710
Epoch 1710, Loss: 0.000019213, Improvement: 0.000014380, Best Loss: 0.000001972 in Epoch 1629
Epoch 1711
Epoch 1711, Loss: 0.000028228, Improvement: 0.000009015, Best Loss: 0.000001972 in Epoch 1629
Epoch 1712
Epoch 1712, Loss: 0.000016385, Improvement: -0.000011843, Best Loss: 0.000001972 in Epoch 1629
Epoch 1713
Epoch 1713, Loss: 0.000014223, Improvement: -0.000002162, Best Loss: 0.000001972 in Epoch 1629
Epoch 1714
Epoch 1714, Loss: 0.000010888, Improvement: -0.000003335, Best Loss: 0.000001972 in Epoch 1629
Epoch 1715
Epoch 1715, Loss: 0.000012884, Improvement: 0.000001996, Best Loss: 0.000001972 in Epoch 1629
Epoch 1716
Epoch 1716, Loss: 0.000008515, Improvement: -0.000004370, Best Loss: 0.000001972 in Epoch 1629
Epoch 1717
Epoch 1717, Loss: 0.000005780, Improvement: -0.000002735, Best Loss: 0.000001972 in Epoch 1629
Epoch 1718
Epoch 1718, Loss: 0.000005637, Improvement: -0.000000143, Best Loss: 0.000001972 in Epoch 1629
Epoch 1719
Epoch 1719, Loss: 0.000004630, Improvement: -0.000001006, Best Loss: 0.000001972 in Epoch 1629
Epoch 1720
Epoch 1720, Loss: 0.000004076, Improvement: -0.000000554, Best Loss: 0.000001972 in Epoch 1629
Epoch 1721
Epoch 1721, Loss: 0.000005869, Improvement: 0.000001792, Best Loss: 0.000001972 in Epoch 1629
Epoch 1722
Epoch 1722, Loss: 0.000006391, Improvement: 0.000000522, Best Loss: 0.000001972 in Epoch 1629
Epoch 1723
Epoch 1723, Loss: 0.000003559, Improvement: -0.000002832, Best Loss: 0.000001972 in Epoch 1629
Epoch 1724
Epoch 1724, Loss: 0.000003312, Improvement: -0.000000247, Best Loss: 0.000001972 in Epoch 1629
Epoch 1725
Epoch 1725, Loss: 0.000003410, Improvement: 0.000000098, Best Loss: 0.000001972 in Epoch 1629
Epoch 1726
Epoch 1726, Loss: 0.000004892, Improvement: 0.000001482, Best Loss: 0.000001972 in Epoch 1629
Epoch 1727
Epoch 1727, Loss: 0.000006726, Improvement: 0.000001834, Best Loss: 0.000001972 in Epoch 1629
Epoch 1728
Epoch 1728, Loss: 0.000005047, Improvement: -0.000001679, Best Loss: 0.000001972 in Epoch 1629
Epoch 1729
Epoch 1729, Loss: 0.000012060, Improvement: 0.000007013, Best Loss: 0.000001972 in Epoch 1629
Epoch 1730
Epoch 1730, Loss: 0.000045354, Improvement: 0.000033294, Best Loss: 0.000001972 in Epoch 1629
Epoch 1731
Epoch 1731, Loss: 0.000028501, Improvement: -0.000016853, Best Loss: 0.000001972 in Epoch 1629
Epoch 1732
Epoch 1732, Loss: 0.000039298, Improvement: 0.000010797, Best Loss: 0.000001972 in Epoch 1629
Epoch 1733
Epoch 1733, Loss: 0.000023913, Improvement: -0.000015385, Best Loss: 0.000001972 in Epoch 1629
Epoch 1734
Epoch 1734, Loss: 0.000022358, Improvement: -0.000001555, Best Loss: 0.000001972 in Epoch 1629
Epoch 1735
Epoch 1735, Loss: 0.000013203, Improvement: -0.000009156, Best Loss: 0.000001972 in Epoch 1629
Epoch 1736
Epoch 1736, Loss: 0.000017330, Improvement: 0.000004128, Best Loss: 0.000001972 in Epoch 1629
Epoch 1737
Epoch 1737, Loss: 0.000024539, Improvement: 0.000007209, Best Loss: 0.000001972 in Epoch 1629
Epoch 1738
Epoch 1738, Loss: 0.000034162, Improvement: 0.000009623, Best Loss: 0.000001972 in Epoch 1629
Epoch 1739
Epoch 1739, Loss: 0.000034457, Improvement: 0.000000295, Best Loss: 0.000001972 in Epoch 1629
Epoch 1740
Epoch 1740, Loss: 0.000012928, Improvement: -0.000021530, Best Loss: 0.000001972 in Epoch 1629
Epoch 1741
Epoch 1741, Loss: 0.000008705, Improvement: -0.000004223, Best Loss: 0.000001972 in Epoch 1629
Epoch 1742
Epoch 1742, Loss: 0.000006487, Improvement: -0.000002218, Best Loss: 0.000001972 in Epoch 1629
Epoch 1743
Epoch 1743, Loss: 0.000006777, Improvement: 0.000000291, Best Loss: 0.000001972 in Epoch 1629
Epoch 1744
Epoch 1744, Loss: 0.000006827, Improvement: 0.000000050, Best Loss: 0.000001972 in Epoch 1629
Epoch 1745
Epoch 1745, Loss: 0.000017782, Improvement: 0.000010954, Best Loss: 0.000001972 in Epoch 1629
Epoch 1746
Epoch 1746, Loss: 0.000007303, Improvement: -0.000010479, Best Loss: 0.000001972 in Epoch 1629
Epoch 1747
Epoch 1747, Loss: 0.000004072, Improvement: -0.000003231, Best Loss: 0.000001972 in Epoch 1629
Epoch 1748
Epoch 1748, Loss: 0.000004038, Improvement: -0.000000034, Best Loss: 0.000001972 in Epoch 1629
Epoch 1749
Epoch 1749, Loss: 0.000003130, Improvement: -0.000000908, Best Loss: 0.000001972 in Epoch 1629
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.000002769, Improvement: -0.000000361, Best Loss: 0.000001972 in Epoch 1629
Epoch 1751
A best model at epoch 1751 has been saved with training error 0.000001894.
Epoch 1751, Loss: 0.000002961, Improvement: 0.000000192, Best Loss: 0.000001894 in Epoch 1751
Epoch 1752
Epoch 1752, Loss: 0.000002978, Improvement: 0.000000017, Best Loss: 0.000001894 in Epoch 1751
Epoch 1753
Epoch 1753, Loss: 0.000002606, Improvement: -0.000000372, Best Loss: 0.000001894 in Epoch 1751
Epoch 1754
Epoch 1754, Loss: 0.000002616, Improvement: 0.000000010, Best Loss: 0.000001894 in Epoch 1751
Epoch 1755
Epoch 1755, Loss: 0.000003125, Improvement: 0.000000509, Best Loss: 0.000001894 in Epoch 1751
Epoch 1756
Epoch 1756, Loss: 0.000003676, Improvement: 0.000000551, Best Loss: 0.000001894 in Epoch 1751
Epoch 1757
Epoch 1757, Loss: 0.000005665, Improvement: 0.000001989, Best Loss: 0.000001894 in Epoch 1751
Epoch 1758
Epoch 1758, Loss: 0.000007918, Improvement: 0.000002253, Best Loss: 0.000001894 in Epoch 1751
Epoch 1759
Epoch 1759, Loss: 0.000007328, Improvement: -0.000000589, Best Loss: 0.000001894 in Epoch 1751
Epoch 1760
Epoch 1760, Loss: 0.000005889, Improvement: -0.000001439, Best Loss: 0.000001894 in Epoch 1751
Epoch 1761
Epoch 1761, Loss: 0.000005765, Improvement: -0.000000124, Best Loss: 0.000001894 in Epoch 1751
Epoch 1762
Epoch 1762, Loss: 0.000005649, Improvement: -0.000000116, Best Loss: 0.000001894 in Epoch 1751
Epoch 1763
Epoch 1763, Loss: 0.000004479, Improvement: -0.000001171, Best Loss: 0.000001894 in Epoch 1751
Epoch 1764
Epoch 1764, Loss: 0.000004629, Improvement: 0.000000150, Best Loss: 0.000001894 in Epoch 1751
Epoch 1765
Epoch 1765, Loss: 0.000004692, Improvement: 0.000000063, Best Loss: 0.000001894 in Epoch 1751
Epoch 1766
Epoch 1766, Loss: 0.000005658, Improvement: 0.000000966, Best Loss: 0.000001894 in Epoch 1751
Epoch 1767
Epoch 1767, Loss: 0.000013485, Improvement: 0.000007827, Best Loss: 0.000001894 in Epoch 1751
Epoch 1768
Epoch 1768, Loss: 0.000007245, Improvement: -0.000006240, Best Loss: 0.000001894 in Epoch 1751
Epoch 1769
Epoch 1769, Loss: 0.000012660, Improvement: 0.000005415, Best Loss: 0.000001894 in Epoch 1751
Epoch 1770
Epoch 1770, Loss: 0.000040321, Improvement: 0.000027661, Best Loss: 0.000001894 in Epoch 1751
Epoch 1771
Epoch 1771, Loss: 0.000037174, Improvement: -0.000003148, Best Loss: 0.000001894 in Epoch 1751
Epoch 1772
Epoch 1772, Loss: 0.000024971, Improvement: -0.000012203, Best Loss: 0.000001894 in Epoch 1751
Epoch 1773
Epoch 1773, Loss: 0.000013085, Improvement: -0.000011886, Best Loss: 0.000001894 in Epoch 1751
Epoch 1774
Epoch 1774, Loss: 0.000005513, Improvement: -0.000007572, Best Loss: 0.000001894 in Epoch 1751
Epoch 1775
Epoch 1775, Loss: 0.000003932, Improvement: -0.000001580, Best Loss: 0.000001894 in Epoch 1751
Epoch 1776
Epoch 1776, Loss: 0.000003250, Improvement: -0.000000682, Best Loss: 0.000001894 in Epoch 1751
Epoch 1777
Epoch 1777, Loss: 0.000003018, Improvement: -0.000000233, Best Loss: 0.000001894 in Epoch 1751
Epoch 1778
Epoch 1778, Loss: 0.000002982, Improvement: -0.000000035, Best Loss: 0.000001894 in Epoch 1751
Epoch 1779
Epoch 1779, Loss: 0.000002833, Improvement: -0.000000149, Best Loss: 0.000001894 in Epoch 1751
Epoch 1780
Epoch 1780, Loss: 0.000003158, Improvement: 0.000000325, Best Loss: 0.000001894 in Epoch 1751
Epoch 1781
Epoch 1781, Loss: 0.000004036, Improvement: 0.000000878, Best Loss: 0.000001894 in Epoch 1751
Epoch 1782
Epoch 1782, Loss: 0.000008247, Improvement: 0.000004211, Best Loss: 0.000001894 in Epoch 1751
Epoch 1783
Epoch 1783, Loss: 0.000037777, Improvement: 0.000029529, Best Loss: 0.000001894 in Epoch 1751
Epoch 1784
Epoch 1784, Loss: 0.000045429, Improvement: 0.000007652, Best Loss: 0.000001894 in Epoch 1751
Epoch 1785
Epoch 1785, Loss: 0.000056148, Improvement: 0.000010720, Best Loss: 0.000001894 in Epoch 1751
Epoch 1786
Epoch 1786, Loss: 0.000028825, Improvement: -0.000027324, Best Loss: 0.000001894 in Epoch 1751
Epoch 1787
Epoch 1787, Loss: 0.000015719, Improvement: -0.000013106, Best Loss: 0.000001894 in Epoch 1751
Epoch 1788
Epoch 1788, Loss: 0.000006504, Improvement: -0.000009214, Best Loss: 0.000001894 in Epoch 1751
Epoch 1789
Epoch 1789, Loss: 0.000003881, Improvement: -0.000002624, Best Loss: 0.000001894 in Epoch 1751
Epoch 1790
Epoch 1790, Loss: 0.000003073, Improvement: -0.000000808, Best Loss: 0.000001894 in Epoch 1751
Epoch 1791
Epoch 1791, Loss: 0.000002978, Improvement: -0.000000095, Best Loss: 0.000001894 in Epoch 1751
Epoch 1792
Epoch 1792, Loss: 0.000002983, Improvement: 0.000000006, Best Loss: 0.000001894 in Epoch 1751
Epoch 1793
Epoch 1793, Loss: 0.000002676, Improvement: -0.000000307, Best Loss: 0.000001894 in Epoch 1751
Epoch 1794
Epoch 1794, Loss: 0.000002574, Improvement: -0.000000102, Best Loss: 0.000001894 in Epoch 1751
Epoch 1795
Epoch 1795, Loss: 0.000003067, Improvement: 0.000000493, Best Loss: 0.000001894 in Epoch 1751
Epoch 1796
Epoch 1796, Loss: 0.000002841, Improvement: -0.000000226, Best Loss: 0.000001894 in Epoch 1751
Epoch 1797
Epoch 1797, Loss: 0.000002794, Improvement: -0.000000047, Best Loss: 0.000001894 in Epoch 1751
Epoch 1798
Epoch 1798, Loss: 0.000003514, Improvement: 0.000000720, Best Loss: 0.000001894 in Epoch 1751
Epoch 1799
Epoch 1799, Loss: 0.000006015, Improvement: 0.000002501, Best Loss: 0.000001894 in Epoch 1751
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.000007059, Improvement: 0.000001045, Best Loss: 0.000001894 in Epoch 1751
Epoch 1801
Epoch 1801, Loss: 0.000007361, Improvement: 0.000000301, Best Loss: 0.000001894 in Epoch 1751
Epoch 1802
Epoch 1802, Loss: 0.000008408, Improvement: 0.000001047, Best Loss: 0.000001894 in Epoch 1751
Epoch 1803
Epoch 1803, Loss: 0.000015928, Improvement: 0.000007521, Best Loss: 0.000001894 in Epoch 1751
Epoch 1804
Epoch 1804, Loss: 0.000022702, Improvement: 0.000006774, Best Loss: 0.000001894 in Epoch 1751
Epoch 1805
Epoch 1805, Loss: 0.000026383, Improvement: 0.000003680, Best Loss: 0.000001894 in Epoch 1751
Epoch 1806
Epoch 1806, Loss: 0.000021327, Improvement: -0.000005056, Best Loss: 0.000001894 in Epoch 1751
Epoch 1807
Epoch 1807, Loss: 0.000010250, Improvement: -0.000011077, Best Loss: 0.000001894 in Epoch 1751
Epoch 1808
Epoch 1808, Loss: 0.000006941, Improvement: -0.000003309, Best Loss: 0.000001894 in Epoch 1751
Epoch 1809
Epoch 1809, Loss: 0.000007270, Improvement: 0.000000329, Best Loss: 0.000001894 in Epoch 1751
Epoch 1810
Epoch 1810, Loss: 0.000007150, Improvement: -0.000000119, Best Loss: 0.000001894 in Epoch 1751
Epoch 1811
Epoch 1811, Loss: 0.000009093, Improvement: 0.000001943, Best Loss: 0.000001894 in Epoch 1751
Epoch 1812
Epoch 1812, Loss: 0.000008294, Improvement: -0.000000799, Best Loss: 0.000001894 in Epoch 1751
Epoch 1813
Epoch 1813, Loss: 0.000007441, Improvement: -0.000000852, Best Loss: 0.000001894 in Epoch 1751
Epoch 1814
Epoch 1814, Loss: 0.000005641, Improvement: -0.000001800, Best Loss: 0.000001894 in Epoch 1751
Epoch 1815
Epoch 1815, Loss: 0.000009139, Improvement: 0.000003498, Best Loss: 0.000001894 in Epoch 1751
Epoch 1816
Epoch 1816, Loss: 0.000015507, Improvement: 0.000006368, Best Loss: 0.000001894 in Epoch 1751
Epoch 1817
Epoch 1817, Loss: 0.000009623, Improvement: -0.000005885, Best Loss: 0.000001894 in Epoch 1751
Epoch 1818
Epoch 1818, Loss: 0.000009486, Improvement: -0.000000136, Best Loss: 0.000001894 in Epoch 1751
Epoch 1819
Epoch 1819, Loss: 0.000006599, Improvement: -0.000002887, Best Loss: 0.000001894 in Epoch 1751
Epoch 1820
Epoch 1820, Loss: 0.000008552, Improvement: 0.000001953, Best Loss: 0.000001894 in Epoch 1751
Epoch 1821
Epoch 1821, Loss: 0.000019096, Improvement: 0.000010543, Best Loss: 0.000001894 in Epoch 1751
Epoch 1822
Epoch 1822, Loss: 0.000031643, Improvement: 0.000012548, Best Loss: 0.000001894 in Epoch 1751
Epoch 1823
Epoch 1823, Loss: 0.000037104, Improvement: 0.000005461, Best Loss: 0.000001894 in Epoch 1751
Epoch 1824
Epoch 1824, Loss: 0.000042354, Improvement: 0.000005250, Best Loss: 0.000001894 in Epoch 1751
Epoch 1825
Epoch 1825, Loss: 0.000032901, Improvement: -0.000009454, Best Loss: 0.000001894 in Epoch 1751
Epoch 1826
Epoch 1826, Loss: 0.000013296, Improvement: -0.000019605, Best Loss: 0.000001894 in Epoch 1751
Epoch 1827
Epoch 1827, Loss: 0.000008376, Improvement: -0.000004920, Best Loss: 0.000001894 in Epoch 1751
Epoch 1828
Epoch 1828, Loss: 0.000004670, Improvement: -0.000003706, Best Loss: 0.000001894 in Epoch 1751
Epoch 1829
Epoch 1829, Loss: 0.000003160, Improvement: -0.000001510, Best Loss: 0.000001894 in Epoch 1751
Epoch 1830
Epoch 1830, Loss: 0.000002751, Improvement: -0.000000409, Best Loss: 0.000001894 in Epoch 1751
Epoch 1831
Epoch 1831, Loss: 0.000002809, Improvement: 0.000000057, Best Loss: 0.000001894 in Epoch 1751
Epoch 1832
Epoch 1832, Loss: 0.000003476, Improvement: 0.000000667, Best Loss: 0.000001894 in Epoch 1751
Epoch 1833
Epoch 1833, Loss: 0.000012930, Improvement: 0.000009454, Best Loss: 0.000001894 in Epoch 1751
Epoch 1834
Epoch 1834, Loss: 0.000035027, Improvement: 0.000022098, Best Loss: 0.000001894 in Epoch 1751
Epoch 1835
Epoch 1835, Loss: 0.000031398, Improvement: -0.000003629, Best Loss: 0.000001894 in Epoch 1751
Epoch 1836
Epoch 1836, Loss: 0.000012123, Improvement: -0.000019275, Best Loss: 0.000001894 in Epoch 1751
Epoch 1837
Epoch 1837, Loss: 0.000019744, Improvement: 0.000007620, Best Loss: 0.000001894 in Epoch 1751
Epoch 1838
Epoch 1838, Loss: 0.000015434, Improvement: -0.000004310, Best Loss: 0.000001894 in Epoch 1751
Epoch 1839
Epoch 1839, Loss: 0.000008695, Improvement: -0.000006739, Best Loss: 0.000001894 in Epoch 1751
Epoch 1840
Epoch 1840, Loss: 0.000006401, Improvement: -0.000002295, Best Loss: 0.000001894 in Epoch 1751
Epoch 1841
Epoch 1841, Loss: 0.000003695, Improvement: -0.000002706, Best Loss: 0.000001894 in Epoch 1751
Epoch 1842
Epoch 1842, Loss: 0.000003711, Improvement: 0.000000016, Best Loss: 0.000001894 in Epoch 1751
Epoch 1843
Epoch 1843, Loss: 0.000003356, Improvement: -0.000000355, Best Loss: 0.000001894 in Epoch 1751
Epoch 1844
Epoch 1844, Loss: 0.000004089, Improvement: 0.000000733, Best Loss: 0.000001894 in Epoch 1751
Epoch 1845
Epoch 1845, Loss: 0.000005291, Improvement: 0.000001202, Best Loss: 0.000001894 in Epoch 1751
Epoch 1846
Epoch 1846, Loss: 0.000028121, Improvement: 0.000022830, Best Loss: 0.000001894 in Epoch 1751
Epoch 1847
Epoch 1847, Loss: 0.000038099, Improvement: 0.000009978, Best Loss: 0.000001894 in Epoch 1751
Epoch 1848
Epoch 1848, Loss: 0.000023994, Improvement: -0.000014105, Best Loss: 0.000001894 in Epoch 1751
Epoch 1849
Epoch 1849, Loss: 0.000022023, Improvement: -0.000001971, Best Loss: 0.000001894 in Epoch 1751
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.000007652, Improvement: -0.000014371, Best Loss: 0.000001894 in Epoch 1751
Epoch 1851
Epoch 1851, Loss: 0.000004785, Improvement: -0.000002868, Best Loss: 0.000001894 in Epoch 1751
Epoch 1852
Epoch 1852, Loss: 0.000004260, Improvement: -0.000000525, Best Loss: 0.000001894 in Epoch 1751
Epoch 1853
Epoch 1853, Loss: 0.000003596, Improvement: -0.000000664, Best Loss: 0.000001894 in Epoch 1751
Epoch 1854
Epoch 1854, Loss: 0.000004540, Improvement: 0.000000944, Best Loss: 0.000001894 in Epoch 1751
Epoch 1855
Epoch 1855, Loss: 0.000016841, Improvement: 0.000012301, Best Loss: 0.000001894 in Epoch 1751
Epoch 1856
Epoch 1856, Loss: 0.000045495, Improvement: 0.000028654, Best Loss: 0.000001894 in Epoch 1751
Epoch 1857
Epoch 1857, Loss: 0.000016883, Improvement: -0.000028612, Best Loss: 0.000001894 in Epoch 1751
Epoch 1858
Epoch 1858, Loss: 0.000011875, Improvement: -0.000005008, Best Loss: 0.000001894 in Epoch 1751
Epoch 1859
Epoch 1859, Loss: 0.000011558, Improvement: -0.000000317, Best Loss: 0.000001894 in Epoch 1751
Epoch 1860
Epoch 1860, Loss: 0.000014289, Improvement: 0.000002731, Best Loss: 0.000001894 in Epoch 1751
Epoch 1861
Epoch 1861, Loss: 0.000024961, Improvement: 0.000010672, Best Loss: 0.000001894 in Epoch 1751
Epoch 1862
Epoch 1862, Loss: 0.000035122, Improvement: 0.000010161, Best Loss: 0.000001894 in Epoch 1751
Epoch 1863
Epoch 1863, Loss: 0.000030628, Improvement: -0.000004495, Best Loss: 0.000001894 in Epoch 1751
Epoch 1864
Epoch 1864, Loss: 0.000012498, Improvement: -0.000018129, Best Loss: 0.000001894 in Epoch 1751
Epoch 1865
Epoch 1865, Loss: 0.000005151, Improvement: -0.000007347, Best Loss: 0.000001894 in Epoch 1751
Epoch 1866
Epoch 1866, Loss: 0.000003999, Improvement: -0.000001153, Best Loss: 0.000001894 in Epoch 1751
Epoch 1867
Epoch 1867, Loss: 0.000003151, Improvement: -0.000000848, Best Loss: 0.000001894 in Epoch 1751
Epoch 1868
Epoch 1868, Loss: 0.000002829, Improvement: -0.000000322, Best Loss: 0.000001894 in Epoch 1751
Epoch 1869
Epoch 1869, Loss: 0.000004258, Improvement: 0.000001429, Best Loss: 0.000001894 in Epoch 1751
Epoch 1870
Epoch 1870, Loss: 0.000005956, Improvement: 0.000001698, Best Loss: 0.000001894 in Epoch 1751
Epoch 1871
Epoch 1871, Loss: 0.000004559, Improvement: -0.000001397, Best Loss: 0.000001894 in Epoch 1751
Epoch 1872
Epoch 1872, Loss: 0.000003505, Improvement: -0.000001053, Best Loss: 0.000001894 in Epoch 1751
Epoch 1873
Epoch 1873, Loss: 0.000003238, Improvement: -0.000000267, Best Loss: 0.000001894 in Epoch 1751
Epoch 1874
Epoch 1874, Loss: 0.000004326, Improvement: 0.000001089, Best Loss: 0.000001894 in Epoch 1751
Epoch 1875
Epoch 1875, Loss: 0.000006960, Improvement: 0.000002634, Best Loss: 0.000001894 in Epoch 1751
Epoch 1876
Epoch 1876, Loss: 0.000004670, Improvement: -0.000002291, Best Loss: 0.000001894 in Epoch 1751
Epoch 1877
Epoch 1877, Loss: 0.000004417, Improvement: -0.000000252, Best Loss: 0.000001894 in Epoch 1751
Epoch 1878
Epoch 1878, Loss: 0.000006552, Improvement: 0.000002135, Best Loss: 0.000001894 in Epoch 1751
Epoch 1879
Epoch 1879, Loss: 0.000006586, Improvement: 0.000000034, Best Loss: 0.000001894 in Epoch 1751
Epoch 1880
Epoch 1880, Loss: 0.000004721, Improvement: -0.000001865, Best Loss: 0.000001894 in Epoch 1751
Epoch 1881
Epoch 1881, Loss: 0.000005754, Improvement: 0.000001034, Best Loss: 0.000001894 in Epoch 1751
Epoch 1882
Epoch 1882, Loss: 0.000010048, Improvement: 0.000004294, Best Loss: 0.000001894 in Epoch 1751
Epoch 1883
Epoch 1883, Loss: 0.000020549, Improvement: 0.000010501, Best Loss: 0.000001894 in Epoch 1751
Epoch 1884
Epoch 1884, Loss: 0.000014125, Improvement: -0.000006424, Best Loss: 0.000001894 in Epoch 1751
Epoch 1885
Epoch 1885, Loss: 0.000014285, Improvement: 0.000000159, Best Loss: 0.000001894 in Epoch 1751
Epoch 1886
Epoch 1886, Loss: 0.000019646, Improvement: 0.000005361, Best Loss: 0.000001894 in Epoch 1751
Epoch 1887
Epoch 1887, Loss: 0.000028492, Improvement: 0.000008846, Best Loss: 0.000001894 in Epoch 1751
Epoch 1888
Epoch 1888, Loss: 0.000037723, Improvement: 0.000009230, Best Loss: 0.000001894 in Epoch 1751
Epoch 1889
Epoch 1889, Loss: 0.000018337, Improvement: -0.000019386, Best Loss: 0.000001894 in Epoch 1751
Epoch 1890
Epoch 1890, Loss: 0.000008938, Improvement: -0.000009400, Best Loss: 0.000001894 in Epoch 1751
Epoch 1891
Epoch 1891, Loss: 0.000005388, Improvement: -0.000003550, Best Loss: 0.000001894 in Epoch 1751
Epoch 1892
Epoch 1892, Loss: 0.000004012, Improvement: -0.000001376, Best Loss: 0.000001894 in Epoch 1751
Epoch 1893
Epoch 1893, Loss: 0.000004584, Improvement: 0.000000572, Best Loss: 0.000001894 in Epoch 1751
Epoch 1894
Epoch 1894, Loss: 0.000005181, Improvement: 0.000000597, Best Loss: 0.000001894 in Epoch 1751
Epoch 1895
Epoch 1895, Loss: 0.000007290, Improvement: 0.000002110, Best Loss: 0.000001894 in Epoch 1751
Epoch 1896
Epoch 1896, Loss: 0.000006213, Improvement: -0.000001078, Best Loss: 0.000001894 in Epoch 1751
Epoch 1897
Epoch 1897, Loss: 0.000003870, Improvement: -0.000002343, Best Loss: 0.000001894 in Epoch 1751
Epoch 1898
Epoch 1898, Loss: 0.000003410, Improvement: -0.000000461, Best Loss: 0.000001894 in Epoch 1751
Epoch 1899
Epoch 1899, Loss: 0.000003608, Improvement: 0.000000198, Best Loss: 0.000001894 in Epoch 1751
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.000003273, Improvement: -0.000000335, Best Loss: 0.000001894 in Epoch 1751
Epoch 1901
Epoch 1901, Loss: 0.000002929, Improvement: -0.000000344, Best Loss: 0.000001894 in Epoch 1751
Epoch 1902
Epoch 1902, Loss: 0.000008669, Improvement: 0.000005740, Best Loss: 0.000001894 in Epoch 1751
Epoch 1903
Epoch 1903, Loss: 0.000035539, Improvement: 0.000026870, Best Loss: 0.000001894 in Epoch 1751
Epoch 1904
Epoch 1904, Loss: 0.000037533, Improvement: 0.000001994, Best Loss: 0.000001894 in Epoch 1751
Epoch 1905
Epoch 1905, Loss: 0.000015493, Improvement: -0.000022040, Best Loss: 0.000001894 in Epoch 1751
Epoch 1906
Epoch 1906, Loss: 0.000012030, Improvement: -0.000003463, Best Loss: 0.000001894 in Epoch 1751
Epoch 1907
Epoch 1907, Loss: 0.000007959, Improvement: -0.000004071, Best Loss: 0.000001894 in Epoch 1751
Epoch 1908
Epoch 1908, Loss: 0.000006134, Improvement: -0.000001825, Best Loss: 0.000001894 in Epoch 1751
Epoch 1909
Epoch 1909, Loss: 0.000004871, Improvement: -0.000001263, Best Loss: 0.000001894 in Epoch 1751
Epoch 1910
Epoch 1910, Loss: 0.000004474, Improvement: -0.000000397, Best Loss: 0.000001894 in Epoch 1751
Epoch 1911
Epoch 1911, Loss: 0.000004247, Improvement: -0.000000227, Best Loss: 0.000001894 in Epoch 1751
Epoch 1912
Epoch 1912, Loss: 0.000003015, Improvement: -0.000001232, Best Loss: 0.000001894 in Epoch 1751
Epoch 1913
Epoch 1913, Loss: 0.000003670, Improvement: 0.000000655, Best Loss: 0.000001894 in Epoch 1751
Epoch 1914
Epoch 1914, Loss: 0.000005543, Improvement: 0.000001873, Best Loss: 0.000001894 in Epoch 1751
Epoch 1915
Epoch 1915, Loss: 0.000005332, Improvement: -0.000000210, Best Loss: 0.000001894 in Epoch 1751
Epoch 1916
Epoch 1916, Loss: 0.000003417, Improvement: -0.000001915, Best Loss: 0.000001894 in Epoch 1751
Epoch 1917
Epoch 1917, Loss: 0.000002685, Improvement: -0.000000732, Best Loss: 0.000001894 in Epoch 1751
Epoch 1918
Epoch 1918, Loss: 0.000002558, Improvement: -0.000000127, Best Loss: 0.000001894 in Epoch 1751
Epoch 1919
A best model at epoch 1919 has been saved with training error 0.000001880.
Epoch 1919, Loss: 0.000003178, Improvement: 0.000000620, Best Loss: 0.000001880 in Epoch 1919
Epoch 1920
Epoch 1920, Loss: 0.000005117, Improvement: 0.000001939, Best Loss: 0.000001880 in Epoch 1919
Epoch 1921
Epoch 1921, Loss: 0.000009079, Improvement: 0.000003961, Best Loss: 0.000001880 in Epoch 1919
Epoch 1922
Epoch 1922, Loss: 0.000009948, Improvement: 0.000000869, Best Loss: 0.000001880 in Epoch 1919
Epoch 1923
Epoch 1923, Loss: 0.000018497, Improvement: 0.000008550, Best Loss: 0.000001880 in Epoch 1919
Epoch 1924
Epoch 1924, Loss: 0.000022704, Improvement: 0.000004207, Best Loss: 0.000001880 in Epoch 1919
Epoch 1925
Epoch 1925, Loss: 0.000026678, Improvement: 0.000003974, Best Loss: 0.000001880 in Epoch 1919
Epoch 1926
Epoch 1926, Loss: 0.000027608, Improvement: 0.000000930, Best Loss: 0.000001880 in Epoch 1919
Epoch 1927
Epoch 1927, Loss: 0.000013687, Improvement: -0.000013921, Best Loss: 0.000001880 in Epoch 1919
Epoch 1928
Epoch 1928, Loss: 0.000010730, Improvement: -0.000002957, Best Loss: 0.000001880 in Epoch 1919
Epoch 1929
Epoch 1929, Loss: 0.000012336, Improvement: 0.000001606, Best Loss: 0.000001880 in Epoch 1919
Epoch 1930
Epoch 1930, Loss: 0.000033575, Improvement: 0.000021239, Best Loss: 0.000001880 in Epoch 1919
Epoch 1931
Epoch 1931, Loss: 0.000057231, Improvement: 0.000023656, Best Loss: 0.000001880 in Epoch 1919
Epoch 1932
Epoch 1932, Loss: 0.000034779, Improvement: -0.000022452, Best Loss: 0.000001880 in Epoch 1919
Epoch 1933
Epoch 1933, Loss: 0.000015490, Improvement: -0.000019289, Best Loss: 0.000001880 in Epoch 1919
Epoch 1934
Epoch 1934, Loss: 0.000006068, Improvement: -0.000009422, Best Loss: 0.000001880 in Epoch 1919
Epoch 1935
Epoch 1935, Loss: 0.000004301, Improvement: -0.000001767, Best Loss: 0.000001880 in Epoch 1919
Epoch 1936
Epoch 1936, Loss: 0.000003542, Improvement: -0.000000759, Best Loss: 0.000001880 in Epoch 1919
Epoch 1937
Epoch 1937, Loss: 0.000002892, Improvement: -0.000000651, Best Loss: 0.000001880 in Epoch 1919
Epoch 1938
A best model at epoch 1938 has been saved with training error 0.000001879.
Epoch 1938, Loss: 0.000002611, Improvement: -0.000000281, Best Loss: 0.000001879 in Epoch 1938
Epoch 1939
A best model at epoch 1939 has been saved with training error 0.000001869.
A best model at epoch 1939 has been saved with training error 0.000001847.
Epoch 1939, Loss: 0.000002584, Improvement: -0.000000027, Best Loss: 0.000001847 in Epoch 1939
Epoch 1940
Epoch 1940, Loss: 0.000002513, Improvement: -0.000000071, Best Loss: 0.000001847 in Epoch 1939
Epoch 1941
A best model at epoch 1941 has been saved with training error 0.000001687.
Epoch 1941, Loss: 0.000002391, Improvement: -0.000000123, Best Loss: 0.000001687 in Epoch 1941
Epoch 1942
Epoch 1942, Loss: 0.000002448, Improvement: 0.000000057, Best Loss: 0.000001687 in Epoch 1941
Epoch 1943
Epoch 1943, Loss: 0.000002374, Improvement: -0.000000074, Best Loss: 0.000001687 in Epoch 1941
Epoch 1944
A best model at epoch 1944 has been saved with training error 0.000001682.
A best model at epoch 1944 has been saved with training error 0.000001661.
Epoch 1944, Loss: 0.000002217, Improvement: -0.000000157, Best Loss: 0.000001661 in Epoch 1944
Epoch 1945
A best model at epoch 1945 has been saved with training error 0.000001600.
Epoch 1945, Loss: 0.000002159, Improvement: -0.000000058, Best Loss: 0.000001600 in Epoch 1945
Epoch 1946
Epoch 1946, Loss: 0.000002494, Improvement: 0.000000336, Best Loss: 0.000001600 in Epoch 1945
Epoch 1947
Epoch 1947, Loss: 0.000002365, Improvement: -0.000000130, Best Loss: 0.000001600 in Epoch 1945
Epoch 1948
Epoch 1948, Loss: 0.000002537, Improvement: 0.000000173, Best Loss: 0.000001600 in Epoch 1945
Epoch 1949
A best model at epoch 1949 has been saved with training error 0.000001557.
Epoch 1949, Loss: 0.000002370, Improvement: -0.000000167, Best Loss: 0.000001557 in Epoch 1949
Epoch 1950
A best model at epoch 1950 has been saved with training error 0.000001332.
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.000002173, Improvement: -0.000000197, Best Loss: 0.000001332 in Epoch 1950
Epoch 1951
Epoch 1951, Loss: 0.000002495, Improvement: 0.000000321, Best Loss: 0.000001332 in Epoch 1950
Epoch 1952
Epoch 1952, Loss: 0.000002433, Improvement: -0.000000062, Best Loss: 0.000001332 in Epoch 1950
Epoch 1953
Epoch 1953, Loss: 0.000003242, Improvement: 0.000000809, Best Loss: 0.000001332 in Epoch 1950
Epoch 1954
Epoch 1954, Loss: 0.000003820, Improvement: 0.000000578, Best Loss: 0.000001332 in Epoch 1950
Epoch 1955
Epoch 1955, Loss: 0.000015518, Improvement: 0.000011698, Best Loss: 0.000001332 in Epoch 1950
Epoch 1956
Epoch 1956, Loss: 0.000039375, Improvement: 0.000023857, Best Loss: 0.000001332 in Epoch 1950
Epoch 1957
Epoch 1957, Loss: 0.000023151, Improvement: -0.000016225, Best Loss: 0.000001332 in Epoch 1950
Epoch 1958
Epoch 1958, Loss: 0.000012992, Improvement: -0.000010159, Best Loss: 0.000001332 in Epoch 1950
Epoch 1959
Epoch 1959, Loss: 0.000011724, Improvement: -0.000001268, Best Loss: 0.000001332 in Epoch 1950
Epoch 1960
Epoch 1960, Loss: 0.000006310, Improvement: -0.000005414, Best Loss: 0.000001332 in Epoch 1950
Epoch 1961
Epoch 1961, Loss: 0.000003967, Improvement: -0.000002343, Best Loss: 0.000001332 in Epoch 1950
Epoch 1962
Epoch 1962, Loss: 0.000004708, Improvement: 0.000000741, Best Loss: 0.000001332 in Epoch 1950
Epoch 1963
Epoch 1963, Loss: 0.000004504, Improvement: -0.000000204, Best Loss: 0.000001332 in Epoch 1950
Epoch 1964
Epoch 1964, Loss: 0.000004628, Improvement: 0.000000123, Best Loss: 0.000001332 in Epoch 1950
Epoch 1965
Epoch 1965, Loss: 0.000005950, Improvement: 0.000001322, Best Loss: 0.000001332 in Epoch 1950
Epoch 1966
Epoch 1966, Loss: 0.000006861, Improvement: 0.000000911, Best Loss: 0.000001332 in Epoch 1950
Epoch 1967
Epoch 1967, Loss: 0.000009649, Improvement: 0.000002788, Best Loss: 0.000001332 in Epoch 1950
Epoch 1968
Epoch 1968, Loss: 0.000007476, Improvement: -0.000002172, Best Loss: 0.000001332 in Epoch 1950
Epoch 1969
Epoch 1969, Loss: 0.000008131, Improvement: 0.000000655, Best Loss: 0.000001332 in Epoch 1950
Epoch 1970
Epoch 1970, Loss: 0.000009563, Improvement: 0.000001432, Best Loss: 0.000001332 in Epoch 1950
Epoch 1971
Epoch 1971, Loss: 0.000006348, Improvement: -0.000003215, Best Loss: 0.000001332 in Epoch 1950
Epoch 1972
Epoch 1972, Loss: 0.000008442, Improvement: 0.000002094, Best Loss: 0.000001332 in Epoch 1950
Epoch 1973
Epoch 1973, Loss: 0.000020239, Improvement: 0.000011796, Best Loss: 0.000001332 in Epoch 1950
Epoch 1974
Epoch 1974, Loss: 0.000024633, Improvement: 0.000004394, Best Loss: 0.000001332 in Epoch 1950
Epoch 1975
Epoch 1975, Loss: 0.000047019, Improvement: 0.000022386, Best Loss: 0.000001332 in Epoch 1950
Epoch 1976
Epoch 1976, Loss: 0.000014495, Improvement: -0.000032525, Best Loss: 0.000001332 in Epoch 1950
Epoch 1977
Epoch 1977, Loss: 0.000006234, Improvement: -0.000008261, Best Loss: 0.000001332 in Epoch 1950
Epoch 1978
Epoch 1978, Loss: 0.000003968, Improvement: -0.000002266, Best Loss: 0.000001332 in Epoch 1950
Epoch 1979
Epoch 1979, Loss: 0.000003444, Improvement: -0.000000524, Best Loss: 0.000001332 in Epoch 1950
Epoch 1980
Epoch 1980, Loss: 0.000005183, Improvement: 0.000001739, Best Loss: 0.000001332 in Epoch 1950
Epoch 1981
Epoch 1981, Loss: 0.000005841, Improvement: 0.000000658, Best Loss: 0.000001332 in Epoch 1950
Epoch 1982
Epoch 1982, Loss: 0.000008956, Improvement: 0.000003115, Best Loss: 0.000001332 in Epoch 1950
Epoch 1983
Epoch 1983, Loss: 0.000015569, Improvement: 0.000006613, Best Loss: 0.000001332 in Epoch 1950
Epoch 1984
Epoch 1984, Loss: 0.000012706, Improvement: -0.000002863, Best Loss: 0.000001332 in Epoch 1950
Epoch 1985
Epoch 1985, Loss: 0.000011981, Improvement: -0.000000725, Best Loss: 0.000001332 in Epoch 1950
Epoch 1986
Epoch 1986, Loss: 0.000039342, Improvement: 0.000027362, Best Loss: 0.000001332 in Epoch 1950
Epoch 1987
Epoch 1987, Loss: 0.000047536, Improvement: 0.000008193, Best Loss: 0.000001332 in Epoch 1950
Epoch 1988
Epoch 1988, Loss: 0.000019490, Improvement: -0.000028046, Best Loss: 0.000001332 in Epoch 1950
Epoch 1989
Epoch 1989, Loss: 0.000015001, Improvement: -0.000004488, Best Loss: 0.000001332 in Epoch 1950
Epoch 1990
Epoch 1990, Loss: 0.000009741, Improvement: -0.000005260, Best Loss: 0.000001332 in Epoch 1950
Epoch 1991
Epoch 1991, Loss: 0.000005530, Improvement: -0.000004211, Best Loss: 0.000001332 in Epoch 1950
Epoch 1992
Epoch 1992, Loss: 0.000003496, Improvement: -0.000002034, Best Loss: 0.000001332 in Epoch 1950
Epoch 1993
Epoch 1993, Loss: 0.000003372, Improvement: -0.000000123, Best Loss: 0.000001332 in Epoch 1950
Epoch 1994
Epoch 1994, Loss: 0.000003605, Improvement: 0.000000233, Best Loss: 0.000001332 in Epoch 1950
Epoch 1995
Epoch 1995, Loss: 0.000003168, Improvement: -0.000000437, Best Loss: 0.000001332 in Epoch 1950
Epoch 1996
Epoch 1996, Loss: 0.000002888, Improvement: -0.000000280, Best Loss: 0.000001332 in Epoch 1950
Epoch 1997
Epoch 1997, Loss: 0.000002352, Improvement: -0.000000536, Best Loss: 0.000001332 in Epoch 1950
Epoch 1998
Epoch 1998, Loss: 0.000002140, Improvement: -0.000000212, Best Loss: 0.000001332 in Epoch 1950
Epoch 1999
Epoch 1999, Loss: 0.000002170, Improvement: 0.000000030, Best Loss: 0.000001332 in Epoch 1950
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.000002992, Improvement: 0.000000822, Best Loss: 0.000001332 in Epoch 1950
Epoch 2001
Epoch 2001, Loss: 0.000002843, Improvement: -0.000000149, Best Loss: 0.000001332 in Epoch 1950
Epoch 2002
Epoch 2002, Loss: 0.000002972, Improvement: 0.000000129, Best Loss: 0.000001332 in Epoch 1950
Epoch 2003
Epoch 2003, Loss: 0.000004478, Improvement: 0.000001506, Best Loss: 0.000001332 in Epoch 1950
Epoch 2004
Epoch 2004, Loss: 0.000003478, Improvement: -0.000001000, Best Loss: 0.000001332 in Epoch 1950
Epoch 2005
Epoch 2005, Loss: 0.000002810, Improvement: -0.000000668, Best Loss: 0.000001332 in Epoch 1950
Epoch 2006
Epoch 2006, Loss: 0.000002596, Improvement: -0.000000214, Best Loss: 0.000001332 in Epoch 1950
Epoch 2007
Epoch 2007, Loss: 0.000004197, Improvement: 0.000001601, Best Loss: 0.000001332 in Epoch 1950
Epoch 2008
Epoch 2008, Loss: 0.000005864, Improvement: 0.000001667, Best Loss: 0.000001332 in Epoch 1950
Epoch 2009
Epoch 2009, Loss: 0.000007066, Improvement: 0.000001202, Best Loss: 0.000001332 in Epoch 1950
Epoch 2010
Epoch 2010, Loss: 0.000013207, Improvement: 0.000006141, Best Loss: 0.000001332 in Epoch 1950
Epoch 2011
Epoch 2011, Loss: 0.000010642, Improvement: -0.000002565, Best Loss: 0.000001332 in Epoch 1950
Epoch 2012
Epoch 2012, Loss: 0.000005786, Improvement: -0.000004855, Best Loss: 0.000001332 in Epoch 1950
Epoch 2013
Epoch 2013, Loss: 0.000004308, Improvement: -0.000001479, Best Loss: 0.000001332 in Epoch 1950
Epoch 2014
Epoch 2014, Loss: 0.000006476, Improvement: 0.000002169, Best Loss: 0.000001332 in Epoch 1950
Epoch 2015
Epoch 2015, Loss: 0.000017610, Improvement: 0.000011133, Best Loss: 0.000001332 in Epoch 1950
Epoch 2016
Epoch 2016, Loss: 0.000019893, Improvement: 0.000002283, Best Loss: 0.000001332 in Epoch 1950
Epoch 2017
Epoch 2017, Loss: 0.000014832, Improvement: -0.000005061, Best Loss: 0.000001332 in Epoch 1950
Epoch 2018
Epoch 2018, Loss: 0.000013352, Improvement: -0.000001480, Best Loss: 0.000001332 in Epoch 1950
Epoch 2019
Epoch 2019, Loss: 0.000011426, Improvement: -0.000001926, Best Loss: 0.000001332 in Epoch 1950
Epoch 2020
Epoch 2020, Loss: 0.000028403, Improvement: 0.000016977, Best Loss: 0.000001332 in Epoch 1950
Epoch 2021
Epoch 2021, Loss: 0.000033442, Improvement: 0.000005039, Best Loss: 0.000001332 in Epoch 1950
Epoch 2022
Epoch 2022, Loss: 0.000026262, Improvement: -0.000007179, Best Loss: 0.000001332 in Epoch 1950
Epoch 2023
Epoch 2023, Loss: 0.000011625, Improvement: -0.000014637, Best Loss: 0.000001332 in Epoch 1950
Epoch 2024
Epoch 2024, Loss: 0.000006965, Improvement: -0.000004660, Best Loss: 0.000001332 in Epoch 1950
Epoch 2025
Epoch 2025, Loss: 0.000004944, Improvement: -0.000002021, Best Loss: 0.000001332 in Epoch 1950
Epoch 2026
Epoch 2026, Loss: 0.000003099, Improvement: -0.000001846, Best Loss: 0.000001332 in Epoch 1950
Epoch 2027
Epoch 2027, Loss: 0.000002824, Improvement: -0.000000275, Best Loss: 0.000001332 in Epoch 1950
Epoch 2028
Epoch 2028, Loss: 0.000002766, Improvement: -0.000000058, Best Loss: 0.000001332 in Epoch 1950
Epoch 2029
Epoch 2029, Loss: 0.000002601, Improvement: -0.000000165, Best Loss: 0.000001332 in Epoch 1950
Epoch 2030
Epoch 2030, Loss: 0.000002849, Improvement: 0.000000247, Best Loss: 0.000001332 in Epoch 1950
Epoch 2031
Epoch 2031, Loss: 0.000003106, Improvement: 0.000000257, Best Loss: 0.000001332 in Epoch 1950
Epoch 2032
Epoch 2032, Loss: 0.000003627, Improvement: 0.000000521, Best Loss: 0.000001332 in Epoch 1950
Epoch 2033
Epoch 2033, Loss: 0.000004899, Improvement: 0.000001272, Best Loss: 0.000001332 in Epoch 1950
Epoch 2034
Epoch 2034, Loss: 0.000003466, Improvement: -0.000001433, Best Loss: 0.000001332 in Epoch 1950
Epoch 2035
Epoch 2035, Loss: 0.000004372, Improvement: 0.000000906, Best Loss: 0.000001332 in Epoch 1950
Epoch 2036
Epoch 2036, Loss: 0.000003933, Improvement: -0.000000439, Best Loss: 0.000001332 in Epoch 1950
Epoch 2037
Epoch 2037, Loss: 0.000003585, Improvement: -0.000000348, Best Loss: 0.000001332 in Epoch 1950
Epoch 2038
Epoch 2038, Loss: 0.000002955, Improvement: -0.000000630, Best Loss: 0.000001332 in Epoch 1950
Epoch 2039
Epoch 2039, Loss: 0.000002686, Improvement: -0.000000268, Best Loss: 0.000001332 in Epoch 1950
Epoch 2040
Epoch 2040, Loss: 0.000004007, Improvement: 0.000001321, Best Loss: 0.000001332 in Epoch 1950
Epoch 2041
Epoch 2041, Loss: 0.000007111, Improvement: 0.000003104, Best Loss: 0.000001332 in Epoch 1950
Epoch 2042
Epoch 2042, Loss: 0.000017783, Improvement: 0.000010672, Best Loss: 0.000001332 in Epoch 1950
Epoch 2043
Epoch 2043, Loss: 0.000014487, Improvement: -0.000003296, Best Loss: 0.000001332 in Epoch 1950
Epoch 2044
Epoch 2044, Loss: 0.000011301, Improvement: -0.000003186, Best Loss: 0.000001332 in Epoch 1950
Epoch 2045
Epoch 2045, Loss: 0.000009320, Improvement: -0.000001981, Best Loss: 0.000001332 in Epoch 1950
Epoch 2046
Epoch 2046, Loss: 0.000007983, Improvement: -0.000001337, Best Loss: 0.000001332 in Epoch 1950
Epoch 2047
Epoch 2047, Loss: 0.000006658, Improvement: -0.000001325, Best Loss: 0.000001332 in Epoch 1950
Epoch 2048
Epoch 2048, Loss: 0.000010738, Improvement: 0.000004081, Best Loss: 0.000001332 in Epoch 1950
Epoch 2049
Epoch 2049, Loss: 0.000018818, Improvement: 0.000008079, Best Loss: 0.000001332 in Epoch 1950
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.000011637, Improvement: -0.000007180, Best Loss: 0.000001332 in Epoch 1950
Epoch 2051
Epoch 2051, Loss: 0.000012012, Improvement: 0.000000374, Best Loss: 0.000001332 in Epoch 1950
Epoch 2052
Epoch 2052, Loss: 0.000007128, Improvement: -0.000004884, Best Loss: 0.000001332 in Epoch 1950
Epoch 2053
Epoch 2053, Loss: 0.000008305, Improvement: 0.000001177, Best Loss: 0.000001332 in Epoch 1950
Epoch 2054
Epoch 2054, Loss: 0.000010726, Improvement: 0.000002421, Best Loss: 0.000001332 in Epoch 1950
Epoch 2055
Epoch 2055, Loss: 0.000024913, Improvement: 0.000014187, Best Loss: 0.000001332 in Epoch 1950
Epoch 2056
Epoch 2056, Loss: 0.000017176, Improvement: -0.000007738, Best Loss: 0.000001332 in Epoch 1950
Epoch 2057
Epoch 2057, Loss: 0.000006739, Improvement: -0.000010437, Best Loss: 0.000001332 in Epoch 1950
Epoch 2058
Epoch 2058, Loss: 0.000004258, Improvement: -0.000002481, Best Loss: 0.000001332 in Epoch 1950
Epoch 2059
Epoch 2059, Loss: 0.000002856, Improvement: -0.000001402, Best Loss: 0.000001332 in Epoch 1950
Epoch 2060
Epoch 2060, Loss: 0.000002322, Improvement: -0.000000535, Best Loss: 0.000001332 in Epoch 1950
Epoch 2061
Epoch 2061, Loss: 0.000002333, Improvement: 0.000000012, Best Loss: 0.000001332 in Epoch 1950
Epoch 2062
Epoch 2062, Loss: 0.000002498, Improvement: 0.000000165, Best Loss: 0.000001332 in Epoch 1950
Epoch 2063
Epoch 2063, Loss: 0.000002329, Improvement: -0.000000169, Best Loss: 0.000001332 in Epoch 1950
Epoch 2064
Epoch 2064, Loss: 0.000002693, Improvement: 0.000000364, Best Loss: 0.000001332 in Epoch 1950
Epoch 2065
Epoch 2065, Loss: 0.000003537, Improvement: 0.000000845, Best Loss: 0.000001332 in Epoch 1950
Epoch 2066
Epoch 2066, Loss: 0.000003030, Improvement: -0.000000507, Best Loss: 0.000001332 in Epoch 1950
Epoch 2067
Epoch 2067, Loss: 0.000003545, Improvement: 0.000000515, Best Loss: 0.000001332 in Epoch 1950
Epoch 2068
Epoch 2068, Loss: 0.000003513, Improvement: -0.000000032, Best Loss: 0.000001332 in Epoch 1950
Epoch 2069
Epoch 2069, Loss: 0.000011004, Improvement: 0.000007490, Best Loss: 0.000001332 in Epoch 1950
Epoch 2070
Epoch 2070, Loss: 0.000020389, Improvement: 0.000009386, Best Loss: 0.000001332 in Epoch 1950
Epoch 2071
Epoch 2071, Loss: 0.000013885, Improvement: -0.000006504, Best Loss: 0.000001332 in Epoch 1950
Epoch 2072
Epoch 2072, Loss: 0.000011735, Improvement: -0.000002150, Best Loss: 0.000001332 in Epoch 1950
Epoch 2073
Epoch 2073, Loss: 0.000030883, Improvement: 0.000019149, Best Loss: 0.000001332 in Epoch 1950
Epoch 2074
Epoch 2074, Loss: 0.000036460, Improvement: 0.000005577, Best Loss: 0.000001332 in Epoch 1950
Epoch 2075
Epoch 2075, Loss: 0.000016607, Improvement: -0.000019853, Best Loss: 0.000001332 in Epoch 1950
Epoch 2076
Epoch 2076, Loss: 0.000009195, Improvement: -0.000007412, Best Loss: 0.000001332 in Epoch 1950
Epoch 2077
Epoch 2077, Loss: 0.000005689, Improvement: -0.000003506, Best Loss: 0.000001332 in Epoch 1950
Epoch 2078
Epoch 2078, Loss: 0.000005590, Improvement: -0.000000098, Best Loss: 0.000001332 in Epoch 1950
Epoch 2079
Epoch 2079, Loss: 0.000006950, Improvement: 0.000001359, Best Loss: 0.000001332 in Epoch 1950
Epoch 2080
Epoch 2080, Loss: 0.000007183, Improvement: 0.000000233, Best Loss: 0.000001332 in Epoch 1950
Epoch 2081
Epoch 2081, Loss: 0.000018043, Improvement: 0.000010861, Best Loss: 0.000001332 in Epoch 1950
Epoch 2082
Epoch 2082, Loss: 0.000015594, Improvement: -0.000002450, Best Loss: 0.000001332 in Epoch 1950
Epoch 2083
Epoch 2083, Loss: 0.000008644, Improvement: -0.000006950, Best Loss: 0.000001332 in Epoch 1950
Epoch 2084
Epoch 2084, Loss: 0.000008880, Improvement: 0.000000237, Best Loss: 0.000001332 in Epoch 1950
Epoch 2085
Epoch 2085, Loss: 0.000006197, Improvement: -0.000002683, Best Loss: 0.000001332 in Epoch 1950
Epoch 2086
Epoch 2086, Loss: 0.000012492, Improvement: 0.000006295, Best Loss: 0.000001332 in Epoch 1950
Epoch 2087
Epoch 2087, Loss: 0.000006374, Improvement: -0.000006118, Best Loss: 0.000001332 in Epoch 1950
Epoch 2088
Epoch 2088, Loss: 0.000003888, Improvement: -0.000002486, Best Loss: 0.000001332 in Epoch 1950
Epoch 2089
Epoch 2089, Loss: 0.000003415, Improvement: -0.000000473, Best Loss: 0.000001332 in Epoch 1950
Epoch 2090
Epoch 2090, Loss: 0.000004393, Improvement: 0.000000978, Best Loss: 0.000001332 in Epoch 1950
Epoch 2091
Epoch 2091, Loss: 0.000003600, Improvement: -0.000000793, Best Loss: 0.000001332 in Epoch 1950
Epoch 2092
Epoch 2092, Loss: 0.000005094, Improvement: 0.000001494, Best Loss: 0.000001332 in Epoch 1950
Epoch 2093
Epoch 2093, Loss: 0.000011219, Improvement: 0.000006125, Best Loss: 0.000001332 in Epoch 1950
Epoch 2094
Epoch 2094, Loss: 0.000012073, Improvement: 0.000000855, Best Loss: 0.000001332 in Epoch 1950
Epoch 2095
Epoch 2095, Loss: 0.000027046, Improvement: 0.000014973, Best Loss: 0.000001332 in Epoch 1950
Epoch 2096
Epoch 2096, Loss: 0.000029096, Improvement: 0.000002050, Best Loss: 0.000001332 in Epoch 1950
Epoch 2097
Epoch 2097, Loss: 0.000032823, Improvement: 0.000003726, Best Loss: 0.000001332 in Epoch 1950
Epoch 2098
Epoch 2098, Loss: 0.000014468, Improvement: -0.000018354, Best Loss: 0.000001332 in Epoch 1950
Epoch 2099
Epoch 2099, Loss: 0.000007215, Improvement: -0.000007254, Best Loss: 0.000001332 in Epoch 1950
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.000007947, Improvement: 0.000000733, Best Loss: 0.000001332 in Epoch 1950
Epoch 2101
Epoch 2101, Loss: 0.000005702, Improvement: -0.000002245, Best Loss: 0.000001332 in Epoch 1950
Epoch 2102
Epoch 2102, Loss: 0.000004324, Improvement: -0.000001378, Best Loss: 0.000001332 in Epoch 1950
Epoch 2103
Epoch 2103, Loss: 0.000004521, Improvement: 0.000000197, Best Loss: 0.000001332 in Epoch 1950
Epoch 2104
Epoch 2104, Loss: 0.000007387, Improvement: 0.000002866, Best Loss: 0.000001332 in Epoch 1950
Epoch 2105
Epoch 2105, Loss: 0.000004438, Improvement: -0.000002949, Best Loss: 0.000001332 in Epoch 1950
Epoch 2106
Epoch 2106, Loss: 0.000007655, Improvement: 0.000003216, Best Loss: 0.000001332 in Epoch 1950
Epoch 2107
Epoch 2107, Loss: 0.000016272, Improvement: 0.000008617, Best Loss: 0.000001332 in Epoch 1950
Epoch 2108
Epoch 2108, Loss: 0.000016047, Improvement: -0.000000224, Best Loss: 0.000001332 in Epoch 1950
Epoch 2109
Epoch 2109, Loss: 0.000037357, Improvement: 0.000021310, Best Loss: 0.000001332 in Epoch 1950
Epoch 2110
Epoch 2110, Loss: 0.000021707, Improvement: -0.000015650, Best Loss: 0.000001332 in Epoch 1950
Epoch 2111
Epoch 2111, Loss: 0.000007710, Improvement: -0.000013998, Best Loss: 0.000001332 in Epoch 1950
Epoch 2112
Epoch 2112, Loss: 0.000004104, Improvement: -0.000003605, Best Loss: 0.000001332 in Epoch 1950
Epoch 2113
Epoch 2113, Loss: 0.000003023, Improvement: -0.000001082, Best Loss: 0.000001332 in Epoch 1950
Epoch 2114
Epoch 2114, Loss: 0.000002532, Improvement: -0.000000491, Best Loss: 0.000001332 in Epoch 1950
Epoch 2115
Epoch 2115, Loss: 0.000002534, Improvement: 0.000000002, Best Loss: 0.000001332 in Epoch 1950
Epoch 2116
Epoch 2116, Loss: 0.000002689, Improvement: 0.000000155, Best Loss: 0.000001332 in Epoch 1950
Epoch 2117
Epoch 2117, Loss: 0.000002424, Improvement: -0.000000265, Best Loss: 0.000001332 in Epoch 1950
Epoch 2118
Epoch 2118, Loss: 0.000002282, Improvement: -0.000000142, Best Loss: 0.000001332 in Epoch 1950
Epoch 2119
Epoch 2119, Loss: 0.000003773, Improvement: 0.000001491, Best Loss: 0.000001332 in Epoch 1950
Epoch 2120
Epoch 2120, Loss: 0.000005796, Improvement: 0.000002023, Best Loss: 0.000001332 in Epoch 1950
Epoch 2121
Epoch 2121, Loss: 0.000023356, Improvement: 0.000017560, Best Loss: 0.000001332 in Epoch 1950
Epoch 2122
Epoch 2122, Loss: 0.000071065, Improvement: 0.000047709, Best Loss: 0.000001332 in Epoch 1950
Epoch 2123
Epoch 2123, Loss: 0.000037500, Improvement: -0.000033565, Best Loss: 0.000001332 in Epoch 1950
Epoch 2124
Epoch 2124, Loss: 0.000016041, Improvement: -0.000021459, Best Loss: 0.000001332 in Epoch 1950
Epoch 2125
Epoch 2125, Loss: 0.000015448, Improvement: -0.000000593, Best Loss: 0.000001332 in Epoch 1950
Epoch 2126
Epoch 2126, Loss: 0.000020642, Improvement: 0.000005194, Best Loss: 0.000001332 in Epoch 1950
Epoch 2127
Epoch 2127, Loss: 0.000015364, Improvement: -0.000005278, Best Loss: 0.000001332 in Epoch 1950
Epoch 2128
Epoch 2128, Loss: 0.000008470, Improvement: -0.000006895, Best Loss: 0.000001332 in Epoch 1950
Epoch 2129
Epoch 2129, Loss: 0.000007390, Improvement: -0.000001080, Best Loss: 0.000001332 in Epoch 1950
Epoch 2130
Epoch 2130, Loss: 0.000005764, Improvement: -0.000001626, Best Loss: 0.000001332 in Epoch 1950
Epoch 2131
Epoch 2131, Loss: 0.000004589, Improvement: -0.000001175, Best Loss: 0.000001332 in Epoch 1950
Epoch 2132
Epoch 2132, Loss: 0.000004297, Improvement: -0.000000292, Best Loss: 0.000001332 in Epoch 1950
Epoch 2133
Epoch 2133, Loss: 0.000003872, Improvement: -0.000000425, Best Loss: 0.000001332 in Epoch 1950
Epoch 2134
Epoch 2134, Loss: 0.000008005, Improvement: 0.000004133, Best Loss: 0.000001332 in Epoch 1950
Epoch 2135
Epoch 2135, Loss: 0.000024909, Improvement: 0.000016904, Best Loss: 0.000001332 in Epoch 1950
Epoch 2136
Epoch 2136, Loss: 0.000015555, Improvement: -0.000009354, Best Loss: 0.000001332 in Epoch 1950
Epoch 2137
Epoch 2137, Loss: 0.000010988, Improvement: -0.000004567, Best Loss: 0.000001332 in Epoch 1950
Epoch 2138
Epoch 2138, Loss: 0.000011847, Improvement: 0.000000858, Best Loss: 0.000001332 in Epoch 1950
Epoch 2139
Epoch 2139, Loss: 0.000007398, Improvement: -0.000004449, Best Loss: 0.000001332 in Epoch 1950
Epoch 2140
Epoch 2140, Loss: 0.000006447, Improvement: -0.000000952, Best Loss: 0.000001332 in Epoch 1950
Epoch 2141
Epoch 2141, Loss: 0.000004277, Improvement: -0.000002169, Best Loss: 0.000001332 in Epoch 1950
Epoch 2142
Epoch 2142, Loss: 0.000002895, Improvement: -0.000001382, Best Loss: 0.000001332 in Epoch 1950
Epoch 2143
Epoch 2143, Loss: 0.000003066, Improvement: 0.000000171, Best Loss: 0.000001332 in Epoch 1950
Epoch 2144
Epoch 2144, Loss: 0.000002467, Improvement: -0.000000599, Best Loss: 0.000001332 in Epoch 1950
Epoch 2145
Epoch 2145, Loss: 0.000002136, Improvement: -0.000000332, Best Loss: 0.000001332 in Epoch 1950
Epoch 2146
A best model at epoch 2146 has been saved with training error 0.000001286.
Epoch 2146, Loss: 0.000001885, Improvement: -0.000000251, Best Loss: 0.000001286 in Epoch 2146
Epoch 2147
Epoch 2147, Loss: 0.000001840, Improvement: -0.000000045, Best Loss: 0.000001286 in Epoch 2146
Epoch 2148
Epoch 2148, Loss: 0.000001886, Improvement: 0.000000046, Best Loss: 0.000001286 in Epoch 2146
Epoch 2149
Epoch 2149, Loss: 0.000001900, Improvement: 0.000000014, Best Loss: 0.000001286 in Epoch 2146
Epoch 2150
A best model at epoch 2150 has been saved with training error 0.000001265.
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.000002113, Improvement: 0.000000213, Best Loss: 0.000001265 in Epoch 2150
Epoch 2151
Epoch 2151, Loss: 0.000003297, Improvement: 0.000001184, Best Loss: 0.000001265 in Epoch 2150
Epoch 2152
Epoch 2152, Loss: 0.000004307, Improvement: 0.000001010, Best Loss: 0.000001265 in Epoch 2150
Epoch 2153
Epoch 2153, Loss: 0.000010402, Improvement: 0.000006095, Best Loss: 0.000001265 in Epoch 2150
Epoch 2154
Epoch 2154, Loss: 0.000030242, Improvement: 0.000019840, Best Loss: 0.000001265 in Epoch 2150
Epoch 2155
Epoch 2155, Loss: 0.000020658, Improvement: -0.000009584, Best Loss: 0.000001265 in Epoch 2150
Epoch 2156
Epoch 2156, Loss: 0.000028975, Improvement: 0.000008317, Best Loss: 0.000001265 in Epoch 2150
Epoch 2157
Epoch 2157, Loss: 0.000076928, Improvement: 0.000047953, Best Loss: 0.000001265 in Epoch 2150
Epoch 2158
Epoch 2158, Loss: 0.000070342, Improvement: -0.000006586, Best Loss: 0.000001265 in Epoch 2150
Epoch 2159
Epoch 2159, Loss: 0.000063064, Improvement: -0.000007279, Best Loss: 0.000001265 in Epoch 2150
Epoch 2160
Epoch 2160, Loss: 0.000027398, Improvement: -0.000035666, Best Loss: 0.000001265 in Epoch 2150
Epoch 2161
Epoch 2161, Loss: 0.000009285, Improvement: -0.000018113, Best Loss: 0.000001265 in Epoch 2150
Epoch 2162
Epoch 2162, Loss: 0.000004295, Improvement: -0.000004990, Best Loss: 0.000001265 in Epoch 2150
Epoch 2163
Epoch 2163, Loss: 0.000003145, Improvement: -0.000001150, Best Loss: 0.000001265 in Epoch 2150
Epoch 2164
Epoch 2164, Loss: 0.000002816, Improvement: -0.000000328, Best Loss: 0.000001265 in Epoch 2150
Epoch 2165
Epoch 2165, Loss: 0.000002561, Improvement: -0.000000255, Best Loss: 0.000001265 in Epoch 2150
Epoch 2166
Epoch 2166, Loss: 0.000002242, Improvement: -0.000000320, Best Loss: 0.000001265 in Epoch 2150
Epoch 2167
Epoch 2167, Loss: 0.000002059, Improvement: -0.000000183, Best Loss: 0.000001265 in Epoch 2150
Epoch 2168
Epoch 2168, Loss: 0.000001993, Improvement: -0.000000066, Best Loss: 0.000001265 in Epoch 2150
Epoch 2169
Epoch 2169, Loss: 0.000001923, Improvement: -0.000000070, Best Loss: 0.000001265 in Epoch 2150
Epoch 2170
Epoch 2170, Loss: 0.000001877, Improvement: -0.000000046, Best Loss: 0.000001265 in Epoch 2150
Epoch 2171
Epoch 2171, Loss: 0.000001885, Improvement: 0.000000008, Best Loss: 0.000001265 in Epoch 2150
Epoch 2172
Epoch 2172, Loss: 0.000001849, Improvement: -0.000000036, Best Loss: 0.000001265 in Epoch 2150
Epoch 2173
Epoch 2173, Loss: 0.000001906, Improvement: 0.000000057, Best Loss: 0.000001265 in Epoch 2150
Epoch 2174
Epoch 2174, Loss: 0.000001931, Improvement: 0.000000025, Best Loss: 0.000001265 in Epoch 2150
Epoch 2175
A best model at epoch 2175 has been saved with training error 0.000001184.
Epoch 2175, Loss: 0.000001783, Improvement: -0.000000148, Best Loss: 0.000001184 in Epoch 2175
Epoch 2176
Epoch 2176, Loss: 0.000001852, Improvement: 0.000000069, Best Loss: 0.000001184 in Epoch 2175
Epoch 2177
Epoch 2177, Loss: 0.000001841, Improvement: -0.000000012, Best Loss: 0.000001184 in Epoch 2175
Epoch 2178
Epoch 2178, Loss: 0.000001845, Improvement: 0.000000005, Best Loss: 0.000001184 in Epoch 2175
Epoch 2179
Epoch 2179, Loss: 0.000001812, Improvement: -0.000000034, Best Loss: 0.000001184 in Epoch 2175
Epoch 2180
Epoch 2180, Loss: 0.000001901, Improvement: 0.000000089, Best Loss: 0.000001184 in Epoch 2175
Epoch 2181
Epoch 2181, Loss: 0.000002553, Improvement: 0.000000652, Best Loss: 0.000001184 in Epoch 2175
Epoch 2182
Epoch 2182, Loss: 0.000003811, Improvement: 0.000001258, Best Loss: 0.000001184 in Epoch 2175
Epoch 2183
Epoch 2183, Loss: 0.000005834, Improvement: 0.000002022, Best Loss: 0.000001184 in Epoch 2175
Epoch 2184
Epoch 2184, Loss: 0.000007384, Improvement: 0.000001550, Best Loss: 0.000001184 in Epoch 2175
Epoch 2185
Epoch 2185, Loss: 0.000009198, Improvement: 0.000001814, Best Loss: 0.000001184 in Epoch 2175
Epoch 2186
Epoch 2186, Loss: 0.000005481, Improvement: -0.000003716, Best Loss: 0.000001184 in Epoch 2175
Epoch 2187
Epoch 2187, Loss: 0.000005354, Improvement: -0.000000127, Best Loss: 0.000001184 in Epoch 2175
Epoch 2188
Epoch 2188, Loss: 0.000003411, Improvement: -0.000001943, Best Loss: 0.000001184 in Epoch 2175
Epoch 2189
Epoch 2189, Loss: 0.000003170, Improvement: -0.000000241, Best Loss: 0.000001184 in Epoch 2175
Epoch 2190
Epoch 2190, Loss: 0.000002491, Improvement: -0.000000679, Best Loss: 0.000001184 in Epoch 2175
Epoch 2191
Epoch 2191, Loss: 0.000005385, Improvement: 0.000002894, Best Loss: 0.000001184 in Epoch 2175
Epoch 2192
Epoch 2192, Loss: 0.000005579, Improvement: 0.000000194, Best Loss: 0.000001184 in Epoch 2175
Epoch 2193
Epoch 2193, Loss: 0.000006087, Improvement: 0.000000509, Best Loss: 0.000001184 in Epoch 2175
Epoch 2194
Epoch 2194, Loss: 0.000018709, Improvement: 0.000012621, Best Loss: 0.000001184 in Epoch 2175
Epoch 2195
Epoch 2195, Loss: 0.000023081, Improvement: 0.000004372, Best Loss: 0.000001184 in Epoch 2175
Epoch 2196
Epoch 2196, Loss: 0.000009373, Improvement: -0.000013708, Best Loss: 0.000001184 in Epoch 2175
Epoch 2197
Epoch 2197, Loss: 0.000011755, Improvement: 0.000002382, Best Loss: 0.000001184 in Epoch 2175
Epoch 2198
Epoch 2198, Loss: 0.000014770, Improvement: 0.000003015, Best Loss: 0.000001184 in Epoch 2175
Epoch 2199
Epoch 2199, Loss: 0.000008328, Improvement: -0.000006442, Best Loss: 0.000001184 in Epoch 2175
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.000005682, Improvement: -0.000002645, Best Loss: 0.000001184 in Epoch 2175
Epoch 2201
Epoch 2201, Loss: 0.000005102, Improvement: -0.000000580, Best Loss: 0.000001184 in Epoch 2175
Epoch 2202
Epoch 2202, Loss: 0.000005153, Improvement: 0.000000050, Best Loss: 0.000001184 in Epoch 2175
Epoch 2203
Epoch 2203, Loss: 0.000006239, Improvement: 0.000001086, Best Loss: 0.000001184 in Epoch 2175
Epoch 2204
Epoch 2204, Loss: 0.000005397, Improvement: -0.000000842, Best Loss: 0.000001184 in Epoch 2175
Epoch 2205
Epoch 2205, Loss: 0.000006940, Improvement: 0.000001543, Best Loss: 0.000001184 in Epoch 2175
Epoch 2206
Epoch 2206, Loss: 0.000006849, Improvement: -0.000000091, Best Loss: 0.000001184 in Epoch 2175
Epoch 2207
Epoch 2207, Loss: 0.000015529, Improvement: 0.000008680, Best Loss: 0.000001184 in Epoch 2175
Epoch 2208
Epoch 2208, Loss: 0.000048783, Improvement: 0.000033254, Best Loss: 0.000001184 in Epoch 2175
Epoch 2209
Epoch 2209, Loss: 0.000041617, Improvement: -0.000007166, Best Loss: 0.000001184 in Epoch 2175
Epoch 2210
Epoch 2210, Loss: 0.000019414, Improvement: -0.000022203, Best Loss: 0.000001184 in Epoch 2175
Epoch 2211
Epoch 2211, Loss: 0.000012278, Improvement: -0.000007136, Best Loss: 0.000001184 in Epoch 2175
Epoch 2212
Epoch 2212, Loss: 0.000005279, Improvement: -0.000006999, Best Loss: 0.000001184 in Epoch 2175
Epoch 2213
Epoch 2213, Loss: 0.000003996, Improvement: -0.000001282, Best Loss: 0.000001184 in Epoch 2175
Epoch 2214
Epoch 2214, Loss: 0.000002819, Improvement: -0.000001177, Best Loss: 0.000001184 in Epoch 2175
Epoch 2215
Epoch 2215, Loss: 0.000002361, Improvement: -0.000000459, Best Loss: 0.000001184 in Epoch 2175
Epoch 2216
Epoch 2216, Loss: 0.000002120, Improvement: -0.000000240, Best Loss: 0.000001184 in Epoch 2175
Epoch 2217
Epoch 2217, Loss: 0.000001937, Improvement: -0.000000183, Best Loss: 0.000001184 in Epoch 2175
Epoch 2218
Epoch 2218, Loss: 0.000002103, Improvement: 0.000000166, Best Loss: 0.000001184 in Epoch 2175
Epoch 2219
Epoch 2219, Loss: 0.000002163, Improvement: 0.000000060, Best Loss: 0.000001184 in Epoch 2175
Epoch 2220
Epoch 2220, Loss: 0.000002365, Improvement: 0.000000202, Best Loss: 0.000001184 in Epoch 2175
Epoch 2221
Epoch 2221, Loss: 0.000002202, Improvement: -0.000000163, Best Loss: 0.000001184 in Epoch 2175
Epoch 2222
Epoch 2222, Loss: 0.000002257, Improvement: 0.000000055, Best Loss: 0.000001184 in Epoch 2175
Epoch 2223
Epoch 2223, Loss: 0.000002429, Improvement: 0.000000173, Best Loss: 0.000001184 in Epoch 2175
Epoch 2224
Epoch 2224, Loss: 0.000002491, Improvement: 0.000000061, Best Loss: 0.000001184 in Epoch 2175
Epoch 2225
Epoch 2225, Loss: 0.000003497, Improvement: 0.000001006, Best Loss: 0.000001184 in Epoch 2175
Epoch 2226
Epoch 2226, Loss: 0.000002883, Improvement: -0.000000614, Best Loss: 0.000001184 in Epoch 2175
Epoch 2227
Epoch 2227, Loss: 0.000002135, Improvement: -0.000000748, Best Loss: 0.000001184 in Epoch 2175
Epoch 2228
Epoch 2228, Loss: 0.000001933, Improvement: -0.000000202, Best Loss: 0.000001184 in Epoch 2175
Epoch 2229
Epoch 2229, Loss: 0.000002192, Improvement: 0.000000258, Best Loss: 0.000001184 in Epoch 2175
Epoch 2230
Epoch 2230, Loss: 0.000002292, Improvement: 0.000000100, Best Loss: 0.000001184 in Epoch 2175
Epoch 2231
Epoch 2231, Loss: 0.000002296, Improvement: 0.000000004, Best Loss: 0.000001184 in Epoch 2175
Epoch 2232
Epoch 2232, Loss: 0.000003227, Improvement: 0.000000931, Best Loss: 0.000001184 in Epoch 2175
Epoch 2233
Epoch 2233, Loss: 0.000005466, Improvement: 0.000002239, Best Loss: 0.000001184 in Epoch 2175
Epoch 2234
Epoch 2234, Loss: 0.000024618, Improvement: 0.000019152, Best Loss: 0.000001184 in Epoch 2175
Epoch 2235
Epoch 2235, Loss: 0.000029714, Improvement: 0.000005096, Best Loss: 0.000001184 in Epoch 2175
Epoch 2236
Epoch 2236, Loss: 0.000020584, Improvement: -0.000009130, Best Loss: 0.000001184 in Epoch 2175
Epoch 2237
Epoch 2237, Loss: 0.000011188, Improvement: -0.000009396, Best Loss: 0.000001184 in Epoch 2175
Epoch 2238
Epoch 2238, Loss: 0.000008379, Improvement: -0.000002809, Best Loss: 0.000001184 in Epoch 2175
Epoch 2239
Epoch 2239, Loss: 0.000006207, Improvement: -0.000002172, Best Loss: 0.000001184 in Epoch 2175
Epoch 2240
Epoch 2240, Loss: 0.000005025, Improvement: -0.000001182, Best Loss: 0.000001184 in Epoch 2175
Epoch 2241
Epoch 2241, Loss: 0.000003008, Improvement: -0.000002016, Best Loss: 0.000001184 in Epoch 2175
Epoch 2242
Epoch 2242, Loss: 0.000003224, Improvement: 0.000000216, Best Loss: 0.000001184 in Epoch 2175
Epoch 2243
Epoch 2243, Loss: 0.000005007, Improvement: 0.000001783, Best Loss: 0.000001184 in Epoch 2175
Epoch 2244
Epoch 2244, Loss: 0.000003872, Improvement: -0.000001134, Best Loss: 0.000001184 in Epoch 2175
Epoch 2245
Epoch 2245, Loss: 0.000006484, Improvement: 0.000002612, Best Loss: 0.000001184 in Epoch 2175
Epoch 2246
Epoch 2246, Loss: 0.000007190, Improvement: 0.000000706, Best Loss: 0.000001184 in Epoch 2175
Epoch 2247
Epoch 2247, Loss: 0.000004763, Improvement: -0.000002427, Best Loss: 0.000001184 in Epoch 2175
Epoch 2248
Epoch 2248, Loss: 0.000003650, Improvement: -0.000001113, Best Loss: 0.000001184 in Epoch 2175
Epoch 2249
Epoch 2249, Loss: 0.000009465, Improvement: 0.000005815, Best Loss: 0.000001184 in Epoch 2175
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.000008267, Improvement: -0.000001198, Best Loss: 0.000001184 in Epoch 2175
Epoch 2251
Epoch 2251, Loss: 0.000009950, Improvement: 0.000001683, Best Loss: 0.000001184 in Epoch 2175
Epoch 2252
Epoch 2252, Loss: 0.000015027, Improvement: 0.000005077, Best Loss: 0.000001184 in Epoch 2175
Epoch 2253
Epoch 2253, Loss: 0.000014509, Improvement: -0.000000518, Best Loss: 0.000001184 in Epoch 2175
Epoch 2254
Epoch 2254, Loss: 0.000008562, Improvement: -0.000005947, Best Loss: 0.000001184 in Epoch 2175
Epoch 2255
Epoch 2255, Loss: 0.000005771, Improvement: -0.000002791, Best Loss: 0.000001184 in Epoch 2175
Epoch 2256
Epoch 2256, Loss: 0.000036297, Improvement: 0.000030526, Best Loss: 0.000001184 in Epoch 2175
Epoch 2257
Epoch 2257, Loss: 0.000040127, Improvement: 0.000003830, Best Loss: 0.000001184 in Epoch 2175
Epoch 2258
Epoch 2258, Loss: 0.000020156, Improvement: -0.000019971, Best Loss: 0.000001184 in Epoch 2175
Epoch 2259
Epoch 2259, Loss: 0.000009420, Improvement: -0.000010736, Best Loss: 0.000001184 in Epoch 2175
Epoch 2260
Epoch 2260, Loss: 0.000004979, Improvement: -0.000004441, Best Loss: 0.000001184 in Epoch 2175
Epoch 2261
Epoch 2261, Loss: 0.000003545, Improvement: -0.000001434, Best Loss: 0.000001184 in Epoch 2175
Epoch 2262
Epoch 2262, Loss: 0.000002695, Improvement: -0.000000850, Best Loss: 0.000001184 in Epoch 2175
Epoch 2263
Epoch 2263, Loss: 0.000002434, Improvement: -0.000000261, Best Loss: 0.000001184 in Epoch 2175
Epoch 2264
Epoch 2264, Loss: 0.000002131, Improvement: -0.000000303, Best Loss: 0.000001184 in Epoch 2175
Epoch 2265
Epoch 2265, Loss: 0.000001850, Improvement: -0.000000281, Best Loss: 0.000001184 in Epoch 2175
Epoch 2266
Epoch 2266, Loss: 0.000001784, Improvement: -0.000000067, Best Loss: 0.000001184 in Epoch 2175
Epoch 2267
Epoch 2267, Loss: 0.000001802, Improvement: 0.000000019, Best Loss: 0.000001184 in Epoch 2175
Epoch 2268
A best model at epoch 2268 has been saved with training error 0.000001146.
Epoch 2268, Loss: 0.000001952, Improvement: 0.000000149, Best Loss: 0.000001146 in Epoch 2268
Epoch 2269
Epoch 2269, Loss: 0.000001889, Improvement: -0.000000063, Best Loss: 0.000001146 in Epoch 2268
Epoch 2270
Epoch 2270, Loss: 0.000002164, Improvement: 0.000000274, Best Loss: 0.000001146 in Epoch 2268
Epoch 2271
Epoch 2271, Loss: 0.000001911, Improvement: -0.000000252, Best Loss: 0.000001146 in Epoch 2268
Epoch 2272
A best model at epoch 2272 has been saved with training error 0.000001088.
Epoch 2272, Loss: 0.000001623, Improvement: -0.000000288, Best Loss: 0.000001088 in Epoch 2272
Epoch 2273
Epoch 2273, Loss: 0.000001679, Improvement: 0.000000056, Best Loss: 0.000001088 in Epoch 2272
Epoch 2274
Epoch 2274, Loss: 0.000001657, Improvement: -0.000000022, Best Loss: 0.000001088 in Epoch 2272
Epoch 2275
Epoch 2275, Loss: 0.000002068, Improvement: 0.000000411, Best Loss: 0.000001088 in Epoch 2272
Epoch 2276
Epoch 2276, Loss: 0.000003923, Improvement: 0.000001855, Best Loss: 0.000001088 in Epoch 2272
Epoch 2277
Epoch 2277, Loss: 0.000005734, Improvement: 0.000001811, Best Loss: 0.000001088 in Epoch 2272
Epoch 2278
Epoch 2278, Loss: 0.000005906, Improvement: 0.000000171, Best Loss: 0.000001088 in Epoch 2272
Epoch 2279
Epoch 2279, Loss: 0.000003390, Improvement: -0.000002516, Best Loss: 0.000001088 in Epoch 2272
Epoch 2280
Epoch 2280, Loss: 0.000007314, Improvement: 0.000003924, Best Loss: 0.000001088 in Epoch 2272
Epoch 2281
Epoch 2281, Loss: 0.000008343, Improvement: 0.000001029, Best Loss: 0.000001088 in Epoch 2272
Epoch 2282
Epoch 2282, Loss: 0.000004578, Improvement: -0.000003765, Best Loss: 0.000001088 in Epoch 2272
Epoch 2283
Epoch 2283, Loss: 0.000004737, Improvement: 0.000000159, Best Loss: 0.000001088 in Epoch 2272
Epoch 2284
Epoch 2284, Loss: 0.000007098, Improvement: 0.000002361, Best Loss: 0.000001088 in Epoch 2272
Epoch 2285
Epoch 2285, Loss: 0.000006414, Improvement: -0.000000684, Best Loss: 0.000001088 in Epoch 2272
Epoch 2286
Epoch 2286, Loss: 0.000006089, Improvement: -0.000000325, Best Loss: 0.000001088 in Epoch 2272
Epoch 2287
Epoch 2287, Loss: 0.000006225, Improvement: 0.000000135, Best Loss: 0.000001088 in Epoch 2272
Epoch 2288
Epoch 2288, Loss: 0.000004935, Improvement: -0.000001290, Best Loss: 0.000001088 in Epoch 2272
Epoch 2289
Epoch 2289, Loss: 0.000004882, Improvement: -0.000000053, Best Loss: 0.000001088 in Epoch 2272
Epoch 2290
Epoch 2290, Loss: 0.000012990, Improvement: 0.000008108, Best Loss: 0.000001088 in Epoch 2272
Epoch 2291
Epoch 2291, Loss: 0.000025946, Improvement: 0.000012956, Best Loss: 0.000001088 in Epoch 2272
Epoch 2292
Epoch 2292, Loss: 0.000016087, Improvement: -0.000009858, Best Loss: 0.000001088 in Epoch 2272
Epoch 2293
Epoch 2293, Loss: 0.000019918, Improvement: 0.000003831, Best Loss: 0.000001088 in Epoch 2272
Epoch 2294
Epoch 2294, Loss: 0.000007568, Improvement: -0.000012350, Best Loss: 0.000001088 in Epoch 2272
Epoch 2295
Epoch 2295, Loss: 0.000003741, Improvement: -0.000003826, Best Loss: 0.000001088 in Epoch 2272
Epoch 2296
Epoch 2296, Loss: 0.000002640, Improvement: -0.000001101, Best Loss: 0.000001088 in Epoch 2272
Epoch 2297
Epoch 2297, Loss: 0.000004288, Improvement: 0.000001648, Best Loss: 0.000001088 in Epoch 2272
Epoch 2298
Epoch 2298, Loss: 0.000004636, Improvement: 0.000000348, Best Loss: 0.000001088 in Epoch 2272
Epoch 2299
Epoch 2299, Loss: 0.000015252, Improvement: 0.000010616, Best Loss: 0.000001088 in Epoch 2272
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.000015416, Improvement: 0.000000164, Best Loss: 0.000001088 in Epoch 2272
Epoch 2301
Epoch 2301, Loss: 0.000008982, Improvement: -0.000006434, Best Loss: 0.000001088 in Epoch 2272
Epoch 2302
Epoch 2302, Loss: 0.000011166, Improvement: 0.000002184, Best Loss: 0.000001088 in Epoch 2272
Epoch 2303
Epoch 2303, Loss: 0.000006658, Improvement: -0.000004508, Best Loss: 0.000001088 in Epoch 2272
Epoch 2304
Epoch 2304, Loss: 0.000005019, Improvement: -0.000001640, Best Loss: 0.000001088 in Epoch 2272
Epoch 2305
Epoch 2305, Loss: 0.000006108, Improvement: 0.000001089, Best Loss: 0.000001088 in Epoch 2272
Epoch 2306
Epoch 2306, Loss: 0.000008590, Improvement: 0.000002482, Best Loss: 0.000001088 in Epoch 2272
Epoch 2307
Epoch 2307, Loss: 0.000017256, Improvement: 0.000008666, Best Loss: 0.000001088 in Epoch 2272
Epoch 2308
Epoch 2308, Loss: 0.000020474, Improvement: 0.000003219, Best Loss: 0.000001088 in Epoch 2272
Epoch 2309
Epoch 2309, Loss: 0.000027342, Improvement: 0.000006868, Best Loss: 0.000001088 in Epoch 2272
Epoch 2310
Epoch 2310, Loss: 0.000039484, Improvement: 0.000012142, Best Loss: 0.000001088 in Epoch 2272
Epoch 2311
Epoch 2311, Loss: 0.000035943, Improvement: -0.000003541, Best Loss: 0.000001088 in Epoch 2272
Epoch 2312
Epoch 2312, Loss: 0.000012822, Improvement: -0.000023121, Best Loss: 0.000001088 in Epoch 2272
Epoch 2313
Epoch 2313, Loss: 0.000004395, Improvement: -0.000008427, Best Loss: 0.000001088 in Epoch 2272
Epoch 2314
Epoch 2314, Loss: 0.000002549, Improvement: -0.000001846, Best Loss: 0.000001088 in Epoch 2272
Epoch 2315
Epoch 2315, Loss: 0.000002200, Improvement: -0.000000349, Best Loss: 0.000001088 in Epoch 2272
Epoch 2316
Epoch 2316, Loss: 0.000002043, Improvement: -0.000000157, Best Loss: 0.000001088 in Epoch 2272
Epoch 2317
Epoch 2317, Loss: 0.000001791, Improvement: -0.000000251, Best Loss: 0.000001088 in Epoch 2272
Epoch 2318
Epoch 2318, Loss: 0.000001746, Improvement: -0.000000045, Best Loss: 0.000001088 in Epoch 2272
Epoch 2319
Epoch 2319, Loss: 0.000001740, Improvement: -0.000000006, Best Loss: 0.000001088 in Epoch 2272
Epoch 2320
Epoch 2320, Loss: 0.000001707, Improvement: -0.000000033, Best Loss: 0.000001088 in Epoch 2272
Epoch 2321
Epoch 2321, Loss: 0.000001727, Improvement: 0.000000020, Best Loss: 0.000001088 in Epoch 2272
Epoch 2322
Epoch 2322, Loss: 0.000001815, Improvement: 0.000000089, Best Loss: 0.000001088 in Epoch 2272
Epoch 2323
Epoch 2323, Loss: 0.000001799, Improvement: -0.000000017, Best Loss: 0.000001088 in Epoch 2272
Epoch 2324
Epoch 2324, Loss: 0.000001661, Improvement: -0.000000138, Best Loss: 0.000001088 in Epoch 2272
Epoch 2325
Epoch 2325, Loss: 0.000001691, Improvement: 0.000000031, Best Loss: 0.000001088 in Epoch 2272
Epoch 2326
Epoch 2326, Loss: 0.000001521, Improvement: -0.000000170, Best Loss: 0.000001088 in Epoch 2272
Epoch 2327
Epoch 2327, Loss: 0.000001608, Improvement: 0.000000087, Best Loss: 0.000001088 in Epoch 2272
Epoch 2328
Epoch 2328, Loss: 0.000001801, Improvement: 0.000000193, Best Loss: 0.000001088 in Epoch 2272
Epoch 2329
Epoch 2329, Loss: 0.000002087, Improvement: 0.000000286, Best Loss: 0.000001088 in Epoch 2272
Epoch 2330
Epoch 2330, Loss: 0.000001759, Improvement: -0.000000329, Best Loss: 0.000001088 in Epoch 2272
Epoch 2331
Epoch 2331, Loss: 0.000001538, Improvement: -0.000000221, Best Loss: 0.000001088 in Epoch 2272
Epoch 2332
Epoch 2332, Loss: 0.000001495, Improvement: -0.000000042, Best Loss: 0.000001088 in Epoch 2272
Epoch 2333
Epoch 2333, Loss: 0.000003221, Improvement: 0.000001726, Best Loss: 0.000001088 in Epoch 2272
Epoch 2334
Epoch 2334, Loss: 0.000004833, Improvement: 0.000001611, Best Loss: 0.000001088 in Epoch 2272
Epoch 2335
Epoch 2335, Loss: 0.000004277, Improvement: -0.000000556, Best Loss: 0.000001088 in Epoch 2272
Epoch 2336
Epoch 2336, Loss: 0.000004126, Improvement: -0.000000151, Best Loss: 0.000001088 in Epoch 2272
Epoch 2337
Epoch 2337, Loss: 0.000009020, Improvement: 0.000004894, Best Loss: 0.000001088 in Epoch 2272
Epoch 2338
Epoch 2338, Loss: 0.000006336, Improvement: -0.000002684, Best Loss: 0.000001088 in Epoch 2272
Epoch 2339
Epoch 2339, Loss: 0.000004142, Improvement: -0.000002194, Best Loss: 0.000001088 in Epoch 2272
Epoch 2340
Epoch 2340, Loss: 0.000002347, Improvement: -0.000001795, Best Loss: 0.000001088 in Epoch 2272
Epoch 2341
Epoch 2341, Loss: 0.000002665, Improvement: 0.000000317, Best Loss: 0.000001088 in Epoch 2272
Epoch 2342
Epoch 2342, Loss: 0.000006047, Improvement: 0.000003382, Best Loss: 0.000001088 in Epoch 2272
Epoch 2343
Epoch 2343, Loss: 0.000015581, Improvement: 0.000009535, Best Loss: 0.000001088 in Epoch 2272
Epoch 2344
Epoch 2344, Loss: 0.000023874, Improvement: 0.000008292, Best Loss: 0.000001088 in Epoch 2272
Epoch 2345
Epoch 2345, Loss: 0.000023103, Improvement: -0.000000770, Best Loss: 0.000001088 in Epoch 2272
Epoch 2346
Epoch 2346, Loss: 0.000013854, Improvement: -0.000009249, Best Loss: 0.000001088 in Epoch 2272
Epoch 2347
Epoch 2347, Loss: 0.000006993, Improvement: -0.000006861, Best Loss: 0.000001088 in Epoch 2272
Epoch 2348
Epoch 2348, Loss: 0.000005179, Improvement: -0.000001814, Best Loss: 0.000001088 in Epoch 2272
Epoch 2349
Epoch 2349, Loss: 0.000007274, Improvement: 0.000002095, Best Loss: 0.000001088 in Epoch 2272
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.000004192, Improvement: -0.000003082, Best Loss: 0.000001088 in Epoch 2272
Epoch 2351
Epoch 2351, Loss: 0.000016067, Improvement: 0.000011875, Best Loss: 0.000001088 in Epoch 2272
Epoch 2352
Epoch 2352, Loss: 0.000021468, Improvement: 0.000005400, Best Loss: 0.000001088 in Epoch 2272
Epoch 2353
Epoch 2353, Loss: 0.000024531, Improvement: 0.000003063, Best Loss: 0.000001088 in Epoch 2272
Epoch 2354
Epoch 2354, Loss: 0.000018697, Improvement: -0.000005834, Best Loss: 0.000001088 in Epoch 2272
Epoch 2355
Epoch 2355, Loss: 0.000018390, Improvement: -0.000000307, Best Loss: 0.000001088 in Epoch 2272
Epoch 2356
Epoch 2356, Loss: 0.000029747, Improvement: 0.000011356, Best Loss: 0.000001088 in Epoch 2272
Epoch 2357
Epoch 2357, Loss: 0.000011847, Improvement: -0.000017900, Best Loss: 0.000001088 in Epoch 2272
Epoch 2358
Epoch 2358, Loss: 0.000012301, Improvement: 0.000000455, Best Loss: 0.000001088 in Epoch 2272
Epoch 2359
Epoch 2359, Loss: 0.000006958, Improvement: -0.000005344, Best Loss: 0.000001088 in Epoch 2272
Epoch 2360
Epoch 2360, Loss: 0.000006472, Improvement: -0.000000485, Best Loss: 0.000001088 in Epoch 2272
Epoch 2361
Epoch 2361, Loss: 0.000004813, Improvement: -0.000001659, Best Loss: 0.000001088 in Epoch 2272
Epoch 2362
Epoch 2362, Loss: 0.000005680, Improvement: 0.000000866, Best Loss: 0.000001088 in Epoch 2272
Epoch 2363
Epoch 2363, Loss: 0.000003579, Improvement: -0.000002100, Best Loss: 0.000001088 in Epoch 2272
Epoch 2364
Epoch 2364, Loss: 0.000005618, Improvement: 0.000002039, Best Loss: 0.000001088 in Epoch 2272
Epoch 2365
Epoch 2365, Loss: 0.000007477, Improvement: 0.000001859, Best Loss: 0.000001088 in Epoch 2272
Epoch 2366
Epoch 2366, Loss: 0.000007674, Improvement: 0.000000196, Best Loss: 0.000001088 in Epoch 2272
Epoch 2367
Epoch 2367, Loss: 0.000028271, Improvement: 0.000020598, Best Loss: 0.000001088 in Epoch 2272
Epoch 2368
Epoch 2368, Loss: 0.000042114, Improvement: 0.000013843, Best Loss: 0.000001088 in Epoch 2272
Epoch 2369
Epoch 2369, Loss: 0.000036589, Improvement: -0.000005525, Best Loss: 0.000001088 in Epoch 2272
Epoch 2370
Epoch 2370, Loss: 0.000013202, Improvement: -0.000023387, Best Loss: 0.000001088 in Epoch 2272
Epoch 2371
Epoch 2371, Loss: 0.000007634, Improvement: -0.000005569, Best Loss: 0.000001088 in Epoch 2272
Epoch 2372
Epoch 2372, Loss: 0.000004899, Improvement: -0.000002735, Best Loss: 0.000001088 in Epoch 2272
Epoch 2373
Epoch 2373, Loss: 0.000003754, Improvement: -0.000001145, Best Loss: 0.000001088 in Epoch 2272
Epoch 2374
Epoch 2374, Loss: 0.000002664, Improvement: -0.000001090, Best Loss: 0.000001088 in Epoch 2272
Epoch 2375
Epoch 2375, Loss: 0.000002021, Improvement: -0.000000643, Best Loss: 0.000001088 in Epoch 2272
Epoch 2376
Epoch 2376, Loss: 0.000001843, Improvement: -0.000000178, Best Loss: 0.000001088 in Epoch 2272
Epoch 2377
Epoch 2377, Loss: 0.000001763, Improvement: -0.000000080, Best Loss: 0.000001088 in Epoch 2272
Epoch 2378
Epoch 2378, Loss: 0.000001729, Improvement: -0.000000034, Best Loss: 0.000001088 in Epoch 2272
Epoch 2379
Epoch 2379, Loss: 0.000001638, Improvement: -0.000000090, Best Loss: 0.000001088 in Epoch 2272
Epoch 2380
Epoch 2380, Loss: 0.000001628, Improvement: -0.000000010, Best Loss: 0.000001088 in Epoch 2272
Epoch 2381
Epoch 2381, Loss: 0.000001530, Improvement: -0.000000098, Best Loss: 0.000001088 in Epoch 2272
Epoch 2382
Epoch 2382, Loss: 0.000001499, Improvement: -0.000000031, Best Loss: 0.000001088 in Epoch 2272
Epoch 2383
A best model at epoch 2383 has been saved with training error 0.000001056.
Epoch 2383, Loss: 0.000001535, Improvement: 0.000000035, Best Loss: 0.000001056 in Epoch 2383
Epoch 2384
Epoch 2384, Loss: 0.000001592, Improvement: 0.000000057, Best Loss: 0.000001056 in Epoch 2383
Epoch 2385
Epoch 2385, Loss: 0.000001996, Improvement: 0.000000405, Best Loss: 0.000001056 in Epoch 2383
Epoch 2386
Epoch 2386, Loss: 0.000002107, Improvement: 0.000000111, Best Loss: 0.000001056 in Epoch 2383
Epoch 2387
Epoch 2387, Loss: 0.000003909, Improvement: 0.000001802, Best Loss: 0.000001056 in Epoch 2383
Epoch 2388
Epoch 2388, Loss: 0.000004625, Improvement: 0.000000716, Best Loss: 0.000001056 in Epoch 2383
Epoch 2389
Epoch 2389, Loss: 0.000002358, Improvement: -0.000002267, Best Loss: 0.000001056 in Epoch 2383
Epoch 2390
Epoch 2390, Loss: 0.000002744, Improvement: 0.000000387, Best Loss: 0.000001056 in Epoch 2383
Epoch 2391
Epoch 2391, Loss: 0.000003060, Improvement: 0.000000316, Best Loss: 0.000001056 in Epoch 2383
Epoch 2392
Epoch 2392, Loss: 0.000006772, Improvement: 0.000003712, Best Loss: 0.000001056 in Epoch 2383
Epoch 2393
Epoch 2393, Loss: 0.000015911, Improvement: 0.000009140, Best Loss: 0.000001056 in Epoch 2383
Epoch 2394
Epoch 2394, Loss: 0.000019035, Improvement: 0.000003123, Best Loss: 0.000001056 in Epoch 2383
Epoch 2395
Epoch 2395, Loss: 0.000010229, Improvement: -0.000008806, Best Loss: 0.000001056 in Epoch 2383
Epoch 2396
Epoch 2396, Loss: 0.000005641, Improvement: -0.000004588, Best Loss: 0.000001056 in Epoch 2383
Epoch 2397
Epoch 2397, Loss: 0.000004643, Improvement: -0.000000998, Best Loss: 0.000001056 in Epoch 2383
Epoch 2398
Epoch 2398, Loss: 0.000003112, Improvement: -0.000001531, Best Loss: 0.000001056 in Epoch 2383
Epoch 2399
Epoch 2399, Loss: 0.000003166, Improvement: 0.000000054, Best Loss: 0.000001056 in Epoch 2383
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.000003519, Improvement: 0.000000354, Best Loss: 0.000001056 in Epoch 2383
Epoch 2401
Epoch 2401, Loss: 0.000004003, Improvement: 0.000000484, Best Loss: 0.000001056 in Epoch 2383
Epoch 2402
Epoch 2402, Loss: 0.000006832, Improvement: 0.000002829, Best Loss: 0.000001056 in Epoch 2383
Epoch 2403
Epoch 2403, Loss: 0.000005084, Improvement: -0.000001748, Best Loss: 0.000001056 in Epoch 2383
Epoch 2404
Epoch 2404, Loss: 0.000005349, Improvement: 0.000000265, Best Loss: 0.000001056 in Epoch 2383
Epoch 2405
Epoch 2405, Loss: 0.000015880, Improvement: 0.000010532, Best Loss: 0.000001056 in Epoch 2383
Epoch 2406
Epoch 2406, Loss: 0.000012404, Improvement: -0.000003476, Best Loss: 0.000001056 in Epoch 2383
Epoch 2407
Epoch 2407, Loss: 0.000020854, Improvement: 0.000008449, Best Loss: 0.000001056 in Epoch 2383
Epoch 2408
Epoch 2408, Loss: 0.000011691, Improvement: -0.000009163, Best Loss: 0.000001056 in Epoch 2383
Epoch 2409
Epoch 2409, Loss: 0.000019727, Improvement: 0.000008036, Best Loss: 0.000001056 in Epoch 2383
Epoch 2410
Epoch 2410, Loss: 0.000023731, Improvement: 0.000004003, Best Loss: 0.000001056 in Epoch 2383
Epoch 2411
Epoch 2411, Loss: 0.000015547, Improvement: -0.000008184, Best Loss: 0.000001056 in Epoch 2383
Epoch 2412
Epoch 2412, Loss: 0.000016001, Improvement: 0.000000455, Best Loss: 0.000001056 in Epoch 2383
Epoch 2413
Epoch 2413, Loss: 0.000015302, Improvement: -0.000000699, Best Loss: 0.000001056 in Epoch 2383
Epoch 2414
Epoch 2414, Loss: 0.000008445, Improvement: -0.000006857, Best Loss: 0.000001056 in Epoch 2383
Epoch 2415
Epoch 2415, Loss: 0.000006111, Improvement: -0.000002334, Best Loss: 0.000001056 in Epoch 2383
Epoch 2416
Epoch 2416, Loss: 0.000007192, Improvement: 0.000001081, Best Loss: 0.000001056 in Epoch 2383
Epoch 2417
Epoch 2417, Loss: 0.000011834, Improvement: 0.000004642, Best Loss: 0.000001056 in Epoch 2383
Epoch 2418
Epoch 2418, Loss: 0.000035183, Improvement: 0.000023349, Best Loss: 0.000001056 in Epoch 2383
Epoch 2419
Epoch 2419, Loss: 0.000021131, Improvement: -0.000014052, Best Loss: 0.000001056 in Epoch 2383
Epoch 2420
Epoch 2420, Loss: 0.000010719, Improvement: -0.000010412, Best Loss: 0.000001056 in Epoch 2383
Epoch 2421
Epoch 2421, Loss: 0.000004242, Improvement: -0.000006477, Best Loss: 0.000001056 in Epoch 2383
Epoch 2422
Epoch 2422, Loss: 0.000003193, Improvement: -0.000001049, Best Loss: 0.000001056 in Epoch 2383
Epoch 2423
Epoch 2423, Loss: 0.000003396, Improvement: 0.000000203, Best Loss: 0.000001056 in Epoch 2383
Epoch 2424
Epoch 2424, Loss: 0.000002579, Improvement: -0.000000817, Best Loss: 0.000001056 in Epoch 2383
Epoch 2425
Epoch 2425, Loss: 0.000002191, Improvement: -0.000000387, Best Loss: 0.000001056 in Epoch 2383
Epoch 2426
Epoch 2426, Loss: 0.000001688, Improvement: -0.000000503, Best Loss: 0.000001056 in Epoch 2383
Epoch 2427
Epoch 2427, Loss: 0.000002163, Improvement: 0.000000475, Best Loss: 0.000001056 in Epoch 2383
Epoch 2428
Epoch 2428, Loss: 0.000001888, Improvement: -0.000000275, Best Loss: 0.000001056 in Epoch 2383
Epoch 2429
Epoch 2429, Loss: 0.000001731, Improvement: -0.000000157, Best Loss: 0.000001056 in Epoch 2383
Epoch 2430
Epoch 2430, Loss: 0.000001885, Improvement: 0.000000154, Best Loss: 0.000001056 in Epoch 2383
Epoch 2431
Epoch 2431, Loss: 0.000002272, Improvement: 0.000000387, Best Loss: 0.000001056 in Epoch 2383
Epoch 2432
Epoch 2432, Loss: 0.000002597, Improvement: 0.000000326, Best Loss: 0.000001056 in Epoch 2383
Epoch 2433
Epoch 2433, Loss: 0.000002756, Improvement: 0.000000159, Best Loss: 0.000001056 in Epoch 2383
Epoch 2434
Epoch 2434, Loss: 0.000007440, Improvement: 0.000004683, Best Loss: 0.000001056 in Epoch 2383
Epoch 2435
Epoch 2435, Loss: 0.000017051, Improvement: 0.000009611, Best Loss: 0.000001056 in Epoch 2383
Epoch 2436
Epoch 2436, Loss: 0.000019776, Improvement: 0.000002725, Best Loss: 0.000001056 in Epoch 2383
Epoch 2437
Epoch 2437, Loss: 0.000010362, Improvement: -0.000009414, Best Loss: 0.000001056 in Epoch 2383
Epoch 2438
Epoch 2438, Loss: 0.000011145, Improvement: 0.000000783, Best Loss: 0.000001056 in Epoch 2383
Epoch 2439
Epoch 2439, Loss: 0.000005971, Improvement: -0.000005173, Best Loss: 0.000001056 in Epoch 2383
Epoch 2440
Epoch 2440, Loss: 0.000005818, Improvement: -0.000000153, Best Loss: 0.000001056 in Epoch 2383
Epoch 2441
Epoch 2441, Loss: 0.000013848, Improvement: 0.000008030, Best Loss: 0.000001056 in Epoch 2383
Epoch 2442
Epoch 2442, Loss: 0.000019706, Improvement: 0.000005857, Best Loss: 0.000001056 in Epoch 2383
Epoch 2443
Epoch 2443, Loss: 0.000031642, Improvement: 0.000011936, Best Loss: 0.000001056 in Epoch 2383
Epoch 2444
Epoch 2444, Loss: 0.000013546, Improvement: -0.000018096, Best Loss: 0.000001056 in Epoch 2383
Epoch 2445
Epoch 2445, Loss: 0.000013283, Improvement: -0.000000263, Best Loss: 0.000001056 in Epoch 2383
Epoch 2446
Epoch 2446, Loss: 0.000008727, Improvement: -0.000004556, Best Loss: 0.000001056 in Epoch 2383
Epoch 2447
Epoch 2447, Loss: 0.000006319, Improvement: -0.000002408, Best Loss: 0.000001056 in Epoch 2383
Epoch 2448
Epoch 2448, Loss: 0.000006017, Improvement: -0.000000302, Best Loss: 0.000001056 in Epoch 2383
Epoch 2449
Epoch 2449, Loss: 0.000005056, Improvement: -0.000000961, Best Loss: 0.000001056 in Epoch 2383
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.000003199, Improvement: -0.000001858, Best Loss: 0.000001056 in Epoch 2383
Epoch 2451
Epoch 2451, Loss: 0.000003195, Improvement: -0.000000003, Best Loss: 0.000001056 in Epoch 2383
Epoch 2452
Epoch 2452, Loss: 0.000012261, Improvement: 0.000009066, Best Loss: 0.000001056 in Epoch 2383
Epoch 2453
Epoch 2453, Loss: 0.000019207, Improvement: 0.000006946, Best Loss: 0.000001056 in Epoch 2383
Epoch 2454
Epoch 2454, Loss: 0.000038491, Improvement: 0.000019284, Best Loss: 0.000001056 in Epoch 2383
Epoch 2455
Epoch 2455, Loss: 0.000023323, Improvement: -0.000015168, Best Loss: 0.000001056 in Epoch 2383
Epoch 2456
Epoch 2456, Loss: 0.000010323, Improvement: -0.000013000, Best Loss: 0.000001056 in Epoch 2383
Epoch 2457
Epoch 2457, Loss: 0.000004373, Improvement: -0.000005950, Best Loss: 0.000001056 in Epoch 2383
Epoch 2458
Epoch 2458, Loss: 0.000002559, Improvement: -0.000001814, Best Loss: 0.000001056 in Epoch 2383
Epoch 2459
Epoch 2459, Loss: 0.000001987, Improvement: -0.000000571, Best Loss: 0.000001056 in Epoch 2383
Epoch 2460
Epoch 2460, Loss: 0.000002017, Improvement: 0.000000030, Best Loss: 0.000001056 in Epoch 2383
Epoch 2461
Epoch 2461, Loss: 0.000002293, Improvement: 0.000000276, Best Loss: 0.000001056 in Epoch 2383
Epoch 2462
Epoch 2462, Loss: 0.000002542, Improvement: 0.000000249, Best Loss: 0.000001056 in Epoch 2383
Epoch 2463
Epoch 2463, Loss: 0.000002326, Improvement: -0.000000216, Best Loss: 0.000001056 in Epoch 2383
Epoch 2464
Epoch 2464, Loss: 0.000002053, Improvement: -0.000000274, Best Loss: 0.000001056 in Epoch 2383
Epoch 2465
Epoch 2465, Loss: 0.000001821, Improvement: -0.000000232, Best Loss: 0.000001056 in Epoch 2383
Epoch 2466
Epoch 2466, Loss: 0.000002207, Improvement: 0.000000386, Best Loss: 0.000001056 in Epoch 2383
Epoch 2467
Epoch 2467, Loss: 0.000002136, Improvement: -0.000000071, Best Loss: 0.000001056 in Epoch 2383
Epoch 2468
Epoch 2468, Loss: 0.000001927, Improvement: -0.000000209, Best Loss: 0.000001056 in Epoch 2383
Epoch 2469
Epoch 2469, Loss: 0.000003500, Improvement: 0.000001573, Best Loss: 0.000001056 in Epoch 2383
Epoch 2470
Epoch 2470, Loss: 0.000002339, Improvement: -0.000001161, Best Loss: 0.000001056 in Epoch 2383
Epoch 2471
Epoch 2471, Loss: 0.000003445, Improvement: 0.000001106, Best Loss: 0.000001056 in Epoch 2383
Epoch 2472
Epoch 2472, Loss: 0.000004022, Improvement: 0.000000577, Best Loss: 0.000001056 in Epoch 2383
Epoch 2473
Epoch 2473, Loss: 0.000005966, Improvement: 0.000001944, Best Loss: 0.000001056 in Epoch 2383
Epoch 2474
Epoch 2474, Loss: 0.000006603, Improvement: 0.000000637, Best Loss: 0.000001056 in Epoch 2383
Epoch 2475
Epoch 2475, Loss: 0.000014515, Improvement: 0.000007911, Best Loss: 0.000001056 in Epoch 2383
Epoch 2476
Epoch 2476, Loss: 0.000011297, Improvement: -0.000003218, Best Loss: 0.000001056 in Epoch 2383
Epoch 2477
Epoch 2477, Loss: 0.000014218, Improvement: 0.000002921, Best Loss: 0.000001056 in Epoch 2383
Epoch 2478
Epoch 2478, Loss: 0.000024571, Improvement: 0.000010353, Best Loss: 0.000001056 in Epoch 2383
Epoch 2479
Epoch 2479, Loss: 0.000024875, Improvement: 0.000000305, Best Loss: 0.000001056 in Epoch 2383
Epoch 2480
Epoch 2480, Loss: 0.000010821, Improvement: -0.000014054, Best Loss: 0.000001056 in Epoch 2383
Epoch 2481
Epoch 2481, Loss: 0.000006219, Improvement: -0.000004602, Best Loss: 0.000001056 in Epoch 2383
Epoch 2482
Epoch 2482, Loss: 0.000003729, Improvement: -0.000002490, Best Loss: 0.000001056 in Epoch 2383
Epoch 2483
Epoch 2483, Loss: 0.000002389, Improvement: -0.000001340, Best Loss: 0.000001056 in Epoch 2383
Epoch 2484
Epoch 2484, Loss: 0.000002102, Improvement: -0.000000287, Best Loss: 0.000001056 in Epoch 2383
Epoch 2485
Epoch 2485, Loss: 0.000002232, Improvement: 0.000000130, Best Loss: 0.000001056 in Epoch 2383
Epoch 2486
Epoch 2486, Loss: 0.000002179, Improvement: -0.000000053, Best Loss: 0.000001056 in Epoch 2383
Epoch 2487
Epoch 2487, Loss: 0.000001899, Improvement: -0.000000280, Best Loss: 0.000001056 in Epoch 2383
Epoch 2488
Epoch 2488, Loss: 0.000001659, Improvement: -0.000000240, Best Loss: 0.000001056 in Epoch 2383
Epoch 2489
Epoch 2489, Loss: 0.000001782, Improvement: 0.000000123, Best Loss: 0.000001056 in Epoch 2383
Epoch 2490
Epoch 2490, Loss: 0.000001842, Improvement: 0.000000060, Best Loss: 0.000001056 in Epoch 2383
Epoch 2491
Epoch 2491, Loss: 0.000001693, Improvement: -0.000000149, Best Loss: 0.000001056 in Epoch 2383
Epoch 2492
Epoch 2492, Loss: 0.000001510, Improvement: -0.000000183, Best Loss: 0.000001056 in Epoch 2383
Epoch 2493
Epoch 2493, Loss: 0.000002178, Improvement: 0.000000668, Best Loss: 0.000001056 in Epoch 2383
Epoch 2494
Epoch 2494, Loss: 0.000001764, Improvement: -0.000000414, Best Loss: 0.000001056 in Epoch 2383
Epoch 2495
Epoch 2495, Loss: 0.000001780, Improvement: 0.000000016, Best Loss: 0.000001056 in Epoch 2383
Epoch 2496
Epoch 2496, Loss: 0.000001678, Improvement: -0.000000101, Best Loss: 0.000001056 in Epoch 2383
Epoch 2497
Epoch 2497, Loss: 0.000001857, Improvement: 0.000000178, Best Loss: 0.000001056 in Epoch 2383
Epoch 2498
Epoch 2498, Loss: 0.000003104, Improvement: 0.000001247, Best Loss: 0.000001056 in Epoch 2383
Epoch 2499
Epoch 2499, Loss: 0.000003066, Improvement: -0.000000038, Best Loss: 0.000001056 in Epoch 2383
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.000002503, Improvement: -0.000000563, Best Loss: 0.000001056 in Epoch 2383
Epoch 2501
Epoch 2501, Loss: 0.000005892, Improvement: 0.000003389, Best Loss: 0.000001056 in Epoch 2383
Epoch 2502
Epoch 2502, Loss: 0.000005370, Improvement: -0.000000522, Best Loss: 0.000001056 in Epoch 2383
Epoch 2503
Epoch 2503, Loss: 0.000007676, Improvement: 0.000002306, Best Loss: 0.000001056 in Epoch 2383
Epoch 2504
Epoch 2504, Loss: 0.000020396, Improvement: 0.000012720, Best Loss: 0.000001056 in Epoch 2383
Epoch 2505
Epoch 2505, Loss: 0.000016251, Improvement: -0.000004145, Best Loss: 0.000001056 in Epoch 2383
Epoch 2506
Epoch 2506, Loss: 0.000015015, Improvement: -0.000001236, Best Loss: 0.000001056 in Epoch 2383
Epoch 2507
Epoch 2507, Loss: 0.000031219, Improvement: 0.000016204, Best Loss: 0.000001056 in Epoch 2383
Epoch 2508
Epoch 2508, Loss: 0.000016866, Improvement: -0.000014354, Best Loss: 0.000001056 in Epoch 2383
Epoch 2509
Epoch 2509, Loss: 0.000011498, Improvement: -0.000005367, Best Loss: 0.000001056 in Epoch 2383
Epoch 2510
Epoch 2510, Loss: 0.000011649, Improvement: 0.000000150, Best Loss: 0.000001056 in Epoch 2383
Epoch 2511
Epoch 2511, Loss: 0.000007178, Improvement: -0.000004470, Best Loss: 0.000001056 in Epoch 2383
Epoch 2512
Epoch 2512, Loss: 0.000005468, Improvement: -0.000001710, Best Loss: 0.000001056 in Epoch 2383
Epoch 2513
Epoch 2513, Loss: 0.000010407, Improvement: 0.000004938, Best Loss: 0.000001056 in Epoch 2383
Epoch 2514
Epoch 2514, Loss: 0.000005156, Improvement: -0.000005250, Best Loss: 0.000001056 in Epoch 2383
Epoch 2515
Epoch 2515, Loss: 0.000012024, Improvement: 0.000006867, Best Loss: 0.000001056 in Epoch 2383
Epoch 2516
Epoch 2516, Loss: 0.000028098, Improvement: 0.000016074, Best Loss: 0.000001056 in Epoch 2383
Epoch 2517
Epoch 2517, Loss: 0.000018103, Improvement: -0.000009995, Best Loss: 0.000001056 in Epoch 2383
Epoch 2518
Epoch 2518, Loss: 0.000018582, Improvement: 0.000000479, Best Loss: 0.000001056 in Epoch 2383
Epoch 2519
Epoch 2519, Loss: 0.000026550, Improvement: 0.000007969, Best Loss: 0.000001056 in Epoch 2383
Epoch 2520
Epoch 2520, Loss: 0.000016150, Improvement: -0.000010401, Best Loss: 0.000001056 in Epoch 2383
Epoch 2521
Epoch 2521, Loss: 0.000006347, Improvement: -0.000009803, Best Loss: 0.000001056 in Epoch 2383
Epoch 2522
Epoch 2522, Loss: 0.000003368, Improvement: -0.000002979, Best Loss: 0.000001056 in Epoch 2383
Epoch 2523
Epoch 2523, Loss: 0.000002442, Improvement: -0.000000925, Best Loss: 0.000001056 in Epoch 2383
Epoch 2524
Epoch 2524, Loss: 0.000002389, Improvement: -0.000000053, Best Loss: 0.000001056 in Epoch 2383
Epoch 2525
Epoch 2525, Loss: 0.000001980, Improvement: -0.000000409, Best Loss: 0.000001056 in Epoch 2383
Epoch 2526
Epoch 2526, Loss: 0.000001748, Improvement: -0.000000232, Best Loss: 0.000001056 in Epoch 2383
Epoch 2527
Epoch 2527, Loss: 0.000001723, Improvement: -0.000000025, Best Loss: 0.000001056 in Epoch 2383
Epoch 2528
Epoch 2528, Loss: 0.000001599, Improvement: -0.000000124, Best Loss: 0.000001056 in Epoch 2383
Epoch 2529
Epoch 2529, Loss: 0.000001621, Improvement: 0.000000022, Best Loss: 0.000001056 in Epoch 2383
Epoch 2530
Epoch 2530, Loss: 0.000001542, Improvement: -0.000000079, Best Loss: 0.000001056 in Epoch 2383
Epoch 2531
Epoch 2531, Loss: 0.000001829, Improvement: 0.000000287, Best Loss: 0.000001056 in Epoch 2383
Epoch 2532
Epoch 2532, Loss: 0.000001855, Improvement: 0.000000027, Best Loss: 0.000001056 in Epoch 2383
Epoch 2533
Epoch 2533, Loss: 0.000001782, Improvement: -0.000000073, Best Loss: 0.000001056 in Epoch 2383
Epoch 2534
Epoch 2534, Loss: 0.000002114, Improvement: 0.000000332, Best Loss: 0.000001056 in Epoch 2383
Epoch 2535
Epoch 2535, Loss: 0.000002003, Improvement: -0.000000111, Best Loss: 0.000001056 in Epoch 2383
Epoch 2536
A best model at epoch 2536 has been saved with training error 0.000000966.
Epoch 2536, Loss: 0.000001708, Improvement: -0.000000295, Best Loss: 0.000000966 in Epoch 2536
Epoch 2537
Epoch 2537, Loss: 0.000001503, Improvement: -0.000000204, Best Loss: 0.000000966 in Epoch 2536
Epoch 2538
Epoch 2538, Loss: 0.000001467, Improvement: -0.000000036, Best Loss: 0.000000966 in Epoch 2536
Epoch 2539
A best model at epoch 2539 has been saved with training error 0.000000914.
Epoch 2539, Loss: 0.000001532, Improvement: 0.000000065, Best Loss: 0.000000914 in Epoch 2539
Epoch 2540
Epoch 2540, Loss: 0.000001611, Improvement: 0.000000079, Best Loss: 0.000000914 in Epoch 2539
Epoch 2541
Epoch 2541, Loss: 0.000001462, Improvement: -0.000000149, Best Loss: 0.000000914 in Epoch 2539
Epoch 2542
Epoch 2542, Loss: 0.000001497, Improvement: 0.000000035, Best Loss: 0.000000914 in Epoch 2539
Epoch 2543
Epoch 2543, Loss: 0.000001639, Improvement: 0.000000142, Best Loss: 0.000000914 in Epoch 2539
Epoch 2544
Epoch 2544, Loss: 0.000002441, Improvement: 0.000000802, Best Loss: 0.000000914 in Epoch 2539
Epoch 2545
Epoch 2545, Loss: 0.000002922, Improvement: 0.000000481, Best Loss: 0.000000914 in Epoch 2539
Epoch 2546
Epoch 2546, Loss: 0.000008360, Improvement: 0.000005439, Best Loss: 0.000000914 in Epoch 2539
Epoch 2547
Epoch 2547, Loss: 0.000036082, Improvement: 0.000027722, Best Loss: 0.000000914 in Epoch 2539
Epoch 2548
Epoch 2548, Loss: 0.000025016, Improvement: -0.000011066, Best Loss: 0.000000914 in Epoch 2539
Epoch 2549
Epoch 2549, Loss: 0.000013294, Improvement: -0.000011722, Best Loss: 0.000000914 in Epoch 2539
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.000010118, Improvement: -0.000003176, Best Loss: 0.000000914 in Epoch 2539
Epoch 2551
Epoch 2551, Loss: 0.000009030, Improvement: -0.000001088, Best Loss: 0.000000914 in Epoch 2539
Epoch 2552
Epoch 2552, Loss: 0.000004998, Improvement: -0.000004032, Best Loss: 0.000000914 in Epoch 2539
Epoch 2553
Epoch 2553, Loss: 0.000019355, Improvement: 0.000014357, Best Loss: 0.000000914 in Epoch 2539
Epoch 2554
Epoch 2554, Loss: 0.000016983, Improvement: -0.000002371, Best Loss: 0.000000914 in Epoch 2539
Epoch 2555
Epoch 2555, Loss: 0.000006237, Improvement: -0.000010746, Best Loss: 0.000000914 in Epoch 2539
Epoch 2556
Epoch 2556, Loss: 0.000003692, Improvement: -0.000002546, Best Loss: 0.000000914 in Epoch 2539
Epoch 2557
Epoch 2557, Loss: 0.000002442, Improvement: -0.000001250, Best Loss: 0.000000914 in Epoch 2539
Epoch 2558
Epoch 2558, Loss: 0.000002053, Improvement: -0.000000388, Best Loss: 0.000000914 in Epoch 2539
Epoch 2559
Epoch 2559, Loss: 0.000002014, Improvement: -0.000000039, Best Loss: 0.000000914 in Epoch 2539
Epoch 2560
Epoch 2560, Loss: 0.000003492, Improvement: 0.000001478, Best Loss: 0.000000914 in Epoch 2539
Epoch 2561
Epoch 2561, Loss: 0.000006654, Improvement: 0.000003162, Best Loss: 0.000000914 in Epoch 2539
Epoch 2562
Epoch 2562, Loss: 0.000008516, Improvement: 0.000001862, Best Loss: 0.000000914 in Epoch 2539
Epoch 2563
Epoch 2563, Loss: 0.000008108, Improvement: -0.000000408, Best Loss: 0.000000914 in Epoch 2539
Epoch 2564
Epoch 2564, Loss: 0.000013654, Improvement: 0.000005546, Best Loss: 0.000000914 in Epoch 2539
Epoch 2565
Epoch 2565, Loss: 0.000010717, Improvement: -0.000002936, Best Loss: 0.000000914 in Epoch 2539
Epoch 2566
Epoch 2566, Loss: 0.000020608, Improvement: 0.000009891, Best Loss: 0.000000914 in Epoch 2539
Epoch 2567
Epoch 2567, Loss: 0.000015225, Improvement: -0.000005384, Best Loss: 0.000000914 in Epoch 2539
Epoch 2568
Epoch 2568, Loss: 0.000006596, Improvement: -0.000008629, Best Loss: 0.000000914 in Epoch 2539
Epoch 2569
Epoch 2569, Loss: 0.000011063, Improvement: 0.000004467, Best Loss: 0.000000914 in Epoch 2539
Epoch 2570
Epoch 2570, Loss: 0.000008644, Improvement: -0.000002419, Best Loss: 0.000000914 in Epoch 2539
Epoch 2571
Epoch 2571, Loss: 0.000011613, Improvement: 0.000002969, Best Loss: 0.000000914 in Epoch 2539
Epoch 2572
Epoch 2572, Loss: 0.000011814, Improvement: 0.000000201, Best Loss: 0.000000914 in Epoch 2539
Epoch 2573
Epoch 2573, Loss: 0.000011336, Improvement: -0.000000478, Best Loss: 0.000000914 in Epoch 2539
Epoch 2574
Epoch 2574, Loss: 0.000011128, Improvement: -0.000000208, Best Loss: 0.000000914 in Epoch 2539
Epoch 2575
Epoch 2575, Loss: 0.000007952, Improvement: -0.000003176, Best Loss: 0.000000914 in Epoch 2539
Epoch 2576
Epoch 2576, Loss: 0.000008113, Improvement: 0.000000161, Best Loss: 0.000000914 in Epoch 2539
Epoch 2577
Epoch 2577, Loss: 0.000019445, Improvement: 0.000011332, Best Loss: 0.000000914 in Epoch 2539
Epoch 2578
Epoch 2578, Loss: 0.000016573, Improvement: -0.000002872, Best Loss: 0.000000914 in Epoch 2539
Epoch 2579
Epoch 2579, Loss: 0.000014107, Improvement: -0.000002466, Best Loss: 0.000000914 in Epoch 2539
Epoch 2580
Epoch 2580, Loss: 0.000014237, Improvement: 0.000000130, Best Loss: 0.000000914 in Epoch 2539
Epoch 2581
Epoch 2581, Loss: 0.000021878, Improvement: 0.000007641, Best Loss: 0.000000914 in Epoch 2539
Epoch 2582
Epoch 2582, Loss: 0.000035533, Improvement: 0.000013654, Best Loss: 0.000000914 in Epoch 2539
Epoch 2583
Epoch 2583, Loss: 0.000012988, Improvement: -0.000022545, Best Loss: 0.000000914 in Epoch 2539
Epoch 2584
Epoch 2584, Loss: 0.000006849, Improvement: -0.000006139, Best Loss: 0.000000914 in Epoch 2539
Epoch 2585
Epoch 2585, Loss: 0.000004583, Improvement: -0.000002266, Best Loss: 0.000000914 in Epoch 2539
Epoch 2586
Epoch 2586, Loss: 0.000002364, Improvement: -0.000002219, Best Loss: 0.000000914 in Epoch 2539
Epoch 2587
Epoch 2587, Loss: 0.000002017, Improvement: -0.000000347, Best Loss: 0.000000914 in Epoch 2539
Epoch 2588
Epoch 2588, Loss: 0.000002370, Improvement: 0.000000353, Best Loss: 0.000000914 in Epoch 2539
Epoch 2589
Epoch 2589, Loss: 0.000003556, Improvement: 0.000001187, Best Loss: 0.000000914 in Epoch 2539
Epoch 2590
Epoch 2590, Loss: 0.000004154, Improvement: 0.000000598, Best Loss: 0.000000914 in Epoch 2539
Epoch 2591
Epoch 2591, Loss: 0.000002578, Improvement: -0.000001576, Best Loss: 0.000000914 in Epoch 2539
Epoch 2592
Epoch 2592, Loss: 0.000003828, Improvement: 0.000001250, Best Loss: 0.000000914 in Epoch 2539
Epoch 2593
Epoch 2593, Loss: 0.000005499, Improvement: 0.000001670, Best Loss: 0.000000914 in Epoch 2539
Epoch 2594
Epoch 2594, Loss: 0.000005501, Improvement: 0.000000003, Best Loss: 0.000000914 in Epoch 2539
Epoch 2595
Epoch 2595, Loss: 0.000007500, Improvement: 0.000001999, Best Loss: 0.000000914 in Epoch 2539
Epoch 2596
Epoch 2596, Loss: 0.000010650, Improvement: 0.000003150, Best Loss: 0.000000914 in Epoch 2539
Epoch 2597
Epoch 2597, Loss: 0.000013264, Improvement: 0.000002615, Best Loss: 0.000000914 in Epoch 2539
Epoch 2598
Epoch 2598, Loss: 0.000012010, Improvement: -0.000001254, Best Loss: 0.000000914 in Epoch 2539
Epoch 2599
Epoch 2599, Loss: 0.000009014, Improvement: -0.000002996, Best Loss: 0.000000914 in Epoch 2539
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.000008006, Improvement: -0.000001008, Best Loss: 0.000000914 in Epoch 2539
Epoch 2601
Epoch 2601, Loss: 0.000010597, Improvement: 0.000002591, Best Loss: 0.000000914 in Epoch 2539
Epoch 2602
Epoch 2602, Loss: 0.000007548, Improvement: -0.000003049, Best Loss: 0.000000914 in Epoch 2539
Epoch 2603
Epoch 2603, Loss: 0.000007949, Improvement: 0.000000401, Best Loss: 0.000000914 in Epoch 2539
Epoch 2604
Epoch 2604, Loss: 0.000018776, Improvement: 0.000010826, Best Loss: 0.000000914 in Epoch 2539
Epoch 2605
Epoch 2605, Loss: 0.000008872, Improvement: -0.000009904, Best Loss: 0.000000914 in Epoch 2539
Epoch 2606
Epoch 2606, Loss: 0.000005921, Improvement: -0.000002950, Best Loss: 0.000000914 in Epoch 2539
Epoch 2607
Epoch 2607, Loss: 0.000003294, Improvement: -0.000002628, Best Loss: 0.000000914 in Epoch 2539
Epoch 2608
Epoch 2608, Loss: 0.000003468, Improvement: 0.000000174, Best Loss: 0.000000914 in Epoch 2539
Epoch 2609
Epoch 2609, Loss: 0.000002868, Improvement: -0.000000599, Best Loss: 0.000000914 in Epoch 2539
Epoch 2610
Epoch 2610, Loss: 0.000002795, Improvement: -0.000000074, Best Loss: 0.000000914 in Epoch 2539
Epoch 2611
Epoch 2611, Loss: 0.000001905, Improvement: -0.000000890, Best Loss: 0.000000914 in Epoch 2539
Epoch 2612
Epoch 2612, Loss: 0.000002225, Improvement: 0.000000320, Best Loss: 0.000000914 in Epoch 2539
Epoch 2613
Epoch 2613, Loss: 0.000002836, Improvement: 0.000000611, Best Loss: 0.000000914 in Epoch 2539
Epoch 2614
Epoch 2614, Loss: 0.000006947, Improvement: 0.000004111, Best Loss: 0.000000914 in Epoch 2539
Epoch 2615
Epoch 2615, Loss: 0.000023643, Improvement: 0.000016696, Best Loss: 0.000000914 in Epoch 2539
Epoch 2616
Epoch 2616, Loss: 0.000016416, Improvement: -0.000007227, Best Loss: 0.000000914 in Epoch 2539
Epoch 2617
Epoch 2617, Loss: 0.000005851, Improvement: -0.000010564, Best Loss: 0.000000914 in Epoch 2539
Epoch 2618
Epoch 2618, Loss: 0.000004503, Improvement: -0.000001349, Best Loss: 0.000000914 in Epoch 2539
Epoch 2619
Epoch 2619, Loss: 0.000007253, Improvement: 0.000002750, Best Loss: 0.000000914 in Epoch 2539
Epoch 2620
Epoch 2620, Loss: 0.000008993, Improvement: 0.000001740, Best Loss: 0.000000914 in Epoch 2539
Epoch 2621
Epoch 2621, Loss: 0.000008944, Improvement: -0.000000049, Best Loss: 0.000000914 in Epoch 2539
Epoch 2622
Epoch 2622, Loss: 0.000007397, Improvement: -0.000001547, Best Loss: 0.000000914 in Epoch 2539
Epoch 2623
Epoch 2623, Loss: 0.000005988, Improvement: -0.000001409, Best Loss: 0.000000914 in Epoch 2539
Epoch 2624
Epoch 2624, Loss: 0.000007335, Improvement: 0.000001348, Best Loss: 0.000000914 in Epoch 2539
Epoch 2625
Epoch 2625, Loss: 0.000006646, Improvement: -0.000000689, Best Loss: 0.000000914 in Epoch 2539
Epoch 2626
Epoch 2626, Loss: 0.000015996, Improvement: 0.000009350, Best Loss: 0.000000914 in Epoch 2539
Epoch 2627
Epoch 2627, Loss: 0.000021574, Improvement: 0.000005579, Best Loss: 0.000000914 in Epoch 2539
Epoch 2628
Epoch 2628, Loss: 0.000021432, Improvement: -0.000000142, Best Loss: 0.000000914 in Epoch 2539
Epoch 2629
Epoch 2629, Loss: 0.000006707, Improvement: -0.000014726, Best Loss: 0.000000914 in Epoch 2539
Epoch 2630
Epoch 2630, Loss: 0.000004710, Improvement: -0.000001996, Best Loss: 0.000000914 in Epoch 2539
Epoch 2631
Epoch 2631, Loss: 0.000002872, Improvement: -0.000001838, Best Loss: 0.000000914 in Epoch 2539
Epoch 2632
Epoch 2632, Loss: 0.000004321, Improvement: 0.000001449, Best Loss: 0.000000914 in Epoch 2539
Epoch 2633
Epoch 2633, Loss: 0.000012667, Improvement: 0.000008346, Best Loss: 0.000000914 in Epoch 2539
Epoch 2634
Epoch 2634, Loss: 0.000013479, Improvement: 0.000000812, Best Loss: 0.000000914 in Epoch 2539
Epoch 2635
Epoch 2635, Loss: 0.000006895, Improvement: -0.000006584, Best Loss: 0.000000914 in Epoch 2539
Epoch 2636
Epoch 2636, Loss: 0.000003862, Improvement: -0.000003033, Best Loss: 0.000000914 in Epoch 2539
Epoch 2637
Epoch 2637, Loss: 0.000002509, Improvement: -0.000001353, Best Loss: 0.000000914 in Epoch 2539
Epoch 2638
Epoch 2638, Loss: 0.000002102, Improvement: -0.000000407, Best Loss: 0.000000914 in Epoch 2539
Epoch 2639
Epoch 2639, Loss: 0.000001721, Improvement: -0.000000381, Best Loss: 0.000000914 in Epoch 2539
Epoch 2640
Epoch 2640, Loss: 0.000001798, Improvement: 0.000000077, Best Loss: 0.000000914 in Epoch 2539
Epoch 2641
Epoch 2641, Loss: 0.000001560, Improvement: -0.000000239, Best Loss: 0.000000914 in Epoch 2539
Epoch 2642
Epoch 2642, Loss: 0.000001469, Improvement: -0.000000090, Best Loss: 0.000000914 in Epoch 2539
Epoch 2643
Epoch 2643, Loss: 0.000001580, Improvement: 0.000000110, Best Loss: 0.000000914 in Epoch 2539
Epoch 2644
Epoch 2644, Loss: 0.000001817, Improvement: 0.000000237, Best Loss: 0.000000914 in Epoch 2539
Epoch 2645
Epoch 2645, Loss: 0.000002109, Improvement: 0.000000292, Best Loss: 0.000000914 in Epoch 2539
Epoch 2646
Epoch 2646, Loss: 0.000001923, Improvement: -0.000000186, Best Loss: 0.000000914 in Epoch 2539
Epoch 2647
Epoch 2647, Loss: 0.000001641, Improvement: -0.000000283, Best Loss: 0.000000914 in Epoch 2539
Epoch 2648
Epoch 2648, Loss: 0.000002253, Improvement: 0.000000613, Best Loss: 0.000000914 in Epoch 2539
Epoch 2649
Epoch 2649, Loss: 0.000006912, Improvement: 0.000004659, Best Loss: 0.000000914 in Epoch 2539
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.000025392, Improvement: 0.000018480, Best Loss: 0.000000914 in Epoch 2539
Epoch 2651
Epoch 2651, Loss: 0.000034997, Improvement: 0.000009605, Best Loss: 0.000000914 in Epoch 2539
Epoch 2652
Epoch 2652, Loss: 0.000018692, Improvement: -0.000016304, Best Loss: 0.000000914 in Epoch 2539
Epoch 2653
Epoch 2653, Loss: 0.000012295, Improvement: -0.000006397, Best Loss: 0.000000914 in Epoch 2539
Epoch 2654
Epoch 2654, Loss: 0.000017731, Improvement: 0.000005436, Best Loss: 0.000000914 in Epoch 2539
Epoch 2655
Epoch 2655, Loss: 0.000041174, Improvement: 0.000023443, Best Loss: 0.000000914 in Epoch 2539
Epoch 2656
Epoch 2656, Loss: 0.000026019, Improvement: -0.000015155, Best Loss: 0.000000914 in Epoch 2539
Epoch 2657
Epoch 2657, Loss: 0.000009526, Improvement: -0.000016493, Best Loss: 0.000000914 in Epoch 2539
Epoch 2658
Epoch 2658, Loss: 0.000003908, Improvement: -0.000005618, Best Loss: 0.000000914 in Epoch 2539
Epoch 2659
Epoch 2659, Loss: 0.000002884, Improvement: -0.000001024, Best Loss: 0.000000914 in Epoch 2539
Epoch 2660
Epoch 2660, Loss: 0.000002442, Improvement: -0.000000442, Best Loss: 0.000000914 in Epoch 2539
Epoch 2661
Epoch 2661, Loss: 0.000002326, Improvement: -0.000000116, Best Loss: 0.000000914 in Epoch 2539
Epoch 2662
Epoch 2662, Loss: 0.000004450, Improvement: 0.000002125, Best Loss: 0.000000914 in Epoch 2539
Epoch 2663
Epoch 2663, Loss: 0.000005171, Improvement: 0.000000720, Best Loss: 0.000000914 in Epoch 2539
Epoch 2664
Epoch 2664, Loss: 0.000006242, Improvement: 0.000001071, Best Loss: 0.000000914 in Epoch 2539
Epoch 2665
Epoch 2665, Loss: 0.000005098, Improvement: -0.000001143, Best Loss: 0.000000914 in Epoch 2539
Epoch 2666
Epoch 2666, Loss: 0.000002655, Improvement: -0.000002443, Best Loss: 0.000000914 in Epoch 2539
Epoch 2667
Epoch 2667, Loss: 0.000003580, Improvement: 0.000000925, Best Loss: 0.000000914 in Epoch 2539
Epoch 2668
Epoch 2668, Loss: 0.000009725, Improvement: 0.000006145, Best Loss: 0.000000914 in Epoch 2539
Epoch 2669
Epoch 2669, Loss: 0.000011624, Improvement: 0.000001900, Best Loss: 0.000000914 in Epoch 2539
Epoch 2670
Epoch 2670, Loss: 0.000015968, Improvement: 0.000004344, Best Loss: 0.000000914 in Epoch 2539
Epoch 2671
Epoch 2671, Loss: 0.000035084, Improvement: 0.000019116, Best Loss: 0.000000914 in Epoch 2539
Epoch 2672
Epoch 2672, Loss: 0.000014567, Improvement: -0.000020517, Best Loss: 0.000000914 in Epoch 2539
Epoch 2673
Epoch 2673, Loss: 0.000014640, Improvement: 0.000000074, Best Loss: 0.000000914 in Epoch 2539
Epoch 2674
Epoch 2674, Loss: 0.000030437, Improvement: 0.000015797, Best Loss: 0.000000914 in Epoch 2539
Epoch 2675
Epoch 2675, Loss: 0.000057911, Improvement: 0.000027474, Best Loss: 0.000000914 in Epoch 2539
Epoch 2676
Epoch 2676, Loss: 0.000030791, Improvement: -0.000027120, Best Loss: 0.000000914 in Epoch 2539
Epoch 2677
Epoch 2677, Loss: 0.000013144, Improvement: -0.000017646, Best Loss: 0.000000914 in Epoch 2539
Epoch 2678
Epoch 2678, Loss: 0.000004528, Improvement: -0.000008616, Best Loss: 0.000000914 in Epoch 2539
Epoch 2679
Epoch 2679, Loss: 0.000002757, Improvement: -0.000001771, Best Loss: 0.000000914 in Epoch 2539
Epoch 2680
Epoch 2680, Loss: 0.000002236, Improvement: -0.000000521, Best Loss: 0.000000914 in Epoch 2539
Epoch 2681
Epoch 2681, Loss: 0.000002249, Improvement: 0.000000012, Best Loss: 0.000000914 in Epoch 2539
Epoch 2682
Epoch 2682, Loss: 0.000002138, Improvement: -0.000000111, Best Loss: 0.000000914 in Epoch 2539
Epoch 2683
Epoch 2683, Loss: 0.000001684, Improvement: -0.000000455, Best Loss: 0.000000914 in Epoch 2539
Epoch 2684
Epoch 2684, Loss: 0.000001784, Improvement: 0.000000100, Best Loss: 0.000000914 in Epoch 2539
Epoch 2685
Epoch 2685, Loss: 0.000001536, Improvement: -0.000000248, Best Loss: 0.000000914 in Epoch 2539
Epoch 2686
Epoch 2686, Loss: 0.000001502, Improvement: -0.000000034, Best Loss: 0.000000914 in Epoch 2539
Epoch 2687
Epoch 2687, Loss: 0.000001457, Improvement: -0.000000045, Best Loss: 0.000000914 in Epoch 2539
Epoch 2688
Epoch 2688, Loss: 0.000001460, Improvement: 0.000000003, Best Loss: 0.000000914 in Epoch 2539
Epoch 2689
Epoch 2689, Loss: 0.000001517, Improvement: 0.000000057, Best Loss: 0.000000914 in Epoch 2539
Epoch 2690
Epoch 2690, Loss: 0.000001629, Improvement: 0.000000112, Best Loss: 0.000000914 in Epoch 2539
Epoch 2691
Epoch 2691, Loss: 0.000001642, Improvement: 0.000000013, Best Loss: 0.000000914 in Epoch 2539
Epoch 2692
Epoch 2692, Loss: 0.000001368, Improvement: -0.000000275, Best Loss: 0.000000914 in Epoch 2539
Epoch 2693
Epoch 2693, Loss: 0.000001568, Improvement: 0.000000201, Best Loss: 0.000000914 in Epoch 2539
Epoch 2694
Epoch 2694, Loss: 0.000001823, Improvement: 0.000000254, Best Loss: 0.000000914 in Epoch 2539
Epoch 2695
Epoch 2695, Loss: 0.000002012, Improvement: 0.000000189, Best Loss: 0.000000914 in Epoch 2539
Epoch 2696
Epoch 2696, Loss: 0.000001607, Improvement: -0.000000404, Best Loss: 0.000000914 in Epoch 2539
Epoch 2697
Epoch 2697, Loss: 0.000001472, Improvement: -0.000000136, Best Loss: 0.000000914 in Epoch 2539
Epoch 2698
Epoch 2698, Loss: 0.000001636, Improvement: 0.000000165, Best Loss: 0.000000914 in Epoch 2539
Epoch 2699
Epoch 2699, Loss: 0.000002459, Improvement: 0.000000822, Best Loss: 0.000000914 in Epoch 2539
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.000002188, Improvement: -0.000000271, Best Loss: 0.000000914 in Epoch 2539
Epoch 2701
Epoch 2701, Loss: 0.000003543, Improvement: 0.000001356, Best Loss: 0.000000914 in Epoch 2539
Epoch 2702
Epoch 2702, Loss: 0.000003909, Improvement: 0.000000366, Best Loss: 0.000000914 in Epoch 2539
Epoch 2703
Epoch 2703, Loss: 0.000003427, Improvement: -0.000000482, Best Loss: 0.000000914 in Epoch 2539
Epoch 2704
Epoch 2704, Loss: 0.000011719, Improvement: 0.000008292, Best Loss: 0.000000914 in Epoch 2539
Epoch 2705
Epoch 2705, Loss: 0.000015996, Improvement: 0.000004277, Best Loss: 0.000000914 in Epoch 2539
Epoch 2706
Epoch 2706, Loss: 0.000007403, Improvement: -0.000008594, Best Loss: 0.000000914 in Epoch 2539
Epoch 2707
Epoch 2707, Loss: 0.000011238, Improvement: 0.000003835, Best Loss: 0.000000914 in Epoch 2539
Epoch 2708
Epoch 2708, Loss: 0.000010577, Improvement: -0.000000661, Best Loss: 0.000000914 in Epoch 2539
Epoch 2709
Epoch 2709, Loss: 0.000020550, Improvement: 0.000009973, Best Loss: 0.000000914 in Epoch 2539
Epoch 2710
Epoch 2710, Loss: 0.000021551, Improvement: 0.000001000, Best Loss: 0.000000914 in Epoch 2539
Epoch 2711
Epoch 2711, Loss: 0.000015739, Improvement: -0.000005812, Best Loss: 0.000000914 in Epoch 2539
Epoch 2712
Epoch 2712, Loss: 0.000011843, Improvement: -0.000003896, Best Loss: 0.000000914 in Epoch 2539
Epoch 2713
Epoch 2713, Loss: 0.000005567, Improvement: -0.000006276, Best Loss: 0.000000914 in Epoch 2539
Epoch 2714
Epoch 2714, Loss: 0.000005269, Improvement: -0.000000298, Best Loss: 0.000000914 in Epoch 2539
Epoch 2715
Epoch 2715, Loss: 0.000003542, Improvement: -0.000001726, Best Loss: 0.000000914 in Epoch 2539
Epoch 2716
Epoch 2716, Loss: 0.000002386, Improvement: -0.000001157, Best Loss: 0.000000914 in Epoch 2539
Epoch 2717
Epoch 2717, Loss: 0.000002330, Improvement: -0.000000056, Best Loss: 0.000000914 in Epoch 2539
Epoch 2718
Epoch 2718, Loss: 0.000002865, Improvement: 0.000000535, Best Loss: 0.000000914 in Epoch 2539
Epoch 2719
Epoch 2719, Loss: 0.000003031, Improvement: 0.000000166, Best Loss: 0.000000914 in Epoch 2539
Epoch 2720
Epoch 2720, Loss: 0.000001746, Improvement: -0.000001286, Best Loss: 0.000000914 in Epoch 2539
Epoch 2721
Epoch 2721, Loss: 0.000001951, Improvement: 0.000000205, Best Loss: 0.000000914 in Epoch 2539
Epoch 2722
Epoch 2722, Loss: 0.000002549, Improvement: 0.000000598, Best Loss: 0.000000914 in Epoch 2539
Epoch 2723
Epoch 2723, Loss: 0.000002004, Improvement: -0.000000545, Best Loss: 0.000000914 in Epoch 2539
Epoch 2724
Epoch 2724, Loss: 0.000002014, Improvement: 0.000000011, Best Loss: 0.000000914 in Epoch 2539
Epoch 2725
Epoch 2725, Loss: 0.000002448, Improvement: 0.000000434, Best Loss: 0.000000914 in Epoch 2539
Epoch 2726
Epoch 2726, Loss: 0.000003071, Improvement: 0.000000623, Best Loss: 0.000000914 in Epoch 2539
Epoch 2727
Epoch 2727, Loss: 0.000003849, Improvement: 0.000000778, Best Loss: 0.000000914 in Epoch 2539
Epoch 2728
Epoch 2728, Loss: 0.000002366, Improvement: -0.000001483, Best Loss: 0.000000914 in Epoch 2539
Epoch 2729
Epoch 2729, Loss: 0.000002803, Improvement: 0.000000437, Best Loss: 0.000000914 in Epoch 2539
Epoch 2730
Epoch 2730, Loss: 0.000016227, Improvement: 0.000013424, Best Loss: 0.000000914 in Epoch 2539
Epoch 2731
Epoch 2731, Loss: 0.000021348, Improvement: 0.000005121, Best Loss: 0.000000914 in Epoch 2539
Epoch 2732
Epoch 2732, Loss: 0.000023550, Improvement: 0.000002202, Best Loss: 0.000000914 in Epoch 2539
Epoch 2733
Epoch 2733, Loss: 0.000012632, Improvement: -0.000010918, Best Loss: 0.000000914 in Epoch 2539
Epoch 2734
Epoch 2734, Loss: 0.000011950, Improvement: -0.000000682, Best Loss: 0.000000914 in Epoch 2539
Epoch 2735
Epoch 2735, Loss: 0.000005158, Improvement: -0.000006792, Best Loss: 0.000000914 in Epoch 2539
Epoch 2736
Epoch 2736, Loss: 0.000003667, Improvement: -0.000001491, Best Loss: 0.000000914 in Epoch 2539
Epoch 2737
Epoch 2737, Loss: 0.000002512, Improvement: -0.000001155, Best Loss: 0.000000914 in Epoch 2539
Epoch 2738
Epoch 2738, Loss: 0.000002223, Improvement: -0.000000289, Best Loss: 0.000000914 in Epoch 2539
Epoch 2739
Epoch 2739, Loss: 0.000001700, Improvement: -0.000000523, Best Loss: 0.000000914 in Epoch 2539
Epoch 2740
Epoch 2740, Loss: 0.000001548, Improvement: -0.000000152, Best Loss: 0.000000914 in Epoch 2539
Epoch 2741
Epoch 2741, Loss: 0.000002020, Improvement: 0.000000472, Best Loss: 0.000000914 in Epoch 2539
Epoch 2742
Epoch 2742, Loss: 0.000001795, Improvement: -0.000000225, Best Loss: 0.000000914 in Epoch 2539
Epoch 2743
Epoch 2743, Loss: 0.000001330, Improvement: -0.000000465, Best Loss: 0.000000914 in Epoch 2539
Epoch 2744
Epoch 2744, Loss: 0.000001285, Improvement: -0.000000045, Best Loss: 0.000000914 in Epoch 2539
Epoch 2745
A best model at epoch 2745 has been saved with training error 0.000000798.
Epoch 2745, Loss: 0.000001229, Improvement: -0.000000056, Best Loss: 0.000000798 in Epoch 2745
Epoch 2746
Epoch 2746, Loss: 0.000001240, Improvement: 0.000000011, Best Loss: 0.000000798 in Epoch 2745
Epoch 2747
Epoch 2747, Loss: 0.000001242, Improvement: 0.000000002, Best Loss: 0.000000798 in Epoch 2745
Epoch 2748
Epoch 2748, Loss: 0.000001617, Improvement: 0.000000376, Best Loss: 0.000000798 in Epoch 2745
Epoch 2749
Epoch 2749, Loss: 0.000001878, Improvement: 0.000000261, Best Loss: 0.000000798 in Epoch 2745
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.000002546, Improvement: 0.000000668, Best Loss: 0.000000798 in Epoch 2745
Epoch 2751
Epoch 2751, Loss: 0.000002756, Improvement: 0.000000210, Best Loss: 0.000000798 in Epoch 2745
Epoch 2752
Epoch 2752, Loss: 0.000002294, Improvement: -0.000000462, Best Loss: 0.000000798 in Epoch 2745
Epoch 2753
Epoch 2753, Loss: 0.000003554, Improvement: 0.000001260, Best Loss: 0.000000798 in Epoch 2745
Epoch 2754
Epoch 2754, Loss: 0.000007841, Improvement: 0.000004287, Best Loss: 0.000000798 in Epoch 2745
Epoch 2755
Epoch 2755, Loss: 0.000007926, Improvement: 0.000000085, Best Loss: 0.000000798 in Epoch 2745
Epoch 2756
Epoch 2756, Loss: 0.000008913, Improvement: 0.000000987, Best Loss: 0.000000798 in Epoch 2745
Epoch 2757
Epoch 2757, Loss: 0.000020179, Improvement: 0.000011266, Best Loss: 0.000000798 in Epoch 2745
Epoch 2758
Epoch 2758, Loss: 0.000019495, Improvement: -0.000000685, Best Loss: 0.000000798 in Epoch 2745
Epoch 2759
Epoch 2759, Loss: 0.000012845, Improvement: -0.000006650, Best Loss: 0.000000798 in Epoch 2745
Epoch 2760
Epoch 2760, Loss: 0.000049113, Improvement: 0.000036268, Best Loss: 0.000000798 in Epoch 2745
Epoch 2761
Epoch 2761, Loss: 0.000025132, Improvement: -0.000023981, Best Loss: 0.000000798 in Epoch 2745
Epoch 2762
Epoch 2762, Loss: 0.000010085, Improvement: -0.000015048, Best Loss: 0.000000798 in Epoch 2745
Epoch 2763
Epoch 2763, Loss: 0.000003459, Improvement: -0.000006625, Best Loss: 0.000000798 in Epoch 2745
Epoch 2764
Epoch 2764, Loss: 0.000002064, Improvement: -0.000001395, Best Loss: 0.000000798 in Epoch 2745
Epoch 2765
Epoch 2765, Loss: 0.000001513, Improvement: -0.000000551, Best Loss: 0.000000798 in Epoch 2745
Epoch 2766
Epoch 2766, Loss: 0.000001439, Improvement: -0.000000074, Best Loss: 0.000000798 in Epoch 2745
Epoch 2767
Epoch 2767, Loss: 0.000001335, Improvement: -0.000000104, Best Loss: 0.000000798 in Epoch 2745
Epoch 2768
Epoch 2768, Loss: 0.000001329, Improvement: -0.000000006, Best Loss: 0.000000798 in Epoch 2745
Epoch 2769
Epoch 2769, Loss: 0.000001504, Improvement: 0.000000175, Best Loss: 0.000000798 in Epoch 2745
Epoch 2770
Epoch 2770, Loss: 0.000001395, Improvement: -0.000000108, Best Loss: 0.000000798 in Epoch 2745
Epoch 2771
Epoch 2771, Loss: 0.000001323, Improvement: -0.000000072, Best Loss: 0.000000798 in Epoch 2745
Epoch 2772
Epoch 2772, Loss: 0.000001322, Improvement: -0.000000002, Best Loss: 0.000000798 in Epoch 2745
Epoch 2773
Epoch 2773, Loss: 0.000001231, Improvement: -0.000000091, Best Loss: 0.000000798 in Epoch 2745
Epoch 2774
Epoch 2774, Loss: 0.000001181, Improvement: -0.000000049, Best Loss: 0.000000798 in Epoch 2745
Epoch 2775
Epoch 2775, Loss: 0.000001173, Improvement: -0.000000009, Best Loss: 0.000000798 in Epoch 2745
Epoch 2776
Epoch 2776, Loss: 0.000001168, Improvement: -0.000000005, Best Loss: 0.000000798 in Epoch 2745
Epoch 2777
Epoch 2777, Loss: 0.000001254, Improvement: 0.000000086, Best Loss: 0.000000798 in Epoch 2745
Epoch 2778
Epoch 2778, Loss: 0.000001226, Improvement: -0.000000028, Best Loss: 0.000000798 in Epoch 2745
Epoch 2779
Epoch 2779, Loss: 0.000001213, Improvement: -0.000000013, Best Loss: 0.000000798 in Epoch 2745
Epoch 2780
Epoch 2780, Loss: 0.000001158, Improvement: -0.000000055, Best Loss: 0.000000798 in Epoch 2745
Epoch 2781
Epoch 2781, Loss: 0.000001290, Improvement: 0.000000131, Best Loss: 0.000000798 in Epoch 2745
Epoch 2782
Epoch 2782, Loss: 0.000001388, Improvement: 0.000000098, Best Loss: 0.000000798 in Epoch 2745
Epoch 2783
Epoch 2783, Loss: 0.000001454, Improvement: 0.000000066, Best Loss: 0.000000798 in Epoch 2745
Epoch 2784
Epoch 2784, Loss: 0.000003330, Improvement: 0.000001876, Best Loss: 0.000000798 in Epoch 2745
Epoch 2785
Epoch 2785, Loss: 0.000002434, Improvement: -0.000000896, Best Loss: 0.000000798 in Epoch 2745
Epoch 2786
Epoch 2786, Loss: 0.000004581, Improvement: 0.000002147, Best Loss: 0.000000798 in Epoch 2745
Epoch 2787
Epoch 2787, Loss: 0.000002709, Improvement: -0.000001872, Best Loss: 0.000000798 in Epoch 2745
Epoch 2788
Epoch 2788, Loss: 0.000001874, Improvement: -0.000000836, Best Loss: 0.000000798 in Epoch 2745
Epoch 2789
Epoch 2789, Loss: 0.000002448, Improvement: 0.000000574, Best Loss: 0.000000798 in Epoch 2745
Epoch 2790
Epoch 2790, Loss: 0.000005029, Improvement: 0.000002582, Best Loss: 0.000000798 in Epoch 2745
Epoch 2791
Epoch 2791, Loss: 0.000004091, Improvement: -0.000000938, Best Loss: 0.000000798 in Epoch 2745
Epoch 2792
Epoch 2792, Loss: 0.000003593, Improvement: -0.000000498, Best Loss: 0.000000798 in Epoch 2745
Epoch 2793
Epoch 2793, Loss: 0.000017708, Improvement: 0.000014116, Best Loss: 0.000000798 in Epoch 2745
Epoch 2794
Epoch 2794, Loss: 0.000016343, Improvement: -0.000001365, Best Loss: 0.000000798 in Epoch 2745
Epoch 2795
Epoch 2795, Loss: 0.000010489, Improvement: -0.000005854, Best Loss: 0.000000798 in Epoch 2745
Epoch 2796
Epoch 2796, Loss: 0.000021264, Improvement: 0.000010775, Best Loss: 0.000000798 in Epoch 2745
Epoch 2797
Epoch 2797, Loss: 0.000009644, Improvement: -0.000011620, Best Loss: 0.000000798 in Epoch 2745
Epoch 2798
Epoch 2798, Loss: 0.000005555, Improvement: -0.000004089, Best Loss: 0.000000798 in Epoch 2745
Epoch 2799
Epoch 2799, Loss: 0.000005154, Improvement: -0.000000401, Best Loss: 0.000000798 in Epoch 2745
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.000004864, Improvement: -0.000000290, Best Loss: 0.000000798 in Epoch 2745
Epoch 2801
Epoch 2801, Loss: 0.000005236, Improvement: 0.000000372, Best Loss: 0.000000798 in Epoch 2745
Epoch 2802
Epoch 2802, Loss: 0.000003810, Improvement: -0.000001426, Best Loss: 0.000000798 in Epoch 2745
Epoch 2803
Epoch 2803, Loss: 0.000013070, Improvement: 0.000009261, Best Loss: 0.000000798 in Epoch 2745
Epoch 2804
Epoch 2804, Loss: 0.000032189, Improvement: 0.000019119, Best Loss: 0.000000798 in Epoch 2745
Epoch 2805
Epoch 2805, Loss: 0.000025314, Improvement: -0.000006876, Best Loss: 0.000000798 in Epoch 2745
Epoch 2806
Epoch 2806, Loss: 0.000008498, Improvement: -0.000016815, Best Loss: 0.000000798 in Epoch 2745
Epoch 2807
Epoch 2807, Loss: 0.000005928, Improvement: -0.000002570, Best Loss: 0.000000798 in Epoch 2745
Epoch 2808
Epoch 2808, Loss: 0.000003867, Improvement: -0.000002061, Best Loss: 0.000000798 in Epoch 2745
Epoch 2809
Epoch 2809, Loss: 0.000002509, Improvement: -0.000001358, Best Loss: 0.000000798 in Epoch 2745
Epoch 2810
Epoch 2810, Loss: 0.000001877, Improvement: -0.000000632, Best Loss: 0.000000798 in Epoch 2745
Epoch 2811
Epoch 2811, Loss: 0.000001460, Improvement: -0.000000417, Best Loss: 0.000000798 in Epoch 2745
Epoch 2812
Epoch 2812, Loss: 0.000001341, Improvement: -0.000000118, Best Loss: 0.000000798 in Epoch 2745
Epoch 2813
Epoch 2813, Loss: 0.000001239, Improvement: -0.000000103, Best Loss: 0.000000798 in Epoch 2745
Epoch 2814
Epoch 2814, Loss: 0.000001233, Improvement: -0.000000006, Best Loss: 0.000000798 in Epoch 2745
Epoch 2815
Epoch 2815, Loss: 0.000001226, Improvement: -0.000000006, Best Loss: 0.000000798 in Epoch 2745
Epoch 2816
Epoch 2816, Loss: 0.000001200, Improvement: -0.000000026, Best Loss: 0.000000798 in Epoch 2745
Epoch 2817
Epoch 2817, Loss: 0.000001254, Improvement: 0.000000054, Best Loss: 0.000000798 in Epoch 2745
Epoch 2818
Epoch 2818, Loss: 0.000001205, Improvement: -0.000000049, Best Loss: 0.000000798 in Epoch 2745
Epoch 2819
A best model at epoch 2819 has been saved with training error 0.000000735.
Epoch 2819, Loss: 0.000001225, Improvement: 0.000000020, Best Loss: 0.000000735 in Epoch 2819
Epoch 2820
Epoch 2820, Loss: 0.000001628, Improvement: 0.000000403, Best Loss: 0.000000735 in Epoch 2819
Epoch 2821
Epoch 2821, Loss: 0.000001847, Improvement: 0.000000219, Best Loss: 0.000000735 in Epoch 2819
Epoch 2822
Epoch 2822, Loss: 0.000002526, Improvement: 0.000000679, Best Loss: 0.000000735 in Epoch 2819
Epoch 2823
Epoch 2823, Loss: 0.000002791, Improvement: 0.000000265, Best Loss: 0.000000735 in Epoch 2819
Epoch 2824
Epoch 2824, Loss: 0.000003210, Improvement: 0.000000419, Best Loss: 0.000000735 in Epoch 2819
Epoch 2825
Epoch 2825, Loss: 0.000007503, Improvement: 0.000004292, Best Loss: 0.000000735 in Epoch 2819
Epoch 2826
Epoch 2826, Loss: 0.000008977, Improvement: 0.000001474, Best Loss: 0.000000735 in Epoch 2819
Epoch 2827
Epoch 2827, Loss: 0.000008524, Improvement: -0.000000453, Best Loss: 0.000000735 in Epoch 2819
Epoch 2828
Epoch 2828, Loss: 0.000007419, Improvement: -0.000001105, Best Loss: 0.000000735 in Epoch 2819
Epoch 2829
Epoch 2829, Loss: 0.000007023, Improvement: -0.000000396, Best Loss: 0.000000735 in Epoch 2819
Epoch 2830
Epoch 2830, Loss: 0.000041728, Improvement: 0.000034706, Best Loss: 0.000000735 in Epoch 2819
Epoch 2831
Epoch 2831, Loss: 0.000036784, Improvement: -0.000004944, Best Loss: 0.000000735 in Epoch 2819
Epoch 2832
Epoch 2832, Loss: 0.000034483, Improvement: -0.000002301, Best Loss: 0.000000735 in Epoch 2819
Epoch 2833
Epoch 2833, Loss: 0.000014246, Improvement: -0.000020236, Best Loss: 0.000000735 in Epoch 2819
Epoch 2834
Epoch 2834, Loss: 0.000008426, Improvement: -0.000005820, Best Loss: 0.000000735 in Epoch 2819
Epoch 2835
Epoch 2835, Loss: 0.000003954, Improvement: -0.000004472, Best Loss: 0.000000735 in Epoch 2819
Epoch 2836
Epoch 2836, Loss: 0.000002099, Improvement: -0.000001855, Best Loss: 0.000000735 in Epoch 2819
Epoch 2837
Epoch 2837, Loss: 0.000001861, Improvement: -0.000000238, Best Loss: 0.000000735 in Epoch 2819
Epoch 2838
Epoch 2838, Loss: 0.000001534, Improvement: -0.000000327, Best Loss: 0.000000735 in Epoch 2819
Epoch 2839
Epoch 2839, Loss: 0.000001419, Improvement: -0.000000115, Best Loss: 0.000000735 in Epoch 2819
Epoch 2840
Epoch 2840, Loss: 0.000001405, Improvement: -0.000000014, Best Loss: 0.000000735 in Epoch 2819
Epoch 2841
Epoch 2841, Loss: 0.000001357, Improvement: -0.000000048, Best Loss: 0.000000735 in Epoch 2819
Epoch 2842
Epoch 2842, Loss: 0.000001334, Improvement: -0.000000023, Best Loss: 0.000000735 in Epoch 2819
Epoch 2843
Epoch 2843, Loss: 0.000001205, Improvement: -0.000000129, Best Loss: 0.000000735 in Epoch 2819
Epoch 2844
Epoch 2844, Loss: 0.000001256, Improvement: 0.000000051, Best Loss: 0.000000735 in Epoch 2819
Epoch 2845
Epoch 2845, Loss: 0.000001364, Improvement: 0.000000108, Best Loss: 0.000000735 in Epoch 2819
Epoch 2846
Epoch 2846, Loss: 0.000001375, Improvement: 0.000000011, Best Loss: 0.000000735 in Epoch 2819
Epoch 2847
Epoch 2847, Loss: 0.000001515, Improvement: 0.000000140, Best Loss: 0.000000735 in Epoch 2819
Epoch 2848
Epoch 2848, Loss: 0.000002255, Improvement: 0.000000739, Best Loss: 0.000000735 in Epoch 2819
Epoch 2849
Epoch 2849, Loss: 0.000001997, Improvement: -0.000000258, Best Loss: 0.000000735 in Epoch 2819
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.000001467, Improvement: -0.000000530, Best Loss: 0.000000735 in Epoch 2819
Epoch 2851
Epoch 2851, Loss: 0.000001277, Improvement: -0.000000190, Best Loss: 0.000000735 in Epoch 2819
Epoch 2852
Epoch 2852, Loss: 0.000001595, Improvement: 0.000000318, Best Loss: 0.000000735 in Epoch 2819
Epoch 2853
Epoch 2853, Loss: 0.000001811, Improvement: 0.000000216, Best Loss: 0.000000735 in Epoch 2819
Epoch 2854
Epoch 2854, Loss: 0.000001672, Improvement: -0.000000139, Best Loss: 0.000000735 in Epoch 2819
Epoch 2855
Epoch 2855, Loss: 0.000002605, Improvement: 0.000000933, Best Loss: 0.000000735 in Epoch 2819
Epoch 2856
Epoch 2856, Loss: 0.000004837, Improvement: 0.000002232, Best Loss: 0.000000735 in Epoch 2819
Epoch 2857
Epoch 2857, Loss: 0.000003172, Improvement: -0.000001665, Best Loss: 0.000000735 in Epoch 2819
Epoch 2858
Epoch 2858, Loss: 0.000002803, Improvement: -0.000000370, Best Loss: 0.000000735 in Epoch 2819
Epoch 2859
Epoch 2859, Loss: 0.000002216, Improvement: -0.000000586, Best Loss: 0.000000735 in Epoch 2819
Epoch 2860
Epoch 2860, Loss: 0.000002860, Improvement: 0.000000644, Best Loss: 0.000000735 in Epoch 2819
Epoch 2861
Epoch 2861, Loss: 0.000002361, Improvement: -0.000000499, Best Loss: 0.000000735 in Epoch 2819
Epoch 2862
Epoch 2862, Loss: 0.000002250, Improvement: -0.000000112, Best Loss: 0.000000735 in Epoch 2819
Epoch 2863
Epoch 2863, Loss: 0.000002087, Improvement: -0.000000162, Best Loss: 0.000000735 in Epoch 2819
Epoch 2864
Epoch 2864, Loss: 0.000002688, Improvement: 0.000000600, Best Loss: 0.000000735 in Epoch 2819
Epoch 2865
Epoch 2865, Loss: 0.000003246, Improvement: 0.000000558, Best Loss: 0.000000735 in Epoch 2819
Epoch 2866
Epoch 2866, Loss: 0.000002865, Improvement: -0.000000381, Best Loss: 0.000000735 in Epoch 2819
Epoch 2867
Epoch 2867, Loss: 0.000007623, Improvement: 0.000004758, Best Loss: 0.000000735 in Epoch 2819
Epoch 2868
Epoch 2868, Loss: 0.000024349, Improvement: 0.000016726, Best Loss: 0.000000735 in Epoch 2819
Epoch 2869
Epoch 2869, Loss: 0.000025912, Improvement: 0.000001563, Best Loss: 0.000000735 in Epoch 2819
Epoch 2870
Epoch 2870, Loss: 0.000013637, Improvement: -0.000012276, Best Loss: 0.000000735 in Epoch 2819
Epoch 2871
Epoch 2871, Loss: 0.000010564, Improvement: -0.000003073, Best Loss: 0.000000735 in Epoch 2819
Epoch 2872
Epoch 2872, Loss: 0.000004272, Improvement: -0.000006292, Best Loss: 0.000000735 in Epoch 2819
Epoch 2873
Epoch 2873, Loss: 0.000004212, Improvement: -0.000000060, Best Loss: 0.000000735 in Epoch 2819
Epoch 2874
Epoch 2874, Loss: 0.000005000, Improvement: 0.000000788, Best Loss: 0.000000735 in Epoch 2819
Epoch 2875
Epoch 2875, Loss: 0.000005587, Improvement: 0.000000587, Best Loss: 0.000000735 in Epoch 2819
Epoch 2876
Epoch 2876, Loss: 0.000003384, Improvement: -0.000002203, Best Loss: 0.000000735 in Epoch 2819
Epoch 2877
Epoch 2877, Loss: 0.000003459, Improvement: 0.000000075, Best Loss: 0.000000735 in Epoch 2819
Epoch 2878
Epoch 2878, Loss: 0.000004475, Improvement: 0.000001017, Best Loss: 0.000000735 in Epoch 2819
Epoch 2879
Epoch 2879, Loss: 0.000005244, Improvement: 0.000000769, Best Loss: 0.000000735 in Epoch 2819
Epoch 2880
Epoch 2880, Loss: 0.000003940, Improvement: -0.000001304, Best Loss: 0.000000735 in Epoch 2819
Epoch 2881
Epoch 2881, Loss: 0.000005424, Improvement: 0.000001484, Best Loss: 0.000000735 in Epoch 2819
Epoch 2882
Epoch 2882, Loss: 0.000004993, Improvement: -0.000000430, Best Loss: 0.000000735 in Epoch 2819
Epoch 2883
Epoch 2883, Loss: 0.000017251, Improvement: 0.000012257, Best Loss: 0.000000735 in Epoch 2819
Epoch 2884
Epoch 2884, Loss: 0.000020523, Improvement: 0.000003273, Best Loss: 0.000000735 in Epoch 2819
Epoch 2885
Epoch 2885, Loss: 0.000029800, Improvement: 0.000009277, Best Loss: 0.000000735 in Epoch 2819
Epoch 2886
Epoch 2886, Loss: 0.000018030, Improvement: -0.000011770, Best Loss: 0.000000735 in Epoch 2819
Epoch 2887
Epoch 2887, Loss: 0.000007435, Improvement: -0.000010595, Best Loss: 0.000000735 in Epoch 2819
Epoch 2888
Epoch 2888, Loss: 0.000004749, Improvement: -0.000002686, Best Loss: 0.000000735 in Epoch 2819
Epoch 2889
Epoch 2889, Loss: 0.000002379, Improvement: -0.000002369, Best Loss: 0.000000735 in Epoch 2819
Epoch 2890
Epoch 2890, Loss: 0.000002424, Improvement: 0.000000044, Best Loss: 0.000000735 in Epoch 2819
Epoch 2891
Epoch 2891, Loss: 0.000001799, Improvement: -0.000000625, Best Loss: 0.000000735 in Epoch 2819
Epoch 2892
Epoch 2892, Loss: 0.000001722, Improvement: -0.000000076, Best Loss: 0.000000735 in Epoch 2819
Epoch 2893
Epoch 2893, Loss: 0.000001706, Improvement: -0.000000017, Best Loss: 0.000000735 in Epoch 2819
Epoch 2894
Epoch 2894, Loss: 0.000001644, Improvement: -0.000000062, Best Loss: 0.000000735 in Epoch 2819
Epoch 2895
Epoch 2895, Loss: 0.000002005, Improvement: 0.000000362, Best Loss: 0.000000735 in Epoch 2819
Epoch 2896
Epoch 2896, Loss: 0.000001439, Improvement: -0.000000567, Best Loss: 0.000000735 in Epoch 2819
Epoch 2897
Epoch 2897, Loss: 0.000001933, Improvement: 0.000000494, Best Loss: 0.000000735 in Epoch 2819
Epoch 2898
Epoch 2898, Loss: 0.000003597, Improvement: 0.000001665, Best Loss: 0.000000735 in Epoch 2819
Epoch 2899
Epoch 2899, Loss: 0.000006712, Improvement: 0.000003115, Best Loss: 0.000000735 in Epoch 2819
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.000005848, Improvement: -0.000000864, Best Loss: 0.000000735 in Epoch 2819
Epoch 2901
Epoch 2901, Loss: 0.000020075, Improvement: 0.000014227, Best Loss: 0.000000735 in Epoch 2819
Epoch 2902
Epoch 2902, Loss: 0.000014029, Improvement: -0.000006046, Best Loss: 0.000000735 in Epoch 2819
Epoch 2903
Epoch 2903, Loss: 0.000008737, Improvement: -0.000005292, Best Loss: 0.000000735 in Epoch 2819
Epoch 2904
Epoch 2904, Loss: 0.000005138, Improvement: -0.000003599, Best Loss: 0.000000735 in Epoch 2819
Epoch 2905
Epoch 2905, Loss: 0.000005287, Improvement: 0.000000150, Best Loss: 0.000000735 in Epoch 2819
Epoch 2906
Epoch 2906, Loss: 0.000002898, Improvement: -0.000002390, Best Loss: 0.000000735 in Epoch 2819
Epoch 2907
Epoch 2907, Loss: 0.000002939, Improvement: 0.000000041, Best Loss: 0.000000735 in Epoch 2819
Epoch 2908
Epoch 2908, Loss: 0.000005390, Improvement: 0.000002452, Best Loss: 0.000000735 in Epoch 2819
Epoch 2909
Epoch 2909, Loss: 0.000009028, Improvement: 0.000003638, Best Loss: 0.000000735 in Epoch 2819
Epoch 2910
Epoch 2910, Loss: 0.000006726, Improvement: -0.000002302, Best Loss: 0.000000735 in Epoch 2819
Epoch 2911
Epoch 2911, Loss: 0.000006470, Improvement: -0.000000257, Best Loss: 0.000000735 in Epoch 2819
Epoch 2912
Epoch 2912, Loss: 0.000012683, Improvement: 0.000006214, Best Loss: 0.000000735 in Epoch 2819
Epoch 2913
Epoch 2913, Loss: 0.000017108, Improvement: 0.000004424, Best Loss: 0.000000735 in Epoch 2819
Epoch 2914
Epoch 2914, Loss: 0.000011805, Improvement: -0.000005302, Best Loss: 0.000000735 in Epoch 2819
Epoch 2915
Epoch 2915, Loss: 0.000005007, Improvement: -0.000006799, Best Loss: 0.000000735 in Epoch 2819
Epoch 2916
Epoch 2916, Loss: 0.000003604, Improvement: -0.000001402, Best Loss: 0.000000735 in Epoch 2819
Epoch 2917
Epoch 2917, Loss: 0.000002609, Improvement: -0.000000995, Best Loss: 0.000000735 in Epoch 2819
Epoch 2918
Epoch 2918, Loss: 0.000002583, Improvement: -0.000000026, Best Loss: 0.000000735 in Epoch 2819
Epoch 2919
Epoch 2919, Loss: 0.000002078, Improvement: -0.000000505, Best Loss: 0.000000735 in Epoch 2819
Epoch 2920
Epoch 2920, Loss: 0.000001842, Improvement: -0.000000236, Best Loss: 0.000000735 in Epoch 2819
Epoch 2921
Epoch 2921, Loss: 0.000001693, Improvement: -0.000000149, Best Loss: 0.000000735 in Epoch 2819
Epoch 2922
Epoch 2922, Loss: 0.000002211, Improvement: 0.000000519, Best Loss: 0.000000735 in Epoch 2819
Epoch 2923
Epoch 2923, Loss: 0.000002514, Improvement: 0.000000303, Best Loss: 0.000000735 in Epoch 2819
Epoch 2924
Epoch 2924, Loss: 0.000003149, Improvement: 0.000000635, Best Loss: 0.000000735 in Epoch 2819
Epoch 2925
Epoch 2925, Loss: 0.000002960, Improvement: -0.000000190, Best Loss: 0.000000735 in Epoch 2819
Epoch 2926
Epoch 2926, Loss: 0.000002593, Improvement: -0.000000366, Best Loss: 0.000000735 in Epoch 2819
Epoch 2927
Epoch 2927, Loss: 0.000003253, Improvement: 0.000000660, Best Loss: 0.000000735 in Epoch 2819
Epoch 2928
Epoch 2928, Loss: 0.000003707, Improvement: 0.000000454, Best Loss: 0.000000735 in Epoch 2819
Epoch 2929
Epoch 2929, Loss: 0.000007332, Improvement: 0.000003625, Best Loss: 0.000000735 in Epoch 2819
Epoch 2930
Epoch 2930, Loss: 0.000028111, Improvement: 0.000020779, Best Loss: 0.000000735 in Epoch 2819
Epoch 2931
Epoch 2931, Loss: 0.000021821, Improvement: -0.000006291, Best Loss: 0.000000735 in Epoch 2819
Epoch 2932
Epoch 2932, Loss: 0.000012445, Improvement: -0.000009375, Best Loss: 0.000000735 in Epoch 2819
Epoch 2933
Epoch 2933, Loss: 0.000010272, Improvement: -0.000002174, Best Loss: 0.000000735 in Epoch 2819
Epoch 2934
Epoch 2934, Loss: 0.000012783, Improvement: 0.000002512, Best Loss: 0.000000735 in Epoch 2819
Epoch 2935
Epoch 2935, Loss: 0.000009724, Improvement: -0.000003059, Best Loss: 0.000000735 in Epoch 2819
Epoch 2936
Epoch 2936, Loss: 0.000004352, Improvement: -0.000005372, Best Loss: 0.000000735 in Epoch 2819
Epoch 2937
Epoch 2937, Loss: 0.000002624, Improvement: -0.000001729, Best Loss: 0.000000735 in Epoch 2819
Epoch 2938
Epoch 2938, Loss: 0.000002040, Improvement: -0.000000584, Best Loss: 0.000000735 in Epoch 2819
Epoch 2939
Epoch 2939, Loss: 0.000003938, Improvement: 0.000001898, Best Loss: 0.000000735 in Epoch 2819
Epoch 2940
Epoch 2940, Loss: 0.000009501, Improvement: 0.000005563, Best Loss: 0.000000735 in Epoch 2819
Epoch 2941
Epoch 2941, Loss: 0.000016659, Improvement: 0.000007158, Best Loss: 0.000000735 in Epoch 2819
Epoch 2942
Epoch 2942, Loss: 0.000011135, Improvement: -0.000005523, Best Loss: 0.000000735 in Epoch 2819
Epoch 2943
Epoch 2943, Loss: 0.000006220, Improvement: -0.000004915, Best Loss: 0.000000735 in Epoch 2819
Epoch 2944
Epoch 2944, Loss: 0.000003754, Improvement: -0.000002466, Best Loss: 0.000000735 in Epoch 2819
Epoch 2945
Epoch 2945, Loss: 0.000003217, Improvement: -0.000000538, Best Loss: 0.000000735 in Epoch 2819
Epoch 2946
Epoch 2946, Loss: 0.000002412, Improvement: -0.000000805, Best Loss: 0.000000735 in Epoch 2819
Epoch 2947
Epoch 2947, Loss: 0.000001721, Improvement: -0.000000691, Best Loss: 0.000000735 in Epoch 2819
Epoch 2948
Epoch 2948, Loss: 0.000001323, Improvement: -0.000000397, Best Loss: 0.000000735 in Epoch 2819
Epoch 2949
Epoch 2949, Loss: 0.000001383, Improvement: 0.000000060, Best Loss: 0.000000735 in Epoch 2819
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.000001470, Improvement: 0.000000087, Best Loss: 0.000000735 in Epoch 2819
Epoch 2951
Epoch 2951, Loss: 0.000002607, Improvement: 0.000001138, Best Loss: 0.000000735 in Epoch 2819
Epoch 2952
Epoch 2952, Loss: 0.000013660, Improvement: 0.000011053, Best Loss: 0.000000735 in Epoch 2819
Epoch 2953
Epoch 2953, Loss: 0.000032297, Improvement: 0.000018636, Best Loss: 0.000000735 in Epoch 2819
Epoch 2954
Epoch 2954, Loss: 0.000014940, Improvement: -0.000017357, Best Loss: 0.000000735 in Epoch 2819
Epoch 2955
Epoch 2955, Loss: 0.000006290, Improvement: -0.000008649, Best Loss: 0.000000735 in Epoch 2819
Epoch 2956
Epoch 2956, Loss: 0.000003122, Improvement: -0.000003168, Best Loss: 0.000000735 in Epoch 2819
Epoch 2957
Epoch 2957, Loss: 0.000003911, Improvement: 0.000000789, Best Loss: 0.000000735 in Epoch 2819
Epoch 2958
Epoch 2958, Loss: 0.000002468, Improvement: -0.000001444, Best Loss: 0.000000735 in Epoch 2819
Epoch 2959
Epoch 2959, Loss: 0.000001593, Improvement: -0.000000875, Best Loss: 0.000000735 in Epoch 2819
Epoch 2960
Epoch 2960, Loss: 0.000001784, Improvement: 0.000000191, Best Loss: 0.000000735 in Epoch 2819
Epoch 2961
Epoch 2961, Loss: 0.000001758, Improvement: -0.000000026, Best Loss: 0.000000735 in Epoch 2819
Epoch 2962
Epoch 2962, Loss: 0.000001356, Improvement: -0.000000402, Best Loss: 0.000000735 in Epoch 2819
Epoch 2963
Epoch 2963, Loss: 0.000001271, Improvement: -0.000000085, Best Loss: 0.000000735 in Epoch 2819
Epoch 2964
Epoch 2964, Loss: 0.000001380, Improvement: 0.000000109, Best Loss: 0.000000735 in Epoch 2819
Epoch 2965
Epoch 2965, Loss: 0.000001398, Improvement: 0.000000018, Best Loss: 0.000000735 in Epoch 2819
Epoch 2966
Epoch 2966, Loss: 0.000002080, Improvement: 0.000000682, Best Loss: 0.000000735 in Epoch 2819
Epoch 2967
Epoch 2967, Loss: 0.000003390, Improvement: 0.000001311, Best Loss: 0.000000735 in Epoch 2819
Epoch 2968
Epoch 2968, Loss: 0.000002800, Improvement: -0.000000590, Best Loss: 0.000000735 in Epoch 2819
Epoch 2969
Epoch 2969, Loss: 0.000001662, Improvement: -0.000001138, Best Loss: 0.000000735 in Epoch 2819
Epoch 2970
Epoch 2970, Loss: 0.000001269, Improvement: -0.000000393, Best Loss: 0.000000735 in Epoch 2819
Epoch 2971
Epoch 2971, Loss: 0.000001237, Improvement: -0.000000032, Best Loss: 0.000000735 in Epoch 2819
Epoch 2972
Epoch 2972, Loss: 0.000001603, Improvement: 0.000000366, Best Loss: 0.000000735 in Epoch 2819
Epoch 2973
Epoch 2973, Loss: 0.000002691, Improvement: 0.000001088, Best Loss: 0.000000735 in Epoch 2819
Epoch 2974
Epoch 2974, Loss: 0.000002556, Improvement: -0.000000136, Best Loss: 0.000000735 in Epoch 2819
Epoch 2975
Epoch 2975, Loss: 0.000002993, Improvement: 0.000000437, Best Loss: 0.000000735 in Epoch 2819
Epoch 2976
Epoch 2976, Loss: 0.000002180, Improvement: -0.000000813, Best Loss: 0.000000735 in Epoch 2819
Epoch 2977
Epoch 2977, Loss: 0.000009038, Improvement: 0.000006858, Best Loss: 0.000000735 in Epoch 2819
Epoch 2978
Epoch 2978, Loss: 0.000020262, Improvement: 0.000011224, Best Loss: 0.000000735 in Epoch 2819
Epoch 2979
Epoch 2979, Loss: 0.000012465, Improvement: -0.000007797, Best Loss: 0.000000735 in Epoch 2819
Epoch 2980
Epoch 2980, Loss: 0.000016923, Improvement: 0.000004459, Best Loss: 0.000000735 in Epoch 2819
Epoch 2981
Epoch 2981, Loss: 0.000024115, Improvement: 0.000007191, Best Loss: 0.000000735 in Epoch 2819
Epoch 2982
Epoch 2982, Loss: 0.000020527, Improvement: -0.000003587, Best Loss: 0.000000735 in Epoch 2819
Epoch 2983
Epoch 2983, Loss: 0.000006599, Improvement: -0.000013928, Best Loss: 0.000000735 in Epoch 2819
Epoch 2984
Epoch 2984, Loss: 0.000004346, Improvement: -0.000002253, Best Loss: 0.000000735 in Epoch 2819
Epoch 2985
Epoch 2985, Loss: 0.000005928, Improvement: 0.000001582, Best Loss: 0.000000735 in Epoch 2819
Epoch 2986
Epoch 2986, Loss: 0.000008162, Improvement: 0.000002234, Best Loss: 0.000000735 in Epoch 2819
Epoch 2987
Epoch 2987, Loss: 0.000004420, Improvement: -0.000003743, Best Loss: 0.000000735 in Epoch 2819
Epoch 2988
Epoch 2988, Loss: 0.000002460, Improvement: -0.000001960, Best Loss: 0.000000735 in Epoch 2819
Epoch 2989
Epoch 2989, Loss: 0.000001538, Improvement: -0.000000922, Best Loss: 0.000000735 in Epoch 2819
Epoch 2990
Epoch 2990, Loss: 0.000001544, Improvement: 0.000000006, Best Loss: 0.000000735 in Epoch 2819
Epoch 2991
Epoch 2991, Loss: 0.000001681, Improvement: 0.000000137, Best Loss: 0.000000735 in Epoch 2819
Epoch 2992
Epoch 2992, Loss: 0.000003575, Improvement: 0.000001894, Best Loss: 0.000000735 in Epoch 2819
Epoch 2993
Epoch 2993, Loss: 0.000005238, Improvement: 0.000001663, Best Loss: 0.000000735 in Epoch 2819
Epoch 2994
Epoch 2994, Loss: 0.000004471, Improvement: -0.000000767, Best Loss: 0.000000735 in Epoch 2819
Epoch 2995
Epoch 2995, Loss: 0.000002608, Improvement: -0.000001863, Best Loss: 0.000000735 in Epoch 2819
Epoch 2996
Epoch 2996, Loss: 0.000002686, Improvement: 0.000000079, Best Loss: 0.000000735 in Epoch 2819
Epoch 2997
Epoch 2997, Loss: 0.000001946, Improvement: -0.000000740, Best Loss: 0.000000735 in Epoch 2819
Epoch 2998
Epoch 2998, Loss: 0.000001507, Improvement: -0.000000439, Best Loss: 0.000000735 in Epoch 2819
Epoch 2999
Epoch 2999, Loss: 0.000001436, Improvement: -0.000000071, Best Loss: 0.000000735 in Epoch 2819
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.000001612, Improvement: 0.000000176, Best Loss: 0.000000735 in Epoch 2819
