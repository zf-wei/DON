/people/weiz828/.conda/envs/test/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/people/weiz828/.conda/envs/test/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
The dimension of y_tensor is torch.Size([5000, 2]).
The dimension of y_expanded is torch.Size([500, 5000, 2]) after expanding.
The dimensions of the initial conditions are: (500, 50)
The dimensions of the solutions are: (500, 100, 50)
The dimension of u_tensor is torch.Size([500, 50]).
The dimension of u_expanded is torch.Size([500, 5000, 50]) after expanding.
The loaded solution dataset has dimension (500, 100, 50),
	 while the arranged linearized dataset has dimension (500, 5000).
The dimension of s_tensor is torch.Size([500, 5000]).
The dimension of s_expanded is torch.Size([500, 5000, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.022380449.
A best model at epoch 1 has been saved with training error 0.018901497.
A best model at epoch 1 has been saved with training error 0.018365612.
A best model at epoch 1 has been saved with training error 0.017963264.
A best model at epoch 1 has been saved with training error 0.017879587.
A best model at epoch 1 has been saved with training error 0.016546192.
A best model at epoch 1 has been saved with training error 0.007068422.
A best model at epoch 1 has been saved with training error 0.005961511.
Epoch 1, Loss: 0.016331968, Improvement: 0.016331968, Best Loss: 0.005961511 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.005093758.
Epoch 2, Loss: 0.010800416, Improvement: -0.005531552, Best Loss: 0.005093758 in Epoch 2
Epoch 3
Epoch 3, Loss: 0.010351903, Improvement: -0.000448513, Best Loss: 0.005093758 in Epoch 2
Epoch 4
Epoch 4, Loss: 0.010034602, Improvement: -0.000317301, Best Loss: 0.005093758 in Epoch 2
Epoch 5
Epoch 5, Loss: 0.009608086, Improvement: -0.000426516, Best Loss: 0.005093758 in Epoch 2
Epoch 6
Epoch 6, Loss: 0.008707879, Improvement: -0.000900207, Best Loss: 0.005093758 in Epoch 2
Epoch 7
A best model at epoch 7 has been saved with training error 0.004779141.
A best model at epoch 7 has been saved with training error 0.004706402.
Epoch 7, Loss: 0.006918896, Improvement: -0.001788983, Best Loss: 0.004706402 in Epoch 7
Epoch 8
A best model at epoch 8 has been saved with training error 0.004058064.
Epoch 8, Loss: 0.006829487, Improvement: -0.000089409, Best Loss: 0.004058064 in Epoch 8
Epoch 9
A best model at epoch 9 has been saved with training error 0.003232434.
Epoch 9, Loss: 0.005836320, Improvement: -0.000993167, Best Loss: 0.003232434 in Epoch 9
Epoch 10
Epoch 10, Loss: 0.005432978, Improvement: -0.000403342, Best Loss: 0.003232434 in Epoch 9
Epoch 11
Epoch 11, Loss: 0.005347794, Improvement: -0.000085184, Best Loss: 0.003232434 in Epoch 9
Epoch 12
Epoch 12, Loss: 0.005423110, Improvement: 0.000075316, Best Loss: 0.003232434 in Epoch 9
Epoch 13
Epoch 13, Loss: 0.005019533, Improvement: -0.000403577, Best Loss: 0.003232434 in Epoch 9
Epoch 14
A best model at epoch 14 has been saved with training error 0.002273221.
Epoch 14, Loss: 0.004914882, Improvement: -0.000104651, Best Loss: 0.002273221 in Epoch 14
Epoch 15
Epoch 15, Loss: 0.004751186, Improvement: -0.000163696, Best Loss: 0.002273221 in Epoch 14
Epoch 16
Epoch 16, Loss: 0.004827239, Improvement: 0.000076053, Best Loss: 0.002273221 in Epoch 14
Epoch 17
Epoch 17, Loss: 0.004538229, Improvement: -0.000289010, Best Loss: 0.002273221 in Epoch 14
Epoch 18
Epoch 18, Loss: 0.004301667, Improvement: -0.000236562, Best Loss: 0.002273221 in Epoch 14
Epoch 19
Epoch 19, Loss: 0.004018068, Improvement: -0.000283599, Best Loss: 0.002273221 in Epoch 14
Epoch 20
Epoch 20, Loss: 0.004447352, Improvement: 0.000429284, Best Loss: 0.002273221 in Epoch 14
Epoch 21
Epoch 21, Loss: 0.004754474, Improvement: 0.000307121, Best Loss: 0.002273221 in Epoch 14
Epoch 22
Epoch 22, Loss: 0.004376075, Improvement: -0.000378399, Best Loss: 0.002273221 in Epoch 14
Epoch 23
Epoch 23, Loss: 0.004100022, Improvement: -0.000276053, Best Loss: 0.002273221 in Epoch 14
Epoch 24
A best model at epoch 24 has been saved with training error 0.002236331.
Epoch 24, Loss: 0.003414730, Improvement: -0.000685292, Best Loss: 0.002236331 in Epoch 24
Epoch 25
A best model at epoch 25 has been saved with training error 0.002128095.
A best model at epoch 25 has been saved with training error 0.001859226.
Epoch 25, Loss: 0.003086709, Improvement: -0.000328021, Best Loss: 0.001859226 in Epoch 25
Epoch 26
A best model at epoch 26 has been saved with training error 0.001710406.
A best model at epoch 26 has been saved with training error 0.001642848.
Epoch 26, Loss: 0.003042931, Improvement: -0.000043778, Best Loss: 0.001642848 in Epoch 26
Epoch 27
Epoch 27, Loss: 0.004363788, Improvement: 0.001320857, Best Loss: 0.001642848 in Epoch 26
Epoch 28
Epoch 28, Loss: 0.003745036, Improvement: -0.000618752, Best Loss: 0.001642848 in Epoch 26
Epoch 29
Epoch 29, Loss: 0.003256907, Improvement: -0.000488129, Best Loss: 0.001642848 in Epoch 26
Epoch 30
Epoch 30, Loss: 0.002773580, Improvement: -0.000483327, Best Loss: 0.001642848 in Epoch 26
Epoch 31
Epoch 31, Loss: 0.002568182, Improvement: -0.000205398, Best Loss: 0.001642848 in Epoch 26
Epoch 32
Epoch 32, Loss: 0.002510192, Improvement: -0.000057990, Best Loss: 0.001642848 in Epoch 26
Epoch 33
A best model at epoch 33 has been saved with training error 0.001594979.
Epoch 33, Loss: 0.002417533, Improvement: -0.000092659, Best Loss: 0.001594979 in Epoch 33
Epoch 34
Epoch 34, Loss: 0.004175171, Improvement: 0.001757638, Best Loss: 0.001594979 in Epoch 33
Epoch 35
Epoch 35, Loss: 0.003385636, Improvement: -0.000789535, Best Loss: 0.001594979 in Epoch 33
Epoch 36
Epoch 36, Loss: 0.002722496, Improvement: -0.000663140, Best Loss: 0.001594979 in Epoch 33
Epoch 37
A best model at epoch 37 has been saved with training error 0.001339616.
Epoch 37, Loss: 0.002414680, Improvement: -0.000307815, Best Loss: 0.001339616 in Epoch 37
Epoch 38
Epoch 38, Loss: 0.002275254, Improvement: -0.000139427, Best Loss: 0.001339616 in Epoch 37
Epoch 39
A best model at epoch 39 has been saved with training error 0.001176973.
Epoch 39, Loss: 0.002204184, Improvement: -0.000071070, Best Loss: 0.001176973 in Epoch 39
Epoch 40
Epoch 40, Loss: 0.002252240, Improvement: 0.000048056, Best Loss: 0.001176973 in Epoch 39
Epoch 41
Epoch 41, Loss: 0.002351057, Improvement: 0.000098817, Best Loss: 0.001176973 in Epoch 39
Epoch 42
Epoch 42, Loss: 0.002154092, Improvement: -0.000196965, Best Loss: 0.001176973 in Epoch 39
Epoch 43
Epoch 43, Loss: 0.002076253, Improvement: -0.000077840, Best Loss: 0.001176973 in Epoch 39
Epoch 44
Epoch 44, Loss: 0.002196018, Improvement: 0.000119765, Best Loss: 0.001176973 in Epoch 39
Epoch 45
Epoch 45, Loss: 0.003536301, Improvement: 0.001340283, Best Loss: 0.001176973 in Epoch 39
Epoch 46
Epoch 46, Loss: 0.002485494, Improvement: -0.001050806, Best Loss: 0.001176973 in Epoch 39
Epoch 47
Epoch 47, Loss: 0.002085688, Improvement: -0.000399807, Best Loss: 0.001176973 in Epoch 39
Epoch 48
A best model at epoch 48 has been saved with training error 0.000958358.
Epoch 48, Loss: 0.001976429, Improvement: -0.000109259, Best Loss: 0.000958358 in Epoch 48
Epoch 49
Epoch 49, Loss: 0.001936119, Improvement: -0.000040310, Best Loss: 0.000958358 in Epoch 48
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.001895724, Improvement: -0.000040395, Best Loss: 0.000958358 in Epoch 48
Epoch 51
A best model at epoch 51 has been saved with training error 0.000940846.
Epoch 51, Loss: 0.001826165, Improvement: -0.000069559, Best Loss: 0.000940846 in Epoch 51
Epoch 52
Epoch 52, Loss: 0.001744455, Improvement: -0.000081710, Best Loss: 0.000940846 in Epoch 51
Epoch 53
Epoch 53, Loss: 0.001859451, Improvement: 0.000114996, Best Loss: 0.000940846 in Epoch 51
Epoch 54
Epoch 54, Loss: 0.004075656, Improvement: 0.002216205, Best Loss: 0.000940846 in Epoch 51
Epoch 55
Epoch 55, Loss: 0.002642682, Improvement: -0.001432974, Best Loss: 0.000940846 in Epoch 51
Epoch 56
Epoch 56, Loss: 0.002110690, Improvement: -0.000531992, Best Loss: 0.000940846 in Epoch 51
Epoch 57
Epoch 57, Loss: 0.001872321, Improvement: -0.000238369, Best Loss: 0.000940846 in Epoch 51
Epoch 58
Epoch 58, Loss: 0.001950772, Improvement: 0.000078451, Best Loss: 0.000940846 in Epoch 51
Epoch 59
Epoch 59, Loss: 0.001817342, Improvement: -0.000133430, Best Loss: 0.000940846 in Epoch 51
Epoch 60
Epoch 60, Loss: 0.001681714, Improvement: -0.000135628, Best Loss: 0.000940846 in Epoch 51
Epoch 61
A best model at epoch 61 has been saved with training error 0.000923777.
Epoch 61, Loss: 0.001609963, Improvement: -0.000071751, Best Loss: 0.000923777 in Epoch 61
Epoch 62
Epoch 62, Loss: 0.001577124, Improvement: -0.000032839, Best Loss: 0.000923777 in Epoch 61
Epoch 63
A best model at epoch 63 has been saved with training error 0.000910515.
Epoch 63, Loss: 0.001528940, Improvement: -0.000048184, Best Loss: 0.000910515 in Epoch 63
Epoch 64
Epoch 64, Loss: 0.001569445, Improvement: 0.000040505, Best Loss: 0.000910515 in Epoch 63
Epoch 65
Epoch 65, Loss: 0.001578613, Improvement: 0.000009168, Best Loss: 0.000910515 in Epoch 63
Epoch 66
Epoch 66, Loss: 0.001718483, Improvement: 0.000139870, Best Loss: 0.000910515 in Epoch 63
Epoch 67
Epoch 67, Loss: 0.004778923, Improvement: 0.003060440, Best Loss: 0.000910515 in Epoch 63
Epoch 68
Epoch 68, Loss: 0.002830217, Improvement: -0.001948706, Best Loss: 0.000910515 in Epoch 63
Epoch 69
Epoch 69, Loss: 0.002006737, Improvement: -0.000823479, Best Loss: 0.000910515 in Epoch 63
Epoch 70
Epoch 70, Loss: 0.001709673, Improvement: -0.000297065, Best Loss: 0.000910515 in Epoch 63
Epoch 71
A best model at epoch 71 has been saved with training error 0.000662002.
Epoch 71, Loss: 0.001578506, Improvement: -0.000131167, Best Loss: 0.000662002 in Epoch 71
Epoch 72
Epoch 72, Loss: 0.001502909, Improvement: -0.000075597, Best Loss: 0.000662002 in Epoch 71
Epoch 73
Epoch 73, Loss: 0.001430366, Improvement: -0.000072542, Best Loss: 0.000662002 in Epoch 71
Epoch 74
Epoch 74, Loss: 0.001392215, Improvement: -0.000038151, Best Loss: 0.000662002 in Epoch 71
Epoch 75
Epoch 75, Loss: 0.001397073, Improvement: 0.000004858, Best Loss: 0.000662002 in Epoch 71
Epoch 76
Epoch 76, Loss: 0.001389378, Improvement: -0.000007695, Best Loss: 0.000662002 in Epoch 71
Epoch 77
Epoch 77, Loss: 0.001419121, Improvement: 0.000029743, Best Loss: 0.000662002 in Epoch 71
Epoch 78
Epoch 78, Loss: 0.001605264, Improvement: 0.000186143, Best Loss: 0.000662002 in Epoch 71
Epoch 79
Epoch 79, Loss: 0.001542105, Improvement: -0.000063159, Best Loss: 0.000662002 in Epoch 71
Epoch 80
Epoch 80, Loss: 0.001354155, Improvement: -0.000187950, Best Loss: 0.000662002 in Epoch 71
Epoch 81
Epoch 81, Loss: 0.001425758, Improvement: 0.000071602, Best Loss: 0.000662002 in Epoch 71
Epoch 82
Epoch 82, Loss: 0.001339849, Improvement: -0.000085909, Best Loss: 0.000662002 in Epoch 71
Epoch 83
Epoch 83, Loss: 0.001427169, Improvement: 0.000087321, Best Loss: 0.000662002 in Epoch 71
Epoch 84
Epoch 84, Loss: 0.002848893, Improvement: 0.001421724, Best Loss: 0.000662002 in Epoch 71
Epoch 85
Epoch 85, Loss: 0.002240733, Improvement: -0.000608160, Best Loss: 0.000662002 in Epoch 71
Epoch 86
Epoch 86, Loss: 0.001635999, Improvement: -0.000604734, Best Loss: 0.000662002 in Epoch 71
Epoch 87
Epoch 87, Loss: 0.001450778, Improvement: -0.000185221, Best Loss: 0.000662002 in Epoch 71
Epoch 88
Epoch 88, Loss: 0.001332353, Improvement: -0.000118424, Best Loss: 0.000662002 in Epoch 71
Epoch 89
Epoch 89, Loss: 0.001217598, Improvement: -0.000114756, Best Loss: 0.000662002 in Epoch 71
Epoch 90
Epoch 90, Loss: 0.001145779, Improvement: -0.000071818, Best Loss: 0.000662002 in Epoch 71
Epoch 91
A best model at epoch 91 has been saved with training error 0.000627634.
Epoch 91, Loss: 0.001122447, Improvement: -0.000023332, Best Loss: 0.000627634 in Epoch 91
Epoch 92
Epoch 92, Loss: 0.001185882, Improvement: 0.000063436, Best Loss: 0.000627634 in Epoch 91
Epoch 93
Epoch 93, Loss: 0.001544264, Improvement: 0.000358382, Best Loss: 0.000627634 in Epoch 91
Epoch 94
Epoch 94, Loss: 0.001204808, Improvement: -0.000339456, Best Loss: 0.000627634 in Epoch 91
Epoch 95
A best model at epoch 95 has been saved with training error 0.000599722.
Epoch 95, Loss: 0.001273490, Improvement: 0.000068682, Best Loss: 0.000599722 in Epoch 95
Epoch 96
Epoch 96, Loss: 0.002684365, Improvement: 0.001410875, Best Loss: 0.000599722 in Epoch 95
Epoch 97
Epoch 97, Loss: 0.001722266, Improvement: -0.000962100, Best Loss: 0.000599722 in Epoch 95
Epoch 98
Epoch 98, Loss: 0.001322037, Improvement: -0.000400229, Best Loss: 0.000599722 in Epoch 95
Epoch 99
Epoch 99, Loss: 0.001153299, Improvement: -0.000168738, Best Loss: 0.000599722 in Epoch 95
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.001102523, Improvement: -0.000050776, Best Loss: 0.000599722 in Epoch 95
Epoch 101
Epoch 101, Loss: 0.001047891, Improvement: -0.000054632, Best Loss: 0.000599722 in Epoch 95
Epoch 102
Epoch 102, Loss: 0.000931674, Improvement: -0.000116217, Best Loss: 0.000599722 in Epoch 95
Epoch 103
A best model at epoch 103 has been saved with training error 0.000439312.
Epoch 103, Loss: 0.000930649, Improvement: -0.000001025, Best Loss: 0.000439312 in Epoch 103
Epoch 104
Epoch 104, Loss: 0.002641619, Improvement: 0.001710970, Best Loss: 0.000439312 in Epoch 103
Epoch 105
Epoch 105, Loss: 0.002317341, Improvement: -0.000324278, Best Loss: 0.000439312 in Epoch 103
Epoch 106
Epoch 106, Loss: 0.001424941, Improvement: -0.000892400, Best Loss: 0.000439312 in Epoch 103
Epoch 107
Epoch 107, Loss: 0.001144931, Improvement: -0.000280010, Best Loss: 0.000439312 in Epoch 103
Epoch 108
Epoch 108, Loss: 0.001078869, Improvement: -0.000066061, Best Loss: 0.000439312 in Epoch 103
Epoch 109
Epoch 109, Loss: 0.001097968, Improvement: 0.000019098, Best Loss: 0.000439312 in Epoch 103
Epoch 110
Epoch 110, Loss: 0.000990964, Improvement: -0.000107004, Best Loss: 0.000439312 in Epoch 103
Epoch 111
Epoch 111, Loss: 0.000882695, Improvement: -0.000108269, Best Loss: 0.000439312 in Epoch 103
Epoch 112
Epoch 112, Loss: 0.000819225, Improvement: -0.000063469, Best Loss: 0.000439312 in Epoch 103
Epoch 113
Epoch 113, Loss: 0.000838331, Improvement: 0.000019106, Best Loss: 0.000439312 in Epoch 103
Epoch 114
Epoch 114, Loss: 0.000780866, Improvement: -0.000057466, Best Loss: 0.000439312 in Epoch 103
Epoch 115
A best model at epoch 115 has been saved with training error 0.000427398.
Epoch 115, Loss: 0.000914154, Improvement: 0.000133289, Best Loss: 0.000427398 in Epoch 115
Epoch 116
Epoch 116, Loss: 0.001859056, Improvement: 0.000944902, Best Loss: 0.000427398 in Epoch 115
Epoch 117
Epoch 117, Loss: 0.001181306, Improvement: -0.000677750, Best Loss: 0.000427398 in Epoch 115
Epoch 118
Epoch 118, Loss: 0.000892954, Improvement: -0.000288352, Best Loss: 0.000427398 in Epoch 115
Epoch 119
Epoch 119, Loss: 0.000797211, Improvement: -0.000095743, Best Loss: 0.000427398 in Epoch 115
Epoch 120
Epoch 120, Loss: 0.000717793, Improvement: -0.000079418, Best Loss: 0.000427398 in Epoch 115
Epoch 121
A best model at epoch 121 has been saved with training error 0.000415422.
Epoch 121, Loss: 0.000663521, Improvement: -0.000054272, Best Loss: 0.000415422 in Epoch 121
Epoch 122
Epoch 122, Loss: 0.000659048, Improvement: -0.000004474, Best Loss: 0.000415422 in Epoch 121
Epoch 123
Epoch 123, Loss: 0.001137716, Improvement: 0.000478669, Best Loss: 0.000415422 in Epoch 121
Epoch 124
Epoch 124, Loss: 0.000948647, Improvement: -0.000189070, Best Loss: 0.000415422 in Epoch 121
Epoch 125
A best model at epoch 125 has been saved with training error 0.000382549.
Epoch 125, Loss: 0.001039429, Improvement: 0.000090782, Best Loss: 0.000382549 in Epoch 125
Epoch 126
Epoch 126, Loss: 0.001049593, Improvement: 0.000010164, Best Loss: 0.000382549 in Epoch 125
Epoch 127
Epoch 127, Loss: 0.000822551, Improvement: -0.000227042, Best Loss: 0.000382549 in Epoch 125
Epoch 128
Epoch 128, Loss: 0.000722217, Improvement: -0.000100334, Best Loss: 0.000382549 in Epoch 125
Epoch 129
Epoch 129, Loss: 0.001392531, Improvement: 0.000670314, Best Loss: 0.000382549 in Epoch 125
Epoch 130
Epoch 130, Loss: 0.001180199, Improvement: -0.000212332, Best Loss: 0.000382549 in Epoch 125
Epoch 131
Epoch 131, Loss: 0.000750283, Improvement: -0.000429916, Best Loss: 0.000382549 in Epoch 125
Epoch 132
Epoch 132, Loss: 0.000642581, Improvement: -0.000107702, Best Loss: 0.000382549 in Epoch 125
Epoch 133
Epoch 133, Loss: 0.000598484, Improvement: -0.000044097, Best Loss: 0.000382549 in Epoch 125
Epoch 134
Epoch 134, Loss: 0.000991391, Improvement: 0.000392908, Best Loss: 0.000382549 in Epoch 125
Epoch 135
Epoch 135, Loss: 0.001427939, Improvement: 0.000436547, Best Loss: 0.000382549 in Epoch 125
Epoch 136
Epoch 136, Loss: 0.001233792, Improvement: -0.000194147, Best Loss: 0.000382549 in Epoch 125
Epoch 137
Epoch 137, Loss: 0.001146752, Improvement: -0.000087040, Best Loss: 0.000382549 in Epoch 125
Epoch 138
Epoch 138, Loss: 0.000802017, Improvement: -0.000344735, Best Loss: 0.000382549 in Epoch 125
Epoch 139
Epoch 139, Loss: 0.000654802, Improvement: -0.000147215, Best Loss: 0.000382549 in Epoch 125
Epoch 140
Epoch 140, Loss: 0.000582321, Improvement: -0.000072481, Best Loss: 0.000382549 in Epoch 125
Epoch 141
Epoch 141, Loss: 0.000541486, Improvement: -0.000040835, Best Loss: 0.000382549 in Epoch 125
Epoch 142
Epoch 142, Loss: 0.000541473, Improvement: -0.000000013, Best Loss: 0.000382549 in Epoch 125
Epoch 143
Epoch 143, Loss: 0.001941671, Improvement: 0.001400198, Best Loss: 0.000382549 in Epoch 125
Epoch 144
Epoch 144, Loss: 0.001827598, Improvement: -0.000114074, Best Loss: 0.000382549 in Epoch 125
Epoch 145
Epoch 145, Loss: 0.001328771, Improvement: -0.000498826, Best Loss: 0.000382549 in Epoch 125
Epoch 146
Epoch 146, Loss: 0.001048371, Improvement: -0.000280400, Best Loss: 0.000382549 in Epoch 125
Epoch 147
Epoch 147, Loss: 0.000833961, Improvement: -0.000214410, Best Loss: 0.000382549 in Epoch 125
Epoch 148
A best model at epoch 148 has been saved with training error 0.000362664.
Epoch 148, Loss: 0.000699203, Improvement: -0.000134759, Best Loss: 0.000362664 in Epoch 148
Epoch 149
Epoch 149, Loss: 0.000619101, Improvement: -0.000080102, Best Loss: 0.000362664 in Epoch 148
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.000595882, Improvement: -0.000023219, Best Loss: 0.000362664 in Epoch 148
Epoch 151
Epoch 151, Loss: 0.000587407, Improvement: -0.000008475, Best Loss: 0.000362664 in Epoch 148
Epoch 152
Epoch 152, Loss: 0.000874110, Improvement: 0.000286703, Best Loss: 0.000362664 in Epoch 148
Epoch 153
Epoch 153, Loss: 0.000902000, Improvement: 0.000027890, Best Loss: 0.000362664 in Epoch 148
Epoch 154
A best model at epoch 154 has been saved with training error 0.000324545.
Epoch 154, Loss: 0.000607517, Improvement: -0.000294483, Best Loss: 0.000324545 in Epoch 154
Epoch 155
Epoch 155, Loss: 0.000531381, Improvement: -0.000076136, Best Loss: 0.000324545 in Epoch 154
Epoch 156
A best model at epoch 156 has been saved with training error 0.000289811.
Epoch 156, Loss: 0.000522750, Improvement: -0.000008631, Best Loss: 0.000289811 in Epoch 156
Epoch 157
Epoch 157, Loss: 0.000480919, Improvement: -0.000041831, Best Loss: 0.000289811 in Epoch 156
Epoch 158
Epoch 158, Loss: 0.000511407, Improvement: 0.000030488, Best Loss: 0.000289811 in Epoch 156
Epoch 159
Epoch 159, Loss: 0.001108963, Improvement: 0.000597556, Best Loss: 0.000289811 in Epoch 156
Epoch 160
Epoch 160, Loss: 0.000812940, Improvement: -0.000296023, Best Loss: 0.000289811 in Epoch 156
Epoch 161
Epoch 161, Loss: 0.000559566, Improvement: -0.000253374, Best Loss: 0.000289811 in Epoch 156
Epoch 162
Epoch 162, Loss: 0.000475298, Improvement: -0.000084268, Best Loss: 0.000289811 in Epoch 156
Epoch 163
A best model at epoch 163 has been saved with training error 0.000248877.
Epoch 163, Loss: 0.000437244, Improvement: -0.000038054, Best Loss: 0.000248877 in Epoch 163
Epoch 164
A best model at epoch 164 has been saved with training error 0.000244247.
Epoch 164, Loss: 0.000424746, Improvement: -0.000012498, Best Loss: 0.000244247 in Epoch 164
Epoch 165
Epoch 165, Loss: 0.000791466, Improvement: 0.000366720, Best Loss: 0.000244247 in Epoch 164
Epoch 166
Epoch 166, Loss: 0.001363858, Improvement: 0.000572391, Best Loss: 0.000244247 in Epoch 164
Epoch 167
Epoch 167, Loss: 0.000932797, Improvement: -0.000431061, Best Loss: 0.000244247 in Epoch 164
Epoch 168
Epoch 168, Loss: 0.000600757, Improvement: -0.000332040, Best Loss: 0.000244247 in Epoch 164
Epoch 169
Epoch 169, Loss: 0.000480270, Improvement: -0.000120487, Best Loss: 0.000244247 in Epoch 164
Epoch 170
Epoch 170, Loss: 0.000437519, Improvement: -0.000042750, Best Loss: 0.000244247 in Epoch 164
Epoch 171
Epoch 171, Loss: 0.000435031, Improvement: -0.000002488, Best Loss: 0.000244247 in Epoch 164
Epoch 172
Epoch 172, Loss: 0.000624010, Improvement: 0.000188978, Best Loss: 0.000244247 in Epoch 164
Epoch 173
Epoch 173, Loss: 0.000861057, Improvement: 0.000237047, Best Loss: 0.000244247 in Epoch 164
Epoch 174
Epoch 174, Loss: 0.000749969, Improvement: -0.000111088, Best Loss: 0.000244247 in Epoch 164
Epoch 175
Epoch 175, Loss: 0.000660808, Improvement: -0.000089160, Best Loss: 0.000244247 in Epoch 164
Epoch 176
Epoch 176, Loss: 0.000586948, Improvement: -0.000073861, Best Loss: 0.000244247 in Epoch 164
Epoch 177
Epoch 177, Loss: 0.000672587, Improvement: 0.000085639, Best Loss: 0.000244247 in Epoch 164
Epoch 178
Epoch 178, Loss: 0.000564145, Improvement: -0.000108442, Best Loss: 0.000244247 in Epoch 164
Epoch 179
Epoch 179, Loss: 0.000838286, Improvement: 0.000274141, Best Loss: 0.000244247 in Epoch 164
Epoch 180
Epoch 180, Loss: 0.000625353, Improvement: -0.000212933, Best Loss: 0.000244247 in Epoch 164
Epoch 181
Epoch 181, Loss: 0.000483411, Improvement: -0.000141942, Best Loss: 0.000244247 in Epoch 164
Epoch 182
Epoch 182, Loss: 0.000420103, Improvement: -0.000063308, Best Loss: 0.000244247 in Epoch 164
Epoch 183
A best model at epoch 183 has been saved with training error 0.000239555.
A best model at epoch 183 has been saved with training error 0.000222435.
Epoch 183, Loss: 0.000389063, Improvement: -0.000031041, Best Loss: 0.000222435 in Epoch 183
Epoch 184
Epoch 184, Loss: 0.000403317, Improvement: 0.000014254, Best Loss: 0.000222435 in Epoch 183
Epoch 185
Epoch 185, Loss: 0.001009382, Improvement: 0.000606065, Best Loss: 0.000222435 in Epoch 183
Epoch 186
Epoch 186, Loss: 0.000619270, Improvement: -0.000390112, Best Loss: 0.000222435 in Epoch 183
Epoch 187
Epoch 187, Loss: 0.000511797, Improvement: -0.000107473, Best Loss: 0.000222435 in Epoch 183
Epoch 188
Epoch 188, Loss: 0.000428930, Improvement: -0.000082867, Best Loss: 0.000222435 in Epoch 183
Epoch 189
Epoch 189, Loss: 0.000395294, Improvement: -0.000033636, Best Loss: 0.000222435 in Epoch 183
Epoch 190
A best model at epoch 190 has been saved with training error 0.000218517.
Epoch 190, Loss: 0.000415940, Improvement: 0.000020646, Best Loss: 0.000218517 in Epoch 190
Epoch 191
Epoch 191, Loss: 0.000356464, Improvement: -0.000059476, Best Loss: 0.000218517 in Epoch 190
Epoch 192
A best model at epoch 192 has been saved with training error 0.000210854.
Epoch 192, Loss: 0.000349351, Improvement: -0.000007112, Best Loss: 0.000210854 in Epoch 192
Epoch 193
Epoch 193, Loss: 0.000334567, Improvement: -0.000014784, Best Loss: 0.000210854 in Epoch 192
Epoch 194
Epoch 194, Loss: 0.000335722, Improvement: 0.000001155, Best Loss: 0.000210854 in Epoch 192
Epoch 195
Epoch 195, Loss: 0.000354064, Improvement: 0.000018341, Best Loss: 0.000210854 in Epoch 192
Epoch 196
Epoch 196, Loss: 0.000377588, Improvement: 0.000023524, Best Loss: 0.000210854 in Epoch 192
Epoch 197
Epoch 197, Loss: 0.000438216, Improvement: 0.000060629, Best Loss: 0.000210854 in Epoch 192
Epoch 198
Epoch 198, Loss: 0.000422908, Improvement: -0.000015309, Best Loss: 0.000210854 in Epoch 192
Epoch 199
Epoch 199, Loss: 0.000366904, Improvement: -0.000056004, Best Loss: 0.000210854 in Epoch 192
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.000337879, Improvement: -0.000029025, Best Loss: 0.000210854 in Epoch 192
Epoch 201
Epoch 201, Loss: 0.000840941, Improvement: 0.000503062, Best Loss: 0.000210854 in Epoch 192
Epoch 202
Epoch 202, Loss: 0.000578894, Improvement: -0.000262047, Best Loss: 0.000210854 in Epoch 192
Epoch 203
Epoch 203, Loss: 0.000494741, Improvement: -0.000084152, Best Loss: 0.000210854 in Epoch 192
Epoch 204
Epoch 204, Loss: 0.000444364, Improvement: -0.000050378, Best Loss: 0.000210854 in Epoch 192
Epoch 205
Epoch 205, Loss: 0.000396561, Improvement: -0.000047802, Best Loss: 0.000210854 in Epoch 192
Epoch 206
Epoch 206, Loss: 0.000358350, Improvement: -0.000038211, Best Loss: 0.000210854 in Epoch 192
Epoch 207
Epoch 207, Loss: 0.000350227, Improvement: -0.000008123, Best Loss: 0.000210854 in Epoch 192
Epoch 208
Epoch 208, Loss: 0.000376740, Improvement: 0.000026512, Best Loss: 0.000210854 in Epoch 192
Epoch 209
Epoch 209, Loss: 0.000742163, Improvement: 0.000365423, Best Loss: 0.000210854 in Epoch 192
Epoch 210
Epoch 210, Loss: 0.000822923, Improvement: 0.000080760, Best Loss: 0.000210854 in Epoch 192
Epoch 211
Epoch 211, Loss: 0.000451314, Improvement: -0.000371608, Best Loss: 0.000210854 in Epoch 192
Epoch 212
Epoch 212, Loss: 0.000366737, Improvement: -0.000084577, Best Loss: 0.000210854 in Epoch 192
Epoch 213
Epoch 213, Loss: 0.000326124, Improvement: -0.000040614, Best Loss: 0.000210854 in Epoch 192
Epoch 214
A best model at epoch 214 has been saved with training error 0.000210486.
Epoch 214, Loss: 0.000309287, Improvement: -0.000016836, Best Loss: 0.000210486 in Epoch 214
Epoch 215
Epoch 215, Loss: 0.000556754, Improvement: 0.000247467, Best Loss: 0.000210486 in Epoch 214
Epoch 216
Epoch 216, Loss: 0.001123933, Improvement: 0.000567179, Best Loss: 0.000210486 in Epoch 214
Epoch 217
Epoch 217, Loss: 0.000767761, Improvement: -0.000356172, Best Loss: 0.000210486 in Epoch 214
Epoch 218
Epoch 218, Loss: 0.000506692, Improvement: -0.000261069, Best Loss: 0.000210486 in Epoch 214
Epoch 219
A best model at epoch 219 has been saved with training error 0.000202644.
Epoch 219, Loss: 0.000377997, Improvement: -0.000128695, Best Loss: 0.000202644 in Epoch 219
Epoch 220
A best model at epoch 220 has been saved with training error 0.000202202.
Epoch 220, Loss: 0.000337047, Improvement: -0.000040950, Best Loss: 0.000202202 in Epoch 220
Epoch 221
Epoch 221, Loss: 0.000311348, Improvement: -0.000025699, Best Loss: 0.000202202 in Epoch 220
Epoch 222
A best model at epoch 222 has been saved with training error 0.000178489.
Epoch 222, Loss: 0.000304192, Improvement: -0.000007155, Best Loss: 0.000178489 in Epoch 222
Epoch 223
Epoch 223, Loss: 0.000305911, Improvement: 0.000001718, Best Loss: 0.000178489 in Epoch 222
Epoch 224
Epoch 224, Loss: 0.000482752, Improvement: 0.000176841, Best Loss: 0.000178489 in Epoch 222
Epoch 225
Epoch 225, Loss: 0.000395621, Improvement: -0.000087131, Best Loss: 0.000178489 in Epoch 222
Epoch 226
Epoch 226, Loss: 0.000351626, Improvement: -0.000043994, Best Loss: 0.000178489 in Epoch 222
Epoch 227
Epoch 227, Loss: 0.000363714, Improvement: 0.000012087, Best Loss: 0.000178489 in Epoch 222
Epoch 228
Epoch 228, Loss: 0.000343744, Improvement: -0.000019969, Best Loss: 0.000178489 in Epoch 222
Epoch 229
Epoch 229, Loss: 0.000491044, Improvement: 0.000147299, Best Loss: 0.000178489 in Epoch 222
Epoch 230
Epoch 230, Loss: 0.000823957, Improvement: 0.000332913, Best Loss: 0.000178489 in Epoch 222
Epoch 231
Epoch 231, Loss: 0.000922022, Improvement: 0.000098066, Best Loss: 0.000178489 in Epoch 222
Epoch 232
Epoch 232, Loss: 0.000717257, Improvement: -0.000204765, Best Loss: 0.000178489 in Epoch 222
Epoch 233
Epoch 233, Loss: 0.000443601, Improvement: -0.000273656, Best Loss: 0.000178489 in Epoch 222
Epoch 234
Epoch 234, Loss: 0.000385476, Improvement: -0.000058125, Best Loss: 0.000178489 in Epoch 222
Epoch 235
Epoch 235, Loss: 0.000376121, Improvement: -0.000009355, Best Loss: 0.000178489 in Epoch 222
Epoch 236
Epoch 236, Loss: 0.000428314, Improvement: 0.000052192, Best Loss: 0.000178489 in Epoch 222
Epoch 237
Epoch 237, Loss: 0.000356195, Improvement: -0.000072119, Best Loss: 0.000178489 in Epoch 222
Epoch 238
Epoch 238, Loss: 0.000303099, Improvement: -0.000053096, Best Loss: 0.000178489 in Epoch 222
Epoch 239
Epoch 239, Loss: 0.000387513, Improvement: 0.000084414, Best Loss: 0.000178489 in Epoch 222
Epoch 240
Epoch 240, Loss: 0.000781175, Improvement: 0.000393662, Best Loss: 0.000178489 in Epoch 222
Epoch 241
Epoch 241, Loss: 0.000633498, Improvement: -0.000147678, Best Loss: 0.000178489 in Epoch 222
Epoch 242
Epoch 242, Loss: 0.000418797, Improvement: -0.000214701, Best Loss: 0.000178489 in Epoch 222
Epoch 243
Epoch 243, Loss: 0.000320170, Improvement: -0.000098627, Best Loss: 0.000178489 in Epoch 222
Epoch 244
A best model at epoch 244 has been saved with training error 0.000175304.
Epoch 244, Loss: 0.000277233, Improvement: -0.000042936, Best Loss: 0.000175304 in Epoch 244
Epoch 245
A best model at epoch 245 has been saved with training error 0.000173735.
Epoch 245, Loss: 0.000258885, Improvement: -0.000018348, Best Loss: 0.000173735 in Epoch 245
Epoch 246
Epoch 246, Loss: 0.000298254, Improvement: 0.000039369, Best Loss: 0.000173735 in Epoch 245
Epoch 247
Epoch 247, Loss: 0.000273085, Improvement: -0.000025169, Best Loss: 0.000173735 in Epoch 245
Epoch 248
Epoch 248, Loss: 0.000571809, Improvement: 0.000298724, Best Loss: 0.000173735 in Epoch 245
Epoch 249
Epoch 249, Loss: 0.000498473, Improvement: -0.000073336, Best Loss: 0.000173735 in Epoch 245
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.000418814, Improvement: -0.000079660, Best Loss: 0.000173735 in Epoch 245
Epoch 251
Epoch 251, Loss: 0.000349056, Improvement: -0.000069758, Best Loss: 0.000173735 in Epoch 245
Epoch 252
Epoch 252, Loss: 0.000329418, Improvement: -0.000019638, Best Loss: 0.000173735 in Epoch 245
Epoch 253
Epoch 253, Loss: 0.000359744, Improvement: 0.000030326, Best Loss: 0.000173735 in Epoch 245
Epoch 254
Epoch 254, Loss: 0.000353114, Improvement: -0.000006630, Best Loss: 0.000173735 in Epoch 245
Epoch 255
Epoch 255, Loss: 0.000349610, Improvement: -0.000003503, Best Loss: 0.000173735 in Epoch 245
Epoch 256
Epoch 256, Loss: 0.000303656, Improvement: -0.000045954, Best Loss: 0.000173735 in Epoch 245
Epoch 257
Epoch 257, Loss: 0.000342013, Improvement: 0.000038357, Best Loss: 0.000173735 in Epoch 245
Epoch 258
Epoch 258, Loss: 0.000471880, Improvement: 0.000129867, Best Loss: 0.000173735 in Epoch 245
Epoch 259
Epoch 259, Loss: 0.000985330, Improvement: 0.000513450, Best Loss: 0.000173735 in Epoch 245
Epoch 260
Epoch 260, Loss: 0.000601010, Improvement: -0.000384320, Best Loss: 0.000173735 in Epoch 245
Epoch 261
Epoch 261, Loss: 0.000412701, Improvement: -0.000188309, Best Loss: 0.000173735 in Epoch 245
Epoch 262
Epoch 262, Loss: 0.000309455, Improvement: -0.000103246, Best Loss: 0.000173735 in Epoch 245
Epoch 263
Epoch 263, Loss: 0.000267984, Improvement: -0.000041471, Best Loss: 0.000173735 in Epoch 245
Epoch 264
A best model at epoch 264 has been saved with training error 0.000163218.
Epoch 264, Loss: 0.000297171, Improvement: 0.000029187, Best Loss: 0.000163218 in Epoch 264
Epoch 265
Epoch 265, Loss: 0.000374440, Improvement: 0.000077268, Best Loss: 0.000163218 in Epoch 264
Epoch 266
Epoch 266, Loss: 0.000322756, Improvement: -0.000051683, Best Loss: 0.000163218 in Epoch 264
Epoch 267
Epoch 267, Loss: 0.000423291, Improvement: 0.000100534, Best Loss: 0.000163218 in Epoch 264
Epoch 268
Epoch 268, Loss: 0.000443616, Improvement: 0.000020325, Best Loss: 0.000163218 in Epoch 264
Epoch 269
Epoch 269, Loss: 0.000619098, Improvement: 0.000175483, Best Loss: 0.000163218 in Epoch 264
Epoch 270
Epoch 270, Loss: 0.000473549, Improvement: -0.000145549, Best Loss: 0.000163218 in Epoch 264
Epoch 271
Epoch 271, Loss: 0.000455056, Improvement: -0.000018494, Best Loss: 0.000163218 in Epoch 264
Epoch 272
Epoch 272, Loss: 0.000322549, Improvement: -0.000132507, Best Loss: 0.000163218 in Epoch 264
Epoch 273
Epoch 273, Loss: 0.000279797, Improvement: -0.000042752, Best Loss: 0.000163218 in Epoch 264
Epoch 274
Epoch 274, Loss: 0.000283730, Improvement: 0.000003933, Best Loss: 0.000163218 in Epoch 264
Epoch 275
Epoch 275, Loss: 0.000316615, Improvement: 0.000032886, Best Loss: 0.000163218 in Epoch 264
Epoch 276
Epoch 276, Loss: 0.000387546, Improvement: 0.000070931, Best Loss: 0.000163218 in Epoch 264
Epoch 277
Epoch 277, Loss: 0.000525767, Improvement: 0.000138221, Best Loss: 0.000163218 in Epoch 264
Epoch 278
Epoch 278, Loss: 0.000332120, Improvement: -0.000193647, Best Loss: 0.000163218 in Epoch 264
Epoch 279
Epoch 279, Loss: 0.000255906, Improvement: -0.000076215, Best Loss: 0.000163218 in Epoch 264
Epoch 280
A best model at epoch 280 has been saved with training error 0.000142860.
Epoch 280, Loss: 0.000253954, Improvement: -0.000001952, Best Loss: 0.000142860 in Epoch 280
Epoch 281
Epoch 281, Loss: 0.000267822, Improvement: 0.000013868, Best Loss: 0.000142860 in Epoch 280
Epoch 282
Epoch 282, Loss: 0.000374511, Improvement: 0.000106690, Best Loss: 0.000142860 in Epoch 280
Epoch 283
Epoch 283, Loss: 0.000695744, Improvement: 0.000321233, Best Loss: 0.000142860 in Epoch 280
Epoch 284
Epoch 284, Loss: 0.000476406, Improvement: -0.000219338, Best Loss: 0.000142860 in Epoch 280
Epoch 285
Epoch 285, Loss: 0.000310096, Improvement: -0.000166310, Best Loss: 0.000142860 in Epoch 280
Epoch 286
Epoch 286, Loss: 0.000249145, Improvement: -0.000060951, Best Loss: 0.000142860 in Epoch 280
Epoch 287
A best model at epoch 287 has been saved with training error 0.000141277.
Epoch 287, Loss: 0.000218313, Improvement: -0.000030831, Best Loss: 0.000141277 in Epoch 287
Epoch 288
A best model at epoch 288 has been saved with training error 0.000136438.
Epoch 288, Loss: 0.000205783, Improvement: -0.000012531, Best Loss: 0.000136438 in Epoch 288
Epoch 289
A best model at epoch 289 has been saved with training error 0.000121507.
Epoch 289, Loss: 0.000194580, Improvement: -0.000011203, Best Loss: 0.000121507 in Epoch 289
Epoch 290
Epoch 290, Loss: 0.000221938, Improvement: 0.000027358, Best Loss: 0.000121507 in Epoch 289
Epoch 291
Epoch 291, Loss: 0.000223345, Improvement: 0.000001407, Best Loss: 0.000121507 in Epoch 289
Epoch 292
Epoch 292, Loss: 0.000346574, Improvement: 0.000123229, Best Loss: 0.000121507 in Epoch 289
Epoch 293
Epoch 293, Loss: 0.000348545, Improvement: 0.000001971, Best Loss: 0.000121507 in Epoch 289
Epoch 294
Epoch 294, Loss: 0.000357862, Improvement: 0.000009317, Best Loss: 0.000121507 in Epoch 289
Epoch 295
Epoch 295, Loss: 0.000303003, Improvement: -0.000054859, Best Loss: 0.000121507 in Epoch 289
Epoch 296
Epoch 296, Loss: 0.000394712, Improvement: 0.000091709, Best Loss: 0.000121507 in Epoch 289
Epoch 297
Epoch 297, Loss: 0.000318071, Improvement: -0.000076640, Best Loss: 0.000121507 in Epoch 289
Epoch 298
Epoch 298, Loss: 0.000234520, Improvement: -0.000083551, Best Loss: 0.000121507 in Epoch 289
Epoch 299
Epoch 299, Loss: 0.000202789, Improvement: -0.000031731, Best Loss: 0.000121507 in Epoch 289
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.000238305, Improvement: 0.000035516, Best Loss: 0.000121507 in Epoch 289
Epoch 301
Epoch 301, Loss: 0.000603356, Improvement: 0.000365051, Best Loss: 0.000121507 in Epoch 289
Epoch 302
Epoch 302, Loss: 0.000504728, Improvement: -0.000098628, Best Loss: 0.000121507 in Epoch 289
Epoch 303
Epoch 303, Loss: 0.000345010, Improvement: -0.000159718, Best Loss: 0.000121507 in Epoch 289
Epoch 304
Epoch 304, Loss: 0.000240067, Improvement: -0.000104943, Best Loss: 0.000121507 in Epoch 289
Epoch 305
A best model at epoch 305 has been saved with training error 0.000119416.
Epoch 305, Loss: 0.000206741, Improvement: -0.000033326, Best Loss: 0.000119416 in Epoch 305
Epoch 306
Epoch 306, Loss: 0.000240588, Improvement: 0.000033847, Best Loss: 0.000119416 in Epoch 305
Epoch 307
Epoch 307, Loss: 0.000235589, Improvement: -0.000004999, Best Loss: 0.000119416 in Epoch 305
Epoch 308
Epoch 308, Loss: 0.000509645, Improvement: 0.000274056, Best Loss: 0.000119416 in Epoch 305
Epoch 309
Epoch 309, Loss: 0.000379574, Improvement: -0.000130071, Best Loss: 0.000119416 in Epoch 305
Epoch 310
Epoch 310, Loss: 0.000286192, Improvement: -0.000093382, Best Loss: 0.000119416 in Epoch 305
Epoch 311
Epoch 311, Loss: 0.000429709, Improvement: 0.000143517, Best Loss: 0.000119416 in Epoch 305
Epoch 312
Epoch 312, Loss: 0.000483507, Improvement: 0.000053798, Best Loss: 0.000119416 in Epoch 305
Epoch 313
Epoch 313, Loss: 0.000294979, Improvement: -0.000188528, Best Loss: 0.000119416 in Epoch 305
Epoch 314
Epoch 314, Loss: 0.000245197, Improvement: -0.000049782, Best Loss: 0.000119416 in Epoch 305
Epoch 315
Epoch 315, Loss: 0.000199022, Improvement: -0.000046175, Best Loss: 0.000119416 in Epoch 305
Epoch 316
A best model at epoch 316 has been saved with training error 0.000118792.
Epoch 316, Loss: 0.000172438, Improvement: -0.000026584, Best Loss: 0.000118792 in Epoch 316
Epoch 317
Epoch 317, Loss: 0.000209470, Improvement: 0.000037032, Best Loss: 0.000118792 in Epoch 316
Epoch 318
A best model at epoch 318 has been saved with training error 0.000107470.
Epoch 318, Loss: 0.000188953, Improvement: -0.000020517, Best Loss: 0.000107470 in Epoch 318
Epoch 319
Epoch 319, Loss: 0.000396952, Improvement: 0.000207999, Best Loss: 0.000107470 in Epoch 318
Epoch 320
Epoch 320, Loss: 0.000289603, Improvement: -0.000107349, Best Loss: 0.000107470 in Epoch 318
Epoch 321
Epoch 321, Loss: 0.000211714, Improvement: -0.000077889, Best Loss: 0.000107470 in Epoch 318
Epoch 322
A best model at epoch 322 has been saved with training error 0.000086578.
Epoch 322, Loss: 0.000172675, Improvement: -0.000039039, Best Loss: 0.000086578 in Epoch 322
Epoch 323
Epoch 323, Loss: 0.000151015, Improvement: -0.000021660, Best Loss: 0.000086578 in Epoch 322
Epoch 324
Epoch 324, Loss: 0.000165518, Improvement: 0.000014503, Best Loss: 0.000086578 in Epoch 322
Epoch 325
Epoch 325, Loss: 0.000248714, Improvement: 0.000083196, Best Loss: 0.000086578 in Epoch 322
Epoch 326
Epoch 326, Loss: 0.000570633, Improvement: 0.000321919, Best Loss: 0.000086578 in Epoch 322
Epoch 327
Epoch 327, Loss: 0.000360921, Improvement: -0.000209711, Best Loss: 0.000086578 in Epoch 322
Epoch 328
Epoch 328, Loss: 0.000224773, Improvement: -0.000136148, Best Loss: 0.000086578 in Epoch 322
Epoch 329
Epoch 329, Loss: 0.000221822, Improvement: -0.000002950, Best Loss: 0.000086578 in Epoch 322
Epoch 330
Epoch 330, Loss: 0.000312095, Improvement: 0.000090273, Best Loss: 0.000086578 in Epoch 322
Epoch 331
Epoch 331, Loss: 0.000194342, Improvement: -0.000117753, Best Loss: 0.000086578 in Epoch 322
Epoch 332
Epoch 332, Loss: 0.000149327, Improvement: -0.000045015, Best Loss: 0.000086578 in Epoch 322
Epoch 333
Epoch 333, Loss: 0.000147241, Improvement: -0.000002086, Best Loss: 0.000086578 in Epoch 322
Epoch 334
Epoch 334, Loss: 0.000152053, Improvement: 0.000004812, Best Loss: 0.000086578 in Epoch 322
Epoch 335
Epoch 335, Loss: 0.000160360, Improvement: 0.000008306, Best Loss: 0.000086578 in Epoch 322
Epoch 336
Epoch 336, Loss: 0.000192680, Improvement: 0.000032320, Best Loss: 0.000086578 in Epoch 322
Epoch 337
Epoch 337, Loss: 0.000433231, Improvement: 0.000240551, Best Loss: 0.000086578 in Epoch 322
Epoch 338
Epoch 338, Loss: 0.000253018, Improvement: -0.000180213, Best Loss: 0.000086578 in Epoch 322
Epoch 339
Epoch 339, Loss: 0.000263512, Improvement: 0.000010494, Best Loss: 0.000086578 in Epoch 322
Epoch 340
Epoch 340, Loss: 0.000198080, Improvement: -0.000065432, Best Loss: 0.000086578 in Epoch 322
Epoch 341
Epoch 341, Loss: 0.000167180, Improvement: -0.000030900, Best Loss: 0.000086578 in Epoch 322
Epoch 342
Epoch 342, Loss: 0.000142215, Improvement: -0.000024965, Best Loss: 0.000086578 in Epoch 322
Epoch 343
Epoch 343, Loss: 0.000131245, Improvement: -0.000010970, Best Loss: 0.000086578 in Epoch 322
Epoch 344
A best model at epoch 344 has been saved with training error 0.000079265.
A best model at epoch 344 has been saved with training error 0.000076380.
Epoch 344, Loss: 0.000122581, Improvement: -0.000008664, Best Loss: 0.000076380 in Epoch 344
Epoch 345
Epoch 345, Loss: 0.000175364, Improvement: 0.000052783, Best Loss: 0.000076380 in Epoch 344
Epoch 346
Epoch 346, Loss: 0.000239234, Improvement: 0.000063871, Best Loss: 0.000076380 in Epoch 344
Epoch 347
Epoch 347, Loss: 0.000476463, Improvement: 0.000237229, Best Loss: 0.000076380 in Epoch 344
Epoch 348
Epoch 348, Loss: 0.000286571, Improvement: -0.000189892, Best Loss: 0.000076380 in Epoch 344
Epoch 349
Epoch 349, Loss: 0.000260324, Improvement: -0.000026248, Best Loss: 0.000076380 in Epoch 344
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.000374810, Improvement: 0.000114487, Best Loss: 0.000076380 in Epoch 344
Epoch 351
Epoch 351, Loss: 0.000214819, Improvement: -0.000159992, Best Loss: 0.000076380 in Epoch 344
Epoch 352
Epoch 352, Loss: 0.000161774, Improvement: -0.000053045, Best Loss: 0.000076380 in Epoch 344
Epoch 353
Epoch 353, Loss: 0.000135494, Improvement: -0.000026281, Best Loss: 0.000076380 in Epoch 344
Epoch 354
Epoch 354, Loss: 0.000179876, Improvement: 0.000044382, Best Loss: 0.000076380 in Epoch 344
Epoch 355
Epoch 355, Loss: 0.000255574, Improvement: 0.000075698, Best Loss: 0.000076380 in Epoch 344
Epoch 356
Epoch 356, Loss: 0.000286175, Improvement: 0.000030601, Best Loss: 0.000076380 in Epoch 344
Epoch 357
Epoch 357, Loss: 0.000269245, Improvement: -0.000016930, Best Loss: 0.000076380 in Epoch 344
Epoch 358
Epoch 358, Loss: 0.000194116, Improvement: -0.000075129, Best Loss: 0.000076380 in Epoch 344
Epoch 359
Epoch 359, Loss: 0.000136189, Improvement: -0.000057927, Best Loss: 0.000076380 in Epoch 344
Epoch 360
Epoch 360, Loss: 0.000155559, Improvement: 0.000019370, Best Loss: 0.000076380 in Epoch 344
Epoch 361
Epoch 361, Loss: 0.000208173, Improvement: 0.000052614, Best Loss: 0.000076380 in Epoch 344
Epoch 362
Epoch 362, Loss: 0.000161658, Improvement: -0.000046515, Best Loss: 0.000076380 in Epoch 344
Epoch 363
A best model at epoch 363 has been saved with training error 0.000065936.
Epoch 363, Loss: 0.000115392, Improvement: -0.000046266, Best Loss: 0.000065936 in Epoch 363
Epoch 364
Epoch 364, Loss: 0.000105341, Improvement: -0.000010051, Best Loss: 0.000065936 in Epoch 363
Epoch 365
A best model at epoch 365 has been saved with training error 0.000063163.
A best model at epoch 365 has been saved with training error 0.000059663.
Epoch 365, Loss: 0.000130860, Improvement: 0.000025520, Best Loss: 0.000059663 in Epoch 365
Epoch 366
Epoch 366, Loss: 0.000159270, Improvement: 0.000028410, Best Loss: 0.000059663 in Epoch 365
Epoch 367
Epoch 367, Loss: 0.000167841, Improvement: 0.000008571, Best Loss: 0.000059663 in Epoch 365
Epoch 368
Epoch 368, Loss: 0.000201723, Improvement: 0.000033882, Best Loss: 0.000059663 in Epoch 365
Epoch 369
Epoch 369, Loss: 0.000261051, Improvement: 0.000059329, Best Loss: 0.000059663 in Epoch 365
Epoch 370
Epoch 370, Loss: 0.000230020, Improvement: -0.000031031, Best Loss: 0.000059663 in Epoch 365
Epoch 371
Epoch 371, Loss: 0.000147162, Improvement: -0.000082858, Best Loss: 0.000059663 in Epoch 365
Epoch 372
Epoch 372, Loss: 0.000186939, Improvement: 0.000039777, Best Loss: 0.000059663 in Epoch 365
Epoch 373
Epoch 373, Loss: 0.000389276, Improvement: 0.000202337, Best Loss: 0.000059663 in Epoch 365
Epoch 374
Epoch 374, Loss: 0.000437803, Improvement: 0.000048527, Best Loss: 0.000059663 in Epoch 365
Epoch 375
Epoch 375, Loss: 0.000301656, Improvement: -0.000136147, Best Loss: 0.000059663 in Epoch 365
Epoch 376
Epoch 376, Loss: 0.000154896, Improvement: -0.000146760, Best Loss: 0.000059663 in Epoch 365
Epoch 377
Epoch 377, Loss: 0.000111696, Improvement: -0.000043200, Best Loss: 0.000059663 in Epoch 365
Epoch 378
Epoch 378, Loss: 0.000087722, Improvement: -0.000023974, Best Loss: 0.000059663 in Epoch 365
Epoch 379
Epoch 379, Loss: 0.000154113, Improvement: 0.000066390, Best Loss: 0.000059663 in Epoch 365
Epoch 380
Epoch 380, Loss: 0.000312567, Improvement: 0.000158454, Best Loss: 0.000059663 in Epoch 365
Epoch 381
Epoch 381, Loss: 0.000206968, Improvement: -0.000105599, Best Loss: 0.000059663 in Epoch 365
Epoch 382
Epoch 382, Loss: 0.000123161, Improvement: -0.000083807, Best Loss: 0.000059663 in Epoch 365
Epoch 383
Epoch 383, Loss: 0.000102157, Improvement: -0.000021004, Best Loss: 0.000059663 in Epoch 365
Epoch 384
Epoch 384, Loss: 0.000098099, Improvement: -0.000004058, Best Loss: 0.000059663 in Epoch 365
Epoch 385
Epoch 385, Loss: 0.000087547, Improvement: -0.000010552, Best Loss: 0.000059663 in Epoch 365
Epoch 386
A best model at epoch 386 has been saved with training error 0.000054857.
Epoch 386, Loss: 0.000084585, Improvement: -0.000002962, Best Loss: 0.000054857 in Epoch 386
Epoch 387
Epoch 387, Loss: 0.000099964, Improvement: 0.000015379, Best Loss: 0.000054857 in Epoch 386
Epoch 388
Epoch 388, Loss: 0.000251539, Improvement: 0.000151575, Best Loss: 0.000054857 in Epoch 386
Epoch 389
Epoch 389, Loss: 0.000183605, Improvement: -0.000067934, Best Loss: 0.000054857 in Epoch 386
Epoch 390
Epoch 390, Loss: 0.000116655, Improvement: -0.000066950, Best Loss: 0.000054857 in Epoch 386
Epoch 391
Epoch 391, Loss: 0.000151699, Improvement: 0.000035043, Best Loss: 0.000054857 in Epoch 386
Epoch 392
Epoch 392, Loss: 0.000319154, Improvement: 0.000167455, Best Loss: 0.000054857 in Epoch 386
Epoch 393
Epoch 393, Loss: 0.000253275, Improvement: -0.000065879, Best Loss: 0.000054857 in Epoch 386
Epoch 394
Epoch 394, Loss: 0.000320562, Improvement: 0.000067288, Best Loss: 0.000054857 in Epoch 386
Epoch 395
Epoch 395, Loss: 0.000203530, Improvement: -0.000117032, Best Loss: 0.000054857 in Epoch 386
Epoch 396
Epoch 396, Loss: 0.000103725, Improvement: -0.000099805, Best Loss: 0.000054857 in Epoch 386
Epoch 397
Epoch 397, Loss: 0.000078217, Improvement: -0.000025509, Best Loss: 0.000054857 in Epoch 386
Epoch 398
A best model at epoch 398 has been saved with training error 0.000053058.
A best model at epoch 398 has been saved with training error 0.000050116.
Epoch 398, Loss: 0.000067281, Improvement: -0.000010936, Best Loss: 0.000050116 in Epoch 398
Epoch 399
A best model at epoch 399 has been saved with training error 0.000046626.
Epoch 399, Loss: 0.000070801, Improvement: 0.000003519, Best Loss: 0.000046626 in Epoch 399
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.000066491, Improvement: -0.000004310, Best Loss: 0.000046626 in Epoch 399
Epoch 401
Epoch 401, Loss: 0.000092471, Improvement: 0.000025980, Best Loss: 0.000046626 in Epoch 399
Epoch 402
Epoch 402, Loss: 0.000243999, Improvement: 0.000151529, Best Loss: 0.000046626 in Epoch 399
Epoch 403
Epoch 403, Loss: 0.000156142, Improvement: -0.000087857, Best Loss: 0.000046626 in Epoch 399
Epoch 404
Epoch 404, Loss: 0.000107391, Improvement: -0.000048751, Best Loss: 0.000046626 in Epoch 399
Epoch 405
Epoch 405, Loss: 0.000080773, Improvement: -0.000026618, Best Loss: 0.000046626 in Epoch 399
Epoch 406
Epoch 406, Loss: 0.000071302, Improvement: -0.000009471, Best Loss: 0.000046626 in Epoch 399
Epoch 407
Epoch 407, Loss: 0.000073713, Improvement: 0.000002411, Best Loss: 0.000046626 in Epoch 399
Epoch 408
Epoch 408, Loss: 0.000071636, Improvement: -0.000002076, Best Loss: 0.000046626 in Epoch 399
Epoch 409
Epoch 409, Loss: 0.000135941, Improvement: 0.000064305, Best Loss: 0.000046626 in Epoch 399
Epoch 410
Epoch 410, Loss: 0.000264441, Improvement: 0.000128500, Best Loss: 0.000046626 in Epoch 399
Epoch 411
Epoch 411, Loss: 0.000195742, Improvement: -0.000068698, Best Loss: 0.000046626 in Epoch 399
Epoch 412
Epoch 412, Loss: 0.000144465, Improvement: -0.000051277, Best Loss: 0.000046626 in Epoch 399
Epoch 413
Epoch 413, Loss: 0.000113513, Improvement: -0.000030952, Best Loss: 0.000046626 in Epoch 399
Epoch 414
Epoch 414, Loss: 0.000094103, Improvement: -0.000019410, Best Loss: 0.000046626 in Epoch 399
Epoch 415
Epoch 415, Loss: 0.000088939, Improvement: -0.000005164, Best Loss: 0.000046626 in Epoch 399
Epoch 416
Epoch 416, Loss: 0.000124313, Improvement: 0.000035374, Best Loss: 0.000046626 in Epoch 399
Epoch 417
Epoch 417, Loss: 0.000206026, Improvement: 0.000081713, Best Loss: 0.000046626 in Epoch 399
Epoch 418
Epoch 418, Loss: 0.000215191, Improvement: 0.000009165, Best Loss: 0.000046626 in Epoch 399
Epoch 419
Epoch 419, Loss: 0.000124795, Improvement: -0.000090396, Best Loss: 0.000046626 in Epoch 399
Epoch 420
Epoch 420, Loss: 0.000077721, Improvement: -0.000047074, Best Loss: 0.000046626 in Epoch 399
Epoch 421
A best model at epoch 421 has been saved with training error 0.000046170.
A best model at epoch 421 has been saved with training error 0.000045151.
Epoch 421, Loss: 0.000059391, Improvement: -0.000018330, Best Loss: 0.000045151 in Epoch 421
Epoch 422
A best model at epoch 422 has been saved with training error 0.000033613.
Epoch 422, Loss: 0.000057632, Improvement: -0.000001758, Best Loss: 0.000033613 in Epoch 422
Epoch 423
Epoch 423, Loss: 0.000086351, Improvement: 0.000028719, Best Loss: 0.000033613 in Epoch 422
Epoch 424
Epoch 424, Loss: 0.000078885, Improvement: -0.000007466, Best Loss: 0.000033613 in Epoch 422
Epoch 425
Epoch 425, Loss: 0.000075299, Improvement: -0.000003587, Best Loss: 0.000033613 in Epoch 422
Epoch 426
Epoch 426, Loss: 0.000074307, Improvement: -0.000000992, Best Loss: 0.000033613 in Epoch 422
Epoch 427
Epoch 427, Loss: 0.000129859, Improvement: 0.000055552, Best Loss: 0.000033613 in Epoch 422
Epoch 428
Epoch 428, Loss: 0.000114672, Improvement: -0.000015187, Best Loss: 0.000033613 in Epoch 422
Epoch 429
Epoch 429, Loss: 0.000132326, Improvement: 0.000017654, Best Loss: 0.000033613 in Epoch 422
Epoch 430
Epoch 430, Loss: 0.000082024, Improvement: -0.000050302, Best Loss: 0.000033613 in Epoch 422
Epoch 431
Epoch 431, Loss: 0.000070440, Improvement: -0.000011584, Best Loss: 0.000033613 in Epoch 422
Epoch 432
Epoch 432, Loss: 0.000085170, Improvement: 0.000014730, Best Loss: 0.000033613 in Epoch 422
Epoch 433
Epoch 433, Loss: 0.000077216, Improvement: -0.000007954, Best Loss: 0.000033613 in Epoch 422
Epoch 434
Epoch 434, Loss: 0.000093195, Improvement: 0.000015979, Best Loss: 0.000033613 in Epoch 422
Epoch 435
Epoch 435, Loss: 0.000116620, Improvement: 0.000023426, Best Loss: 0.000033613 in Epoch 422
Epoch 436
Epoch 436, Loss: 0.000074559, Improvement: -0.000042061, Best Loss: 0.000033613 in Epoch 422
Epoch 437
Epoch 437, Loss: 0.000077083, Improvement: 0.000002524, Best Loss: 0.000033613 in Epoch 422
Epoch 438
Epoch 438, Loss: 0.000086428, Improvement: 0.000009344, Best Loss: 0.000033613 in Epoch 422
Epoch 439
Epoch 439, Loss: 0.000096730, Improvement: 0.000010302, Best Loss: 0.000033613 in Epoch 422
Epoch 440
Epoch 440, Loss: 0.000081931, Improvement: -0.000014799, Best Loss: 0.000033613 in Epoch 422
Epoch 441
Epoch 441, Loss: 0.000080725, Improvement: -0.000001206, Best Loss: 0.000033613 in Epoch 422
Epoch 442
Epoch 442, Loss: 0.000099871, Improvement: 0.000019146, Best Loss: 0.000033613 in Epoch 422
Epoch 443
Epoch 443, Loss: 0.000132691, Improvement: 0.000032819, Best Loss: 0.000033613 in Epoch 422
Epoch 444
Epoch 444, Loss: 0.000110947, Improvement: -0.000021743, Best Loss: 0.000033613 in Epoch 422
Epoch 445
Epoch 445, Loss: 0.000068375, Improvement: -0.000042573, Best Loss: 0.000033613 in Epoch 422
Epoch 446
Epoch 446, Loss: 0.000072682, Improvement: 0.000004307, Best Loss: 0.000033613 in Epoch 422
Epoch 447
Epoch 447, Loss: 0.000069735, Improvement: -0.000002946, Best Loss: 0.000033613 in Epoch 422
Epoch 448
Epoch 448, Loss: 0.000084741, Improvement: 0.000015006, Best Loss: 0.000033613 in Epoch 422
Epoch 449
Epoch 449, Loss: 0.000227280, Improvement: 0.000142538, Best Loss: 0.000033613 in Epoch 422
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.000173394, Improvement: -0.000053886, Best Loss: 0.000033613 in Epoch 422
Epoch 451
Epoch 451, Loss: 0.000097497, Improvement: -0.000075897, Best Loss: 0.000033613 in Epoch 422
Epoch 452
Epoch 452, Loss: 0.000142554, Improvement: 0.000045057, Best Loss: 0.000033613 in Epoch 422
Epoch 453
Epoch 453, Loss: 0.000187860, Improvement: 0.000045306, Best Loss: 0.000033613 in Epoch 422
Epoch 454
Epoch 454, Loss: 0.000139101, Improvement: -0.000048760, Best Loss: 0.000033613 in Epoch 422
Epoch 455
Epoch 455, Loss: 0.000136890, Improvement: -0.000002210, Best Loss: 0.000033613 in Epoch 422
Epoch 456
Epoch 456, Loss: 0.000170772, Improvement: 0.000033882, Best Loss: 0.000033613 in Epoch 422
Epoch 457
Epoch 457, Loss: 0.000086141, Improvement: -0.000084631, Best Loss: 0.000033613 in Epoch 422
Epoch 458
Epoch 458, Loss: 0.000061050, Improvement: -0.000025090, Best Loss: 0.000033613 in Epoch 422
Epoch 459
Epoch 459, Loss: 0.000076123, Improvement: 0.000015073, Best Loss: 0.000033613 in Epoch 422
Epoch 460
Epoch 460, Loss: 0.000072990, Improvement: -0.000003133, Best Loss: 0.000033613 in Epoch 422
Epoch 461
Epoch 461, Loss: 0.000055921, Improvement: -0.000017068, Best Loss: 0.000033613 in Epoch 422
Epoch 462
A best model at epoch 462 has been saved with training error 0.000033005.
A best model at epoch 462 has been saved with training error 0.000030910.
A best model at epoch 462 has been saved with training error 0.000030031.
Epoch 462, Loss: 0.000042198, Improvement: -0.000013723, Best Loss: 0.000030031 in Epoch 462
Epoch 463
A best model at epoch 463 has been saved with training error 0.000029909.
A best model at epoch 463 has been saved with training error 0.000029252.
A best model at epoch 463 has been saved with training error 0.000023831.
Epoch 463, Loss: 0.000034852, Improvement: -0.000007347, Best Loss: 0.000023831 in Epoch 463
Epoch 464
Epoch 464, Loss: 0.000031588, Improvement: -0.000003263, Best Loss: 0.000023831 in Epoch 463
Epoch 465
A best model at epoch 465 has been saved with training error 0.000021231.
Epoch 465, Loss: 0.000032229, Improvement: 0.000000640, Best Loss: 0.000021231 in Epoch 465
Epoch 466
Epoch 466, Loss: 0.000041138, Improvement: 0.000008909, Best Loss: 0.000021231 in Epoch 465
Epoch 467
Epoch 467, Loss: 0.000132984, Improvement: 0.000091846, Best Loss: 0.000021231 in Epoch 465
Epoch 468
Epoch 468, Loss: 0.000129739, Improvement: -0.000003244, Best Loss: 0.000021231 in Epoch 465
Epoch 469
Epoch 469, Loss: 0.000082432, Improvement: -0.000047307, Best Loss: 0.000021231 in Epoch 465
Epoch 470
Epoch 470, Loss: 0.000055183, Improvement: -0.000027249, Best Loss: 0.000021231 in Epoch 465
Epoch 471
Epoch 471, Loss: 0.000042002, Improvement: -0.000013181, Best Loss: 0.000021231 in Epoch 465
Epoch 472
Epoch 472, Loss: 0.000054034, Improvement: 0.000012032, Best Loss: 0.000021231 in Epoch 465
Epoch 473
Epoch 473, Loss: 0.000035865, Improvement: -0.000018169, Best Loss: 0.000021231 in Epoch 465
Epoch 474
Epoch 474, Loss: 0.000035366, Improvement: -0.000000499, Best Loss: 0.000021231 in Epoch 465
Epoch 475
Epoch 475, Loss: 0.000039491, Improvement: 0.000004125, Best Loss: 0.000021231 in Epoch 465
Epoch 476
Epoch 476, Loss: 0.000035464, Improvement: -0.000004027, Best Loss: 0.000021231 in Epoch 465
Epoch 477
Epoch 477, Loss: 0.000041751, Improvement: 0.000006287, Best Loss: 0.000021231 in Epoch 465
Epoch 478
Epoch 478, Loss: 0.000068441, Improvement: 0.000026690, Best Loss: 0.000021231 in Epoch 465
Epoch 479
Epoch 479, Loss: 0.000074555, Improvement: 0.000006114, Best Loss: 0.000021231 in Epoch 465
Epoch 480
Epoch 480, Loss: 0.000099095, Improvement: 0.000024540, Best Loss: 0.000021231 in Epoch 465
Epoch 481
Epoch 481, Loss: 0.000166331, Improvement: 0.000067236, Best Loss: 0.000021231 in Epoch 465
Epoch 482
Epoch 482, Loss: 0.000097889, Improvement: -0.000068442, Best Loss: 0.000021231 in Epoch 465
Epoch 483
Epoch 483, Loss: 0.000062853, Improvement: -0.000035035, Best Loss: 0.000021231 in Epoch 465
Epoch 484
Epoch 484, Loss: 0.000060641, Improvement: -0.000002213, Best Loss: 0.000021231 in Epoch 465
Epoch 485
Epoch 485, Loss: 0.000043002, Improvement: -0.000017639, Best Loss: 0.000021231 in Epoch 465
Epoch 486
Epoch 486, Loss: 0.000035153, Improvement: -0.000007848, Best Loss: 0.000021231 in Epoch 465
Epoch 487
Epoch 487, Loss: 0.000058789, Improvement: 0.000023636, Best Loss: 0.000021231 in Epoch 465
Epoch 488
Epoch 488, Loss: 0.000116381, Improvement: 0.000057592, Best Loss: 0.000021231 in Epoch 465
Epoch 489
Epoch 489, Loss: 0.000150254, Improvement: 0.000033873, Best Loss: 0.000021231 in Epoch 465
Epoch 490
Epoch 490, Loss: 0.000092902, Improvement: -0.000057352, Best Loss: 0.000021231 in Epoch 465
Epoch 491
Epoch 491, Loss: 0.000064282, Improvement: -0.000028620, Best Loss: 0.000021231 in Epoch 465
Epoch 492
Epoch 492, Loss: 0.000047020, Improvement: -0.000017262, Best Loss: 0.000021231 in Epoch 465
Epoch 493
Epoch 493, Loss: 0.000038619, Improvement: -0.000008400, Best Loss: 0.000021231 in Epoch 465
Epoch 494
Epoch 494, Loss: 0.000036316, Improvement: -0.000002303, Best Loss: 0.000021231 in Epoch 465
Epoch 495
A best model at epoch 495 has been saved with training error 0.000020994.
Epoch 495, Loss: 0.000030814, Improvement: -0.000005502, Best Loss: 0.000020994 in Epoch 495
Epoch 496
Epoch 496, Loss: 0.000029904, Improvement: -0.000000910, Best Loss: 0.000020994 in Epoch 495
Epoch 497
A best model at epoch 497 has been saved with training error 0.000020842.
Epoch 497, Loss: 0.000029507, Improvement: -0.000000396, Best Loss: 0.000020842 in Epoch 497
Epoch 498
A best model at epoch 498 has been saved with training error 0.000019342.
Epoch 498, Loss: 0.000031355, Improvement: 0.000001848, Best Loss: 0.000019342 in Epoch 498
Epoch 499
Epoch 499, Loss: 0.000122091, Improvement: 0.000090736, Best Loss: 0.000019342 in Epoch 498
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.000152667, Improvement: 0.000030575, Best Loss: 0.000019342 in Epoch 498
Epoch 501
Epoch 501, Loss: 0.000094335, Improvement: -0.000058332, Best Loss: 0.000019342 in Epoch 498
Epoch 502
Epoch 502, Loss: 0.000092613, Improvement: -0.000001722, Best Loss: 0.000019342 in Epoch 498
Epoch 503
Epoch 503, Loss: 0.000077609, Improvement: -0.000015004, Best Loss: 0.000019342 in Epoch 498
Epoch 504
Epoch 504, Loss: 0.000050986, Improvement: -0.000026623, Best Loss: 0.000019342 in Epoch 498
Epoch 505
Epoch 505, Loss: 0.000041346, Improvement: -0.000009640, Best Loss: 0.000019342 in Epoch 498
Epoch 506
Epoch 506, Loss: 0.000044576, Improvement: 0.000003230, Best Loss: 0.000019342 in Epoch 498
Epoch 507
Epoch 507, Loss: 0.000140988, Improvement: 0.000096412, Best Loss: 0.000019342 in Epoch 498
Epoch 508
Epoch 508, Loss: 0.000078250, Improvement: -0.000062738, Best Loss: 0.000019342 in Epoch 498
Epoch 509
Epoch 509, Loss: 0.000047192, Improvement: -0.000031058, Best Loss: 0.000019342 in Epoch 498
Epoch 510
Epoch 510, Loss: 0.000055624, Improvement: 0.000008432, Best Loss: 0.000019342 in Epoch 498
Epoch 511
Epoch 511, Loss: 0.000053433, Improvement: -0.000002192, Best Loss: 0.000019342 in Epoch 498
Epoch 512
Epoch 512, Loss: 0.000054060, Improvement: 0.000000627, Best Loss: 0.000019342 in Epoch 498
Epoch 513
Epoch 513, Loss: 0.000071874, Improvement: 0.000017815, Best Loss: 0.000019342 in Epoch 498
Epoch 514
Epoch 514, Loss: 0.000065706, Improvement: -0.000006168, Best Loss: 0.000019342 in Epoch 498
Epoch 515
Epoch 515, Loss: 0.000061821, Improvement: -0.000003885, Best Loss: 0.000019342 in Epoch 498
Epoch 516
Epoch 516, Loss: 0.000044199, Improvement: -0.000017622, Best Loss: 0.000019342 in Epoch 498
Epoch 517
Epoch 517, Loss: 0.000030209, Improvement: -0.000013990, Best Loss: 0.000019342 in Epoch 498
Epoch 518
Epoch 518, Loss: 0.000031567, Improvement: 0.000001358, Best Loss: 0.000019342 in Epoch 498
Epoch 519
Epoch 519, Loss: 0.000051941, Improvement: 0.000020374, Best Loss: 0.000019342 in Epoch 498
Epoch 520
Epoch 520, Loss: 0.000101824, Improvement: 0.000049883, Best Loss: 0.000019342 in Epoch 498
Epoch 521
Epoch 521, Loss: 0.000130491, Improvement: 0.000028668, Best Loss: 0.000019342 in Epoch 498
Epoch 522
Epoch 522, Loss: 0.000087334, Improvement: -0.000043157, Best Loss: 0.000019342 in Epoch 498
Epoch 523
Epoch 523, Loss: 0.000082544, Improvement: -0.000004790, Best Loss: 0.000019342 in Epoch 498
Epoch 524
Epoch 524, Loss: 0.000063409, Improvement: -0.000019135, Best Loss: 0.000019342 in Epoch 498
Epoch 525
Epoch 525, Loss: 0.000052090, Improvement: -0.000011319, Best Loss: 0.000019342 in Epoch 498
Epoch 526
Epoch 526, Loss: 0.000045968, Improvement: -0.000006123, Best Loss: 0.000019342 in Epoch 498
Epoch 527
A best model at epoch 527 has been saved with training error 0.000018893.
Epoch 527, Loss: 0.000033177, Improvement: -0.000012790, Best Loss: 0.000018893 in Epoch 527
Epoch 528
Epoch 528, Loss: 0.000037033, Improvement: 0.000003856, Best Loss: 0.000018893 in Epoch 527
Epoch 529
Epoch 529, Loss: 0.000095163, Improvement: 0.000058130, Best Loss: 0.000018893 in Epoch 527
Epoch 530
Epoch 530, Loss: 0.000106085, Improvement: 0.000010922, Best Loss: 0.000018893 in Epoch 527
Epoch 531
Epoch 531, Loss: 0.000083913, Improvement: -0.000022172, Best Loss: 0.000018893 in Epoch 527
Epoch 532
Epoch 532, Loss: 0.000053961, Improvement: -0.000029952, Best Loss: 0.000018893 in Epoch 527
Epoch 533
Epoch 533, Loss: 0.000044138, Improvement: -0.000009823, Best Loss: 0.000018893 in Epoch 527
Epoch 534
Epoch 534, Loss: 0.000050445, Improvement: 0.000006307, Best Loss: 0.000018893 in Epoch 527
Epoch 535
Epoch 535, Loss: 0.000036130, Improvement: -0.000014315, Best Loss: 0.000018893 in Epoch 527
Epoch 536
A best model at epoch 536 has been saved with training error 0.000018107.
A best model at epoch 536 has been saved with training error 0.000017784.
Epoch 536, Loss: 0.000027397, Improvement: -0.000008733, Best Loss: 0.000017784 in Epoch 536
Epoch 537
A best model at epoch 537 has been saved with training error 0.000015155.
Epoch 537, Loss: 0.000028898, Improvement: 0.000001500, Best Loss: 0.000015155 in Epoch 537
Epoch 538
Epoch 538, Loss: 0.000061793, Improvement: 0.000032895, Best Loss: 0.000015155 in Epoch 537
Epoch 539
Epoch 539, Loss: 0.000079396, Improvement: 0.000017603, Best Loss: 0.000015155 in Epoch 537
Epoch 540
Epoch 540, Loss: 0.000108016, Improvement: 0.000028619, Best Loss: 0.000015155 in Epoch 537
Epoch 541
Epoch 541, Loss: 0.000072803, Improvement: -0.000035213, Best Loss: 0.000015155 in Epoch 537
Epoch 542
Epoch 542, Loss: 0.000082102, Improvement: 0.000009299, Best Loss: 0.000015155 in Epoch 537
Epoch 543
Epoch 543, Loss: 0.000058305, Improvement: -0.000023797, Best Loss: 0.000015155 in Epoch 537
Epoch 544
Epoch 544, Loss: 0.000045611, Improvement: -0.000012695, Best Loss: 0.000015155 in Epoch 537
Epoch 545
Epoch 545, Loss: 0.000054122, Improvement: 0.000008511, Best Loss: 0.000015155 in Epoch 537
Epoch 546
Epoch 546, Loss: 0.000094870, Improvement: 0.000040748, Best Loss: 0.000015155 in Epoch 537
Epoch 547
Epoch 547, Loss: 0.000080336, Improvement: -0.000014534, Best Loss: 0.000015155 in Epoch 537
Epoch 548
Epoch 548, Loss: 0.000078766, Improvement: -0.000001570, Best Loss: 0.000015155 in Epoch 537
Epoch 549
Epoch 549, Loss: 0.000069548, Improvement: -0.000009218, Best Loss: 0.000015155 in Epoch 537
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.000033553, Improvement: -0.000035995, Best Loss: 0.000015155 in Epoch 537
Epoch 551
A best model at epoch 551 has been saved with training error 0.000015098.
Epoch 551, Loss: 0.000026737, Improvement: -0.000006816, Best Loss: 0.000015098 in Epoch 551
Epoch 552
Epoch 552, Loss: 0.000022280, Improvement: -0.000004457, Best Loss: 0.000015098 in Epoch 551
Epoch 553
Epoch 553, Loss: 0.000029043, Improvement: 0.000006763, Best Loss: 0.000015098 in Epoch 551
Epoch 554
Epoch 554, Loss: 0.000031437, Improvement: 0.000002394, Best Loss: 0.000015098 in Epoch 551
Epoch 555
Epoch 555, Loss: 0.000033596, Improvement: 0.000002158, Best Loss: 0.000015098 in Epoch 551
Epoch 556
Epoch 556, Loss: 0.000038770, Improvement: 0.000005174, Best Loss: 0.000015098 in Epoch 551
Epoch 557
Epoch 557, Loss: 0.000038934, Improvement: 0.000000165, Best Loss: 0.000015098 in Epoch 551
Epoch 558
Epoch 558, Loss: 0.000031272, Improvement: -0.000007663, Best Loss: 0.000015098 in Epoch 551
Epoch 559
Epoch 559, Loss: 0.000047350, Improvement: 0.000016079, Best Loss: 0.000015098 in Epoch 551
Epoch 560
Epoch 560, Loss: 0.000088600, Improvement: 0.000041249, Best Loss: 0.000015098 in Epoch 551
Epoch 561
Epoch 561, Loss: 0.000066234, Improvement: -0.000022366, Best Loss: 0.000015098 in Epoch 551
Epoch 562
Epoch 562, Loss: 0.000084342, Improvement: 0.000018108, Best Loss: 0.000015098 in Epoch 551
Epoch 563
Epoch 563, Loss: 0.000054770, Improvement: -0.000029572, Best Loss: 0.000015098 in Epoch 551
Epoch 564
Epoch 564, Loss: 0.000077504, Improvement: 0.000022734, Best Loss: 0.000015098 in Epoch 551
Epoch 565
Epoch 565, Loss: 0.000186225, Improvement: 0.000108721, Best Loss: 0.000015098 in Epoch 551
Epoch 566
Epoch 566, Loss: 0.000115815, Improvement: -0.000070410, Best Loss: 0.000015098 in Epoch 551
Epoch 567
Epoch 567, Loss: 0.000078670, Improvement: -0.000037145, Best Loss: 0.000015098 in Epoch 551
Epoch 568
Epoch 568, Loss: 0.000066671, Improvement: -0.000011999, Best Loss: 0.000015098 in Epoch 551
Epoch 569
Epoch 569, Loss: 0.000041802, Improvement: -0.000024869, Best Loss: 0.000015098 in Epoch 551
Epoch 570
Epoch 570, Loss: 0.000055548, Improvement: 0.000013746, Best Loss: 0.000015098 in Epoch 551
Epoch 571
Epoch 571, Loss: 0.000040937, Improvement: -0.000014611, Best Loss: 0.000015098 in Epoch 551
Epoch 572
Epoch 572, Loss: 0.000030545, Improvement: -0.000010392, Best Loss: 0.000015098 in Epoch 551
Epoch 573
Epoch 573, Loss: 0.000033962, Improvement: 0.000003417, Best Loss: 0.000015098 in Epoch 551
Epoch 574
Epoch 574, Loss: 0.000039046, Improvement: 0.000005084, Best Loss: 0.000015098 in Epoch 551
Epoch 575
Epoch 575, Loss: 0.000037247, Improvement: -0.000001799, Best Loss: 0.000015098 in Epoch 551
Epoch 576
Epoch 576, Loss: 0.000046191, Improvement: 0.000008944, Best Loss: 0.000015098 in Epoch 551
Epoch 577
Epoch 577, Loss: 0.000069503, Improvement: 0.000023312, Best Loss: 0.000015098 in Epoch 551
Epoch 578
Epoch 578, Loss: 0.000106751, Improvement: 0.000037248, Best Loss: 0.000015098 in Epoch 551
Epoch 579
Epoch 579, Loss: 0.000119542, Improvement: 0.000012791, Best Loss: 0.000015098 in Epoch 551
Epoch 580
Epoch 580, Loss: 0.000074187, Improvement: -0.000045355, Best Loss: 0.000015098 in Epoch 551
Epoch 581
Epoch 581, Loss: 0.000044630, Improvement: -0.000029557, Best Loss: 0.000015098 in Epoch 551
Epoch 582
Epoch 582, Loss: 0.000038125, Improvement: -0.000006505, Best Loss: 0.000015098 in Epoch 551
Epoch 583
Epoch 583, Loss: 0.000040183, Improvement: 0.000002057, Best Loss: 0.000015098 in Epoch 551
Epoch 584
Epoch 584, Loss: 0.000042678, Improvement: 0.000002496, Best Loss: 0.000015098 in Epoch 551
Epoch 585
Epoch 585, Loss: 0.000037477, Improvement: -0.000005202, Best Loss: 0.000015098 in Epoch 551
Epoch 586
A best model at epoch 586 has been saved with training error 0.000013955.
Epoch 586, Loss: 0.000026620, Improvement: -0.000010857, Best Loss: 0.000013955 in Epoch 586
Epoch 587
A best model at epoch 587 has been saved with training error 0.000010980.
Epoch 587, Loss: 0.000021883, Improvement: -0.000004736, Best Loss: 0.000010980 in Epoch 587
Epoch 588
Epoch 588, Loss: 0.000044739, Improvement: 0.000022856, Best Loss: 0.000010980 in Epoch 587
Epoch 589
Epoch 589, Loss: 0.000043587, Improvement: -0.000001152, Best Loss: 0.000010980 in Epoch 587
Epoch 590
Epoch 590, Loss: 0.000070501, Improvement: 0.000026914, Best Loss: 0.000010980 in Epoch 587
Epoch 591
Epoch 591, Loss: 0.000060067, Improvement: -0.000010434, Best Loss: 0.000010980 in Epoch 587
Epoch 592
Epoch 592, Loss: 0.000038249, Improvement: -0.000021818, Best Loss: 0.000010980 in Epoch 587
Epoch 593
Epoch 593, Loss: 0.000042415, Improvement: 0.000004166, Best Loss: 0.000010980 in Epoch 587
Epoch 594
Epoch 594, Loss: 0.000050574, Improvement: 0.000008159, Best Loss: 0.000010980 in Epoch 587
Epoch 595
Epoch 595, Loss: 0.000025125, Improvement: -0.000025448, Best Loss: 0.000010980 in Epoch 587
Epoch 596
Epoch 596, Loss: 0.000033237, Improvement: 0.000008112, Best Loss: 0.000010980 in Epoch 587
Epoch 597
Epoch 597, Loss: 0.000078831, Improvement: 0.000045594, Best Loss: 0.000010980 in Epoch 587
Epoch 598
Epoch 598, Loss: 0.000161347, Improvement: 0.000082516, Best Loss: 0.000010980 in Epoch 587
Epoch 599
Epoch 599, Loss: 0.000119258, Improvement: -0.000042089, Best Loss: 0.000010980 in Epoch 587
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.000043093, Improvement: -0.000076166, Best Loss: 0.000010980 in Epoch 587
Epoch 601
Epoch 601, Loss: 0.000027135, Improvement: -0.000015958, Best Loss: 0.000010980 in Epoch 587
Epoch 602
Epoch 602, Loss: 0.000019599, Improvement: -0.000007535, Best Loss: 0.000010980 in Epoch 587
Epoch 603
Epoch 603, Loss: 0.000015904, Improvement: -0.000003695, Best Loss: 0.000010980 in Epoch 587
Epoch 604
A best model at epoch 604 has been saved with training error 0.000010439.
Epoch 604, Loss: 0.000013833, Improvement: -0.000002071, Best Loss: 0.000010439 in Epoch 604
Epoch 605
A best model at epoch 605 has been saved with training error 0.000009207.
Epoch 605, Loss: 0.000015008, Improvement: 0.000001175, Best Loss: 0.000009207 in Epoch 605
Epoch 606
Epoch 606, Loss: 0.000016783, Improvement: 0.000001775, Best Loss: 0.000009207 in Epoch 605
Epoch 607
Epoch 607, Loss: 0.000017153, Improvement: 0.000000370, Best Loss: 0.000009207 in Epoch 605
Epoch 608
Epoch 608, Loss: 0.000018143, Improvement: 0.000000990, Best Loss: 0.000009207 in Epoch 605
Epoch 609
Epoch 609, Loss: 0.000017399, Improvement: -0.000000744, Best Loss: 0.000009207 in Epoch 605
Epoch 610
Epoch 610, Loss: 0.000022296, Improvement: 0.000004897, Best Loss: 0.000009207 in Epoch 605
Epoch 611
Epoch 611, Loss: 0.000036002, Improvement: 0.000013706, Best Loss: 0.000009207 in Epoch 605
Epoch 612
Epoch 612, Loss: 0.000087073, Improvement: 0.000051072, Best Loss: 0.000009207 in Epoch 605
Epoch 613
Epoch 613, Loss: 0.000108797, Improvement: 0.000021723, Best Loss: 0.000009207 in Epoch 605
Epoch 614
Epoch 614, Loss: 0.000099231, Improvement: -0.000009566, Best Loss: 0.000009207 in Epoch 605
Epoch 615
Epoch 615, Loss: 0.000076649, Improvement: -0.000022582, Best Loss: 0.000009207 in Epoch 605
Epoch 616
Epoch 616, Loss: 0.000041054, Improvement: -0.000035595, Best Loss: 0.000009207 in Epoch 605
Epoch 617
Epoch 617, Loss: 0.000034231, Improvement: -0.000006823, Best Loss: 0.000009207 in Epoch 605
Epoch 618
Epoch 618, Loss: 0.000027704, Improvement: -0.000006527, Best Loss: 0.000009207 in Epoch 605
Epoch 619
Epoch 619, Loss: 0.000029243, Improvement: 0.000001539, Best Loss: 0.000009207 in Epoch 605
Epoch 620
Epoch 620, Loss: 0.000039804, Improvement: 0.000010561, Best Loss: 0.000009207 in Epoch 605
Epoch 621
Epoch 621, Loss: 0.000029280, Improvement: -0.000010524, Best Loss: 0.000009207 in Epoch 605
Epoch 622
Epoch 622, Loss: 0.000022215, Improvement: -0.000007065, Best Loss: 0.000009207 in Epoch 605
Epoch 623
Epoch 623, Loss: 0.000035478, Improvement: 0.000013263, Best Loss: 0.000009207 in Epoch 605
Epoch 624
Epoch 624, Loss: 0.000053488, Improvement: 0.000018011, Best Loss: 0.000009207 in Epoch 605
Epoch 625
Epoch 625, Loss: 0.000036903, Improvement: -0.000016586, Best Loss: 0.000009207 in Epoch 605
Epoch 626
Epoch 626, Loss: 0.000068755, Improvement: 0.000031852, Best Loss: 0.000009207 in Epoch 605
Epoch 627
Epoch 627, Loss: 0.000110635, Improvement: 0.000041880, Best Loss: 0.000009207 in Epoch 605
Epoch 628
Epoch 628, Loss: 0.000137688, Improvement: 0.000027053, Best Loss: 0.000009207 in Epoch 605
Epoch 629
Epoch 629, Loss: 0.000074069, Improvement: -0.000063618, Best Loss: 0.000009207 in Epoch 605
Epoch 630
Epoch 630, Loss: 0.000042123, Improvement: -0.000031947, Best Loss: 0.000009207 in Epoch 605
Epoch 631
Epoch 631, Loss: 0.000029463, Improvement: -0.000012660, Best Loss: 0.000009207 in Epoch 605
Epoch 632
Epoch 632, Loss: 0.000020415, Improvement: -0.000009048, Best Loss: 0.000009207 in Epoch 605
Epoch 633
Epoch 633, Loss: 0.000014295, Improvement: -0.000006120, Best Loss: 0.000009207 in Epoch 605
Epoch 634
Epoch 634, Loss: 0.000012527, Improvement: -0.000001768, Best Loss: 0.000009207 in Epoch 605
Epoch 635
Epoch 635, Loss: 0.000012589, Improvement: 0.000000062, Best Loss: 0.000009207 in Epoch 605
Epoch 636
Epoch 636, Loss: 0.000016190, Improvement: 0.000003601, Best Loss: 0.000009207 in Epoch 605
Epoch 637
Epoch 637, Loss: 0.000029424, Improvement: 0.000013234, Best Loss: 0.000009207 in Epoch 605
Epoch 638
Epoch 638, Loss: 0.000019120, Improvement: -0.000010304, Best Loss: 0.000009207 in Epoch 605
Epoch 639
Epoch 639, Loss: 0.000021917, Improvement: 0.000002797, Best Loss: 0.000009207 in Epoch 605
Epoch 640
Epoch 640, Loss: 0.000030308, Improvement: 0.000008391, Best Loss: 0.000009207 in Epoch 605
Epoch 641
Epoch 641, Loss: 0.000025788, Improvement: -0.000004520, Best Loss: 0.000009207 in Epoch 605
Epoch 642
Epoch 642, Loss: 0.000019738, Improvement: -0.000006050, Best Loss: 0.000009207 in Epoch 605
Epoch 643
Epoch 643, Loss: 0.000018758, Improvement: -0.000000980, Best Loss: 0.000009207 in Epoch 605
Epoch 644
Epoch 644, Loss: 0.000017321, Improvement: -0.000001437, Best Loss: 0.000009207 in Epoch 605
Epoch 645
A best model at epoch 645 has been saved with training error 0.000008841.
A best model at epoch 645 has been saved with training error 0.000008789.
Epoch 645, Loss: 0.000011967, Improvement: -0.000005355, Best Loss: 0.000008789 in Epoch 645
Epoch 646
Epoch 646, Loss: 0.000011504, Improvement: -0.000000463, Best Loss: 0.000008789 in Epoch 645
Epoch 647
A best model at epoch 647 has been saved with training error 0.000007573.
Epoch 647, Loss: 0.000018492, Improvement: 0.000006988, Best Loss: 0.000007573 in Epoch 647
Epoch 648
Epoch 648, Loss: 0.000080286, Improvement: 0.000061794, Best Loss: 0.000007573 in Epoch 647
Epoch 649
Epoch 649, Loss: 0.000125321, Improvement: 0.000045035, Best Loss: 0.000007573 in Epoch 647
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.000073265, Improvement: -0.000052056, Best Loss: 0.000007573 in Epoch 647
Epoch 651
Epoch 651, Loss: 0.000040581, Improvement: -0.000032684, Best Loss: 0.000007573 in Epoch 647
Epoch 652
Epoch 652, Loss: 0.000025822, Improvement: -0.000014758, Best Loss: 0.000007573 in Epoch 647
Epoch 653
Epoch 653, Loss: 0.000022633, Improvement: -0.000003189, Best Loss: 0.000007573 in Epoch 647
Epoch 654
Epoch 654, Loss: 0.000028570, Improvement: 0.000005937, Best Loss: 0.000007573 in Epoch 647
Epoch 655
Epoch 655, Loss: 0.000034184, Improvement: 0.000005614, Best Loss: 0.000007573 in Epoch 647
Epoch 656
Epoch 656, Loss: 0.000038674, Improvement: 0.000004490, Best Loss: 0.000007573 in Epoch 647
Epoch 657
Epoch 657, Loss: 0.000037037, Improvement: -0.000001638, Best Loss: 0.000007573 in Epoch 647
Epoch 658
Epoch 658, Loss: 0.000040200, Improvement: 0.000003163, Best Loss: 0.000007573 in Epoch 647
Epoch 659
Epoch 659, Loss: 0.000049391, Improvement: 0.000009191, Best Loss: 0.000007573 in Epoch 647
Epoch 660
Epoch 660, Loss: 0.000039206, Improvement: -0.000010185, Best Loss: 0.000007573 in Epoch 647
Epoch 661
Epoch 661, Loss: 0.000029632, Improvement: -0.000009574, Best Loss: 0.000007573 in Epoch 647
Epoch 662
Epoch 662, Loss: 0.000024700, Improvement: -0.000004932, Best Loss: 0.000007573 in Epoch 647
Epoch 663
Epoch 663, Loss: 0.000020521, Improvement: -0.000004179, Best Loss: 0.000007573 in Epoch 647
Epoch 664
Epoch 664, Loss: 0.000018128, Improvement: -0.000002393, Best Loss: 0.000007573 in Epoch 647
Epoch 665
Epoch 665, Loss: 0.000024653, Improvement: 0.000006524, Best Loss: 0.000007573 in Epoch 647
Epoch 666
Epoch 666, Loss: 0.000037182, Improvement: 0.000012529, Best Loss: 0.000007573 in Epoch 647
Epoch 667
Epoch 667, Loss: 0.000089992, Improvement: 0.000052810, Best Loss: 0.000007573 in Epoch 647
Epoch 668
Epoch 668, Loss: 0.000081131, Improvement: -0.000008861, Best Loss: 0.000007573 in Epoch 647
Epoch 669
Epoch 669, Loss: 0.000036885, Improvement: -0.000044246, Best Loss: 0.000007573 in Epoch 647
Epoch 670
Epoch 670, Loss: 0.000024740, Improvement: -0.000012146, Best Loss: 0.000007573 in Epoch 647
Epoch 671
Epoch 671, Loss: 0.000014937, Improvement: -0.000009803, Best Loss: 0.000007573 in Epoch 647
Epoch 672
Epoch 672, Loss: 0.000012353, Improvement: -0.000002584, Best Loss: 0.000007573 in Epoch 647
Epoch 673
A best model at epoch 673 has been saved with training error 0.000007450.
Epoch 673, Loss: 0.000015924, Improvement: 0.000003570, Best Loss: 0.000007450 in Epoch 673
Epoch 674
Epoch 674, Loss: 0.000025178, Improvement: 0.000009255, Best Loss: 0.000007450 in Epoch 673
Epoch 675
Epoch 675, Loss: 0.000020890, Improvement: -0.000004289, Best Loss: 0.000007450 in Epoch 673
Epoch 676
Epoch 676, Loss: 0.000025311, Improvement: 0.000004422, Best Loss: 0.000007450 in Epoch 673
Epoch 677
Epoch 677, Loss: 0.000029088, Improvement: 0.000003777, Best Loss: 0.000007450 in Epoch 673
Epoch 678
Epoch 678, Loss: 0.000034206, Improvement: 0.000005118, Best Loss: 0.000007450 in Epoch 673
Epoch 679
Epoch 679, Loss: 0.000025060, Improvement: -0.000009146, Best Loss: 0.000007450 in Epoch 673
Epoch 680
Epoch 680, Loss: 0.000016841, Improvement: -0.000008219, Best Loss: 0.000007450 in Epoch 673
Epoch 681
Epoch 681, Loss: 0.000022710, Improvement: 0.000005869, Best Loss: 0.000007450 in Epoch 673
Epoch 682
Epoch 682, Loss: 0.000025398, Improvement: 0.000002687, Best Loss: 0.000007450 in Epoch 673
Epoch 683
Epoch 683, Loss: 0.000018537, Improvement: -0.000006861, Best Loss: 0.000007450 in Epoch 673
Epoch 684
Epoch 684, Loss: 0.000028370, Improvement: 0.000009833, Best Loss: 0.000007450 in Epoch 673
Epoch 685
Epoch 685, Loss: 0.000026497, Improvement: -0.000001873, Best Loss: 0.000007450 in Epoch 673
Epoch 686
Epoch 686, Loss: 0.000035552, Improvement: 0.000009055, Best Loss: 0.000007450 in Epoch 673
Epoch 687
Epoch 687, Loss: 0.000036234, Improvement: 0.000000682, Best Loss: 0.000007450 in Epoch 673
Epoch 688
Epoch 688, Loss: 0.000049692, Improvement: 0.000013458, Best Loss: 0.000007450 in Epoch 673
Epoch 689
Epoch 689, Loss: 0.000034814, Improvement: -0.000014879, Best Loss: 0.000007450 in Epoch 673
Epoch 690
Epoch 690, Loss: 0.000040623, Improvement: 0.000005809, Best Loss: 0.000007450 in Epoch 673
Epoch 691
Epoch 691, Loss: 0.000034456, Improvement: -0.000006167, Best Loss: 0.000007450 in Epoch 673
Epoch 692
Epoch 692, Loss: 0.000033375, Improvement: -0.000001081, Best Loss: 0.000007450 in Epoch 673
Epoch 693
Epoch 693, Loss: 0.000030099, Improvement: -0.000003275, Best Loss: 0.000007450 in Epoch 673
Epoch 694
Epoch 694, Loss: 0.000020195, Improvement: -0.000009905, Best Loss: 0.000007450 in Epoch 673
Epoch 695
Epoch 695, Loss: 0.000021386, Improvement: 0.000001191, Best Loss: 0.000007450 in Epoch 673
Epoch 696
Epoch 696, Loss: 0.000033032, Improvement: 0.000011646, Best Loss: 0.000007450 in Epoch 673
Epoch 697
Epoch 697, Loss: 0.000038900, Improvement: 0.000005868, Best Loss: 0.000007450 in Epoch 673
Epoch 698
Epoch 698, Loss: 0.000075583, Improvement: 0.000036683, Best Loss: 0.000007450 in Epoch 673
Epoch 699
Epoch 699, Loss: 0.000097276, Improvement: 0.000021693, Best Loss: 0.000007450 in Epoch 673
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.000044497, Improvement: -0.000052779, Best Loss: 0.000007450 in Epoch 673
Epoch 701
Epoch 701, Loss: 0.000032520, Improvement: -0.000011978, Best Loss: 0.000007450 in Epoch 673
Epoch 702
Epoch 702, Loss: 0.000020283, Improvement: -0.000012237, Best Loss: 0.000007450 in Epoch 673
Epoch 703
Epoch 703, Loss: 0.000015005, Improvement: -0.000005278, Best Loss: 0.000007450 in Epoch 673
Epoch 704
Epoch 704, Loss: 0.000011248, Improvement: -0.000003757, Best Loss: 0.000007450 in Epoch 673
Epoch 705
A best model at epoch 705 has been saved with training error 0.000007426.
A best model at epoch 705 has been saved with training error 0.000006862.
Epoch 705, Loss: 0.000009335, Improvement: -0.000001913, Best Loss: 0.000006862 in Epoch 705
Epoch 706
A best model at epoch 706 has been saved with training error 0.000006044.
A best model at epoch 706 has been saved with training error 0.000005728.
Epoch 706, Loss: 0.000007767, Improvement: -0.000001568, Best Loss: 0.000005728 in Epoch 706
Epoch 707
A best model at epoch 707 has been saved with training error 0.000005471.
A best model at epoch 707 has been saved with training error 0.000005116.
A best model at epoch 707 has been saved with training error 0.000004478.
Epoch 707, Loss: 0.000007494, Improvement: -0.000000273, Best Loss: 0.000004478 in Epoch 707
Epoch 708
Epoch 708, Loss: 0.000015154, Improvement: 0.000007660, Best Loss: 0.000004478 in Epoch 707
Epoch 709
Epoch 709, Loss: 0.000030298, Improvement: 0.000015144, Best Loss: 0.000004478 in Epoch 707
Epoch 710
Epoch 710, Loss: 0.000021878, Improvement: -0.000008420, Best Loss: 0.000004478 in Epoch 707
Epoch 711
Epoch 711, Loss: 0.000015188, Improvement: -0.000006690, Best Loss: 0.000004478 in Epoch 707
Epoch 712
Epoch 712, Loss: 0.000024923, Improvement: 0.000009735, Best Loss: 0.000004478 in Epoch 707
Epoch 713
Epoch 713, Loss: 0.000024071, Improvement: -0.000000852, Best Loss: 0.000004478 in Epoch 707
Epoch 714
Epoch 714, Loss: 0.000044628, Improvement: 0.000020557, Best Loss: 0.000004478 in Epoch 707
Epoch 715
Epoch 715, Loss: 0.000039760, Improvement: -0.000004868, Best Loss: 0.000004478 in Epoch 707
Epoch 716
Epoch 716, Loss: 0.000021255, Improvement: -0.000018505, Best Loss: 0.000004478 in Epoch 707
Epoch 717
Epoch 717, Loss: 0.000016010, Improvement: -0.000005246, Best Loss: 0.000004478 in Epoch 707
Epoch 718
Epoch 718, Loss: 0.000020711, Improvement: 0.000004701, Best Loss: 0.000004478 in Epoch 707
Epoch 719
Epoch 719, Loss: 0.000031799, Improvement: 0.000011088, Best Loss: 0.000004478 in Epoch 707
Epoch 720
Epoch 720, Loss: 0.000031053, Improvement: -0.000000746, Best Loss: 0.000004478 in Epoch 707
Epoch 721
Epoch 721, Loss: 0.000029828, Improvement: -0.000001225, Best Loss: 0.000004478 in Epoch 707
Epoch 722
Epoch 722, Loss: 0.000022967, Improvement: -0.000006861, Best Loss: 0.000004478 in Epoch 707
Epoch 723
Epoch 723, Loss: 0.000027241, Improvement: 0.000004274, Best Loss: 0.000004478 in Epoch 707
Epoch 724
Epoch 724, Loss: 0.000043567, Improvement: 0.000016326, Best Loss: 0.000004478 in Epoch 707
Epoch 725
Epoch 725, Loss: 0.000024729, Improvement: -0.000018838, Best Loss: 0.000004478 in Epoch 707
Epoch 726
Epoch 726, Loss: 0.000016683, Improvement: -0.000008046, Best Loss: 0.000004478 in Epoch 707
Epoch 727
Epoch 727, Loss: 0.000014030, Improvement: -0.000002654, Best Loss: 0.000004478 in Epoch 707
Epoch 728
Epoch 728, Loss: 0.000011727, Improvement: -0.000002302, Best Loss: 0.000004478 in Epoch 707
Epoch 729
Epoch 729, Loss: 0.000017319, Improvement: 0.000005592, Best Loss: 0.000004478 in Epoch 707
Epoch 730
Epoch 730, Loss: 0.000029077, Improvement: 0.000011757, Best Loss: 0.000004478 in Epoch 707
Epoch 731
Epoch 731, Loss: 0.000033092, Improvement: 0.000004015, Best Loss: 0.000004478 in Epoch 707
Epoch 732
Epoch 732, Loss: 0.000033645, Improvement: 0.000000553, Best Loss: 0.000004478 in Epoch 707
Epoch 733
Epoch 733, Loss: 0.000025721, Improvement: -0.000007924, Best Loss: 0.000004478 in Epoch 707
Epoch 734
Epoch 734, Loss: 0.000013462, Improvement: -0.000012259, Best Loss: 0.000004478 in Epoch 707
Epoch 735
Epoch 735, Loss: 0.000014543, Improvement: 0.000001081, Best Loss: 0.000004478 in Epoch 707
Epoch 736
Epoch 736, Loss: 0.000024084, Improvement: 0.000009541, Best Loss: 0.000004478 in Epoch 707
Epoch 737
Epoch 737, Loss: 0.000026802, Improvement: 0.000002718, Best Loss: 0.000004478 in Epoch 707
Epoch 738
Epoch 738, Loss: 0.000027630, Improvement: 0.000000828, Best Loss: 0.000004478 in Epoch 707
Epoch 739
Epoch 739, Loss: 0.000062284, Improvement: 0.000034654, Best Loss: 0.000004478 in Epoch 707
Epoch 740
Epoch 740, Loss: 0.000091354, Improvement: 0.000029070, Best Loss: 0.000004478 in Epoch 707
Epoch 741
Epoch 741, Loss: 0.000063284, Improvement: -0.000028070, Best Loss: 0.000004478 in Epoch 707
Epoch 742
Epoch 742, Loss: 0.000027691, Improvement: -0.000035594, Best Loss: 0.000004478 in Epoch 707
Epoch 743
Epoch 743, Loss: 0.000021884, Improvement: -0.000005807, Best Loss: 0.000004478 in Epoch 707
Epoch 744
Epoch 744, Loss: 0.000039393, Improvement: 0.000017509, Best Loss: 0.000004478 in Epoch 707
Epoch 745
Epoch 745, Loss: 0.000041596, Improvement: 0.000002203, Best Loss: 0.000004478 in Epoch 707
Epoch 746
Epoch 746, Loss: 0.000034439, Improvement: -0.000007157, Best Loss: 0.000004478 in Epoch 707
Epoch 747
Epoch 747, Loss: 0.000028573, Improvement: -0.000005866, Best Loss: 0.000004478 in Epoch 707
Epoch 748
Epoch 748, Loss: 0.000023372, Improvement: -0.000005201, Best Loss: 0.000004478 in Epoch 707
Epoch 749
Epoch 749, Loss: 0.000020636, Improvement: -0.000002735, Best Loss: 0.000004478 in Epoch 707
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.000034179, Improvement: 0.000013542, Best Loss: 0.000004478 in Epoch 707
Epoch 751
Epoch 751, Loss: 0.000023732, Improvement: -0.000010447, Best Loss: 0.000004478 in Epoch 707
Epoch 752
Epoch 752, Loss: 0.000015955, Improvement: -0.000007777, Best Loss: 0.000004478 in Epoch 707
Epoch 753
Epoch 753, Loss: 0.000011760, Improvement: -0.000004195, Best Loss: 0.000004478 in Epoch 707
Epoch 754
Epoch 754, Loss: 0.000011604, Improvement: -0.000000156, Best Loss: 0.000004478 in Epoch 707
Epoch 755
Epoch 755, Loss: 0.000011185, Improvement: -0.000000419, Best Loss: 0.000004478 in Epoch 707
Epoch 756
Epoch 756, Loss: 0.000016318, Improvement: 0.000005133, Best Loss: 0.000004478 in Epoch 707
Epoch 757
Epoch 757, Loss: 0.000019447, Improvement: 0.000003129, Best Loss: 0.000004478 in Epoch 707
Epoch 758
Epoch 758, Loss: 0.000028640, Improvement: 0.000009193, Best Loss: 0.000004478 in Epoch 707
Epoch 759
Epoch 759, Loss: 0.000028231, Improvement: -0.000000409, Best Loss: 0.000004478 in Epoch 707
Epoch 760
Epoch 760, Loss: 0.000031741, Improvement: 0.000003510, Best Loss: 0.000004478 in Epoch 707
Epoch 761
Epoch 761, Loss: 0.000039563, Improvement: 0.000007821, Best Loss: 0.000004478 in Epoch 707
Epoch 762
Epoch 762, Loss: 0.000062728, Improvement: 0.000023165, Best Loss: 0.000004478 in Epoch 707
Epoch 763
Epoch 763, Loss: 0.000044675, Improvement: -0.000018052, Best Loss: 0.000004478 in Epoch 707
Epoch 764
Epoch 764, Loss: 0.000032480, Improvement: -0.000012195, Best Loss: 0.000004478 in Epoch 707
Epoch 765
Epoch 765, Loss: 0.000042784, Improvement: 0.000010304, Best Loss: 0.000004478 in Epoch 707
Epoch 766
Epoch 766, Loss: 0.000056077, Improvement: 0.000013293, Best Loss: 0.000004478 in Epoch 707
Epoch 767
Epoch 767, Loss: 0.000053789, Improvement: -0.000002288, Best Loss: 0.000004478 in Epoch 707
Epoch 768
Epoch 768, Loss: 0.000040395, Improvement: -0.000013394, Best Loss: 0.000004478 in Epoch 707
Epoch 769
Epoch 769, Loss: 0.000022572, Improvement: -0.000017823, Best Loss: 0.000004478 in Epoch 707
Epoch 770
Epoch 770, Loss: 0.000010837, Improvement: -0.000011735, Best Loss: 0.000004478 in Epoch 707
Epoch 771
Epoch 771, Loss: 0.000011831, Improvement: 0.000000994, Best Loss: 0.000004478 in Epoch 707
Epoch 772
Epoch 772, Loss: 0.000019593, Improvement: 0.000007762, Best Loss: 0.000004478 in Epoch 707
Epoch 773
Epoch 773, Loss: 0.000011597, Improvement: -0.000007996, Best Loss: 0.000004478 in Epoch 707
Epoch 774
Epoch 774, Loss: 0.000010154, Improvement: -0.000001444, Best Loss: 0.000004478 in Epoch 707
Epoch 775
Epoch 775, Loss: 0.000023258, Improvement: 0.000013104, Best Loss: 0.000004478 in Epoch 707
Epoch 776
Epoch 776, Loss: 0.000035660, Improvement: 0.000012402, Best Loss: 0.000004478 in Epoch 707
Epoch 777
Epoch 777, Loss: 0.000025608, Improvement: -0.000010051, Best Loss: 0.000004478 in Epoch 707
Epoch 778
Epoch 778, Loss: 0.000012795, Improvement: -0.000012813, Best Loss: 0.000004478 in Epoch 707
Epoch 779
Epoch 779, Loss: 0.000015157, Improvement: 0.000002362, Best Loss: 0.000004478 in Epoch 707
Epoch 780
Epoch 780, Loss: 0.000033819, Improvement: 0.000018662, Best Loss: 0.000004478 in Epoch 707
Epoch 781
Epoch 781, Loss: 0.000035519, Improvement: 0.000001700, Best Loss: 0.000004478 in Epoch 707
Epoch 782
Epoch 782, Loss: 0.000017496, Improvement: -0.000018023, Best Loss: 0.000004478 in Epoch 707
Epoch 783
Epoch 783, Loss: 0.000017129, Improvement: -0.000000367, Best Loss: 0.000004478 in Epoch 707
Epoch 784
Epoch 784, Loss: 0.000025312, Improvement: 0.000008183, Best Loss: 0.000004478 in Epoch 707
Epoch 785
Epoch 785, Loss: 0.000023488, Improvement: -0.000001824, Best Loss: 0.000004478 in Epoch 707
Epoch 786
Epoch 786, Loss: 0.000023210, Improvement: -0.000000277, Best Loss: 0.000004478 in Epoch 707
Epoch 787
Epoch 787, Loss: 0.000017574, Improvement: -0.000005636, Best Loss: 0.000004478 in Epoch 707
Epoch 788
Epoch 788, Loss: 0.000013061, Improvement: -0.000004513, Best Loss: 0.000004478 in Epoch 707
Epoch 789
Epoch 789, Loss: 0.000019159, Improvement: 0.000006098, Best Loss: 0.000004478 in Epoch 707
Epoch 790
Epoch 790, Loss: 0.000020630, Improvement: 0.000001471, Best Loss: 0.000004478 in Epoch 707
Epoch 791
Epoch 791, Loss: 0.000033812, Improvement: 0.000013182, Best Loss: 0.000004478 in Epoch 707
Epoch 792
Epoch 792, Loss: 0.000018521, Improvement: -0.000015291, Best Loss: 0.000004478 in Epoch 707
Epoch 793
Epoch 793, Loss: 0.000013762, Improvement: -0.000004759, Best Loss: 0.000004478 in Epoch 707
Epoch 794
Epoch 794, Loss: 0.000039123, Improvement: 0.000025362, Best Loss: 0.000004478 in Epoch 707
Epoch 795
Epoch 795, Loss: 0.000042926, Improvement: 0.000003803, Best Loss: 0.000004478 in Epoch 707
Epoch 796
Epoch 796, Loss: 0.000019221, Improvement: -0.000023705, Best Loss: 0.000004478 in Epoch 707
Epoch 797
Epoch 797, Loss: 0.000023345, Improvement: 0.000004123, Best Loss: 0.000004478 in Epoch 707
Epoch 798
Epoch 798, Loss: 0.000051680, Improvement: 0.000028335, Best Loss: 0.000004478 in Epoch 707
Epoch 799
Epoch 799, Loss: 0.000029890, Improvement: -0.000021790, Best Loss: 0.000004478 in Epoch 707
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.000015497, Improvement: -0.000014393, Best Loss: 0.000004478 in Epoch 707
Epoch 801
Epoch 801, Loss: 0.000009726, Improvement: -0.000005771, Best Loss: 0.000004478 in Epoch 707
Epoch 802
Epoch 802, Loss: 0.000008025, Improvement: -0.000001702, Best Loss: 0.000004478 in Epoch 707
Epoch 803
Epoch 803, Loss: 0.000007148, Improvement: -0.000000876, Best Loss: 0.000004478 in Epoch 707
Epoch 804
Epoch 804, Loss: 0.000009323, Improvement: 0.000002174, Best Loss: 0.000004478 in Epoch 707
Epoch 805
Epoch 805, Loss: 0.000007239, Improvement: -0.000002083, Best Loss: 0.000004478 in Epoch 707
Epoch 806
A best model at epoch 806 has been saved with training error 0.000004213.
Epoch 806, Loss: 0.000007099, Improvement: -0.000000140, Best Loss: 0.000004213 in Epoch 806
Epoch 807
Epoch 807, Loss: 0.000010841, Improvement: 0.000003742, Best Loss: 0.000004213 in Epoch 806
Epoch 808
Epoch 808, Loss: 0.000037931, Improvement: 0.000027089, Best Loss: 0.000004213 in Epoch 806
Epoch 809
Epoch 809, Loss: 0.000038745, Improvement: 0.000000814, Best Loss: 0.000004213 in Epoch 806
Epoch 810
Epoch 810, Loss: 0.000032526, Improvement: -0.000006219, Best Loss: 0.000004213 in Epoch 806
Epoch 811
Epoch 811, Loss: 0.000022659, Improvement: -0.000009867, Best Loss: 0.000004213 in Epoch 806
Epoch 812
Epoch 812, Loss: 0.000013646, Improvement: -0.000009013, Best Loss: 0.000004213 in Epoch 806
Epoch 813
Epoch 813, Loss: 0.000011161, Improvement: -0.000002485, Best Loss: 0.000004213 in Epoch 806
Epoch 814
Epoch 814, Loss: 0.000013861, Improvement: 0.000002700, Best Loss: 0.000004213 in Epoch 806
Epoch 815
Epoch 815, Loss: 0.000032938, Improvement: 0.000019078, Best Loss: 0.000004213 in Epoch 806
Epoch 816
Epoch 816, Loss: 0.000032151, Improvement: -0.000000787, Best Loss: 0.000004213 in Epoch 806
Epoch 817
Epoch 817, Loss: 0.000018262, Improvement: -0.000013889, Best Loss: 0.000004213 in Epoch 806
Epoch 818
Epoch 818, Loss: 0.000024029, Improvement: 0.000005767, Best Loss: 0.000004213 in Epoch 806
Epoch 819
Epoch 819, Loss: 0.000049400, Improvement: 0.000025370, Best Loss: 0.000004213 in Epoch 806
Epoch 820
Epoch 820, Loss: 0.000032307, Improvement: -0.000017092, Best Loss: 0.000004213 in Epoch 806
Epoch 821
Epoch 821, Loss: 0.000026295, Improvement: -0.000006013, Best Loss: 0.000004213 in Epoch 806
Epoch 822
Epoch 822, Loss: 0.000013172, Improvement: -0.000013123, Best Loss: 0.000004213 in Epoch 806
Epoch 823
Epoch 823, Loss: 0.000008576, Improvement: -0.000004596, Best Loss: 0.000004213 in Epoch 806
Epoch 824
Epoch 824, Loss: 0.000010665, Improvement: 0.000002089, Best Loss: 0.000004213 in Epoch 806
Epoch 825
Epoch 825, Loss: 0.000018211, Improvement: 0.000007546, Best Loss: 0.000004213 in Epoch 806
Epoch 826
Epoch 826, Loss: 0.000010394, Improvement: -0.000007817, Best Loss: 0.000004213 in Epoch 806
Epoch 827
Epoch 827, Loss: 0.000011788, Improvement: 0.000001394, Best Loss: 0.000004213 in Epoch 806
Epoch 828
Epoch 828, Loss: 0.000027064, Improvement: 0.000015276, Best Loss: 0.000004213 in Epoch 806
Epoch 829
Epoch 829, Loss: 0.000035648, Improvement: 0.000008584, Best Loss: 0.000004213 in Epoch 806
Epoch 830
Epoch 830, Loss: 0.000026031, Improvement: -0.000009617, Best Loss: 0.000004213 in Epoch 806
Epoch 831
Epoch 831, Loss: 0.000028415, Improvement: 0.000002384, Best Loss: 0.000004213 in Epoch 806
Epoch 832
Epoch 832, Loss: 0.000022426, Improvement: -0.000005990, Best Loss: 0.000004213 in Epoch 806
Epoch 833
Epoch 833, Loss: 0.000019716, Improvement: -0.000002709, Best Loss: 0.000004213 in Epoch 806
Epoch 834
Epoch 834, Loss: 0.000014770, Improvement: -0.000004946, Best Loss: 0.000004213 in Epoch 806
Epoch 835
Epoch 835, Loss: 0.000012294, Improvement: -0.000002476, Best Loss: 0.000004213 in Epoch 806
Epoch 836
Epoch 836, Loss: 0.000040798, Improvement: 0.000028504, Best Loss: 0.000004213 in Epoch 806
Epoch 837
Epoch 837, Loss: 0.000031915, Improvement: -0.000008883, Best Loss: 0.000004213 in Epoch 806
Epoch 838
Epoch 838, Loss: 0.000019954, Improvement: -0.000011962, Best Loss: 0.000004213 in Epoch 806
Epoch 839
Epoch 839, Loss: 0.000012886, Improvement: -0.000007068, Best Loss: 0.000004213 in Epoch 806
Epoch 840
Epoch 840, Loss: 0.000017818, Improvement: 0.000004933, Best Loss: 0.000004213 in Epoch 806
Epoch 841
Epoch 841, Loss: 0.000012252, Improvement: -0.000005566, Best Loss: 0.000004213 in Epoch 806
Epoch 842
Epoch 842, Loss: 0.000017495, Improvement: 0.000005243, Best Loss: 0.000004213 in Epoch 806
Epoch 843
Epoch 843, Loss: 0.000026654, Improvement: 0.000009159, Best Loss: 0.000004213 in Epoch 806
Epoch 844
Epoch 844, Loss: 0.000014857, Improvement: -0.000011797, Best Loss: 0.000004213 in Epoch 806
Epoch 845
Epoch 845, Loss: 0.000021575, Improvement: 0.000006718, Best Loss: 0.000004213 in Epoch 806
Epoch 846
Epoch 846, Loss: 0.000044739, Improvement: 0.000023164, Best Loss: 0.000004213 in Epoch 806
Epoch 847
Epoch 847, Loss: 0.000049858, Improvement: 0.000005119, Best Loss: 0.000004213 in Epoch 806
Epoch 848
Epoch 848, Loss: 0.000037968, Improvement: -0.000011890, Best Loss: 0.000004213 in Epoch 806
Epoch 849
Epoch 849, Loss: 0.000019080, Improvement: -0.000018888, Best Loss: 0.000004213 in Epoch 806
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.000018380, Improvement: -0.000000700, Best Loss: 0.000004213 in Epoch 806
Epoch 851
Epoch 851, Loss: 0.000031465, Improvement: 0.000013084, Best Loss: 0.000004213 in Epoch 806
Epoch 852
Epoch 852, Loss: 0.000020255, Improvement: -0.000011209, Best Loss: 0.000004213 in Epoch 806
Epoch 853
Epoch 853, Loss: 0.000021273, Improvement: 0.000001017, Best Loss: 0.000004213 in Epoch 806
Epoch 854
Epoch 854, Loss: 0.000015756, Improvement: -0.000005517, Best Loss: 0.000004213 in Epoch 806
Epoch 855
Epoch 855, Loss: 0.000009823, Improvement: -0.000005933, Best Loss: 0.000004213 in Epoch 806
Epoch 856
Epoch 856, Loss: 0.000006755, Improvement: -0.000003068, Best Loss: 0.000004213 in Epoch 806
Epoch 857
Epoch 857, Loss: 0.000008748, Improvement: 0.000001993, Best Loss: 0.000004213 in Epoch 806
Epoch 858
Epoch 858, Loss: 0.000010864, Improvement: 0.000002116, Best Loss: 0.000004213 in Epoch 806
Epoch 859
Epoch 859, Loss: 0.000015453, Improvement: 0.000004590, Best Loss: 0.000004213 in Epoch 806
Epoch 860
Epoch 860, Loss: 0.000018795, Improvement: 0.000003341, Best Loss: 0.000004213 in Epoch 806
Epoch 861
Epoch 861, Loss: 0.000017742, Improvement: -0.000001053, Best Loss: 0.000004213 in Epoch 806
Epoch 862
Epoch 862, Loss: 0.000011169, Improvement: -0.000006573, Best Loss: 0.000004213 in Epoch 806
Epoch 863
Epoch 863, Loss: 0.000010815, Improvement: -0.000000354, Best Loss: 0.000004213 in Epoch 806
Epoch 864
Epoch 864, Loss: 0.000012632, Improvement: 0.000001817, Best Loss: 0.000004213 in Epoch 806
Epoch 865
Epoch 865, Loss: 0.000011670, Improvement: -0.000000962, Best Loss: 0.000004213 in Epoch 806
Epoch 866
Epoch 866, Loss: 0.000018877, Improvement: 0.000007208, Best Loss: 0.000004213 in Epoch 806
Epoch 867
Epoch 867, Loss: 0.000032552, Improvement: 0.000013675, Best Loss: 0.000004213 in Epoch 806
Epoch 868
Epoch 868, Loss: 0.000049801, Improvement: 0.000017249, Best Loss: 0.000004213 in Epoch 806
Epoch 869
Epoch 869, Loss: 0.000037342, Improvement: -0.000012459, Best Loss: 0.000004213 in Epoch 806
Epoch 870
Epoch 870, Loss: 0.000023188, Improvement: -0.000014154, Best Loss: 0.000004213 in Epoch 806
Epoch 871
Epoch 871, Loss: 0.000020343, Improvement: -0.000002845, Best Loss: 0.000004213 in Epoch 806
Epoch 872
Epoch 872, Loss: 0.000015567, Improvement: -0.000004775, Best Loss: 0.000004213 in Epoch 806
Epoch 873
Epoch 873, Loss: 0.000008740, Improvement: -0.000006827, Best Loss: 0.000004213 in Epoch 806
Epoch 874
A best model at epoch 874 has been saved with training error 0.000003588.
Epoch 874, Loss: 0.000006673, Improvement: -0.000002068, Best Loss: 0.000003588 in Epoch 874
Epoch 875
A best model at epoch 875 has been saved with training error 0.000003314.
Epoch 875, Loss: 0.000005221, Improvement: -0.000001452, Best Loss: 0.000003314 in Epoch 875
Epoch 876
Epoch 876, Loss: 0.000006124, Improvement: 0.000000903, Best Loss: 0.000003314 in Epoch 875
Epoch 877
Epoch 877, Loss: 0.000008130, Improvement: 0.000002006, Best Loss: 0.000003314 in Epoch 875
Epoch 878
Epoch 878, Loss: 0.000011006, Improvement: 0.000002876, Best Loss: 0.000003314 in Epoch 875
Epoch 879
Epoch 879, Loss: 0.000014952, Improvement: 0.000003947, Best Loss: 0.000003314 in Epoch 875
Epoch 880
Epoch 880, Loss: 0.000009814, Improvement: -0.000005138, Best Loss: 0.000003314 in Epoch 875
Epoch 881
Epoch 881, Loss: 0.000011808, Improvement: 0.000001994, Best Loss: 0.000003314 in Epoch 875
Epoch 882
Epoch 882, Loss: 0.000007799, Improvement: -0.000004008, Best Loss: 0.000003314 in Epoch 875
Epoch 883
Epoch 883, Loss: 0.000010434, Improvement: 0.000002635, Best Loss: 0.000003314 in Epoch 875
Epoch 884
Epoch 884, Loss: 0.000028747, Improvement: 0.000018313, Best Loss: 0.000003314 in Epoch 875
Epoch 885
Epoch 885, Loss: 0.000034510, Improvement: 0.000005763, Best Loss: 0.000003314 in Epoch 875
Epoch 886
Epoch 886, Loss: 0.000022190, Improvement: -0.000012320, Best Loss: 0.000003314 in Epoch 875
Epoch 887
Epoch 887, Loss: 0.000011118, Improvement: -0.000011071, Best Loss: 0.000003314 in Epoch 875
Epoch 888
Epoch 888, Loss: 0.000007585, Improvement: -0.000003533, Best Loss: 0.000003314 in Epoch 875
Epoch 889
Epoch 889, Loss: 0.000004749, Improvement: -0.000002836, Best Loss: 0.000003314 in Epoch 875
Epoch 890
Epoch 890, Loss: 0.000005769, Improvement: 0.000001019, Best Loss: 0.000003314 in Epoch 875
Epoch 891
A best model at epoch 891 has been saved with training error 0.000003302.
Epoch 891, Loss: 0.000005837, Improvement: 0.000000068, Best Loss: 0.000003302 in Epoch 891
Epoch 892
Epoch 892, Loss: 0.000005817, Improvement: -0.000000020, Best Loss: 0.000003302 in Epoch 891
Epoch 893
Epoch 893, Loss: 0.000020033, Improvement: 0.000014217, Best Loss: 0.000003302 in Epoch 891
Epoch 894
Epoch 894, Loss: 0.000031508, Improvement: 0.000011474, Best Loss: 0.000003302 in Epoch 891
Epoch 895
Epoch 895, Loss: 0.000014582, Improvement: -0.000016926, Best Loss: 0.000003302 in Epoch 891
Epoch 896
Epoch 896, Loss: 0.000030587, Improvement: 0.000016006, Best Loss: 0.000003302 in Epoch 891
Epoch 897
Epoch 897, Loss: 0.000030583, Improvement: -0.000000005, Best Loss: 0.000003302 in Epoch 891
Epoch 898
Epoch 898, Loss: 0.000021891, Improvement: -0.000008691, Best Loss: 0.000003302 in Epoch 891
Epoch 899
Epoch 899, Loss: 0.000022005, Improvement: 0.000000114, Best Loss: 0.000003302 in Epoch 891
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.000028474, Improvement: 0.000006469, Best Loss: 0.000003302 in Epoch 891
Epoch 901
Epoch 901, Loss: 0.000031078, Improvement: 0.000002605, Best Loss: 0.000003302 in Epoch 891
Epoch 902
Epoch 902, Loss: 0.000038530, Improvement: 0.000007451, Best Loss: 0.000003302 in Epoch 891
Epoch 903
Epoch 903, Loss: 0.000044864, Improvement: 0.000006334, Best Loss: 0.000003302 in Epoch 891
Epoch 904
Epoch 904, Loss: 0.000029738, Improvement: -0.000015126, Best Loss: 0.000003302 in Epoch 891
Epoch 905
Epoch 905, Loss: 0.000015397, Improvement: -0.000014341, Best Loss: 0.000003302 in Epoch 891
Epoch 906
Epoch 906, Loss: 0.000008636, Improvement: -0.000006761, Best Loss: 0.000003302 in Epoch 891
Epoch 907
Epoch 907, Loss: 0.000007316, Improvement: -0.000001320, Best Loss: 0.000003302 in Epoch 891
Epoch 908
A best model at epoch 908 has been saved with training error 0.000003279.
Epoch 908, Loss: 0.000005408, Improvement: -0.000001908, Best Loss: 0.000003279 in Epoch 908
Epoch 909
Epoch 909, Loss: 0.000009022, Improvement: 0.000003614, Best Loss: 0.000003279 in Epoch 908
Epoch 910
Epoch 910, Loss: 0.000008448, Improvement: -0.000000574, Best Loss: 0.000003279 in Epoch 908
Epoch 911
Epoch 911, Loss: 0.000029178, Improvement: 0.000020730, Best Loss: 0.000003279 in Epoch 908
Epoch 912
Epoch 912, Loss: 0.000010675, Improvement: -0.000018503, Best Loss: 0.000003279 in Epoch 908
Epoch 913
Epoch 913, Loss: 0.000007218, Improvement: -0.000003458, Best Loss: 0.000003279 in Epoch 908
Epoch 914
Epoch 914, Loss: 0.000007843, Improvement: 0.000000626, Best Loss: 0.000003279 in Epoch 908
Epoch 915
Epoch 915, Loss: 0.000014593, Improvement: 0.000006750, Best Loss: 0.000003279 in Epoch 908
Epoch 916
Epoch 916, Loss: 0.000021002, Improvement: 0.000006409, Best Loss: 0.000003279 in Epoch 908
Epoch 917
Epoch 917, Loss: 0.000017886, Improvement: -0.000003117, Best Loss: 0.000003279 in Epoch 908
Epoch 918
Epoch 918, Loss: 0.000016706, Improvement: -0.000001180, Best Loss: 0.000003279 in Epoch 908
Epoch 919
Epoch 919, Loss: 0.000030371, Improvement: 0.000013666, Best Loss: 0.000003279 in Epoch 908
Epoch 920
Epoch 920, Loss: 0.000022216, Improvement: -0.000008155, Best Loss: 0.000003279 in Epoch 908
Epoch 921
Epoch 921, Loss: 0.000013828, Improvement: -0.000008388, Best Loss: 0.000003279 in Epoch 908
Epoch 922
Epoch 922, Loss: 0.000010877, Improvement: -0.000002951, Best Loss: 0.000003279 in Epoch 908
Epoch 923
Epoch 923, Loss: 0.000014226, Improvement: 0.000003350, Best Loss: 0.000003279 in Epoch 908
Epoch 924
Epoch 924, Loss: 0.000030451, Improvement: 0.000016225, Best Loss: 0.000003279 in Epoch 908
Epoch 925
Epoch 925, Loss: 0.000015096, Improvement: -0.000015355, Best Loss: 0.000003279 in Epoch 908
Epoch 926
Epoch 926, Loss: 0.000007715, Improvement: -0.000007381, Best Loss: 0.000003279 in Epoch 908
Epoch 927
Epoch 927, Loss: 0.000010984, Improvement: 0.000003269, Best Loss: 0.000003279 in Epoch 908
Epoch 928
Epoch 928, Loss: 0.000010136, Improvement: -0.000000849, Best Loss: 0.000003279 in Epoch 908
Epoch 929
Epoch 929, Loss: 0.000008976, Improvement: -0.000001160, Best Loss: 0.000003279 in Epoch 908
Epoch 930
Epoch 930, Loss: 0.000018186, Improvement: 0.000009210, Best Loss: 0.000003279 in Epoch 908
Epoch 931
Epoch 931, Loss: 0.000013864, Improvement: -0.000004322, Best Loss: 0.000003279 in Epoch 908
Epoch 932
Epoch 932, Loss: 0.000009896, Improvement: -0.000003968, Best Loss: 0.000003279 in Epoch 908
Epoch 933
Epoch 933, Loss: 0.000015815, Improvement: 0.000005919, Best Loss: 0.000003279 in Epoch 908
Epoch 934
Epoch 934, Loss: 0.000019114, Improvement: 0.000003300, Best Loss: 0.000003279 in Epoch 908
Epoch 935
Epoch 935, Loss: 0.000027596, Improvement: 0.000008482, Best Loss: 0.000003279 in Epoch 908
Epoch 936
Epoch 936, Loss: 0.000028013, Improvement: 0.000000417, Best Loss: 0.000003279 in Epoch 908
Epoch 937
Epoch 937, Loss: 0.000014762, Improvement: -0.000013251, Best Loss: 0.000003279 in Epoch 908
Epoch 938
Epoch 938, Loss: 0.000015544, Improvement: 0.000000782, Best Loss: 0.000003279 in Epoch 908
Epoch 939
Epoch 939, Loss: 0.000016646, Improvement: 0.000001102, Best Loss: 0.000003279 in Epoch 908
Epoch 940
Epoch 940, Loss: 0.000022807, Improvement: 0.000006161, Best Loss: 0.000003279 in Epoch 908
Epoch 941
Epoch 941, Loss: 0.000017154, Improvement: -0.000005653, Best Loss: 0.000003279 in Epoch 908
Epoch 942
Epoch 942, Loss: 0.000022764, Improvement: 0.000005609, Best Loss: 0.000003279 in Epoch 908
Epoch 943
Epoch 943, Loss: 0.000039069, Improvement: 0.000016305, Best Loss: 0.000003279 in Epoch 908
Epoch 944
Epoch 944, Loss: 0.000030028, Improvement: -0.000009041, Best Loss: 0.000003279 in Epoch 908
Epoch 945
Epoch 945, Loss: 0.000032504, Improvement: 0.000002476, Best Loss: 0.000003279 in Epoch 908
Epoch 946
Epoch 946, Loss: 0.000014895, Improvement: -0.000017609, Best Loss: 0.000003279 in Epoch 908
Epoch 947
Epoch 947, Loss: 0.000009020, Improvement: -0.000005875, Best Loss: 0.000003279 in Epoch 908
Epoch 948
Epoch 948, Loss: 0.000006048, Improvement: -0.000002972, Best Loss: 0.000003279 in Epoch 908
Epoch 949
A best model at epoch 949 has been saved with training error 0.000002869.
Epoch 949, Loss: 0.000005453, Improvement: -0.000000595, Best Loss: 0.000002869 in Epoch 949
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.000005058, Improvement: -0.000000395, Best Loss: 0.000002869 in Epoch 949
Epoch 951
Epoch 951, Loss: 0.000013266, Improvement: 0.000008208, Best Loss: 0.000002869 in Epoch 949
Epoch 952
Epoch 952, Loss: 0.000018028, Improvement: 0.000004762, Best Loss: 0.000002869 in Epoch 949
Epoch 953
Epoch 953, Loss: 0.000019200, Improvement: 0.000001172, Best Loss: 0.000002869 in Epoch 949
Epoch 954
Epoch 954, Loss: 0.000014261, Improvement: -0.000004938, Best Loss: 0.000002869 in Epoch 949
Epoch 955
Epoch 955, Loss: 0.000011197, Improvement: -0.000003064, Best Loss: 0.000002869 in Epoch 949
Epoch 956
Epoch 956, Loss: 0.000007626, Improvement: -0.000003570, Best Loss: 0.000002869 in Epoch 949
Epoch 957
Epoch 957, Loss: 0.000008528, Improvement: 0.000000901, Best Loss: 0.000002869 in Epoch 949
Epoch 958
Epoch 958, Loss: 0.000007062, Improvement: -0.000001465, Best Loss: 0.000002869 in Epoch 949
Epoch 959
Epoch 959, Loss: 0.000004903, Improvement: -0.000002160, Best Loss: 0.000002869 in Epoch 949
Epoch 960
Epoch 960, Loss: 0.000007585, Improvement: 0.000002682, Best Loss: 0.000002869 in Epoch 949
Epoch 961
Epoch 961, Loss: 0.000034021, Improvement: 0.000026436, Best Loss: 0.000002869 in Epoch 949
Epoch 962
Epoch 962, Loss: 0.000047444, Improvement: 0.000013423, Best Loss: 0.000002869 in Epoch 949
Epoch 963
Epoch 963, Loss: 0.000026567, Improvement: -0.000020877, Best Loss: 0.000002869 in Epoch 949
Epoch 964
Epoch 964, Loss: 0.000013573, Improvement: -0.000012994, Best Loss: 0.000002869 in Epoch 949
Epoch 965
Epoch 965, Loss: 0.000005843, Improvement: -0.000007729, Best Loss: 0.000002869 in Epoch 949
Epoch 966
Epoch 966, Loss: 0.000004442, Improvement: -0.000001401, Best Loss: 0.000002869 in Epoch 949
Epoch 967
A best model at epoch 967 has been saved with training error 0.000002862.
Epoch 967, Loss: 0.000005749, Improvement: 0.000001307, Best Loss: 0.000002862 in Epoch 967
Epoch 968
Epoch 968, Loss: 0.000005483, Improvement: -0.000000266, Best Loss: 0.000002862 in Epoch 967
Epoch 969
Epoch 969, Loss: 0.000014059, Improvement: 0.000008576, Best Loss: 0.000002862 in Epoch 967
Epoch 970
Epoch 970, Loss: 0.000014886, Improvement: 0.000000827, Best Loss: 0.000002862 in Epoch 967
Epoch 971
Epoch 971, Loss: 0.000019487, Improvement: 0.000004600, Best Loss: 0.000002862 in Epoch 967
Epoch 972
Epoch 972, Loss: 0.000012100, Improvement: -0.000007387, Best Loss: 0.000002862 in Epoch 967
Epoch 973
Epoch 973, Loss: 0.000006995, Improvement: -0.000005105, Best Loss: 0.000002862 in Epoch 967
Epoch 974
Epoch 974, Loss: 0.000005998, Improvement: -0.000000997, Best Loss: 0.000002862 in Epoch 967
Epoch 975
Epoch 975, Loss: 0.000009601, Improvement: 0.000003603, Best Loss: 0.000002862 in Epoch 967
Epoch 976
Epoch 976, Loss: 0.000013860, Improvement: 0.000004259, Best Loss: 0.000002862 in Epoch 967
Epoch 977
Epoch 977, Loss: 0.000028646, Improvement: 0.000014785, Best Loss: 0.000002862 in Epoch 967
Epoch 978
Epoch 978, Loss: 0.000032031, Improvement: 0.000003385, Best Loss: 0.000002862 in Epoch 967
Epoch 979
Epoch 979, Loss: 0.000045981, Improvement: 0.000013950, Best Loss: 0.000002862 in Epoch 967
Epoch 980
Epoch 980, Loss: 0.000025714, Improvement: -0.000020267, Best Loss: 0.000002862 in Epoch 967
Epoch 981
Epoch 981, Loss: 0.000014832, Improvement: -0.000010883, Best Loss: 0.000002862 in Epoch 967
Epoch 982
Epoch 982, Loss: 0.000007917, Improvement: -0.000006915, Best Loss: 0.000002862 in Epoch 967
Epoch 983
Epoch 983, Loss: 0.000007410, Improvement: -0.000000507, Best Loss: 0.000002862 in Epoch 967
Epoch 984
Epoch 984, Loss: 0.000011482, Improvement: 0.000004072, Best Loss: 0.000002862 in Epoch 967
Epoch 985
Epoch 985, Loss: 0.000012168, Improvement: 0.000000686, Best Loss: 0.000002862 in Epoch 967
Epoch 986
Epoch 986, Loss: 0.000012571, Improvement: 0.000000403, Best Loss: 0.000002862 in Epoch 967
Epoch 987
Epoch 987, Loss: 0.000011256, Improvement: -0.000001315, Best Loss: 0.000002862 in Epoch 967
Epoch 988
Epoch 988, Loss: 0.000016314, Improvement: 0.000005058, Best Loss: 0.000002862 in Epoch 967
Epoch 989
Epoch 989, Loss: 0.000010667, Improvement: -0.000005648, Best Loss: 0.000002862 in Epoch 967
Epoch 990
Epoch 990, Loss: 0.000012660, Improvement: 0.000001994, Best Loss: 0.000002862 in Epoch 967
Epoch 991
Epoch 991, Loss: 0.000011885, Improvement: -0.000000776, Best Loss: 0.000002862 in Epoch 967
Epoch 992
Epoch 992, Loss: 0.000007977, Improvement: -0.000003907, Best Loss: 0.000002862 in Epoch 967
Epoch 993
Epoch 993, Loss: 0.000006448, Improvement: -0.000001529, Best Loss: 0.000002862 in Epoch 967
Epoch 994
Epoch 994, Loss: 0.000008901, Improvement: 0.000002452, Best Loss: 0.000002862 in Epoch 967
Epoch 995
Epoch 995, Loss: 0.000013243, Improvement: 0.000004342, Best Loss: 0.000002862 in Epoch 967
Epoch 996
Epoch 996, Loss: 0.000036894, Improvement: 0.000023651, Best Loss: 0.000002862 in Epoch 967
Epoch 997
Epoch 997, Loss: 0.000024313, Improvement: -0.000012581, Best Loss: 0.000002862 in Epoch 967
Epoch 998
Epoch 998, Loss: 0.000012046, Improvement: -0.000012266, Best Loss: 0.000002862 in Epoch 967
Epoch 999
Epoch 999, Loss: 0.000009574, Improvement: -0.000002473, Best Loss: 0.000002862 in Epoch 967
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.000013186, Improvement: 0.000003613, Best Loss: 0.000002862 in Epoch 967
Epoch 1001
Epoch 1001, Loss: 0.000029799, Improvement: 0.000016613, Best Loss: 0.000002862 in Epoch 967
Epoch 1002
Epoch 1002, Loss: 0.000025065, Improvement: -0.000004734, Best Loss: 0.000002862 in Epoch 967
Epoch 1003
Epoch 1003, Loss: 0.000042873, Improvement: 0.000017808, Best Loss: 0.000002862 in Epoch 967
Epoch 1004
Epoch 1004, Loss: 0.000054643, Improvement: 0.000011770, Best Loss: 0.000002862 in Epoch 967
Epoch 1005
Epoch 1005, Loss: 0.000047037, Improvement: -0.000007606, Best Loss: 0.000002862 in Epoch 967
Epoch 1006
Epoch 1006, Loss: 0.000013157, Improvement: -0.000033880, Best Loss: 0.000002862 in Epoch 967
Epoch 1007
Epoch 1007, Loss: 0.000005886, Improvement: -0.000007271, Best Loss: 0.000002862 in Epoch 967
Epoch 1008
Epoch 1008, Loss: 0.000007815, Improvement: 0.000001929, Best Loss: 0.000002862 in Epoch 967
Epoch 1009
Epoch 1009, Loss: 0.000006845, Improvement: -0.000000970, Best Loss: 0.000002862 in Epoch 967
Epoch 1010
Epoch 1010, Loss: 0.000005668, Improvement: -0.000001177, Best Loss: 0.000002862 in Epoch 967
Epoch 1011
Epoch 1011, Loss: 0.000006809, Improvement: 0.000001142, Best Loss: 0.000002862 in Epoch 967
Epoch 1012
Epoch 1012, Loss: 0.000005538, Improvement: -0.000001271, Best Loss: 0.000002862 in Epoch 967
Epoch 1013
A best model at epoch 1013 has been saved with training error 0.000002815.
A best model at epoch 1013 has been saved with training error 0.000002582.
Epoch 1013, Loss: 0.000004490, Improvement: -0.000001048, Best Loss: 0.000002582 in Epoch 1013
Epoch 1014
A best model at epoch 1014 has been saved with training error 0.000002206.
Epoch 1014, Loss: 0.000004201, Improvement: -0.000000289, Best Loss: 0.000002206 in Epoch 1014
Epoch 1015
Epoch 1015, Loss: 0.000005503, Improvement: 0.000001302, Best Loss: 0.000002206 in Epoch 1014
Epoch 1016
Epoch 1016, Loss: 0.000009674, Improvement: 0.000004171, Best Loss: 0.000002206 in Epoch 1014
Epoch 1017
Epoch 1017, Loss: 0.000008232, Improvement: -0.000001442, Best Loss: 0.000002206 in Epoch 1014
Epoch 1018
Epoch 1018, Loss: 0.000006459, Improvement: -0.000001773, Best Loss: 0.000002206 in Epoch 1014
Epoch 1019
Epoch 1019, Loss: 0.000012324, Improvement: 0.000005865, Best Loss: 0.000002206 in Epoch 1014
Epoch 1020
Epoch 1020, Loss: 0.000016571, Improvement: 0.000004247, Best Loss: 0.000002206 in Epoch 1014
Epoch 1021
Epoch 1021, Loss: 0.000009403, Improvement: -0.000007168, Best Loss: 0.000002206 in Epoch 1014
Epoch 1022
Epoch 1022, Loss: 0.000004460, Improvement: -0.000004943, Best Loss: 0.000002206 in Epoch 1014
Epoch 1023
Epoch 1023, Loss: 0.000005214, Improvement: 0.000000755, Best Loss: 0.000002206 in Epoch 1014
Epoch 1024
Epoch 1024, Loss: 0.000007476, Improvement: 0.000002261, Best Loss: 0.000002206 in Epoch 1014
Epoch 1025
Epoch 1025, Loss: 0.000015178, Improvement: 0.000007702, Best Loss: 0.000002206 in Epoch 1014
Epoch 1026
Epoch 1026, Loss: 0.000025002, Improvement: 0.000009825, Best Loss: 0.000002206 in Epoch 1014
Epoch 1027
Epoch 1027, Loss: 0.000016632, Improvement: -0.000008371, Best Loss: 0.000002206 in Epoch 1014
Epoch 1028
Epoch 1028, Loss: 0.000016819, Improvement: 0.000000187, Best Loss: 0.000002206 in Epoch 1014
Epoch 1029
Epoch 1029, Loss: 0.000019400, Improvement: 0.000002581, Best Loss: 0.000002206 in Epoch 1014
Epoch 1030
Epoch 1030, Loss: 0.000016096, Improvement: -0.000003305, Best Loss: 0.000002206 in Epoch 1014
Epoch 1031
Epoch 1031, Loss: 0.000020141, Improvement: 0.000004046, Best Loss: 0.000002206 in Epoch 1014
Epoch 1032
Epoch 1032, Loss: 0.000023935, Improvement: 0.000003794, Best Loss: 0.000002206 in Epoch 1014
Epoch 1033
Epoch 1033, Loss: 0.000025706, Improvement: 0.000001771, Best Loss: 0.000002206 in Epoch 1014
Epoch 1034
Epoch 1034, Loss: 0.000025685, Improvement: -0.000000021, Best Loss: 0.000002206 in Epoch 1014
Epoch 1035
Epoch 1035, Loss: 0.000015551, Improvement: -0.000010134, Best Loss: 0.000002206 in Epoch 1014
Epoch 1036
Epoch 1036, Loss: 0.000015642, Improvement: 0.000000091, Best Loss: 0.000002206 in Epoch 1014
Epoch 1037
Epoch 1037, Loss: 0.000009097, Improvement: -0.000006545, Best Loss: 0.000002206 in Epoch 1014
Epoch 1038
Epoch 1038, Loss: 0.000011022, Improvement: 0.000001926, Best Loss: 0.000002206 in Epoch 1014
Epoch 1039
Epoch 1039, Loss: 0.000012883, Improvement: 0.000001861, Best Loss: 0.000002206 in Epoch 1014
Epoch 1040
Epoch 1040, Loss: 0.000015219, Improvement: 0.000002336, Best Loss: 0.000002206 in Epoch 1014
Epoch 1041
Epoch 1041, Loss: 0.000010794, Improvement: -0.000004425, Best Loss: 0.000002206 in Epoch 1014
Epoch 1042
Epoch 1042, Loss: 0.000008193, Improvement: -0.000002601, Best Loss: 0.000002206 in Epoch 1014
Epoch 1043
Epoch 1043, Loss: 0.000014321, Improvement: 0.000006128, Best Loss: 0.000002206 in Epoch 1014
Epoch 1044
Epoch 1044, Loss: 0.000015616, Improvement: 0.000001294, Best Loss: 0.000002206 in Epoch 1014
Epoch 1045
Epoch 1045, Loss: 0.000031995, Improvement: 0.000016379, Best Loss: 0.000002206 in Epoch 1014
Epoch 1046
Epoch 1046, Loss: 0.000031479, Improvement: -0.000000516, Best Loss: 0.000002206 in Epoch 1014
Epoch 1047
Epoch 1047, Loss: 0.000012433, Improvement: -0.000019046, Best Loss: 0.000002206 in Epoch 1014
Epoch 1048
Epoch 1048, Loss: 0.000008620, Improvement: -0.000003813, Best Loss: 0.000002206 in Epoch 1014
Epoch 1049
Epoch 1049, Loss: 0.000005875, Improvement: -0.000002745, Best Loss: 0.000002206 in Epoch 1014
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.000009653, Improvement: 0.000003778, Best Loss: 0.000002206 in Epoch 1014
Epoch 1051
Epoch 1051, Loss: 0.000022429, Improvement: 0.000012776, Best Loss: 0.000002206 in Epoch 1014
Epoch 1052
Epoch 1052, Loss: 0.000013336, Improvement: -0.000009093, Best Loss: 0.000002206 in Epoch 1014
Epoch 1053
Epoch 1053, Loss: 0.000008820, Improvement: -0.000004516, Best Loss: 0.000002206 in Epoch 1014
Epoch 1054
Epoch 1054, Loss: 0.000007825, Improvement: -0.000000996, Best Loss: 0.000002206 in Epoch 1014
Epoch 1055
Epoch 1055, Loss: 0.000006899, Improvement: -0.000000926, Best Loss: 0.000002206 in Epoch 1014
Epoch 1056
Epoch 1056, Loss: 0.000004681, Improvement: -0.000002218, Best Loss: 0.000002206 in Epoch 1014
Epoch 1057
Epoch 1057, Loss: 0.000006842, Improvement: 0.000002162, Best Loss: 0.000002206 in Epoch 1014
Epoch 1058
Epoch 1058, Loss: 0.000011633, Improvement: 0.000004790, Best Loss: 0.000002206 in Epoch 1014
Epoch 1059
Epoch 1059, Loss: 0.000013811, Improvement: 0.000002179, Best Loss: 0.000002206 in Epoch 1014
Epoch 1060
Epoch 1060, Loss: 0.000025177, Improvement: 0.000011366, Best Loss: 0.000002206 in Epoch 1014
Epoch 1061
Epoch 1061, Loss: 0.000049763, Improvement: 0.000024585, Best Loss: 0.000002206 in Epoch 1014
Epoch 1062
Epoch 1062, Loss: 0.000027792, Improvement: -0.000021971, Best Loss: 0.000002206 in Epoch 1014
Epoch 1063
Epoch 1063, Loss: 0.000010158, Improvement: -0.000017634, Best Loss: 0.000002206 in Epoch 1014
Epoch 1064
Epoch 1064, Loss: 0.000006558, Improvement: -0.000003600, Best Loss: 0.000002206 in Epoch 1014
Epoch 1065
Epoch 1065, Loss: 0.000003639, Improvement: -0.000002920, Best Loss: 0.000002206 in Epoch 1014
Epoch 1066
A best model at epoch 1066 has been saved with training error 0.000002075.
Epoch 1066, Loss: 0.000002925, Improvement: -0.000000714, Best Loss: 0.000002075 in Epoch 1066
Epoch 1067
A best model at epoch 1067 has been saved with training error 0.000001781.
Epoch 1067, Loss: 0.000003423, Improvement: 0.000000498, Best Loss: 0.000001781 in Epoch 1067
Epoch 1068
Epoch 1068, Loss: 0.000004235, Improvement: 0.000000813, Best Loss: 0.000001781 in Epoch 1067
Epoch 1069
A best model at epoch 1069 has been saved with training error 0.000001637.
Epoch 1069, Loss: 0.000003873, Improvement: -0.000000362, Best Loss: 0.000001637 in Epoch 1069
Epoch 1070
Epoch 1070, Loss: 0.000002873, Improvement: -0.000001000, Best Loss: 0.000001637 in Epoch 1069
Epoch 1071
Epoch 1071, Loss: 0.000002599, Improvement: -0.000000274, Best Loss: 0.000001637 in Epoch 1069
Epoch 1072
Epoch 1072, Loss: 0.000003928, Improvement: 0.000001329, Best Loss: 0.000001637 in Epoch 1069
Epoch 1073
Epoch 1073, Loss: 0.000003875, Improvement: -0.000000054, Best Loss: 0.000001637 in Epoch 1069
Epoch 1074
Epoch 1074, Loss: 0.000003825, Improvement: -0.000000049, Best Loss: 0.000001637 in Epoch 1069
Epoch 1075
Epoch 1075, Loss: 0.000002947, Improvement: -0.000000879, Best Loss: 0.000001637 in Epoch 1069
Epoch 1076
Epoch 1076, Loss: 0.000002512, Improvement: -0.000000435, Best Loss: 0.000001637 in Epoch 1069
Epoch 1077
Epoch 1077, Loss: 0.000002538, Improvement: 0.000000026, Best Loss: 0.000001637 in Epoch 1069
Epoch 1078
Epoch 1078, Loss: 0.000003079, Improvement: 0.000000541, Best Loss: 0.000001637 in Epoch 1069
Epoch 1079
Epoch 1079, Loss: 0.000002760, Improvement: -0.000000319, Best Loss: 0.000001637 in Epoch 1069
Epoch 1080
Epoch 1080, Loss: 0.000003101, Improvement: 0.000000341, Best Loss: 0.000001637 in Epoch 1069
Epoch 1081
Epoch 1081, Loss: 0.000004321, Improvement: 0.000001220, Best Loss: 0.000001637 in Epoch 1069
Epoch 1082
Epoch 1082, Loss: 0.000007650, Improvement: 0.000003329, Best Loss: 0.000001637 in Epoch 1069
Epoch 1083
Epoch 1083, Loss: 0.000005682, Improvement: -0.000001968, Best Loss: 0.000001637 in Epoch 1069
Epoch 1084
Epoch 1084, Loss: 0.000003897, Improvement: -0.000001784, Best Loss: 0.000001637 in Epoch 1069
Epoch 1085
Epoch 1085, Loss: 0.000004294, Improvement: 0.000000397, Best Loss: 0.000001637 in Epoch 1069
Epoch 1086
Epoch 1086, Loss: 0.000009868, Improvement: 0.000005573, Best Loss: 0.000001637 in Epoch 1069
Epoch 1087
Epoch 1087, Loss: 0.000016169, Improvement: 0.000006301, Best Loss: 0.000001637 in Epoch 1069
Epoch 1088
Epoch 1088, Loss: 0.000020967, Improvement: 0.000004798, Best Loss: 0.000001637 in Epoch 1069
Epoch 1089
Epoch 1089, Loss: 0.000017756, Improvement: -0.000003211, Best Loss: 0.000001637 in Epoch 1069
Epoch 1090
Epoch 1090, Loss: 0.000014830, Improvement: -0.000002926, Best Loss: 0.000001637 in Epoch 1069
Epoch 1091
Epoch 1091, Loss: 0.000018914, Improvement: 0.000004084, Best Loss: 0.000001637 in Epoch 1069
Epoch 1092
Epoch 1092, Loss: 0.000034334, Improvement: 0.000015420, Best Loss: 0.000001637 in Epoch 1069
Epoch 1093
Epoch 1093, Loss: 0.000026382, Improvement: -0.000007952, Best Loss: 0.000001637 in Epoch 1069
Epoch 1094
Epoch 1094, Loss: 0.000030983, Improvement: 0.000004601, Best Loss: 0.000001637 in Epoch 1069
Epoch 1095
Epoch 1095, Loss: 0.000033500, Improvement: 0.000002517, Best Loss: 0.000001637 in Epoch 1069
Epoch 1096
Epoch 1096, Loss: 0.000008755, Improvement: -0.000024745, Best Loss: 0.000001637 in Epoch 1069
Epoch 1097
Epoch 1097, Loss: 0.000005685, Improvement: -0.000003070, Best Loss: 0.000001637 in Epoch 1069
Epoch 1098
Epoch 1098, Loss: 0.000003935, Improvement: -0.000001751, Best Loss: 0.000001637 in Epoch 1069
Epoch 1099
Epoch 1099, Loss: 0.000005862, Improvement: 0.000001928, Best Loss: 0.000001637 in Epoch 1069
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.000010117, Improvement: 0.000004255, Best Loss: 0.000001637 in Epoch 1069
Epoch 1101
Epoch 1101, Loss: 0.000014526, Improvement: 0.000004409, Best Loss: 0.000001637 in Epoch 1069
Epoch 1102
Epoch 1102, Loss: 0.000018384, Improvement: 0.000003858, Best Loss: 0.000001637 in Epoch 1069
Epoch 1103
Epoch 1103, Loss: 0.000011449, Improvement: -0.000006935, Best Loss: 0.000001637 in Epoch 1069
Epoch 1104
Epoch 1104, Loss: 0.000005088, Improvement: -0.000006361, Best Loss: 0.000001637 in Epoch 1069
Epoch 1105
Epoch 1105, Loss: 0.000004110, Improvement: -0.000000978, Best Loss: 0.000001637 in Epoch 1069
Epoch 1106
Epoch 1106, Loss: 0.000008058, Improvement: 0.000003948, Best Loss: 0.000001637 in Epoch 1069
Epoch 1107
Epoch 1107, Loss: 0.000005506, Improvement: -0.000002552, Best Loss: 0.000001637 in Epoch 1069
Epoch 1108
Epoch 1108, Loss: 0.000009530, Improvement: 0.000004024, Best Loss: 0.000001637 in Epoch 1069
Epoch 1109
Epoch 1109, Loss: 0.000007075, Improvement: -0.000002456, Best Loss: 0.000001637 in Epoch 1069
Epoch 1110
Epoch 1110, Loss: 0.000006046, Improvement: -0.000001028, Best Loss: 0.000001637 in Epoch 1069
Epoch 1111
Epoch 1111, Loss: 0.000008662, Improvement: 0.000002616, Best Loss: 0.000001637 in Epoch 1069
Epoch 1112
Epoch 1112, Loss: 0.000008574, Improvement: -0.000000088, Best Loss: 0.000001637 in Epoch 1069
Epoch 1113
Epoch 1113, Loss: 0.000011394, Improvement: 0.000002820, Best Loss: 0.000001637 in Epoch 1069
Epoch 1114
Epoch 1114, Loss: 0.000012127, Improvement: 0.000000733, Best Loss: 0.000001637 in Epoch 1069
Epoch 1115
Epoch 1115, Loss: 0.000008951, Improvement: -0.000003175, Best Loss: 0.000001637 in Epoch 1069
Epoch 1116
Epoch 1116, Loss: 0.000010155, Improvement: 0.000001204, Best Loss: 0.000001637 in Epoch 1069
Epoch 1117
Epoch 1117, Loss: 0.000008624, Improvement: -0.000001531, Best Loss: 0.000001637 in Epoch 1069
Epoch 1118
Epoch 1118, Loss: 0.000009011, Improvement: 0.000000387, Best Loss: 0.000001637 in Epoch 1069
Epoch 1119
Epoch 1119, Loss: 0.000010260, Improvement: 0.000001249, Best Loss: 0.000001637 in Epoch 1069
Epoch 1120
Epoch 1120, Loss: 0.000008539, Improvement: -0.000001720, Best Loss: 0.000001637 in Epoch 1069
Epoch 1121
Epoch 1121, Loss: 0.000012965, Improvement: 0.000004426, Best Loss: 0.000001637 in Epoch 1069
Epoch 1122
Epoch 1122, Loss: 0.000013666, Improvement: 0.000000701, Best Loss: 0.000001637 in Epoch 1069
Epoch 1123
Epoch 1123, Loss: 0.000026427, Improvement: 0.000012761, Best Loss: 0.000001637 in Epoch 1069
Epoch 1124
Epoch 1124, Loss: 0.000032776, Improvement: 0.000006349, Best Loss: 0.000001637 in Epoch 1069
Epoch 1125
Epoch 1125, Loss: 0.000012689, Improvement: -0.000020088, Best Loss: 0.000001637 in Epoch 1069
Epoch 1126
Epoch 1126, Loss: 0.000011727, Improvement: -0.000000961, Best Loss: 0.000001637 in Epoch 1069
Epoch 1127
Epoch 1127, Loss: 0.000007798, Improvement: -0.000003929, Best Loss: 0.000001637 in Epoch 1069
Epoch 1128
Epoch 1128, Loss: 0.000007779, Improvement: -0.000000019, Best Loss: 0.000001637 in Epoch 1069
Epoch 1129
Epoch 1129, Loss: 0.000004532, Improvement: -0.000003247, Best Loss: 0.000001637 in Epoch 1069
Epoch 1130
Epoch 1130, Loss: 0.000003183, Improvement: -0.000001350, Best Loss: 0.000001637 in Epoch 1069
Epoch 1131
Epoch 1131, Loss: 0.000004314, Improvement: 0.000001131, Best Loss: 0.000001637 in Epoch 1069
Epoch 1132
Epoch 1132, Loss: 0.000004546, Improvement: 0.000000232, Best Loss: 0.000001637 in Epoch 1069
Epoch 1133
Epoch 1133, Loss: 0.000008738, Improvement: 0.000004192, Best Loss: 0.000001637 in Epoch 1069
Epoch 1134
Epoch 1134, Loss: 0.000012201, Improvement: 0.000003463, Best Loss: 0.000001637 in Epoch 1069
Epoch 1135
Epoch 1135, Loss: 0.000009302, Improvement: -0.000002899, Best Loss: 0.000001637 in Epoch 1069
Epoch 1136
Epoch 1136, Loss: 0.000011634, Improvement: 0.000002332, Best Loss: 0.000001637 in Epoch 1069
Epoch 1137
Epoch 1137, Loss: 0.000011107, Improvement: -0.000000527, Best Loss: 0.000001637 in Epoch 1069
Epoch 1138
Epoch 1138, Loss: 0.000021042, Improvement: 0.000009935, Best Loss: 0.000001637 in Epoch 1069
Epoch 1139
Epoch 1139, Loss: 0.000024349, Improvement: 0.000003306, Best Loss: 0.000001637 in Epoch 1069
Epoch 1140
Epoch 1140, Loss: 0.000027767, Improvement: 0.000003418, Best Loss: 0.000001637 in Epoch 1069
Epoch 1141
Epoch 1141, Loss: 0.000019144, Improvement: -0.000008623, Best Loss: 0.000001637 in Epoch 1069
Epoch 1142
Epoch 1142, Loss: 0.000011635, Improvement: -0.000007509, Best Loss: 0.000001637 in Epoch 1069
Epoch 1143
Epoch 1143, Loss: 0.000008424, Improvement: -0.000003210, Best Loss: 0.000001637 in Epoch 1069
Epoch 1144
Epoch 1144, Loss: 0.000006751, Improvement: -0.000001674, Best Loss: 0.000001637 in Epoch 1069
Epoch 1145
Epoch 1145, Loss: 0.000007801, Improvement: 0.000001051, Best Loss: 0.000001637 in Epoch 1069
Epoch 1146
Epoch 1146, Loss: 0.000005357, Improvement: -0.000002444, Best Loss: 0.000001637 in Epoch 1069
Epoch 1147
Epoch 1147, Loss: 0.000007415, Improvement: 0.000002057, Best Loss: 0.000001637 in Epoch 1069
Epoch 1148
Epoch 1148, Loss: 0.000008655, Improvement: 0.000001241, Best Loss: 0.000001637 in Epoch 1069
Epoch 1149
Epoch 1149, Loss: 0.000011022, Improvement: 0.000002367, Best Loss: 0.000001637 in Epoch 1069
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.000013457, Improvement: 0.000002435, Best Loss: 0.000001637 in Epoch 1069
Epoch 1151
Epoch 1151, Loss: 0.000029275, Improvement: 0.000015817, Best Loss: 0.000001637 in Epoch 1069
Epoch 1152
Epoch 1152, Loss: 0.000015647, Improvement: -0.000013628, Best Loss: 0.000001637 in Epoch 1069
Epoch 1153
Epoch 1153, Loss: 0.000006622, Improvement: -0.000009025, Best Loss: 0.000001637 in Epoch 1069
Epoch 1154
Epoch 1154, Loss: 0.000004576, Improvement: -0.000002046, Best Loss: 0.000001637 in Epoch 1069
Epoch 1155
Epoch 1155, Loss: 0.000005782, Improvement: 0.000001206, Best Loss: 0.000001637 in Epoch 1069
Epoch 1156
Epoch 1156, Loss: 0.000009127, Improvement: 0.000003345, Best Loss: 0.000001637 in Epoch 1069
Epoch 1157
Epoch 1157, Loss: 0.000043945, Improvement: 0.000034818, Best Loss: 0.000001637 in Epoch 1069
Epoch 1158
Epoch 1158, Loss: 0.000049528, Improvement: 0.000005583, Best Loss: 0.000001637 in Epoch 1069
Epoch 1159
Epoch 1159, Loss: 0.000042280, Improvement: -0.000007247, Best Loss: 0.000001637 in Epoch 1069
Epoch 1160
Epoch 1160, Loss: 0.000016363, Improvement: -0.000025917, Best Loss: 0.000001637 in Epoch 1069
Epoch 1161
Epoch 1161, Loss: 0.000006358, Improvement: -0.000010005, Best Loss: 0.000001637 in Epoch 1069
Epoch 1162
Epoch 1162, Loss: 0.000004339, Improvement: -0.000002019, Best Loss: 0.000001637 in Epoch 1069
Epoch 1163
Epoch 1163, Loss: 0.000002878, Improvement: -0.000001461, Best Loss: 0.000001637 in Epoch 1069
Epoch 1164
Epoch 1164, Loss: 0.000002994, Improvement: 0.000000115, Best Loss: 0.000001637 in Epoch 1069
Epoch 1165
Epoch 1165, Loss: 0.000002837, Improvement: -0.000000157, Best Loss: 0.000001637 in Epoch 1069
Epoch 1166
A best model at epoch 1166 has been saved with training error 0.000001509.
A best model at epoch 1166 has been saved with training error 0.000001326.
Epoch 1166, Loss: 0.000002173, Improvement: -0.000000664, Best Loss: 0.000001326 in Epoch 1166
Epoch 1167
Epoch 1167, Loss: 0.000003796, Improvement: 0.000001623, Best Loss: 0.000001326 in Epoch 1166
Epoch 1168
Epoch 1168, Loss: 0.000004932, Improvement: 0.000001136, Best Loss: 0.000001326 in Epoch 1166
Epoch 1169
Epoch 1169, Loss: 0.000004480, Improvement: -0.000000452, Best Loss: 0.000001326 in Epoch 1166
Epoch 1170
Epoch 1170, Loss: 0.000002580, Improvement: -0.000001900, Best Loss: 0.000001326 in Epoch 1166
Epoch 1171
Epoch 1171, Loss: 0.000003000, Improvement: 0.000000420, Best Loss: 0.000001326 in Epoch 1166
Epoch 1172
Epoch 1172, Loss: 0.000004709, Improvement: 0.000001709, Best Loss: 0.000001326 in Epoch 1166
Epoch 1173
Epoch 1173, Loss: 0.000003573, Improvement: -0.000001136, Best Loss: 0.000001326 in Epoch 1166
Epoch 1174
Epoch 1174, Loss: 0.000005970, Improvement: 0.000002397, Best Loss: 0.000001326 in Epoch 1166
Epoch 1175
Epoch 1175, Loss: 0.000008740, Improvement: 0.000002770, Best Loss: 0.000001326 in Epoch 1166
Epoch 1176
Epoch 1176, Loss: 0.000008826, Improvement: 0.000000085, Best Loss: 0.000001326 in Epoch 1166
Epoch 1177
Epoch 1177, Loss: 0.000007066, Improvement: -0.000001760, Best Loss: 0.000001326 in Epoch 1166
Epoch 1178
Epoch 1178, Loss: 0.000009783, Improvement: 0.000002717, Best Loss: 0.000001326 in Epoch 1166
Epoch 1179
Epoch 1179, Loss: 0.000007413, Improvement: -0.000002370, Best Loss: 0.000001326 in Epoch 1166
Epoch 1180
Epoch 1180, Loss: 0.000009099, Improvement: 0.000001685, Best Loss: 0.000001326 in Epoch 1166
Epoch 1181
Epoch 1181, Loss: 0.000011719, Improvement: 0.000002620, Best Loss: 0.000001326 in Epoch 1166
Epoch 1182
Epoch 1182, Loss: 0.000012795, Improvement: 0.000001077, Best Loss: 0.000001326 in Epoch 1166
Epoch 1183
Epoch 1183, Loss: 0.000009029, Improvement: -0.000003766, Best Loss: 0.000001326 in Epoch 1166
Epoch 1184
Epoch 1184, Loss: 0.000016962, Improvement: 0.000007933, Best Loss: 0.000001326 in Epoch 1166
Epoch 1185
Epoch 1185, Loss: 0.000014066, Improvement: -0.000002896, Best Loss: 0.000001326 in Epoch 1166
Epoch 1186
Epoch 1186, Loss: 0.000011637, Improvement: -0.000002429, Best Loss: 0.000001326 in Epoch 1166
Epoch 1187
Epoch 1187, Loss: 0.000024647, Improvement: 0.000013010, Best Loss: 0.000001326 in Epoch 1166
Epoch 1188
Epoch 1188, Loss: 0.000046472, Improvement: 0.000021824, Best Loss: 0.000001326 in Epoch 1166
Epoch 1189
Epoch 1189, Loss: 0.000039878, Improvement: -0.000006594, Best Loss: 0.000001326 in Epoch 1166
Epoch 1190
Epoch 1190, Loss: 0.000025098, Improvement: -0.000014780, Best Loss: 0.000001326 in Epoch 1166
Epoch 1191
Epoch 1191, Loss: 0.000019927, Improvement: -0.000005171, Best Loss: 0.000001326 in Epoch 1166
Epoch 1192
Epoch 1192, Loss: 0.000008155, Improvement: -0.000011772, Best Loss: 0.000001326 in Epoch 1166
Epoch 1193
Epoch 1193, Loss: 0.000003944, Improvement: -0.000004211, Best Loss: 0.000001326 in Epoch 1166
Epoch 1194
Epoch 1194, Loss: 0.000003668, Improvement: -0.000000276, Best Loss: 0.000001326 in Epoch 1166
Epoch 1195
Epoch 1195, Loss: 0.000003276, Improvement: -0.000000392, Best Loss: 0.000001326 in Epoch 1166
Epoch 1196
Epoch 1196, Loss: 0.000002755, Improvement: -0.000000521, Best Loss: 0.000001326 in Epoch 1166
Epoch 1197
Epoch 1197, Loss: 0.000002390, Improvement: -0.000000366, Best Loss: 0.000001326 in Epoch 1166
Epoch 1198
Epoch 1198, Loss: 0.000002238, Improvement: -0.000000152, Best Loss: 0.000001326 in Epoch 1166
Epoch 1199
A best model at epoch 1199 has been saved with training error 0.000000950.
Epoch 1199, Loss: 0.000002479, Improvement: 0.000000241, Best Loss: 0.000000950 in Epoch 1199
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.000008883, Improvement: 0.000006404, Best Loss: 0.000000950 in Epoch 1199
Epoch 1201
Epoch 1201, Loss: 0.000010680, Improvement: 0.000001797, Best Loss: 0.000000950 in Epoch 1199
Epoch 1202
Epoch 1202, Loss: 0.000007750, Improvement: -0.000002930, Best Loss: 0.000000950 in Epoch 1199
Epoch 1203
Epoch 1203, Loss: 0.000008617, Improvement: 0.000000867, Best Loss: 0.000000950 in Epoch 1199
Epoch 1204
Epoch 1204, Loss: 0.000009341, Improvement: 0.000000725, Best Loss: 0.000000950 in Epoch 1199
Epoch 1205
Epoch 1205, Loss: 0.000006281, Improvement: -0.000003060, Best Loss: 0.000000950 in Epoch 1199
Epoch 1206
Epoch 1206, Loss: 0.000003594, Improvement: -0.000002687, Best Loss: 0.000000950 in Epoch 1199
Epoch 1207
Epoch 1207, Loss: 0.000003003, Improvement: -0.000000592, Best Loss: 0.000000950 in Epoch 1199
Epoch 1208
Epoch 1208, Loss: 0.000003633, Improvement: 0.000000630, Best Loss: 0.000000950 in Epoch 1199
Epoch 1209
Epoch 1209, Loss: 0.000003376, Improvement: -0.000000257, Best Loss: 0.000000950 in Epoch 1199
Epoch 1210
Epoch 1210, Loss: 0.000005495, Improvement: 0.000002120, Best Loss: 0.000000950 in Epoch 1199
Epoch 1211
Epoch 1211, Loss: 0.000006439, Improvement: 0.000000944, Best Loss: 0.000000950 in Epoch 1199
Epoch 1212
Epoch 1212, Loss: 0.000009346, Improvement: 0.000002906, Best Loss: 0.000000950 in Epoch 1199
Epoch 1213
Epoch 1213, Loss: 0.000009369, Improvement: 0.000000023, Best Loss: 0.000000950 in Epoch 1199
Epoch 1214
Epoch 1214, Loss: 0.000008690, Improvement: -0.000000679, Best Loss: 0.000000950 in Epoch 1199
Epoch 1215
Epoch 1215, Loss: 0.000009811, Improvement: 0.000001121, Best Loss: 0.000000950 in Epoch 1199
Epoch 1216
Epoch 1216, Loss: 0.000007129, Improvement: -0.000002682, Best Loss: 0.000000950 in Epoch 1199
Epoch 1217
Epoch 1217, Loss: 0.000007813, Improvement: 0.000000684, Best Loss: 0.000000950 in Epoch 1199
Epoch 1218
Epoch 1218, Loss: 0.000013052, Improvement: 0.000005239, Best Loss: 0.000000950 in Epoch 1199
Epoch 1219
Epoch 1219, Loss: 0.000013106, Improvement: 0.000000054, Best Loss: 0.000000950 in Epoch 1199
Epoch 1220
Epoch 1220, Loss: 0.000021033, Improvement: 0.000007927, Best Loss: 0.000000950 in Epoch 1199
Epoch 1221
Epoch 1221, Loss: 0.000017489, Improvement: -0.000003544, Best Loss: 0.000000950 in Epoch 1199
Epoch 1222
Epoch 1222, Loss: 0.000015020, Improvement: -0.000002469, Best Loss: 0.000000950 in Epoch 1199
Epoch 1223
Epoch 1223, Loss: 0.000008669, Improvement: -0.000006351, Best Loss: 0.000000950 in Epoch 1199
Epoch 1224
Epoch 1224, Loss: 0.000006793, Improvement: -0.000001877, Best Loss: 0.000000950 in Epoch 1199
Epoch 1225
Epoch 1225, Loss: 0.000004666, Improvement: -0.000002127, Best Loss: 0.000000950 in Epoch 1199
Epoch 1226
Epoch 1226, Loss: 0.000003400, Improvement: -0.000001266, Best Loss: 0.000000950 in Epoch 1199
Epoch 1227
Epoch 1227, Loss: 0.000002610, Improvement: -0.000000790, Best Loss: 0.000000950 in Epoch 1199
Epoch 1228
Epoch 1228, Loss: 0.000004061, Improvement: 0.000001451, Best Loss: 0.000000950 in Epoch 1199
Epoch 1229
Epoch 1229, Loss: 0.000003458, Improvement: -0.000000603, Best Loss: 0.000000950 in Epoch 1199
Epoch 1230
Epoch 1230, Loss: 0.000006839, Improvement: 0.000003381, Best Loss: 0.000000950 in Epoch 1199
Epoch 1231
Epoch 1231, Loss: 0.000016063, Improvement: 0.000009224, Best Loss: 0.000000950 in Epoch 1199
Epoch 1232
Epoch 1232, Loss: 0.000025731, Improvement: 0.000009668, Best Loss: 0.000000950 in Epoch 1199
Epoch 1233
Epoch 1233, Loss: 0.000018466, Improvement: -0.000007265, Best Loss: 0.000000950 in Epoch 1199
Epoch 1234
Epoch 1234, Loss: 0.000010765, Improvement: -0.000007701, Best Loss: 0.000000950 in Epoch 1199
Epoch 1235
Epoch 1235, Loss: 0.000011520, Improvement: 0.000000755, Best Loss: 0.000000950 in Epoch 1199
Epoch 1236
Epoch 1236, Loss: 0.000008987, Improvement: -0.000002533, Best Loss: 0.000000950 in Epoch 1199
Epoch 1237
Epoch 1237, Loss: 0.000008197, Improvement: -0.000000790, Best Loss: 0.000000950 in Epoch 1199
Epoch 1238
Epoch 1238, Loss: 0.000007981, Improvement: -0.000000216, Best Loss: 0.000000950 in Epoch 1199
Epoch 1239
Epoch 1239, Loss: 0.000031799, Improvement: 0.000023818, Best Loss: 0.000000950 in Epoch 1199
Epoch 1240
Epoch 1240, Loss: 0.000027274, Improvement: -0.000004526, Best Loss: 0.000000950 in Epoch 1199
Epoch 1241
Epoch 1241, Loss: 0.000009662, Improvement: -0.000017612, Best Loss: 0.000000950 in Epoch 1199
Epoch 1242
Epoch 1242, Loss: 0.000006662, Improvement: -0.000002999, Best Loss: 0.000000950 in Epoch 1199
Epoch 1243
Epoch 1243, Loss: 0.000006739, Improvement: 0.000000076, Best Loss: 0.000000950 in Epoch 1199
Epoch 1244
Epoch 1244, Loss: 0.000003887, Improvement: -0.000002852, Best Loss: 0.000000950 in Epoch 1199
Epoch 1245
Epoch 1245, Loss: 0.000003639, Improvement: -0.000000248, Best Loss: 0.000000950 in Epoch 1199
Epoch 1246
Epoch 1246, Loss: 0.000004971, Improvement: 0.000001332, Best Loss: 0.000000950 in Epoch 1199
Epoch 1247
Epoch 1247, Loss: 0.000003575, Improvement: -0.000001396, Best Loss: 0.000000950 in Epoch 1199
Epoch 1248
Epoch 1248, Loss: 0.000003031, Improvement: -0.000000544, Best Loss: 0.000000950 in Epoch 1199
Epoch 1249
Epoch 1249, Loss: 0.000002649, Improvement: -0.000000382, Best Loss: 0.000000950 in Epoch 1199
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.000002320, Improvement: -0.000000329, Best Loss: 0.000000950 in Epoch 1199
Epoch 1251
Epoch 1251, Loss: 0.000002046, Improvement: -0.000000274, Best Loss: 0.000000950 in Epoch 1199
Epoch 1252
Epoch 1252, Loss: 0.000001906, Improvement: -0.000000140, Best Loss: 0.000000950 in Epoch 1199
Epoch 1253
Epoch 1253, Loss: 0.000002842, Improvement: 0.000000936, Best Loss: 0.000000950 in Epoch 1199
Epoch 1254
Epoch 1254, Loss: 0.000005089, Improvement: 0.000002247, Best Loss: 0.000000950 in Epoch 1199
Epoch 1255
Epoch 1255, Loss: 0.000005180, Improvement: 0.000000091, Best Loss: 0.000000950 in Epoch 1199
Epoch 1256
Epoch 1256, Loss: 0.000005450, Improvement: 0.000000270, Best Loss: 0.000000950 in Epoch 1199
Epoch 1257
Epoch 1257, Loss: 0.000003883, Improvement: -0.000001567, Best Loss: 0.000000950 in Epoch 1199
Epoch 1258
Epoch 1258, Loss: 0.000007191, Improvement: 0.000003309, Best Loss: 0.000000950 in Epoch 1199
Epoch 1259
Epoch 1259, Loss: 0.000015863, Improvement: 0.000008671, Best Loss: 0.000000950 in Epoch 1199
Epoch 1260
Epoch 1260, Loss: 0.000012427, Improvement: -0.000003436, Best Loss: 0.000000950 in Epoch 1199
Epoch 1261
Epoch 1261, Loss: 0.000010093, Improvement: -0.000002333, Best Loss: 0.000000950 in Epoch 1199
Epoch 1262
Epoch 1262, Loss: 0.000011950, Improvement: 0.000001856, Best Loss: 0.000000950 in Epoch 1199
Epoch 1263
Epoch 1263, Loss: 0.000020534, Improvement: 0.000008584, Best Loss: 0.000000950 in Epoch 1199
Epoch 1264
Epoch 1264, Loss: 0.000018126, Improvement: -0.000002409, Best Loss: 0.000000950 in Epoch 1199
Epoch 1265
Epoch 1265, Loss: 0.000017141, Improvement: -0.000000985, Best Loss: 0.000000950 in Epoch 1199
Epoch 1266
Epoch 1266, Loss: 0.000013286, Improvement: -0.000003854, Best Loss: 0.000000950 in Epoch 1199
Epoch 1267
Epoch 1267, Loss: 0.000007489, Improvement: -0.000005798, Best Loss: 0.000000950 in Epoch 1199
Epoch 1268
Epoch 1268, Loss: 0.000005303, Improvement: -0.000002186, Best Loss: 0.000000950 in Epoch 1199
Epoch 1269
Epoch 1269, Loss: 0.000011154, Improvement: 0.000005851, Best Loss: 0.000000950 in Epoch 1199
Epoch 1270
Epoch 1270, Loss: 0.000020805, Improvement: 0.000009651, Best Loss: 0.000000950 in Epoch 1199
Epoch 1271
Epoch 1271, Loss: 0.000019570, Improvement: -0.000001235, Best Loss: 0.000000950 in Epoch 1199
Epoch 1272
Epoch 1272, Loss: 0.000007330, Improvement: -0.000012240, Best Loss: 0.000000950 in Epoch 1199
Epoch 1273
Epoch 1273, Loss: 0.000005424, Improvement: -0.000001906, Best Loss: 0.000000950 in Epoch 1199
Epoch 1274
Epoch 1274, Loss: 0.000005549, Improvement: 0.000000126, Best Loss: 0.000000950 in Epoch 1199
Epoch 1275
Epoch 1275, Loss: 0.000008288, Improvement: 0.000002739, Best Loss: 0.000000950 in Epoch 1199
Epoch 1276
Epoch 1276, Loss: 0.000034064, Improvement: 0.000025776, Best Loss: 0.000000950 in Epoch 1199
Epoch 1277
Epoch 1277, Loss: 0.000030614, Improvement: -0.000003449, Best Loss: 0.000000950 in Epoch 1199
Epoch 1278
Epoch 1278, Loss: 0.000020003, Improvement: -0.000010612, Best Loss: 0.000000950 in Epoch 1199
Epoch 1279
Epoch 1279, Loss: 0.000011262, Improvement: -0.000008740, Best Loss: 0.000000950 in Epoch 1199
Epoch 1280
Epoch 1280, Loss: 0.000008042, Improvement: -0.000003220, Best Loss: 0.000000950 in Epoch 1199
Epoch 1281
Epoch 1281, Loss: 0.000003563, Improvement: -0.000004480, Best Loss: 0.000000950 in Epoch 1199
Epoch 1282
Epoch 1282, Loss: 0.000003233, Improvement: -0.000000330, Best Loss: 0.000000950 in Epoch 1199
Epoch 1283
Epoch 1283, Loss: 0.000003781, Improvement: 0.000000548, Best Loss: 0.000000950 in Epoch 1199
Epoch 1284
Epoch 1284, Loss: 0.000006357, Improvement: 0.000002576, Best Loss: 0.000000950 in Epoch 1199
Epoch 1285
Epoch 1285, Loss: 0.000008861, Improvement: 0.000002504, Best Loss: 0.000000950 in Epoch 1199
Epoch 1286
Epoch 1286, Loss: 0.000016223, Improvement: 0.000007362, Best Loss: 0.000000950 in Epoch 1199
Epoch 1287
Epoch 1287, Loss: 0.000008496, Improvement: -0.000007727, Best Loss: 0.000000950 in Epoch 1199
Epoch 1288
Epoch 1288, Loss: 0.000007591, Improvement: -0.000000905, Best Loss: 0.000000950 in Epoch 1199
Epoch 1289
Epoch 1289, Loss: 0.000003843, Improvement: -0.000003748, Best Loss: 0.000000950 in Epoch 1199
Epoch 1290
Epoch 1290, Loss: 0.000002342, Improvement: -0.000001502, Best Loss: 0.000000950 in Epoch 1199
Epoch 1291
Epoch 1291, Loss: 0.000002249, Improvement: -0.000000093, Best Loss: 0.000000950 in Epoch 1199
Epoch 1292
Epoch 1292, Loss: 0.000002460, Improvement: 0.000000211, Best Loss: 0.000000950 in Epoch 1199
Epoch 1293
Epoch 1293, Loss: 0.000004819, Improvement: 0.000002359, Best Loss: 0.000000950 in Epoch 1199
Epoch 1294
Epoch 1294, Loss: 0.000003767, Improvement: -0.000001052, Best Loss: 0.000000950 in Epoch 1199
Epoch 1295
Epoch 1295, Loss: 0.000002704, Improvement: -0.000001063, Best Loss: 0.000000950 in Epoch 1199
Epoch 1296
Epoch 1296, Loss: 0.000003163, Improvement: 0.000000459, Best Loss: 0.000000950 in Epoch 1199
Epoch 1297
Epoch 1297, Loss: 0.000007768, Improvement: 0.000004606, Best Loss: 0.000000950 in Epoch 1199
Epoch 1298
Epoch 1298, Loss: 0.000011245, Improvement: 0.000003476, Best Loss: 0.000000950 in Epoch 1199
Epoch 1299
Epoch 1299, Loss: 0.000007012, Improvement: -0.000004233, Best Loss: 0.000000950 in Epoch 1199
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.000006470, Improvement: -0.000000542, Best Loss: 0.000000950 in Epoch 1199
Epoch 1301
Epoch 1301, Loss: 0.000010757, Improvement: 0.000004287, Best Loss: 0.000000950 in Epoch 1199
Epoch 1302
Epoch 1302, Loss: 0.000010196, Improvement: -0.000000561, Best Loss: 0.000000950 in Epoch 1199
Epoch 1303
Epoch 1303, Loss: 0.000013411, Improvement: 0.000003216, Best Loss: 0.000000950 in Epoch 1199
Epoch 1304
Epoch 1304, Loss: 0.000019650, Improvement: 0.000006239, Best Loss: 0.000000950 in Epoch 1199
Epoch 1305
Epoch 1305, Loss: 0.000009239, Improvement: -0.000010411, Best Loss: 0.000000950 in Epoch 1199
Epoch 1306
Epoch 1306, Loss: 0.000007424, Improvement: -0.000001815, Best Loss: 0.000000950 in Epoch 1199
Epoch 1307
Epoch 1307, Loss: 0.000012843, Improvement: 0.000005420, Best Loss: 0.000000950 in Epoch 1199
Epoch 1308
Epoch 1308, Loss: 0.000021833, Improvement: 0.000008990, Best Loss: 0.000000950 in Epoch 1199
Epoch 1309
Epoch 1309, Loss: 0.000015985, Improvement: -0.000005848, Best Loss: 0.000000950 in Epoch 1199
Epoch 1310
Epoch 1310, Loss: 0.000008384, Improvement: -0.000007601, Best Loss: 0.000000950 in Epoch 1199
Epoch 1311
Epoch 1311, Loss: 0.000007671, Improvement: -0.000000714, Best Loss: 0.000000950 in Epoch 1199
Epoch 1312
Epoch 1312, Loss: 0.000015217, Improvement: 0.000007546, Best Loss: 0.000000950 in Epoch 1199
Epoch 1313
Epoch 1313, Loss: 0.000009167, Improvement: -0.000006050, Best Loss: 0.000000950 in Epoch 1199
Epoch 1314
Epoch 1314, Loss: 0.000009103, Improvement: -0.000000064, Best Loss: 0.000000950 in Epoch 1199
Epoch 1315
Epoch 1315, Loss: 0.000012527, Improvement: 0.000003424, Best Loss: 0.000000950 in Epoch 1199
Epoch 1316
Epoch 1316, Loss: 0.000013275, Improvement: 0.000000748, Best Loss: 0.000000950 in Epoch 1199
Epoch 1317
Epoch 1317, Loss: 0.000014650, Improvement: 0.000001375, Best Loss: 0.000000950 in Epoch 1199
Epoch 1318
Epoch 1318, Loss: 0.000011845, Improvement: -0.000002804, Best Loss: 0.000000950 in Epoch 1199
Epoch 1319
Epoch 1319, Loss: 0.000007400, Improvement: -0.000004446, Best Loss: 0.000000950 in Epoch 1199
Epoch 1320
Epoch 1320, Loss: 0.000004674, Improvement: -0.000002726, Best Loss: 0.000000950 in Epoch 1199
Epoch 1321
Epoch 1321, Loss: 0.000003713, Improvement: -0.000000961, Best Loss: 0.000000950 in Epoch 1199
Epoch 1322
Epoch 1322, Loss: 0.000004562, Improvement: 0.000000849, Best Loss: 0.000000950 in Epoch 1199
Epoch 1323
Epoch 1323, Loss: 0.000005695, Improvement: 0.000001133, Best Loss: 0.000000950 in Epoch 1199
Epoch 1324
Epoch 1324, Loss: 0.000005196, Improvement: -0.000000499, Best Loss: 0.000000950 in Epoch 1199
Epoch 1325
Epoch 1325, Loss: 0.000003147, Improvement: -0.000002050, Best Loss: 0.000000950 in Epoch 1199
Epoch 1326
Epoch 1326, Loss: 0.000003874, Improvement: 0.000000727, Best Loss: 0.000000950 in Epoch 1199
Epoch 1327
Epoch 1327, Loss: 0.000003309, Improvement: -0.000000564, Best Loss: 0.000000950 in Epoch 1199
Epoch 1328
Epoch 1328, Loss: 0.000004265, Improvement: 0.000000956, Best Loss: 0.000000950 in Epoch 1199
Epoch 1329
Epoch 1329, Loss: 0.000004850, Improvement: 0.000000585, Best Loss: 0.000000950 in Epoch 1199
Epoch 1330
Epoch 1330, Loss: 0.000005013, Improvement: 0.000000162, Best Loss: 0.000000950 in Epoch 1199
Epoch 1331
Epoch 1331, Loss: 0.000007606, Improvement: 0.000002593, Best Loss: 0.000000950 in Epoch 1199
Epoch 1332
Epoch 1332, Loss: 0.000006506, Improvement: -0.000001099, Best Loss: 0.000000950 in Epoch 1199
Epoch 1333
Epoch 1333, Loss: 0.000004754, Improvement: -0.000001753, Best Loss: 0.000000950 in Epoch 1199
Epoch 1334
Epoch 1334, Loss: 0.000012198, Improvement: 0.000007444, Best Loss: 0.000000950 in Epoch 1199
Epoch 1335
Epoch 1335, Loss: 0.000019381, Improvement: 0.000007183, Best Loss: 0.000000950 in Epoch 1199
Epoch 1336
Epoch 1336, Loss: 0.000016468, Improvement: -0.000002914, Best Loss: 0.000000950 in Epoch 1199
Epoch 1337
Epoch 1337, Loss: 0.000009326, Improvement: -0.000007142, Best Loss: 0.000000950 in Epoch 1199
Epoch 1338
Epoch 1338, Loss: 0.000006002, Improvement: -0.000003324, Best Loss: 0.000000950 in Epoch 1199
Epoch 1339
Epoch 1339, Loss: 0.000003940, Improvement: -0.000002062, Best Loss: 0.000000950 in Epoch 1199
Epoch 1340
Epoch 1340, Loss: 0.000004102, Improvement: 0.000000162, Best Loss: 0.000000950 in Epoch 1199
Epoch 1341
Epoch 1341, Loss: 0.000002500, Improvement: -0.000001602, Best Loss: 0.000000950 in Epoch 1199
Epoch 1342
Epoch 1342, Loss: 0.000002086, Improvement: -0.000000414, Best Loss: 0.000000950 in Epoch 1199
Epoch 1343
Epoch 1343, Loss: 0.000002185, Improvement: 0.000000098, Best Loss: 0.000000950 in Epoch 1199
Epoch 1344
Epoch 1344, Loss: 0.000002640, Improvement: 0.000000456, Best Loss: 0.000000950 in Epoch 1199
Epoch 1345
Epoch 1345, Loss: 0.000004737, Improvement: 0.000002097, Best Loss: 0.000000950 in Epoch 1199
Epoch 1346
Epoch 1346, Loss: 0.000015448, Improvement: 0.000010711, Best Loss: 0.000000950 in Epoch 1199
Epoch 1347
Epoch 1347, Loss: 0.000026938, Improvement: 0.000011490, Best Loss: 0.000000950 in Epoch 1199
Epoch 1348
Epoch 1348, Loss: 0.000028944, Improvement: 0.000002006, Best Loss: 0.000000950 in Epoch 1199
Epoch 1349
Epoch 1349, Loss: 0.000020168, Improvement: -0.000008777, Best Loss: 0.000000950 in Epoch 1199
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.000010669, Improvement: -0.000009499, Best Loss: 0.000000950 in Epoch 1199
Epoch 1351
Epoch 1351, Loss: 0.000004038, Improvement: -0.000006631, Best Loss: 0.000000950 in Epoch 1199
Epoch 1352
Epoch 1352, Loss: 0.000002488, Improvement: -0.000001550, Best Loss: 0.000000950 in Epoch 1199
Epoch 1353
Epoch 1353, Loss: 0.000001826, Improvement: -0.000000661, Best Loss: 0.000000950 in Epoch 1199
Epoch 1354
Epoch 1354, Loss: 0.000003428, Improvement: 0.000001602, Best Loss: 0.000000950 in Epoch 1199
Epoch 1355
Epoch 1355, Loss: 0.000003988, Improvement: 0.000000559, Best Loss: 0.000000950 in Epoch 1199
Epoch 1356
Epoch 1356, Loss: 0.000003829, Improvement: -0.000000159, Best Loss: 0.000000950 in Epoch 1199
Epoch 1357
Epoch 1357, Loss: 0.000002556, Improvement: -0.000001273, Best Loss: 0.000000950 in Epoch 1199
Epoch 1358
Epoch 1358, Loss: 0.000002736, Improvement: 0.000000181, Best Loss: 0.000000950 in Epoch 1199
Epoch 1359
Epoch 1359, Loss: 0.000003684, Improvement: 0.000000947, Best Loss: 0.000000950 in Epoch 1199
Epoch 1360
Epoch 1360, Loss: 0.000006213, Improvement: 0.000002530, Best Loss: 0.000000950 in Epoch 1199
Epoch 1361
Epoch 1361, Loss: 0.000009061, Improvement: 0.000002848, Best Loss: 0.000000950 in Epoch 1199
Epoch 1362
Epoch 1362, Loss: 0.000006812, Improvement: -0.000002249, Best Loss: 0.000000950 in Epoch 1199
Epoch 1363
Epoch 1363, Loss: 0.000005693, Improvement: -0.000001119, Best Loss: 0.000000950 in Epoch 1199
Epoch 1364
Epoch 1364, Loss: 0.000032506, Improvement: 0.000026813, Best Loss: 0.000000950 in Epoch 1199
Epoch 1365
Epoch 1365, Loss: 0.000034070, Improvement: 0.000001564, Best Loss: 0.000000950 in Epoch 1199
Epoch 1366
Epoch 1366, Loss: 0.000015263, Improvement: -0.000018807, Best Loss: 0.000000950 in Epoch 1199
Epoch 1367
Epoch 1367, Loss: 0.000008855, Improvement: -0.000006408, Best Loss: 0.000000950 in Epoch 1199
Epoch 1368
Epoch 1368, Loss: 0.000007589, Improvement: -0.000001266, Best Loss: 0.000000950 in Epoch 1199
Epoch 1369
Epoch 1369, Loss: 0.000004818, Improvement: -0.000002771, Best Loss: 0.000000950 in Epoch 1199
Epoch 1370
Epoch 1370, Loss: 0.000008130, Improvement: 0.000003312, Best Loss: 0.000000950 in Epoch 1199
Epoch 1371
Epoch 1371, Loss: 0.000004556, Improvement: -0.000003573, Best Loss: 0.000000950 in Epoch 1199
Epoch 1372
Epoch 1372, Loss: 0.000003267, Improvement: -0.000001290, Best Loss: 0.000000950 in Epoch 1199
Epoch 1373
Epoch 1373, Loss: 0.000001705, Improvement: -0.000001562, Best Loss: 0.000000950 in Epoch 1199
Epoch 1374
Epoch 1374, Loss: 0.000001882, Improvement: 0.000000176, Best Loss: 0.000000950 in Epoch 1199
Epoch 1375
Epoch 1375, Loss: 0.000004509, Improvement: 0.000002628, Best Loss: 0.000000950 in Epoch 1199
Epoch 1376
Epoch 1376, Loss: 0.000006913, Improvement: 0.000002403, Best Loss: 0.000000950 in Epoch 1199
Epoch 1377
Epoch 1377, Loss: 0.000010893, Improvement: 0.000003980, Best Loss: 0.000000950 in Epoch 1199
Epoch 1378
Epoch 1378, Loss: 0.000016341, Improvement: 0.000005449, Best Loss: 0.000000950 in Epoch 1199
Epoch 1379
Epoch 1379, Loss: 0.000013285, Improvement: -0.000003056, Best Loss: 0.000000950 in Epoch 1199
Epoch 1380
Epoch 1380, Loss: 0.000025148, Improvement: 0.000011863, Best Loss: 0.000000950 in Epoch 1199
Epoch 1381
Epoch 1381, Loss: 0.000012153, Improvement: -0.000012994, Best Loss: 0.000000950 in Epoch 1199
Epoch 1382
Epoch 1382, Loss: 0.000004517, Improvement: -0.000007637, Best Loss: 0.000000950 in Epoch 1199
Epoch 1383
Epoch 1383, Loss: 0.000002932, Improvement: -0.000001585, Best Loss: 0.000000950 in Epoch 1199
Epoch 1384
Epoch 1384, Loss: 0.000002216, Improvement: -0.000000716, Best Loss: 0.000000950 in Epoch 1199
Epoch 1385
Epoch 1385, Loss: 0.000001720, Improvement: -0.000000496, Best Loss: 0.000000950 in Epoch 1199
Epoch 1386
Epoch 1386, Loss: 0.000001608, Improvement: -0.000000112, Best Loss: 0.000000950 in Epoch 1199
Epoch 1387
Epoch 1387, Loss: 0.000001515, Improvement: -0.000000093, Best Loss: 0.000000950 in Epoch 1199
Epoch 1388
Epoch 1388, Loss: 0.000001641, Improvement: 0.000000126, Best Loss: 0.000000950 in Epoch 1199
Epoch 1389
Epoch 1389, Loss: 0.000002364, Improvement: 0.000000723, Best Loss: 0.000000950 in Epoch 1199
Epoch 1390
Epoch 1390, Loss: 0.000003127, Improvement: 0.000000763, Best Loss: 0.000000950 in Epoch 1199
Epoch 1391
Epoch 1391, Loss: 0.000002019, Improvement: -0.000001108, Best Loss: 0.000000950 in Epoch 1199
Epoch 1392
Epoch 1392, Loss: 0.000001690, Improvement: -0.000000329, Best Loss: 0.000000950 in Epoch 1199
Epoch 1393
Epoch 1393, Loss: 0.000002138, Improvement: 0.000000448, Best Loss: 0.000000950 in Epoch 1199
Epoch 1394
Epoch 1394, Loss: 0.000003289, Improvement: 0.000001151, Best Loss: 0.000000950 in Epoch 1199
Epoch 1395
Epoch 1395, Loss: 0.000008144, Improvement: 0.000004855, Best Loss: 0.000000950 in Epoch 1199
Epoch 1396
Epoch 1396, Loss: 0.000014746, Improvement: 0.000006602, Best Loss: 0.000000950 in Epoch 1199
Epoch 1397
Epoch 1397, Loss: 0.000028894, Improvement: 0.000014148, Best Loss: 0.000000950 in Epoch 1199
Epoch 1398
Epoch 1398, Loss: 0.000034177, Improvement: 0.000005283, Best Loss: 0.000000950 in Epoch 1199
Epoch 1399
Epoch 1399, Loss: 0.000024224, Improvement: -0.000009954, Best Loss: 0.000000950 in Epoch 1199
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.000006996, Improvement: -0.000017228, Best Loss: 0.000000950 in Epoch 1199
Epoch 1401
Epoch 1401, Loss: 0.000004450, Improvement: -0.000002546, Best Loss: 0.000000950 in Epoch 1199
Epoch 1402
Epoch 1402, Loss: 0.000002767, Improvement: -0.000001683, Best Loss: 0.000000950 in Epoch 1199
Epoch 1403
Epoch 1403, Loss: 0.000002092, Improvement: -0.000000674, Best Loss: 0.000000950 in Epoch 1199
Epoch 1404
Epoch 1404, Loss: 0.000001665, Improvement: -0.000000427, Best Loss: 0.000000950 in Epoch 1199
Epoch 1405
Epoch 1405, Loss: 0.000003042, Improvement: 0.000001377, Best Loss: 0.000000950 in Epoch 1199
Epoch 1406
Epoch 1406, Loss: 0.000003018, Improvement: -0.000000024, Best Loss: 0.000000950 in Epoch 1199
Epoch 1407
Epoch 1407, Loss: 0.000002595, Improvement: -0.000000423, Best Loss: 0.000000950 in Epoch 1199
Epoch 1408
Epoch 1408, Loss: 0.000001807, Improvement: -0.000000788, Best Loss: 0.000000950 in Epoch 1199
Epoch 1409
Epoch 1409, Loss: 0.000001659, Improvement: -0.000000148, Best Loss: 0.000000950 in Epoch 1199
Epoch 1410
Epoch 1410, Loss: 0.000002240, Improvement: 0.000000581, Best Loss: 0.000000950 in Epoch 1199
Epoch 1411
Epoch 1411, Loss: 0.000001627, Improvement: -0.000000614, Best Loss: 0.000000950 in Epoch 1199
Epoch 1412
A best model at epoch 1412 has been saved with training error 0.000000868.
Epoch 1412, Loss: 0.000001664, Improvement: 0.000000037, Best Loss: 0.000000868 in Epoch 1412
Epoch 1413
A best model at epoch 1413 has been saved with training error 0.000000853.
Epoch 1413, Loss: 0.000001337, Improvement: -0.000000326, Best Loss: 0.000000853 in Epoch 1413
Epoch 1414
Epoch 1414, Loss: 0.000002403, Improvement: 0.000001066, Best Loss: 0.000000853 in Epoch 1413
Epoch 1415
Epoch 1415, Loss: 0.000003199, Improvement: 0.000000795, Best Loss: 0.000000853 in Epoch 1413
Epoch 1416
Epoch 1416, Loss: 0.000001798, Improvement: -0.000001401, Best Loss: 0.000000853 in Epoch 1413
Epoch 1417
Epoch 1417, Loss: 0.000002038, Improvement: 0.000000240, Best Loss: 0.000000853 in Epoch 1413
Epoch 1418
Epoch 1418, Loss: 0.000002150, Improvement: 0.000000112, Best Loss: 0.000000853 in Epoch 1413
Epoch 1419
Epoch 1419, Loss: 0.000002358, Improvement: 0.000000208, Best Loss: 0.000000853 in Epoch 1413
Epoch 1420
Epoch 1420, Loss: 0.000002586, Improvement: 0.000000228, Best Loss: 0.000000853 in Epoch 1413
Epoch 1421
Epoch 1421, Loss: 0.000002117, Improvement: -0.000000470, Best Loss: 0.000000853 in Epoch 1413
Epoch 1422
Epoch 1422, Loss: 0.000003871, Improvement: 0.000001754, Best Loss: 0.000000853 in Epoch 1413
Epoch 1423
Epoch 1423, Loss: 0.000006560, Improvement: 0.000002689, Best Loss: 0.000000853 in Epoch 1413
Epoch 1424
Epoch 1424, Loss: 0.000008551, Improvement: 0.000001990, Best Loss: 0.000000853 in Epoch 1413
Epoch 1425
Epoch 1425, Loss: 0.000009198, Improvement: 0.000000648, Best Loss: 0.000000853 in Epoch 1413
Epoch 1426
Epoch 1426, Loss: 0.000012571, Improvement: 0.000003373, Best Loss: 0.000000853 in Epoch 1413
Epoch 1427
Epoch 1427, Loss: 0.000013777, Improvement: 0.000001206, Best Loss: 0.000000853 in Epoch 1413
Epoch 1428
Epoch 1428, Loss: 0.000006128, Improvement: -0.000007649, Best Loss: 0.000000853 in Epoch 1413
Epoch 1429
Epoch 1429, Loss: 0.000004921, Improvement: -0.000001206, Best Loss: 0.000000853 in Epoch 1413
Epoch 1430
Epoch 1430, Loss: 0.000007198, Improvement: 0.000002277, Best Loss: 0.000000853 in Epoch 1413
Epoch 1431
Epoch 1431, Loss: 0.000011282, Improvement: 0.000004084, Best Loss: 0.000000853 in Epoch 1413
Epoch 1432
Epoch 1432, Loss: 0.000006733, Improvement: -0.000004549, Best Loss: 0.000000853 in Epoch 1413
Epoch 1433
Epoch 1433, Loss: 0.000007994, Improvement: 0.000001261, Best Loss: 0.000000853 in Epoch 1413
Epoch 1434
Epoch 1434, Loss: 0.000006461, Improvement: -0.000001532, Best Loss: 0.000000853 in Epoch 1413
Epoch 1435
Epoch 1435, Loss: 0.000006317, Improvement: -0.000000144, Best Loss: 0.000000853 in Epoch 1413
Epoch 1436
Epoch 1436, Loss: 0.000015835, Improvement: 0.000009517, Best Loss: 0.000000853 in Epoch 1413
Epoch 1437
Epoch 1437, Loss: 0.000007259, Improvement: -0.000008576, Best Loss: 0.000000853 in Epoch 1413
Epoch 1438
Epoch 1438, Loss: 0.000005399, Improvement: -0.000001860, Best Loss: 0.000000853 in Epoch 1413
Epoch 1439
Epoch 1439, Loss: 0.000004395, Improvement: -0.000001003, Best Loss: 0.000000853 in Epoch 1413
Epoch 1440
Epoch 1440, Loss: 0.000004991, Improvement: 0.000000595, Best Loss: 0.000000853 in Epoch 1413
Epoch 1441
Epoch 1441, Loss: 0.000008202, Improvement: 0.000003211, Best Loss: 0.000000853 in Epoch 1413
Epoch 1442
Epoch 1442, Loss: 0.000010991, Improvement: 0.000002789, Best Loss: 0.000000853 in Epoch 1413
Epoch 1443
Epoch 1443, Loss: 0.000015610, Improvement: 0.000004619, Best Loss: 0.000000853 in Epoch 1413
Epoch 1444
Epoch 1444, Loss: 0.000017937, Improvement: 0.000002327, Best Loss: 0.000000853 in Epoch 1413
Epoch 1445
Epoch 1445, Loss: 0.000011428, Improvement: -0.000006509, Best Loss: 0.000000853 in Epoch 1413
Epoch 1446
Epoch 1446, Loss: 0.000010325, Improvement: -0.000001103, Best Loss: 0.000000853 in Epoch 1413
Epoch 1447
Epoch 1447, Loss: 0.000010414, Improvement: 0.000000088, Best Loss: 0.000000853 in Epoch 1413
Epoch 1448
Epoch 1448, Loss: 0.000013613, Improvement: 0.000003200, Best Loss: 0.000000853 in Epoch 1413
Epoch 1449
Epoch 1449, Loss: 0.000019597, Improvement: 0.000005983, Best Loss: 0.000000853 in Epoch 1413
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.000028002, Improvement: 0.000008405, Best Loss: 0.000000853 in Epoch 1413
Epoch 1451
Epoch 1451, Loss: 0.000017347, Improvement: -0.000010654, Best Loss: 0.000000853 in Epoch 1413
Epoch 1452
Epoch 1452, Loss: 0.000015676, Improvement: -0.000001672, Best Loss: 0.000000853 in Epoch 1413
Epoch 1453
Epoch 1453, Loss: 0.000007354, Improvement: -0.000008321, Best Loss: 0.000000853 in Epoch 1413
Epoch 1454
Epoch 1454, Loss: 0.000004033, Improvement: -0.000003322, Best Loss: 0.000000853 in Epoch 1413
Epoch 1455
Epoch 1455, Loss: 0.000002657, Improvement: -0.000001376, Best Loss: 0.000000853 in Epoch 1413
Epoch 1456
Epoch 1456, Loss: 0.000002138, Improvement: -0.000000519, Best Loss: 0.000000853 in Epoch 1413
Epoch 1457
Epoch 1457, Loss: 0.000002093, Improvement: -0.000000045, Best Loss: 0.000000853 in Epoch 1413
Epoch 1458
Epoch 1458, Loss: 0.000001704, Improvement: -0.000000389, Best Loss: 0.000000853 in Epoch 1413
Epoch 1459
Epoch 1459, Loss: 0.000001694, Improvement: -0.000000010, Best Loss: 0.000000853 in Epoch 1413
Epoch 1460
Epoch 1460, Loss: 0.000001721, Improvement: 0.000000027, Best Loss: 0.000000853 in Epoch 1413
Epoch 1461
Epoch 1461, Loss: 0.000001478, Improvement: -0.000000243, Best Loss: 0.000000853 in Epoch 1413
Epoch 1462
Epoch 1462, Loss: 0.000002263, Improvement: 0.000000785, Best Loss: 0.000000853 in Epoch 1413
Epoch 1463
Epoch 1463, Loss: 0.000001878, Improvement: -0.000000386, Best Loss: 0.000000853 in Epoch 1413
Epoch 1464
Epoch 1464, Loss: 0.000003704, Improvement: 0.000001827, Best Loss: 0.000000853 in Epoch 1413
Epoch 1465
Epoch 1465, Loss: 0.000003694, Improvement: -0.000000010, Best Loss: 0.000000853 in Epoch 1413
Epoch 1466
Epoch 1466, Loss: 0.000002420, Improvement: -0.000001274, Best Loss: 0.000000853 in Epoch 1413
Epoch 1467
Epoch 1467, Loss: 0.000004370, Improvement: 0.000001949, Best Loss: 0.000000853 in Epoch 1413
Epoch 1468
Epoch 1468, Loss: 0.000009687, Improvement: 0.000005317, Best Loss: 0.000000853 in Epoch 1413
Epoch 1469
Epoch 1469, Loss: 0.000008460, Improvement: -0.000001226, Best Loss: 0.000000853 in Epoch 1413
Epoch 1470
Epoch 1470, Loss: 0.000012630, Improvement: 0.000004169, Best Loss: 0.000000853 in Epoch 1413
Epoch 1471
Epoch 1471, Loss: 0.000028186, Improvement: 0.000015556, Best Loss: 0.000000853 in Epoch 1413
Epoch 1472
Epoch 1472, Loss: 0.000019243, Improvement: -0.000008943, Best Loss: 0.000000853 in Epoch 1413
Epoch 1473
Epoch 1473, Loss: 0.000020036, Improvement: 0.000000793, Best Loss: 0.000000853 in Epoch 1413
Epoch 1474
Epoch 1474, Loss: 0.000009501, Improvement: -0.000010535, Best Loss: 0.000000853 in Epoch 1413
Epoch 1475
Epoch 1475, Loss: 0.000004457, Improvement: -0.000005044, Best Loss: 0.000000853 in Epoch 1413
Epoch 1476
Epoch 1476, Loss: 0.000004611, Improvement: 0.000000154, Best Loss: 0.000000853 in Epoch 1413
Epoch 1477
Epoch 1477, Loss: 0.000004111, Improvement: -0.000000500, Best Loss: 0.000000853 in Epoch 1413
Epoch 1478
Epoch 1478, Loss: 0.000003759, Improvement: -0.000000352, Best Loss: 0.000000853 in Epoch 1413
Epoch 1479
Epoch 1479, Loss: 0.000008376, Improvement: 0.000004617, Best Loss: 0.000000853 in Epoch 1413
Epoch 1480
Epoch 1480, Loss: 0.000005767, Improvement: -0.000002609, Best Loss: 0.000000853 in Epoch 1413
Epoch 1481
Epoch 1481, Loss: 0.000006826, Improvement: 0.000001059, Best Loss: 0.000000853 in Epoch 1413
Epoch 1482
Epoch 1482, Loss: 0.000011944, Improvement: 0.000005118, Best Loss: 0.000000853 in Epoch 1413
Epoch 1483
Epoch 1483, Loss: 0.000037085, Improvement: 0.000025141, Best Loss: 0.000000853 in Epoch 1413
Epoch 1484
Epoch 1484, Loss: 0.000032417, Improvement: -0.000004668, Best Loss: 0.000000853 in Epoch 1413
Epoch 1485
Epoch 1485, Loss: 0.000036038, Improvement: 0.000003621, Best Loss: 0.000000853 in Epoch 1413
Epoch 1486
Epoch 1486, Loss: 0.000017576, Improvement: -0.000018462, Best Loss: 0.000000853 in Epoch 1413
Epoch 1487
Epoch 1487, Loss: 0.000008886, Improvement: -0.000008689, Best Loss: 0.000000853 in Epoch 1413
Epoch 1488
Epoch 1488, Loss: 0.000003662, Improvement: -0.000005225, Best Loss: 0.000000853 in Epoch 1413
Epoch 1489
Epoch 1489, Loss: 0.000002687, Improvement: -0.000000975, Best Loss: 0.000000853 in Epoch 1413
Epoch 1490
Epoch 1490, Loss: 0.000001686, Improvement: -0.000001000, Best Loss: 0.000000853 in Epoch 1413
Epoch 1491
Epoch 1491, Loss: 0.000001783, Improvement: 0.000000097, Best Loss: 0.000000853 in Epoch 1413
Epoch 1492
Epoch 1492, Loss: 0.000001707, Improvement: -0.000000076, Best Loss: 0.000000853 in Epoch 1413
Epoch 1493
Epoch 1493, Loss: 0.000001437, Improvement: -0.000000270, Best Loss: 0.000000853 in Epoch 1413
Epoch 1494
Epoch 1494, Loss: 0.000001531, Improvement: 0.000000094, Best Loss: 0.000000853 in Epoch 1413
Epoch 1495
A best model at epoch 1495 has been saved with training error 0.000000769.
Epoch 1495, Loss: 0.000001137, Improvement: -0.000000394, Best Loss: 0.000000769 in Epoch 1495
Epoch 1496
Epoch 1496, Loss: 0.000001105, Improvement: -0.000000032, Best Loss: 0.000000769 in Epoch 1495
Epoch 1497
A best model at epoch 1497 has been saved with training error 0.000000712.
Epoch 1497, Loss: 0.000001009, Improvement: -0.000000096, Best Loss: 0.000000712 in Epoch 1497
Epoch 1498
Epoch 1498, Loss: 0.000001078, Improvement: 0.000000069, Best Loss: 0.000000712 in Epoch 1497
Epoch 1499
Epoch 1499, Loss: 0.000001873, Improvement: 0.000000795, Best Loss: 0.000000712 in Epoch 1497
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.000001915, Improvement: 0.000000042, Best Loss: 0.000000712 in Epoch 1497
Epoch 1501
Epoch 1501, Loss: 0.000001545, Improvement: -0.000000370, Best Loss: 0.000000712 in Epoch 1497
Epoch 1502
Epoch 1502, Loss: 0.000002016, Improvement: 0.000000471, Best Loss: 0.000000712 in Epoch 1497
Epoch 1503
Epoch 1503, Loss: 0.000001235, Improvement: -0.000000781, Best Loss: 0.000000712 in Epoch 1497
Epoch 1504
Epoch 1504, Loss: 0.000001257, Improvement: 0.000000023, Best Loss: 0.000000712 in Epoch 1497
Epoch 1505
Epoch 1505, Loss: 0.000002661, Improvement: 0.000001403, Best Loss: 0.000000712 in Epoch 1497
Epoch 1506
Epoch 1506, Loss: 0.000006019, Improvement: 0.000003358, Best Loss: 0.000000712 in Epoch 1497
Epoch 1507
Epoch 1507, Loss: 0.000005584, Improvement: -0.000000436, Best Loss: 0.000000712 in Epoch 1497
Epoch 1508
Epoch 1508, Loss: 0.000003459, Improvement: -0.000002124, Best Loss: 0.000000712 in Epoch 1497
Epoch 1509
Epoch 1509, Loss: 0.000006500, Improvement: 0.000003040, Best Loss: 0.000000712 in Epoch 1497
Epoch 1510
Epoch 1510, Loss: 0.000006026, Improvement: -0.000000473, Best Loss: 0.000000712 in Epoch 1497
Epoch 1511
Epoch 1511, Loss: 0.000010897, Improvement: 0.000004870, Best Loss: 0.000000712 in Epoch 1497
Epoch 1512
Epoch 1512, Loss: 0.000005136, Improvement: -0.000005760, Best Loss: 0.000000712 in Epoch 1497
Epoch 1513
Epoch 1513, Loss: 0.000005262, Improvement: 0.000000126, Best Loss: 0.000000712 in Epoch 1497
Epoch 1514
Epoch 1514, Loss: 0.000004928, Improvement: -0.000000335, Best Loss: 0.000000712 in Epoch 1497
Epoch 1515
Epoch 1515, Loss: 0.000003900, Improvement: -0.000001028, Best Loss: 0.000000712 in Epoch 1497
Epoch 1516
Epoch 1516, Loss: 0.000002922, Improvement: -0.000000977, Best Loss: 0.000000712 in Epoch 1497
Epoch 1517
Epoch 1517, Loss: 0.000002618, Improvement: -0.000000304, Best Loss: 0.000000712 in Epoch 1497
Epoch 1518
Epoch 1518, Loss: 0.000005741, Improvement: 0.000003123, Best Loss: 0.000000712 in Epoch 1497
Epoch 1519
Epoch 1519, Loss: 0.000005794, Improvement: 0.000000053, Best Loss: 0.000000712 in Epoch 1497
Epoch 1520
Epoch 1520, Loss: 0.000006086, Improvement: 0.000000292, Best Loss: 0.000000712 in Epoch 1497
Epoch 1521
Epoch 1521, Loss: 0.000005356, Improvement: -0.000000729, Best Loss: 0.000000712 in Epoch 1497
Epoch 1522
Epoch 1522, Loss: 0.000007091, Improvement: 0.000001735, Best Loss: 0.000000712 in Epoch 1497
Epoch 1523
Epoch 1523, Loss: 0.000005393, Improvement: -0.000001698, Best Loss: 0.000000712 in Epoch 1497
Epoch 1524
Epoch 1524, Loss: 0.000004999, Improvement: -0.000000394, Best Loss: 0.000000712 in Epoch 1497
Epoch 1525
Epoch 1525, Loss: 0.000004419, Improvement: -0.000000580, Best Loss: 0.000000712 in Epoch 1497
Epoch 1526
Epoch 1526, Loss: 0.000007349, Improvement: 0.000002930, Best Loss: 0.000000712 in Epoch 1497
Epoch 1527
Epoch 1527, Loss: 0.000010869, Improvement: 0.000003520, Best Loss: 0.000000712 in Epoch 1497
Epoch 1528
Epoch 1528, Loss: 0.000011263, Improvement: 0.000000394, Best Loss: 0.000000712 in Epoch 1497
Epoch 1529
Epoch 1529, Loss: 0.000006461, Improvement: -0.000004802, Best Loss: 0.000000712 in Epoch 1497
Epoch 1530
Epoch 1530, Loss: 0.000007736, Improvement: 0.000001275, Best Loss: 0.000000712 in Epoch 1497
Epoch 1531
Epoch 1531, Loss: 0.000008740, Improvement: 0.000001004, Best Loss: 0.000000712 in Epoch 1497
Epoch 1532
Epoch 1532, Loss: 0.000011655, Improvement: 0.000002915, Best Loss: 0.000000712 in Epoch 1497
Epoch 1533
Epoch 1533, Loss: 0.000010588, Improvement: -0.000001067, Best Loss: 0.000000712 in Epoch 1497
Epoch 1534
Epoch 1534, Loss: 0.000032253, Improvement: 0.000021665, Best Loss: 0.000000712 in Epoch 1497
Epoch 1535
Epoch 1535, Loss: 0.000036768, Improvement: 0.000004515, Best Loss: 0.000000712 in Epoch 1497
Epoch 1536
Epoch 1536, Loss: 0.000050077, Improvement: 0.000013309, Best Loss: 0.000000712 in Epoch 1497
Epoch 1537
Epoch 1537, Loss: 0.000037738, Improvement: -0.000012339, Best Loss: 0.000000712 in Epoch 1497
Epoch 1538
Epoch 1538, Loss: 0.000011000, Improvement: -0.000026738, Best Loss: 0.000000712 in Epoch 1497
Epoch 1539
Epoch 1539, Loss: 0.000003707, Improvement: -0.000007293, Best Loss: 0.000000712 in Epoch 1497
Epoch 1540
Epoch 1540, Loss: 0.000002284, Improvement: -0.000001423, Best Loss: 0.000000712 in Epoch 1497
Epoch 1541
Epoch 1541, Loss: 0.000001577, Improvement: -0.000000707, Best Loss: 0.000000712 in Epoch 1497
Epoch 1542
Epoch 1542, Loss: 0.000001727, Improvement: 0.000000150, Best Loss: 0.000000712 in Epoch 1497
Epoch 1543
Epoch 1543, Loss: 0.000001534, Improvement: -0.000000193, Best Loss: 0.000000712 in Epoch 1497
Epoch 1544
Epoch 1544, Loss: 0.000001830, Improvement: 0.000000296, Best Loss: 0.000000712 in Epoch 1497
Epoch 1545
Epoch 1545, Loss: 0.000001484, Improvement: -0.000000346, Best Loss: 0.000000712 in Epoch 1497
Epoch 1546
Epoch 1546, Loss: 0.000002286, Improvement: 0.000000802, Best Loss: 0.000000712 in Epoch 1497
Epoch 1547
Epoch 1547, Loss: 0.000001626, Improvement: -0.000000660, Best Loss: 0.000000712 in Epoch 1497
Epoch 1548
Epoch 1548, Loss: 0.000001398, Improvement: -0.000000228, Best Loss: 0.000000712 in Epoch 1497
Epoch 1549
Epoch 1549, Loss: 0.000002035, Improvement: 0.000000637, Best Loss: 0.000000712 in Epoch 1497
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.000002746, Improvement: 0.000000711, Best Loss: 0.000000712 in Epoch 1497
Epoch 1551
Epoch 1551, Loss: 0.000004187, Improvement: 0.000001441, Best Loss: 0.000000712 in Epoch 1497
Epoch 1552
Epoch 1552, Loss: 0.000003990, Improvement: -0.000000197, Best Loss: 0.000000712 in Epoch 1497
Epoch 1553
Epoch 1553, Loss: 0.000001727, Improvement: -0.000002262, Best Loss: 0.000000712 in Epoch 1497
Epoch 1554
Epoch 1554, Loss: 0.000001124, Improvement: -0.000000604, Best Loss: 0.000000712 in Epoch 1497
Epoch 1555
Epoch 1555, Loss: 0.000001443, Improvement: 0.000000319, Best Loss: 0.000000712 in Epoch 1497
Epoch 1556
Epoch 1556, Loss: 0.000003354, Improvement: 0.000001911, Best Loss: 0.000000712 in Epoch 1497
Epoch 1557
Epoch 1557, Loss: 0.000002351, Improvement: -0.000001003, Best Loss: 0.000000712 in Epoch 1497
Epoch 1558
Epoch 1558, Loss: 0.000002134, Improvement: -0.000000217, Best Loss: 0.000000712 in Epoch 1497
Epoch 1559
Epoch 1559, Loss: 0.000002682, Improvement: 0.000000548, Best Loss: 0.000000712 in Epoch 1497
Epoch 1560
Epoch 1560, Loss: 0.000001910, Improvement: -0.000000773, Best Loss: 0.000000712 in Epoch 1497
Epoch 1561
Epoch 1561, Loss: 0.000003426, Improvement: 0.000001516, Best Loss: 0.000000712 in Epoch 1497
Epoch 1562
Epoch 1562, Loss: 0.000008898, Improvement: 0.000005472, Best Loss: 0.000000712 in Epoch 1497
Epoch 1563
Epoch 1563, Loss: 0.000008844, Improvement: -0.000000054, Best Loss: 0.000000712 in Epoch 1497
Epoch 1564
Epoch 1564, Loss: 0.000003965, Improvement: -0.000004879, Best Loss: 0.000000712 in Epoch 1497
Epoch 1565
Epoch 1565, Loss: 0.000004053, Improvement: 0.000000088, Best Loss: 0.000000712 in Epoch 1497
Epoch 1566
Epoch 1566, Loss: 0.000003045, Improvement: -0.000001008, Best Loss: 0.000000712 in Epoch 1497
Epoch 1567
Epoch 1567, Loss: 0.000003449, Improvement: 0.000000404, Best Loss: 0.000000712 in Epoch 1497
Epoch 1568
Epoch 1568, Loss: 0.000003584, Improvement: 0.000000134, Best Loss: 0.000000712 in Epoch 1497
Epoch 1569
Epoch 1569, Loss: 0.000004135, Improvement: 0.000000551, Best Loss: 0.000000712 in Epoch 1497
Epoch 1570
Epoch 1570, Loss: 0.000003401, Improvement: -0.000000733, Best Loss: 0.000000712 in Epoch 1497
Epoch 1571
Epoch 1571, Loss: 0.000002764, Improvement: -0.000000638, Best Loss: 0.000000712 in Epoch 1497
Epoch 1572
Epoch 1572, Loss: 0.000002577, Improvement: -0.000000186, Best Loss: 0.000000712 in Epoch 1497
Epoch 1573
Epoch 1573, Loss: 0.000011158, Improvement: 0.000008580, Best Loss: 0.000000712 in Epoch 1497
Epoch 1574
Epoch 1574, Loss: 0.000013892, Improvement: 0.000002734, Best Loss: 0.000000712 in Epoch 1497
Epoch 1575
Epoch 1575, Loss: 0.000015200, Improvement: 0.000001308, Best Loss: 0.000000712 in Epoch 1497
Epoch 1576
Epoch 1576, Loss: 0.000014339, Improvement: -0.000000861, Best Loss: 0.000000712 in Epoch 1497
Epoch 1577
Epoch 1577, Loss: 0.000019596, Improvement: 0.000005258, Best Loss: 0.000000712 in Epoch 1497
Epoch 1578
Epoch 1578, Loss: 0.000028292, Improvement: 0.000008696, Best Loss: 0.000000712 in Epoch 1497
Epoch 1579
Epoch 1579, Loss: 0.000011666, Improvement: -0.000016626, Best Loss: 0.000000712 in Epoch 1497
Epoch 1580
Epoch 1580, Loss: 0.000013534, Improvement: 0.000001868, Best Loss: 0.000000712 in Epoch 1497
Epoch 1581
Epoch 1581, Loss: 0.000019940, Improvement: 0.000006406, Best Loss: 0.000000712 in Epoch 1497
Epoch 1582
Epoch 1582, Loss: 0.000011003, Improvement: -0.000008937, Best Loss: 0.000000712 in Epoch 1497
Epoch 1583
Epoch 1583, Loss: 0.000003465, Improvement: -0.000007538, Best Loss: 0.000000712 in Epoch 1497
Epoch 1584
Epoch 1584, Loss: 0.000002354, Improvement: -0.000001111, Best Loss: 0.000000712 in Epoch 1497
Epoch 1585
Epoch 1585, Loss: 0.000003430, Improvement: 0.000001076, Best Loss: 0.000000712 in Epoch 1497
Epoch 1586
Epoch 1586, Loss: 0.000017411, Improvement: 0.000013981, Best Loss: 0.000000712 in Epoch 1497
Epoch 1587
Epoch 1587, Loss: 0.000014298, Improvement: -0.000003113, Best Loss: 0.000000712 in Epoch 1497
Epoch 1588
Epoch 1588, Loss: 0.000005370, Improvement: -0.000008929, Best Loss: 0.000000712 in Epoch 1497
Epoch 1589
Epoch 1589, Loss: 0.000002438, Improvement: -0.000002931, Best Loss: 0.000000712 in Epoch 1497
Epoch 1590
Epoch 1590, Loss: 0.000002385, Improvement: -0.000000054, Best Loss: 0.000000712 in Epoch 1497
Epoch 1591
Epoch 1591, Loss: 0.000001364, Improvement: -0.000001020, Best Loss: 0.000000712 in Epoch 1497
Epoch 1592
Epoch 1592, Loss: 0.000001455, Improvement: 0.000000091, Best Loss: 0.000000712 in Epoch 1497
Epoch 1593
Epoch 1593, Loss: 0.000002676, Improvement: 0.000001221, Best Loss: 0.000000712 in Epoch 1497
Epoch 1594
Epoch 1594, Loss: 0.000010566, Improvement: 0.000007890, Best Loss: 0.000000712 in Epoch 1497
Epoch 1595
Epoch 1595, Loss: 0.000010094, Improvement: -0.000000472, Best Loss: 0.000000712 in Epoch 1497
Epoch 1596
Epoch 1596, Loss: 0.000020392, Improvement: 0.000010298, Best Loss: 0.000000712 in Epoch 1497
Epoch 1597
Epoch 1597, Loss: 0.000010217, Improvement: -0.000010175, Best Loss: 0.000000712 in Epoch 1497
Epoch 1598
Epoch 1598, Loss: 0.000006472, Improvement: -0.000003744, Best Loss: 0.000000712 in Epoch 1497
Epoch 1599
Epoch 1599, Loss: 0.000006376, Improvement: -0.000000096, Best Loss: 0.000000712 in Epoch 1497
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.000004557, Improvement: -0.000001819, Best Loss: 0.000000712 in Epoch 1497
Epoch 1601
Epoch 1601, Loss: 0.000005686, Improvement: 0.000001129, Best Loss: 0.000000712 in Epoch 1497
Epoch 1602
Epoch 1602, Loss: 0.000004011, Improvement: -0.000001675, Best Loss: 0.000000712 in Epoch 1497
Epoch 1603
Epoch 1603, Loss: 0.000005954, Improvement: 0.000001943, Best Loss: 0.000000712 in Epoch 1497
Epoch 1604
Epoch 1604, Loss: 0.000010185, Improvement: 0.000004232, Best Loss: 0.000000712 in Epoch 1497
Epoch 1605
Epoch 1605, Loss: 0.000022007, Improvement: 0.000011822, Best Loss: 0.000000712 in Epoch 1497
Epoch 1606
Epoch 1606, Loss: 0.000020469, Improvement: -0.000001538, Best Loss: 0.000000712 in Epoch 1497
Epoch 1607
Epoch 1607, Loss: 0.000019185, Improvement: -0.000001283, Best Loss: 0.000000712 in Epoch 1497
Epoch 1608
Epoch 1608, Loss: 0.000018696, Improvement: -0.000000489, Best Loss: 0.000000712 in Epoch 1497
Epoch 1609
Epoch 1609, Loss: 0.000011608, Improvement: -0.000007088, Best Loss: 0.000000712 in Epoch 1497
Epoch 1610
Epoch 1610, Loss: 0.000008213, Improvement: -0.000003396, Best Loss: 0.000000712 in Epoch 1497
Epoch 1611
Epoch 1611, Loss: 0.000004097, Improvement: -0.000004115, Best Loss: 0.000000712 in Epoch 1497
Epoch 1612
Epoch 1612, Loss: 0.000002643, Improvement: -0.000001454, Best Loss: 0.000000712 in Epoch 1497
Epoch 1613
Epoch 1613, Loss: 0.000001318, Improvement: -0.000001325, Best Loss: 0.000000712 in Epoch 1497
Epoch 1614
Epoch 1614, Loss: 0.000001643, Improvement: 0.000000325, Best Loss: 0.000000712 in Epoch 1497
Epoch 1615
Epoch 1615, Loss: 0.000001858, Improvement: 0.000000215, Best Loss: 0.000000712 in Epoch 1497
Epoch 1616
Epoch 1616, Loss: 0.000002126, Improvement: 0.000000268, Best Loss: 0.000000712 in Epoch 1497
Epoch 1617
Epoch 1617, Loss: 0.000002973, Improvement: 0.000000847, Best Loss: 0.000000712 in Epoch 1497
Epoch 1618
Epoch 1618, Loss: 0.000003883, Improvement: 0.000000910, Best Loss: 0.000000712 in Epoch 1497
Epoch 1619
Epoch 1619, Loss: 0.000003617, Improvement: -0.000000266, Best Loss: 0.000000712 in Epoch 1497
Epoch 1620
Epoch 1620, Loss: 0.000007138, Improvement: 0.000003521, Best Loss: 0.000000712 in Epoch 1497
Epoch 1621
Epoch 1621, Loss: 0.000004621, Improvement: -0.000002517, Best Loss: 0.000000712 in Epoch 1497
Epoch 1622
Epoch 1622, Loss: 0.000004032, Improvement: -0.000000589, Best Loss: 0.000000712 in Epoch 1497
Epoch 1623
Epoch 1623, Loss: 0.000003793, Improvement: -0.000000239, Best Loss: 0.000000712 in Epoch 1497
Epoch 1624
Epoch 1624, Loss: 0.000003096, Improvement: -0.000000697, Best Loss: 0.000000712 in Epoch 1497
Epoch 1625
Epoch 1625, Loss: 0.000005842, Improvement: 0.000002746, Best Loss: 0.000000712 in Epoch 1497
Epoch 1626
Epoch 1626, Loss: 0.000007191, Improvement: 0.000001349, Best Loss: 0.000000712 in Epoch 1497
Epoch 1627
Epoch 1627, Loss: 0.000005146, Improvement: -0.000002044, Best Loss: 0.000000712 in Epoch 1497
Epoch 1628
Epoch 1628, Loss: 0.000011139, Improvement: 0.000005992, Best Loss: 0.000000712 in Epoch 1497
Epoch 1629
Epoch 1629, Loss: 0.000016278, Improvement: 0.000005140, Best Loss: 0.000000712 in Epoch 1497
Epoch 1630
Epoch 1630, Loss: 0.000018162, Improvement: 0.000001883, Best Loss: 0.000000712 in Epoch 1497
Epoch 1631
Epoch 1631, Loss: 0.000011971, Improvement: -0.000006190, Best Loss: 0.000000712 in Epoch 1497
Epoch 1632
Epoch 1632, Loss: 0.000009759, Improvement: -0.000002213, Best Loss: 0.000000712 in Epoch 1497
Epoch 1633
Epoch 1633, Loss: 0.000008603, Improvement: -0.000001155, Best Loss: 0.000000712 in Epoch 1497
Epoch 1634
Epoch 1634, Loss: 0.000005132, Improvement: -0.000003471, Best Loss: 0.000000712 in Epoch 1497
Epoch 1635
Epoch 1635, Loss: 0.000003354, Improvement: -0.000001778, Best Loss: 0.000000712 in Epoch 1497
Epoch 1636
Epoch 1636, Loss: 0.000003529, Improvement: 0.000000174, Best Loss: 0.000000712 in Epoch 1497
Epoch 1637
Epoch 1637, Loss: 0.000002647, Improvement: -0.000000881, Best Loss: 0.000000712 in Epoch 1497
Epoch 1638
Epoch 1638, Loss: 0.000002282, Improvement: -0.000000365, Best Loss: 0.000000712 in Epoch 1497
Epoch 1639
Epoch 1639, Loss: 0.000002583, Improvement: 0.000000301, Best Loss: 0.000000712 in Epoch 1497
Epoch 1640
Epoch 1640, Loss: 0.000002141, Improvement: -0.000000441, Best Loss: 0.000000712 in Epoch 1497
Epoch 1641
Epoch 1641, Loss: 0.000002815, Improvement: 0.000000674, Best Loss: 0.000000712 in Epoch 1497
Epoch 1642
Epoch 1642, Loss: 0.000002821, Improvement: 0.000000006, Best Loss: 0.000000712 in Epoch 1497
Epoch 1643
Epoch 1643, Loss: 0.000002268, Improvement: -0.000000553, Best Loss: 0.000000712 in Epoch 1497
Epoch 1644
Epoch 1644, Loss: 0.000001546, Improvement: -0.000000722, Best Loss: 0.000000712 in Epoch 1497
Epoch 1645
Epoch 1645, Loss: 0.000002100, Improvement: 0.000000554, Best Loss: 0.000000712 in Epoch 1497
Epoch 1646
Epoch 1646, Loss: 0.000002714, Improvement: 0.000000614, Best Loss: 0.000000712 in Epoch 1497
Epoch 1647
Epoch 1647, Loss: 0.000002229, Improvement: -0.000000486, Best Loss: 0.000000712 in Epoch 1497
Epoch 1648
Epoch 1648, Loss: 0.000006769, Improvement: 0.000004541, Best Loss: 0.000000712 in Epoch 1497
Epoch 1649
Epoch 1649, Loss: 0.000012081, Improvement: 0.000005312, Best Loss: 0.000000712 in Epoch 1497
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.000020248, Improvement: 0.000008167, Best Loss: 0.000000712 in Epoch 1497
Epoch 1651
Epoch 1651, Loss: 0.000009569, Improvement: -0.000010679, Best Loss: 0.000000712 in Epoch 1497
Epoch 1652
Epoch 1652, Loss: 0.000007573, Improvement: -0.000001996, Best Loss: 0.000000712 in Epoch 1497
Epoch 1653
Epoch 1653, Loss: 0.000006673, Improvement: -0.000000900, Best Loss: 0.000000712 in Epoch 1497
Epoch 1654
Epoch 1654, Loss: 0.000004131, Improvement: -0.000002542, Best Loss: 0.000000712 in Epoch 1497
Epoch 1655
Epoch 1655, Loss: 0.000004125, Improvement: -0.000000007, Best Loss: 0.000000712 in Epoch 1497
Epoch 1656
Epoch 1656, Loss: 0.000004660, Improvement: 0.000000535, Best Loss: 0.000000712 in Epoch 1497
Epoch 1657
Epoch 1657, Loss: 0.000003481, Improvement: -0.000001179, Best Loss: 0.000000712 in Epoch 1497
Epoch 1658
Epoch 1658, Loss: 0.000004414, Improvement: 0.000000933, Best Loss: 0.000000712 in Epoch 1497
Epoch 1659
Epoch 1659, Loss: 0.000006724, Improvement: 0.000002310, Best Loss: 0.000000712 in Epoch 1497
Epoch 1660
Epoch 1660, Loss: 0.000014037, Improvement: 0.000007314, Best Loss: 0.000000712 in Epoch 1497
Epoch 1661
Epoch 1661, Loss: 0.000016963, Improvement: 0.000002926, Best Loss: 0.000000712 in Epoch 1497
Epoch 1662
Epoch 1662, Loss: 0.000014962, Improvement: -0.000002001, Best Loss: 0.000000712 in Epoch 1497
Epoch 1663
Epoch 1663, Loss: 0.000012037, Improvement: -0.000002926, Best Loss: 0.000000712 in Epoch 1497
Epoch 1664
Epoch 1664, Loss: 0.000009698, Improvement: -0.000002338, Best Loss: 0.000000712 in Epoch 1497
Epoch 1665
Epoch 1665, Loss: 0.000010722, Improvement: 0.000001024, Best Loss: 0.000000712 in Epoch 1497
Epoch 1666
Epoch 1666, Loss: 0.000028625, Improvement: 0.000017902, Best Loss: 0.000000712 in Epoch 1497
Epoch 1667
Epoch 1667, Loss: 0.000035585, Improvement: 0.000006960, Best Loss: 0.000000712 in Epoch 1497
Epoch 1668
Epoch 1668, Loss: 0.000020072, Improvement: -0.000015513, Best Loss: 0.000000712 in Epoch 1497
Epoch 1669
Epoch 1669, Loss: 0.000005983, Improvement: -0.000014089, Best Loss: 0.000000712 in Epoch 1497
Epoch 1670
Epoch 1670, Loss: 0.000003654, Improvement: -0.000002329, Best Loss: 0.000000712 in Epoch 1497
Epoch 1671
Epoch 1671, Loss: 0.000002342, Improvement: -0.000001312, Best Loss: 0.000000712 in Epoch 1497
Epoch 1672
Epoch 1672, Loss: 0.000002137, Improvement: -0.000000205, Best Loss: 0.000000712 in Epoch 1497
Epoch 1673
A best model at epoch 1673 has been saved with training error 0.000000690.
Epoch 1673, Loss: 0.000001797, Improvement: -0.000000340, Best Loss: 0.000000690 in Epoch 1673
Epoch 1674
Epoch 1674, Loss: 0.000002371, Improvement: 0.000000574, Best Loss: 0.000000690 in Epoch 1673
Epoch 1675
Epoch 1675, Loss: 0.000002256, Improvement: -0.000000115, Best Loss: 0.000000690 in Epoch 1673
Epoch 1676
Epoch 1676, Loss: 0.000001531, Improvement: -0.000000725, Best Loss: 0.000000690 in Epoch 1673
Epoch 1677
Epoch 1677, Loss: 0.000001486, Improvement: -0.000000045, Best Loss: 0.000000690 in Epoch 1673
Epoch 1678
Epoch 1678, Loss: 0.000001858, Improvement: 0.000000372, Best Loss: 0.000000690 in Epoch 1673
Epoch 1679
Epoch 1679, Loss: 0.000002565, Improvement: 0.000000707, Best Loss: 0.000000690 in Epoch 1673
Epoch 1680
Epoch 1680, Loss: 0.000008736, Improvement: 0.000006171, Best Loss: 0.000000690 in Epoch 1673
Epoch 1681
Epoch 1681, Loss: 0.000006516, Improvement: -0.000002220, Best Loss: 0.000000690 in Epoch 1673
Epoch 1682
Epoch 1682, Loss: 0.000004146, Improvement: -0.000002370, Best Loss: 0.000000690 in Epoch 1673
Epoch 1683
Epoch 1683, Loss: 0.000006350, Improvement: 0.000002203, Best Loss: 0.000000690 in Epoch 1673
Epoch 1684
Epoch 1684, Loss: 0.000007210, Improvement: 0.000000860, Best Loss: 0.000000690 in Epoch 1673
Epoch 1685
Epoch 1685, Loss: 0.000002277, Improvement: -0.000004933, Best Loss: 0.000000690 in Epoch 1673
Epoch 1686
Epoch 1686, Loss: 0.000001470, Improvement: -0.000000807, Best Loss: 0.000000690 in Epoch 1673
Epoch 1687
Epoch 1687, Loss: 0.000001151, Improvement: -0.000000319, Best Loss: 0.000000690 in Epoch 1673
Epoch 1688
Epoch 1688, Loss: 0.000001209, Improvement: 0.000000058, Best Loss: 0.000000690 in Epoch 1673
Epoch 1689
Epoch 1689, Loss: 0.000001531, Improvement: 0.000000322, Best Loss: 0.000000690 in Epoch 1673
Epoch 1690
Epoch 1690, Loss: 0.000001761, Improvement: 0.000000231, Best Loss: 0.000000690 in Epoch 1673
Epoch 1691
Epoch 1691, Loss: 0.000002040, Improvement: 0.000000279, Best Loss: 0.000000690 in Epoch 1673
Epoch 1692
Epoch 1692, Loss: 0.000001860, Improvement: -0.000000181, Best Loss: 0.000000690 in Epoch 1673
Epoch 1693
Epoch 1693, Loss: 0.000002603, Improvement: 0.000000744, Best Loss: 0.000000690 in Epoch 1673
Epoch 1694
Epoch 1694, Loss: 0.000002273, Improvement: -0.000000330, Best Loss: 0.000000690 in Epoch 1673
Epoch 1695
Epoch 1695, Loss: 0.000002597, Improvement: 0.000000324, Best Loss: 0.000000690 in Epoch 1673
Epoch 1696
Epoch 1696, Loss: 0.000002019, Improvement: -0.000000579, Best Loss: 0.000000690 in Epoch 1673
Epoch 1697
Epoch 1697, Loss: 0.000002954, Improvement: 0.000000935, Best Loss: 0.000000690 in Epoch 1673
Epoch 1698
Epoch 1698, Loss: 0.000004386, Improvement: 0.000001432, Best Loss: 0.000000690 in Epoch 1673
Epoch 1699
Epoch 1699, Loss: 0.000012299, Improvement: 0.000007913, Best Loss: 0.000000690 in Epoch 1673
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.000023958, Improvement: 0.000011660, Best Loss: 0.000000690 in Epoch 1673
Epoch 1701
Epoch 1701, Loss: 0.000019607, Improvement: -0.000004352, Best Loss: 0.000000690 in Epoch 1673
Epoch 1702
Epoch 1702, Loss: 0.000008040, Improvement: -0.000011567, Best Loss: 0.000000690 in Epoch 1673
Epoch 1703
Epoch 1703, Loss: 0.000005290, Improvement: -0.000002750, Best Loss: 0.000000690 in Epoch 1673
Epoch 1704
Epoch 1704, Loss: 0.000003690, Improvement: -0.000001600, Best Loss: 0.000000690 in Epoch 1673
Epoch 1705
Epoch 1705, Loss: 0.000002088, Improvement: -0.000001601, Best Loss: 0.000000690 in Epoch 1673
Epoch 1706
Epoch 1706, Loss: 0.000001915, Improvement: -0.000000174, Best Loss: 0.000000690 in Epoch 1673
Epoch 1707
Epoch 1707, Loss: 0.000001374, Improvement: -0.000000541, Best Loss: 0.000000690 in Epoch 1673
Epoch 1708
Epoch 1708, Loss: 0.000001042, Improvement: -0.000000332, Best Loss: 0.000000690 in Epoch 1673
Epoch 1709
A best model at epoch 1709 has been saved with training error 0.000000542.
Epoch 1709, Loss: 0.000000997, Improvement: -0.000000045, Best Loss: 0.000000542 in Epoch 1709
Epoch 1710
Epoch 1710, Loss: 0.000001770, Improvement: 0.000000773, Best Loss: 0.000000542 in Epoch 1709
Epoch 1711
Epoch 1711, Loss: 0.000002799, Improvement: 0.000001029, Best Loss: 0.000000542 in Epoch 1709
Epoch 1712
Epoch 1712, Loss: 0.000001769, Improvement: -0.000001030, Best Loss: 0.000000542 in Epoch 1709
Epoch 1713
Epoch 1713, Loss: 0.000003119, Improvement: 0.000001350, Best Loss: 0.000000542 in Epoch 1709
Epoch 1714
Epoch 1714, Loss: 0.000002531, Improvement: -0.000000588, Best Loss: 0.000000542 in Epoch 1709
Epoch 1715
Epoch 1715, Loss: 0.000002746, Improvement: 0.000000215, Best Loss: 0.000000542 in Epoch 1709
Epoch 1716
Epoch 1716, Loss: 0.000003260, Improvement: 0.000000514, Best Loss: 0.000000542 in Epoch 1709
Epoch 1717
Epoch 1717, Loss: 0.000002703, Improvement: -0.000000558, Best Loss: 0.000000542 in Epoch 1709
Epoch 1718
Epoch 1718, Loss: 0.000001501, Improvement: -0.000001201, Best Loss: 0.000000542 in Epoch 1709
Epoch 1719
Epoch 1719, Loss: 0.000001372, Improvement: -0.000000129, Best Loss: 0.000000542 in Epoch 1709
Epoch 1720
Epoch 1720, Loss: 0.000001636, Improvement: 0.000000265, Best Loss: 0.000000542 in Epoch 1709
Epoch 1721
Epoch 1721, Loss: 0.000007195, Improvement: 0.000005559, Best Loss: 0.000000542 in Epoch 1709
Epoch 1722
Epoch 1722, Loss: 0.000006652, Improvement: -0.000000543, Best Loss: 0.000000542 in Epoch 1709
Epoch 1723
Epoch 1723, Loss: 0.000005359, Improvement: -0.000001294, Best Loss: 0.000000542 in Epoch 1709
Epoch 1724
Epoch 1724, Loss: 0.000005765, Improvement: 0.000000406, Best Loss: 0.000000542 in Epoch 1709
Epoch 1725
Epoch 1725, Loss: 0.000003958, Improvement: -0.000001806, Best Loss: 0.000000542 in Epoch 1709
Epoch 1726
Epoch 1726, Loss: 0.000003698, Improvement: -0.000000261, Best Loss: 0.000000542 in Epoch 1709
Epoch 1727
Epoch 1727, Loss: 0.000007108, Improvement: 0.000003410, Best Loss: 0.000000542 in Epoch 1709
Epoch 1728
Epoch 1728, Loss: 0.000014644, Improvement: 0.000007536, Best Loss: 0.000000542 in Epoch 1709
Epoch 1729
Epoch 1729, Loss: 0.000025722, Improvement: 0.000011078, Best Loss: 0.000000542 in Epoch 1709
Epoch 1730
Epoch 1730, Loss: 0.000020647, Improvement: -0.000005075, Best Loss: 0.000000542 in Epoch 1709
Epoch 1731
Epoch 1731, Loss: 0.000014311, Improvement: -0.000006336, Best Loss: 0.000000542 in Epoch 1709
Epoch 1732
Epoch 1732, Loss: 0.000022029, Improvement: 0.000007718, Best Loss: 0.000000542 in Epoch 1709
Epoch 1733
Epoch 1733, Loss: 0.000008772, Improvement: -0.000013257, Best Loss: 0.000000542 in Epoch 1709
Epoch 1734
Epoch 1734, Loss: 0.000005483, Improvement: -0.000003289, Best Loss: 0.000000542 in Epoch 1709
Epoch 1735
Epoch 1735, Loss: 0.000002107, Improvement: -0.000003376, Best Loss: 0.000000542 in Epoch 1709
Epoch 1736
Epoch 1736, Loss: 0.000001566, Improvement: -0.000000541, Best Loss: 0.000000542 in Epoch 1709
Epoch 1737
Epoch 1737, Loss: 0.000002058, Improvement: 0.000000492, Best Loss: 0.000000542 in Epoch 1709
Epoch 1738
Epoch 1738, Loss: 0.000007738, Improvement: 0.000005680, Best Loss: 0.000000542 in Epoch 1709
Epoch 1739
Epoch 1739, Loss: 0.000003382, Improvement: -0.000004356, Best Loss: 0.000000542 in Epoch 1709
Epoch 1740
Epoch 1740, Loss: 0.000001997, Improvement: -0.000001385, Best Loss: 0.000000542 in Epoch 1709
Epoch 1741
Epoch 1741, Loss: 0.000001183, Improvement: -0.000000814, Best Loss: 0.000000542 in Epoch 1709
Epoch 1742
Epoch 1742, Loss: 0.000002069, Improvement: 0.000000887, Best Loss: 0.000000542 in Epoch 1709
Epoch 1743
Epoch 1743, Loss: 0.000004680, Improvement: 0.000002611, Best Loss: 0.000000542 in Epoch 1709
Epoch 1744
Epoch 1744, Loss: 0.000010536, Improvement: 0.000005856, Best Loss: 0.000000542 in Epoch 1709
Epoch 1745
Epoch 1745, Loss: 0.000008513, Improvement: -0.000002023, Best Loss: 0.000000542 in Epoch 1709
Epoch 1746
Epoch 1746, Loss: 0.000005411, Improvement: -0.000003102, Best Loss: 0.000000542 in Epoch 1709
Epoch 1747
Epoch 1747, Loss: 0.000004492, Improvement: -0.000000919, Best Loss: 0.000000542 in Epoch 1709
Epoch 1748
Epoch 1748, Loss: 0.000003450, Improvement: -0.000001043, Best Loss: 0.000000542 in Epoch 1709
Epoch 1749
Epoch 1749, Loss: 0.000003935, Improvement: 0.000000486, Best Loss: 0.000000542 in Epoch 1709
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.000008775, Improvement: 0.000004839, Best Loss: 0.000000542 in Epoch 1709
Epoch 1751
Epoch 1751, Loss: 0.000006237, Improvement: -0.000002538, Best Loss: 0.000000542 in Epoch 1709
Epoch 1752
Epoch 1752, Loss: 0.000004198, Improvement: -0.000002039, Best Loss: 0.000000542 in Epoch 1709
Epoch 1753
Epoch 1753, Loss: 0.000003167, Improvement: -0.000001031, Best Loss: 0.000000542 in Epoch 1709
Epoch 1754
Epoch 1754, Loss: 0.000004982, Improvement: 0.000001815, Best Loss: 0.000000542 in Epoch 1709
Epoch 1755
Epoch 1755, Loss: 0.000006151, Improvement: 0.000001169, Best Loss: 0.000000542 in Epoch 1709
Epoch 1756
Epoch 1756, Loss: 0.000014170, Improvement: 0.000008019, Best Loss: 0.000000542 in Epoch 1709
Epoch 1757
Epoch 1757, Loss: 0.000012366, Improvement: -0.000001804, Best Loss: 0.000000542 in Epoch 1709
Epoch 1758
Epoch 1758, Loss: 0.000025667, Improvement: 0.000013301, Best Loss: 0.000000542 in Epoch 1709
Epoch 1759
Epoch 1759, Loss: 0.000031373, Improvement: 0.000005706, Best Loss: 0.000000542 in Epoch 1709
Epoch 1760
Epoch 1760, Loss: 0.000024673, Improvement: -0.000006700, Best Loss: 0.000000542 in Epoch 1709
Epoch 1761
Epoch 1761, Loss: 0.000015449, Improvement: -0.000009223, Best Loss: 0.000000542 in Epoch 1709
Epoch 1762
Epoch 1762, Loss: 0.000008390, Improvement: -0.000007060, Best Loss: 0.000000542 in Epoch 1709
Epoch 1763
Epoch 1763, Loss: 0.000004891, Improvement: -0.000003499, Best Loss: 0.000000542 in Epoch 1709
Epoch 1764
Epoch 1764, Loss: 0.000002716, Improvement: -0.000002174, Best Loss: 0.000000542 in Epoch 1709
Epoch 1765
Epoch 1765, Loss: 0.000002413, Improvement: -0.000000304, Best Loss: 0.000000542 in Epoch 1709
Epoch 1766
Epoch 1766, Loss: 0.000001754, Improvement: -0.000000659, Best Loss: 0.000000542 in Epoch 1709
Epoch 1767
Epoch 1767, Loss: 0.000001051, Improvement: -0.000000704, Best Loss: 0.000000542 in Epoch 1709
Epoch 1768
Epoch 1768, Loss: 0.000001005, Improvement: -0.000000045, Best Loss: 0.000000542 in Epoch 1709
Epoch 1769
Epoch 1769, Loss: 0.000001264, Improvement: 0.000000258, Best Loss: 0.000000542 in Epoch 1709
Epoch 1770
Epoch 1770, Loss: 0.000001417, Improvement: 0.000000153, Best Loss: 0.000000542 in Epoch 1709
Epoch 1771
Epoch 1771, Loss: 0.000001622, Improvement: 0.000000205, Best Loss: 0.000000542 in Epoch 1709
Epoch 1772
Epoch 1772, Loss: 0.000002633, Improvement: 0.000001010, Best Loss: 0.000000542 in Epoch 1709
Epoch 1773
Epoch 1773, Loss: 0.000004880, Improvement: 0.000002248, Best Loss: 0.000000542 in Epoch 1709
Epoch 1774
Epoch 1774, Loss: 0.000005907, Improvement: 0.000001026, Best Loss: 0.000000542 in Epoch 1709
Epoch 1775
Epoch 1775, Loss: 0.000004024, Improvement: -0.000001883, Best Loss: 0.000000542 in Epoch 1709
Epoch 1776
Epoch 1776, Loss: 0.000002416, Improvement: -0.000001608, Best Loss: 0.000000542 in Epoch 1709
Epoch 1777
Epoch 1777, Loss: 0.000002629, Improvement: 0.000000214, Best Loss: 0.000000542 in Epoch 1709
Epoch 1778
Epoch 1778, Loss: 0.000003586, Improvement: 0.000000957, Best Loss: 0.000000542 in Epoch 1709
Epoch 1779
Epoch 1779, Loss: 0.000002333, Improvement: -0.000001253, Best Loss: 0.000000542 in Epoch 1709
Epoch 1780
Epoch 1780, Loss: 0.000002519, Improvement: 0.000000187, Best Loss: 0.000000542 in Epoch 1709
Epoch 1781
Epoch 1781, Loss: 0.000001623, Improvement: -0.000000896, Best Loss: 0.000000542 in Epoch 1709
Epoch 1782
Epoch 1782, Loss: 0.000002574, Improvement: 0.000000951, Best Loss: 0.000000542 in Epoch 1709
Epoch 1783
Epoch 1783, Loss: 0.000008030, Improvement: 0.000005455, Best Loss: 0.000000542 in Epoch 1709
Epoch 1784
Epoch 1784, Loss: 0.000033394, Improvement: 0.000025365, Best Loss: 0.000000542 in Epoch 1709
Epoch 1785
Epoch 1785, Loss: 0.000022452, Improvement: -0.000010942, Best Loss: 0.000000542 in Epoch 1709
Epoch 1786
Epoch 1786, Loss: 0.000006564, Improvement: -0.000015889, Best Loss: 0.000000542 in Epoch 1709
Epoch 1787
Epoch 1787, Loss: 0.000003086, Improvement: -0.000003477, Best Loss: 0.000000542 in Epoch 1709
Epoch 1788
Epoch 1788, Loss: 0.000001523, Improvement: -0.000001563, Best Loss: 0.000000542 in Epoch 1709
Epoch 1789
Epoch 1789, Loss: 0.000001949, Improvement: 0.000000426, Best Loss: 0.000000542 in Epoch 1709
Epoch 1790
Epoch 1790, Loss: 0.000001162, Improvement: -0.000000786, Best Loss: 0.000000542 in Epoch 1709
Epoch 1791
Epoch 1791, Loss: 0.000001343, Improvement: 0.000000181, Best Loss: 0.000000542 in Epoch 1709
Epoch 1792
Epoch 1792, Loss: 0.000002949, Improvement: 0.000001606, Best Loss: 0.000000542 in Epoch 1709
Epoch 1793
Epoch 1793, Loss: 0.000002298, Improvement: -0.000000651, Best Loss: 0.000000542 in Epoch 1709
Epoch 1794
Epoch 1794, Loss: 0.000001844, Improvement: -0.000000454, Best Loss: 0.000000542 in Epoch 1709
Epoch 1795
Epoch 1795, Loss: 0.000003967, Improvement: 0.000002124, Best Loss: 0.000000542 in Epoch 1709
Epoch 1796
Epoch 1796, Loss: 0.000002392, Improvement: -0.000001575, Best Loss: 0.000000542 in Epoch 1709
Epoch 1797
Epoch 1797, Loss: 0.000002167, Improvement: -0.000000225, Best Loss: 0.000000542 in Epoch 1709
Epoch 1798
Epoch 1798, Loss: 0.000001882, Improvement: -0.000000286, Best Loss: 0.000000542 in Epoch 1709
Epoch 1799
Epoch 1799, Loss: 0.000001295, Improvement: -0.000000587, Best Loss: 0.000000542 in Epoch 1709
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.000001024, Improvement: -0.000000270, Best Loss: 0.000000542 in Epoch 1709
Epoch 1801
Epoch 1801, Loss: 0.000001097, Improvement: 0.000000073, Best Loss: 0.000000542 in Epoch 1709
Epoch 1802
Epoch 1802, Loss: 0.000001096, Improvement: -0.000000001, Best Loss: 0.000000542 in Epoch 1709
Epoch 1803
Epoch 1803, Loss: 0.000001332, Improvement: 0.000000236, Best Loss: 0.000000542 in Epoch 1709
Epoch 1804
Epoch 1804, Loss: 0.000002224, Improvement: 0.000000892, Best Loss: 0.000000542 in Epoch 1709
Epoch 1805
Epoch 1805, Loss: 0.000002167, Improvement: -0.000000057, Best Loss: 0.000000542 in Epoch 1709
Epoch 1806
Epoch 1806, Loss: 0.000008834, Improvement: 0.000006666, Best Loss: 0.000000542 in Epoch 1709
Epoch 1807
Epoch 1807, Loss: 0.000010889, Improvement: 0.000002056, Best Loss: 0.000000542 in Epoch 1709
Epoch 1808
Epoch 1808, Loss: 0.000006856, Improvement: -0.000004033, Best Loss: 0.000000542 in Epoch 1709
Epoch 1809
Epoch 1809, Loss: 0.000006540, Improvement: -0.000000316, Best Loss: 0.000000542 in Epoch 1709
Epoch 1810
Epoch 1810, Loss: 0.000011993, Improvement: 0.000005453, Best Loss: 0.000000542 in Epoch 1709
Epoch 1811
Epoch 1811, Loss: 0.000016662, Improvement: 0.000004668, Best Loss: 0.000000542 in Epoch 1709
Epoch 1812
Epoch 1812, Loss: 0.000020178, Improvement: 0.000003517, Best Loss: 0.000000542 in Epoch 1709
Epoch 1813
Epoch 1813, Loss: 0.000014462, Improvement: -0.000005716, Best Loss: 0.000000542 in Epoch 1709
Epoch 1814
Epoch 1814, Loss: 0.000005011, Improvement: -0.000009451, Best Loss: 0.000000542 in Epoch 1709
Epoch 1815
Epoch 1815, Loss: 0.000004502, Improvement: -0.000000509, Best Loss: 0.000000542 in Epoch 1709
Epoch 1816
Epoch 1816, Loss: 0.000002835, Improvement: -0.000001667, Best Loss: 0.000000542 in Epoch 1709
Epoch 1817
Epoch 1817, Loss: 0.000002736, Improvement: -0.000000099, Best Loss: 0.000000542 in Epoch 1709
Epoch 1818
Epoch 1818, Loss: 0.000002499, Improvement: -0.000000237, Best Loss: 0.000000542 in Epoch 1709
Epoch 1819
Epoch 1819, Loss: 0.000003730, Improvement: 0.000001231, Best Loss: 0.000000542 in Epoch 1709
Epoch 1820
Epoch 1820, Loss: 0.000006936, Improvement: 0.000003206, Best Loss: 0.000000542 in Epoch 1709
Epoch 1821
Epoch 1821, Loss: 0.000010740, Improvement: 0.000003804, Best Loss: 0.000000542 in Epoch 1709
Epoch 1822
Epoch 1822, Loss: 0.000015558, Improvement: 0.000004818, Best Loss: 0.000000542 in Epoch 1709
Epoch 1823
Epoch 1823, Loss: 0.000009327, Improvement: -0.000006231, Best Loss: 0.000000542 in Epoch 1709
Epoch 1824
Epoch 1824, Loss: 0.000006076, Improvement: -0.000003250, Best Loss: 0.000000542 in Epoch 1709
Epoch 1825
Epoch 1825, Loss: 0.000004448, Improvement: -0.000001629, Best Loss: 0.000000542 in Epoch 1709
Epoch 1826
Epoch 1826, Loss: 0.000004212, Improvement: -0.000000235, Best Loss: 0.000000542 in Epoch 1709
Epoch 1827
Epoch 1827, Loss: 0.000003297, Improvement: -0.000000915, Best Loss: 0.000000542 in Epoch 1709
Epoch 1828
Epoch 1828, Loss: 0.000002461, Improvement: -0.000000835, Best Loss: 0.000000542 in Epoch 1709
Epoch 1829
Epoch 1829, Loss: 0.000002056, Improvement: -0.000000405, Best Loss: 0.000000542 in Epoch 1709
Epoch 1830
Epoch 1830, Loss: 0.000002211, Improvement: 0.000000154, Best Loss: 0.000000542 in Epoch 1709
Epoch 1831
Epoch 1831, Loss: 0.000001727, Improvement: -0.000000484, Best Loss: 0.000000542 in Epoch 1709
Epoch 1832
Epoch 1832, Loss: 0.000001359, Improvement: -0.000000368, Best Loss: 0.000000542 in Epoch 1709
Epoch 1833
Epoch 1833, Loss: 0.000001515, Improvement: 0.000000155, Best Loss: 0.000000542 in Epoch 1709
Epoch 1834
Epoch 1834, Loss: 0.000002055, Improvement: 0.000000540, Best Loss: 0.000000542 in Epoch 1709
Epoch 1835
Epoch 1835, Loss: 0.000002099, Improvement: 0.000000044, Best Loss: 0.000000542 in Epoch 1709
Epoch 1836
Epoch 1836, Loss: 0.000002706, Improvement: 0.000000607, Best Loss: 0.000000542 in Epoch 1709
Epoch 1837
Epoch 1837, Loss: 0.000007002, Improvement: 0.000004296, Best Loss: 0.000000542 in Epoch 1709
Epoch 1838
Epoch 1838, Loss: 0.000007574, Improvement: 0.000000573, Best Loss: 0.000000542 in Epoch 1709
Epoch 1839
Epoch 1839, Loss: 0.000005199, Improvement: -0.000002376, Best Loss: 0.000000542 in Epoch 1709
Epoch 1840
Epoch 1840, Loss: 0.000002880, Improvement: -0.000002319, Best Loss: 0.000000542 in Epoch 1709
Epoch 1841
Epoch 1841, Loss: 0.000003440, Improvement: 0.000000560, Best Loss: 0.000000542 in Epoch 1709
Epoch 1842
Epoch 1842, Loss: 0.000011262, Improvement: 0.000007822, Best Loss: 0.000000542 in Epoch 1709
Epoch 1843
Epoch 1843, Loss: 0.000052417, Improvement: 0.000041155, Best Loss: 0.000000542 in Epoch 1709
Epoch 1844
Epoch 1844, Loss: 0.000015053, Improvement: -0.000037364, Best Loss: 0.000000542 in Epoch 1709
Epoch 1845
Epoch 1845, Loss: 0.000004365, Improvement: -0.000010688, Best Loss: 0.000000542 in Epoch 1709
Epoch 1846
Epoch 1846, Loss: 0.000001659, Improvement: -0.000002706, Best Loss: 0.000000542 in Epoch 1709
Epoch 1847
Epoch 1847, Loss: 0.000001025, Improvement: -0.000000634, Best Loss: 0.000000542 in Epoch 1709
Epoch 1848
Epoch 1848, Loss: 0.000000858, Improvement: -0.000000167, Best Loss: 0.000000542 in Epoch 1709
Epoch 1849
Epoch 1849, Loss: 0.000001006, Improvement: 0.000000148, Best Loss: 0.000000542 in Epoch 1709
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.000000894, Improvement: -0.000000112, Best Loss: 0.000000542 in Epoch 1709
Epoch 1851
A best model at epoch 1851 has been saved with training error 0.000000539.
A best model at epoch 1851 has been saved with training error 0.000000452.
Epoch 1851, Loss: 0.000000859, Improvement: -0.000000035, Best Loss: 0.000000452 in Epoch 1851
Epoch 1852
Epoch 1852, Loss: 0.000000955, Improvement: 0.000000096, Best Loss: 0.000000452 in Epoch 1851
Epoch 1853
Epoch 1853, Loss: 0.000000844, Improvement: -0.000000111, Best Loss: 0.000000452 in Epoch 1851
Epoch 1854
Epoch 1854, Loss: 0.000000614, Improvement: -0.000000229, Best Loss: 0.000000452 in Epoch 1851
Epoch 1855
A best model at epoch 1855 has been saved with training error 0.000000449.
A best model at epoch 1855 has been saved with training error 0.000000438.
Epoch 1855, Loss: 0.000000583, Improvement: -0.000000031, Best Loss: 0.000000438 in Epoch 1855
Epoch 1856
Epoch 1856, Loss: 0.000000569, Improvement: -0.000000014, Best Loss: 0.000000438 in Epoch 1855
Epoch 1857
Epoch 1857, Loss: 0.000000620, Improvement: 0.000000051, Best Loss: 0.000000438 in Epoch 1855
Epoch 1858
A best model at epoch 1858 has been saved with training error 0.000000437.
Epoch 1858, Loss: 0.000000619, Improvement: -0.000000000, Best Loss: 0.000000437 in Epoch 1858
Epoch 1859
Epoch 1859, Loss: 0.000000607, Improvement: -0.000000012, Best Loss: 0.000000437 in Epoch 1858
Epoch 1860
A best model at epoch 1860 has been saved with training error 0.000000427.
Epoch 1860, Loss: 0.000000580, Improvement: -0.000000028, Best Loss: 0.000000427 in Epoch 1860
Epoch 1861
A best model at epoch 1861 has been saved with training error 0.000000419.
A best model at epoch 1861 has been saved with training error 0.000000398.
Epoch 1861, Loss: 0.000000548, Improvement: -0.000000032, Best Loss: 0.000000398 in Epoch 1861
Epoch 1862
Epoch 1862, Loss: 0.000001243, Improvement: 0.000000695, Best Loss: 0.000000398 in Epoch 1861
Epoch 1863
Epoch 1863, Loss: 0.000002403, Improvement: 0.000001160, Best Loss: 0.000000398 in Epoch 1861
Epoch 1864
Epoch 1864, Loss: 0.000002352, Improvement: -0.000000051, Best Loss: 0.000000398 in Epoch 1861
Epoch 1865
Epoch 1865, Loss: 0.000001538, Improvement: -0.000000814, Best Loss: 0.000000398 in Epoch 1861
Epoch 1866
Epoch 1866, Loss: 0.000001500, Improvement: -0.000000038, Best Loss: 0.000000398 in Epoch 1861
Epoch 1867
Epoch 1867, Loss: 0.000002148, Improvement: 0.000000648, Best Loss: 0.000000398 in Epoch 1861
Epoch 1868
Epoch 1868, Loss: 0.000002355, Improvement: 0.000000207, Best Loss: 0.000000398 in Epoch 1861
Epoch 1869
Epoch 1869, Loss: 0.000004536, Improvement: 0.000002181, Best Loss: 0.000000398 in Epoch 1861
Epoch 1870
Epoch 1870, Loss: 0.000005591, Improvement: 0.000001055, Best Loss: 0.000000398 in Epoch 1861
Epoch 1871
Epoch 1871, Loss: 0.000002803, Improvement: -0.000002788, Best Loss: 0.000000398 in Epoch 1861
Epoch 1872
Epoch 1872, Loss: 0.000002792, Improvement: -0.000000011, Best Loss: 0.000000398 in Epoch 1861
Epoch 1873
Epoch 1873, Loss: 0.000001616, Improvement: -0.000001176, Best Loss: 0.000000398 in Epoch 1861
Epoch 1874
Epoch 1874, Loss: 0.000000955, Improvement: -0.000000661, Best Loss: 0.000000398 in Epoch 1861
Epoch 1875
Epoch 1875, Loss: 0.000000741, Improvement: -0.000000214, Best Loss: 0.000000398 in Epoch 1861
Epoch 1876
Epoch 1876, Loss: 0.000000639, Improvement: -0.000000102, Best Loss: 0.000000398 in Epoch 1861
Epoch 1877
Epoch 1877, Loss: 0.000000729, Improvement: 0.000000090, Best Loss: 0.000000398 in Epoch 1861
Epoch 1878
Epoch 1878, Loss: 0.000000884, Improvement: 0.000000155, Best Loss: 0.000000398 in Epoch 1861
Epoch 1879
Epoch 1879, Loss: 0.000000790, Improvement: -0.000000094, Best Loss: 0.000000398 in Epoch 1861
Epoch 1880
Epoch 1880, Loss: 0.000000871, Improvement: 0.000000080, Best Loss: 0.000000398 in Epoch 1861
Epoch 1881
Epoch 1881, Loss: 0.000000797, Improvement: -0.000000073, Best Loss: 0.000000398 in Epoch 1861
Epoch 1882
Epoch 1882, Loss: 0.000001105, Improvement: 0.000000308, Best Loss: 0.000000398 in Epoch 1861
Epoch 1883
Epoch 1883, Loss: 0.000001124, Improvement: 0.000000019, Best Loss: 0.000000398 in Epoch 1861
Epoch 1884
Epoch 1884, Loss: 0.000001075, Improvement: -0.000000049, Best Loss: 0.000000398 in Epoch 1861
Epoch 1885
Epoch 1885, Loss: 0.000001570, Improvement: 0.000000495, Best Loss: 0.000000398 in Epoch 1861
Epoch 1886
Epoch 1886, Loss: 0.000001538, Improvement: -0.000000032, Best Loss: 0.000000398 in Epoch 1861
Epoch 1887
Epoch 1887, Loss: 0.000002488, Improvement: 0.000000950, Best Loss: 0.000000398 in Epoch 1861
Epoch 1888
Epoch 1888, Loss: 0.000002794, Improvement: 0.000000306, Best Loss: 0.000000398 in Epoch 1861
Epoch 1889
Epoch 1889, Loss: 0.000004672, Improvement: 0.000001878, Best Loss: 0.000000398 in Epoch 1861
Epoch 1890
Epoch 1890, Loss: 0.000003056, Improvement: -0.000001616, Best Loss: 0.000000398 in Epoch 1861
Epoch 1891
Epoch 1891, Loss: 0.000001297, Improvement: -0.000001759, Best Loss: 0.000000398 in Epoch 1861
Epoch 1892
Epoch 1892, Loss: 0.000003090, Improvement: 0.000001793, Best Loss: 0.000000398 in Epoch 1861
Epoch 1893
Epoch 1893, Loss: 0.000004422, Improvement: 0.000001332, Best Loss: 0.000000398 in Epoch 1861
Epoch 1894
Epoch 1894, Loss: 0.000006192, Improvement: 0.000001770, Best Loss: 0.000000398 in Epoch 1861
Epoch 1895
Epoch 1895, Loss: 0.000005739, Improvement: -0.000000453, Best Loss: 0.000000398 in Epoch 1861
Epoch 1896
Epoch 1896, Loss: 0.000007644, Improvement: 0.000001905, Best Loss: 0.000000398 in Epoch 1861
Epoch 1897
Epoch 1897, Loss: 0.000005402, Improvement: -0.000002242, Best Loss: 0.000000398 in Epoch 1861
Epoch 1898
Epoch 1898, Loss: 0.000003868, Improvement: -0.000001534, Best Loss: 0.000000398 in Epoch 1861
Epoch 1899
Epoch 1899, Loss: 0.000002837, Improvement: -0.000001031, Best Loss: 0.000000398 in Epoch 1861
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.000005084, Improvement: 0.000002247, Best Loss: 0.000000398 in Epoch 1861
Epoch 1901
Epoch 1901, Loss: 0.000015151, Improvement: 0.000010068, Best Loss: 0.000000398 in Epoch 1861
Epoch 1902
Epoch 1902, Loss: 0.000019552, Improvement: 0.000004400, Best Loss: 0.000000398 in Epoch 1861
Epoch 1903
Epoch 1903, Loss: 0.000013419, Improvement: -0.000006132, Best Loss: 0.000000398 in Epoch 1861
Epoch 1904
Epoch 1904, Loss: 0.000003866, Improvement: -0.000009553, Best Loss: 0.000000398 in Epoch 1861
Epoch 1905
Epoch 1905, Loss: 0.000001946, Improvement: -0.000001920, Best Loss: 0.000000398 in Epoch 1861
Epoch 1906
Epoch 1906, Loss: 0.000001515, Improvement: -0.000000431, Best Loss: 0.000000398 in Epoch 1861
Epoch 1907
Epoch 1907, Loss: 0.000001532, Improvement: 0.000000016, Best Loss: 0.000000398 in Epoch 1861
Epoch 1908
Epoch 1908, Loss: 0.000001040, Improvement: -0.000000492, Best Loss: 0.000000398 in Epoch 1861
Epoch 1909
Epoch 1909, Loss: 0.000001319, Improvement: 0.000000279, Best Loss: 0.000000398 in Epoch 1861
Epoch 1910
Epoch 1910, Loss: 0.000003577, Improvement: 0.000002259, Best Loss: 0.000000398 in Epoch 1861
Epoch 1911
Epoch 1911, Loss: 0.000013093, Improvement: 0.000009516, Best Loss: 0.000000398 in Epoch 1861
Epoch 1912
Epoch 1912, Loss: 0.000020369, Improvement: 0.000007276, Best Loss: 0.000000398 in Epoch 1861
Epoch 1913
Epoch 1913, Loss: 0.000008334, Improvement: -0.000012036, Best Loss: 0.000000398 in Epoch 1861
Epoch 1914
Epoch 1914, Loss: 0.000004278, Improvement: -0.000004056, Best Loss: 0.000000398 in Epoch 1861
Epoch 1915
Epoch 1915, Loss: 0.000003254, Improvement: -0.000001024, Best Loss: 0.000000398 in Epoch 1861
Epoch 1916
Epoch 1916, Loss: 0.000002231, Improvement: -0.000001024, Best Loss: 0.000000398 in Epoch 1861
Epoch 1917
Epoch 1917, Loss: 0.000000950, Improvement: -0.000001281, Best Loss: 0.000000398 in Epoch 1861
Epoch 1918
Epoch 1918, Loss: 0.000000929, Improvement: -0.000000021, Best Loss: 0.000000398 in Epoch 1861
Epoch 1919
Epoch 1919, Loss: 0.000001064, Improvement: 0.000000135, Best Loss: 0.000000398 in Epoch 1861
Epoch 1920
Epoch 1920, Loss: 0.000001636, Improvement: 0.000000573, Best Loss: 0.000000398 in Epoch 1861
Epoch 1921
Epoch 1921, Loss: 0.000001374, Improvement: -0.000000263, Best Loss: 0.000000398 in Epoch 1861
Epoch 1922
Epoch 1922, Loss: 0.000005455, Improvement: 0.000004082, Best Loss: 0.000000398 in Epoch 1861
Epoch 1923
Epoch 1923, Loss: 0.000005536, Improvement: 0.000000080, Best Loss: 0.000000398 in Epoch 1861
Epoch 1924
Epoch 1924, Loss: 0.000004354, Improvement: -0.000001181, Best Loss: 0.000000398 in Epoch 1861
Epoch 1925
Epoch 1925, Loss: 0.000002485, Improvement: -0.000001869, Best Loss: 0.000000398 in Epoch 1861
Epoch 1926
Epoch 1926, Loss: 0.000002127, Improvement: -0.000000357, Best Loss: 0.000000398 in Epoch 1861
Epoch 1927
Epoch 1927, Loss: 0.000003502, Improvement: 0.000001375, Best Loss: 0.000000398 in Epoch 1861
Epoch 1928
Epoch 1928, Loss: 0.000008992, Improvement: 0.000005490, Best Loss: 0.000000398 in Epoch 1861
Epoch 1929
Epoch 1929, Loss: 0.000009178, Improvement: 0.000000186, Best Loss: 0.000000398 in Epoch 1861
Epoch 1930
Epoch 1930, Loss: 0.000009385, Improvement: 0.000000207, Best Loss: 0.000000398 in Epoch 1861
Epoch 1931
Epoch 1931, Loss: 0.000004769, Improvement: -0.000004616, Best Loss: 0.000000398 in Epoch 1861
Epoch 1932
Epoch 1932, Loss: 0.000013309, Improvement: 0.000008540, Best Loss: 0.000000398 in Epoch 1861
Epoch 1933
Epoch 1933, Loss: 0.000022182, Improvement: 0.000008873, Best Loss: 0.000000398 in Epoch 1861
Epoch 1934
Epoch 1934, Loss: 0.000021219, Improvement: -0.000000963, Best Loss: 0.000000398 in Epoch 1861
Epoch 1935
Epoch 1935, Loss: 0.000017410, Improvement: -0.000003809, Best Loss: 0.000000398 in Epoch 1861
Epoch 1936
Epoch 1936, Loss: 0.000009624, Improvement: -0.000007786, Best Loss: 0.000000398 in Epoch 1861
Epoch 1937
Epoch 1937, Loss: 0.000004912, Improvement: -0.000004712, Best Loss: 0.000000398 in Epoch 1861
Epoch 1938
Epoch 1938, Loss: 0.000003114, Improvement: -0.000001798, Best Loss: 0.000000398 in Epoch 1861
Epoch 1939
Epoch 1939, Loss: 0.000002343, Improvement: -0.000000770, Best Loss: 0.000000398 in Epoch 1861
Epoch 1940
Epoch 1940, Loss: 0.000001630, Improvement: -0.000000714, Best Loss: 0.000000398 in Epoch 1861
Epoch 1941
Epoch 1941, Loss: 0.000001202, Improvement: -0.000000428, Best Loss: 0.000000398 in Epoch 1861
Epoch 1942
Epoch 1942, Loss: 0.000000808, Improvement: -0.000000393, Best Loss: 0.000000398 in Epoch 1861
Epoch 1943
Epoch 1943, Loss: 0.000001069, Improvement: 0.000000261, Best Loss: 0.000000398 in Epoch 1861
Epoch 1944
Epoch 1944, Loss: 0.000001068, Improvement: -0.000000001, Best Loss: 0.000000398 in Epoch 1861
Epoch 1945
Epoch 1945, Loss: 0.000001053, Improvement: -0.000000014, Best Loss: 0.000000398 in Epoch 1861
Epoch 1946
Epoch 1946, Loss: 0.000000863, Improvement: -0.000000190, Best Loss: 0.000000398 in Epoch 1861
Epoch 1947
Epoch 1947, Loss: 0.000000757, Improvement: -0.000000106, Best Loss: 0.000000398 in Epoch 1861
Epoch 1948
Epoch 1948, Loss: 0.000000941, Improvement: 0.000000185, Best Loss: 0.000000398 in Epoch 1861
Epoch 1949
Epoch 1949, Loss: 0.000000844, Improvement: -0.000000097, Best Loss: 0.000000398 in Epoch 1861
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.000001694, Improvement: 0.000000849, Best Loss: 0.000000398 in Epoch 1861
Epoch 1951
Epoch 1951, Loss: 0.000001519, Improvement: -0.000000175, Best Loss: 0.000000398 in Epoch 1861
Epoch 1952
Epoch 1952, Loss: 0.000001154, Improvement: -0.000000365, Best Loss: 0.000000398 in Epoch 1861
Epoch 1953
Epoch 1953, Loss: 0.000001458, Improvement: 0.000000304, Best Loss: 0.000000398 in Epoch 1861
Epoch 1954
Epoch 1954, Loss: 0.000001629, Improvement: 0.000000172, Best Loss: 0.000000398 in Epoch 1861
Epoch 1955
Epoch 1955, Loss: 0.000001658, Improvement: 0.000000028, Best Loss: 0.000000398 in Epoch 1861
Epoch 1956
Epoch 1956, Loss: 0.000002743, Improvement: 0.000001086, Best Loss: 0.000000398 in Epoch 1861
Epoch 1957
Epoch 1957, Loss: 0.000002547, Improvement: -0.000000196, Best Loss: 0.000000398 in Epoch 1861
Epoch 1958
Epoch 1958, Loss: 0.000002407, Improvement: -0.000000140, Best Loss: 0.000000398 in Epoch 1861
Epoch 1959
Epoch 1959, Loss: 0.000003526, Improvement: 0.000001119, Best Loss: 0.000000398 in Epoch 1861
Epoch 1960
Epoch 1960, Loss: 0.000004052, Improvement: 0.000000526, Best Loss: 0.000000398 in Epoch 1861
Epoch 1961
Epoch 1961, Loss: 0.000004543, Improvement: 0.000000490, Best Loss: 0.000000398 in Epoch 1861
Epoch 1962
Epoch 1962, Loss: 0.000007479, Improvement: 0.000002937, Best Loss: 0.000000398 in Epoch 1861
Epoch 1963
Epoch 1963, Loss: 0.000018799, Improvement: 0.000011319, Best Loss: 0.000000398 in Epoch 1861
Epoch 1964
Epoch 1964, Loss: 0.000014905, Improvement: -0.000003893, Best Loss: 0.000000398 in Epoch 1861
Epoch 1965
Epoch 1965, Loss: 0.000009945, Improvement: -0.000004960, Best Loss: 0.000000398 in Epoch 1861
Epoch 1966
Epoch 1966, Loss: 0.000004773, Improvement: -0.000005172, Best Loss: 0.000000398 in Epoch 1861
Epoch 1967
Epoch 1967, Loss: 0.000006877, Improvement: 0.000002104, Best Loss: 0.000000398 in Epoch 1861
Epoch 1968
Epoch 1968, Loss: 0.000003955, Improvement: -0.000002922, Best Loss: 0.000000398 in Epoch 1861
Epoch 1969
Epoch 1969, Loss: 0.000002483, Improvement: -0.000001472, Best Loss: 0.000000398 in Epoch 1861
Epoch 1970
Epoch 1970, Loss: 0.000001485, Improvement: -0.000000998, Best Loss: 0.000000398 in Epoch 1861
Epoch 1971
Epoch 1971, Loss: 0.000001771, Improvement: 0.000000286, Best Loss: 0.000000398 in Epoch 1861
Epoch 1972
Epoch 1972, Loss: 0.000001154, Improvement: -0.000000617, Best Loss: 0.000000398 in Epoch 1861
Epoch 1973
Epoch 1973, Loss: 0.000001585, Improvement: 0.000000431, Best Loss: 0.000000398 in Epoch 1861
Epoch 1974
Epoch 1974, Loss: 0.000001213, Improvement: -0.000000372, Best Loss: 0.000000398 in Epoch 1861
Epoch 1975
Epoch 1975, Loss: 0.000000867, Improvement: -0.000000345, Best Loss: 0.000000398 in Epoch 1861
Epoch 1976
Epoch 1976, Loss: 0.000000892, Improvement: 0.000000024, Best Loss: 0.000000398 in Epoch 1861
Epoch 1977
Epoch 1977, Loss: 0.000001200, Improvement: 0.000000308, Best Loss: 0.000000398 in Epoch 1861
Epoch 1978
Epoch 1978, Loss: 0.000001889, Improvement: 0.000000690, Best Loss: 0.000000398 in Epoch 1861
Epoch 1979
Epoch 1979, Loss: 0.000001426, Improvement: -0.000000464, Best Loss: 0.000000398 in Epoch 1861
Epoch 1980
Epoch 1980, Loss: 0.000001021, Improvement: -0.000000405, Best Loss: 0.000000398 in Epoch 1861
Epoch 1981
Epoch 1981, Loss: 0.000001030, Improvement: 0.000000010, Best Loss: 0.000000398 in Epoch 1861
Epoch 1982
Epoch 1982, Loss: 0.000001188, Improvement: 0.000000158, Best Loss: 0.000000398 in Epoch 1861
Epoch 1983
Epoch 1983, Loss: 0.000003559, Improvement: 0.000002370, Best Loss: 0.000000398 in Epoch 1861
Epoch 1984
Epoch 1984, Loss: 0.000002921, Improvement: -0.000000638, Best Loss: 0.000000398 in Epoch 1861
Epoch 1985
Epoch 1985, Loss: 0.000008150, Improvement: 0.000005230, Best Loss: 0.000000398 in Epoch 1861
Epoch 1986
Epoch 1986, Loss: 0.000010970, Improvement: 0.000002820, Best Loss: 0.000000398 in Epoch 1861
Epoch 1987
Epoch 1987, Loss: 0.000010316, Improvement: -0.000000654, Best Loss: 0.000000398 in Epoch 1861
Epoch 1988
Epoch 1988, Loss: 0.000012889, Improvement: 0.000002573, Best Loss: 0.000000398 in Epoch 1861
Epoch 1989
Epoch 1989, Loss: 0.000014432, Improvement: 0.000001544, Best Loss: 0.000000398 in Epoch 1861
Epoch 1990
Epoch 1990, Loss: 0.000013981, Improvement: -0.000000451, Best Loss: 0.000000398 in Epoch 1861
Epoch 1991
Epoch 1991, Loss: 0.000006772, Improvement: -0.000007209, Best Loss: 0.000000398 in Epoch 1861
Epoch 1992
Epoch 1992, Loss: 0.000008701, Improvement: 0.000001929, Best Loss: 0.000000398 in Epoch 1861
Epoch 1993
Epoch 1993, Loss: 0.000005351, Improvement: -0.000003350, Best Loss: 0.000000398 in Epoch 1861
Epoch 1994
Epoch 1994, Loss: 0.000004510, Improvement: -0.000000841, Best Loss: 0.000000398 in Epoch 1861
Epoch 1995
Epoch 1995, Loss: 0.000002940, Improvement: -0.000001570, Best Loss: 0.000000398 in Epoch 1861
Epoch 1996
Epoch 1996, Loss: 0.000006263, Improvement: 0.000003324, Best Loss: 0.000000398 in Epoch 1861
Epoch 1997
Epoch 1997, Loss: 0.000028784, Improvement: 0.000022521, Best Loss: 0.000000398 in Epoch 1861
Epoch 1998
Epoch 1998, Loss: 0.000016681, Improvement: -0.000012103, Best Loss: 0.000000398 in Epoch 1861
Epoch 1999
Epoch 1999, Loss: 0.000015661, Improvement: -0.000001020, Best Loss: 0.000000398 in Epoch 1861
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.000006967, Improvement: -0.000008694, Best Loss: 0.000000398 in Epoch 1861
Epoch 2001
Epoch 2001, Loss: 0.000002775, Improvement: -0.000004192, Best Loss: 0.000000398 in Epoch 1861
Epoch 2002
Epoch 2002, Loss: 0.000001424, Improvement: -0.000001351, Best Loss: 0.000000398 in Epoch 1861
Epoch 2003
Epoch 2003, Loss: 0.000001028, Improvement: -0.000000396, Best Loss: 0.000000398 in Epoch 1861
Epoch 2004
Epoch 2004, Loss: 0.000000754, Improvement: -0.000000274, Best Loss: 0.000000398 in Epoch 1861
Epoch 2005
Epoch 2005, Loss: 0.000000706, Improvement: -0.000000048, Best Loss: 0.000000398 in Epoch 1861
Epoch 2006
Epoch 2006, Loss: 0.000000580, Improvement: -0.000000126, Best Loss: 0.000000398 in Epoch 1861
Epoch 2007
Epoch 2007, Loss: 0.000000548, Improvement: -0.000000031, Best Loss: 0.000000398 in Epoch 1861
Epoch 2008
A best model at epoch 2008 has been saved with training error 0.000000385.
A best model at epoch 2008 has been saved with training error 0.000000344.
Epoch 2008, Loss: 0.000000582, Improvement: 0.000000033, Best Loss: 0.000000344 in Epoch 2008
Epoch 2009
Epoch 2009, Loss: 0.000000945, Improvement: 0.000000363, Best Loss: 0.000000344 in Epoch 2008
Epoch 2010
Epoch 2010, Loss: 0.000001147, Improvement: 0.000000203, Best Loss: 0.000000344 in Epoch 2008
Epoch 2011
Epoch 2011, Loss: 0.000001329, Improvement: 0.000000182, Best Loss: 0.000000344 in Epoch 2008
Epoch 2012
Epoch 2012, Loss: 0.000001301, Improvement: -0.000000028, Best Loss: 0.000000344 in Epoch 2008
Epoch 2013
Epoch 2013, Loss: 0.000001581, Improvement: 0.000000280, Best Loss: 0.000000344 in Epoch 2008
Epoch 2014
Epoch 2014, Loss: 0.000006321, Improvement: 0.000004740, Best Loss: 0.000000344 in Epoch 2008
Epoch 2015
Epoch 2015, Loss: 0.000007999, Improvement: 0.000001678, Best Loss: 0.000000344 in Epoch 2008
Epoch 2016
Epoch 2016, Loss: 0.000006615, Improvement: -0.000001383, Best Loss: 0.000000344 in Epoch 2008
Epoch 2017
Epoch 2017, Loss: 0.000003290, Improvement: -0.000003325, Best Loss: 0.000000344 in Epoch 2008
Epoch 2018
Epoch 2018, Loss: 0.000004326, Improvement: 0.000001036, Best Loss: 0.000000344 in Epoch 2008
Epoch 2019
Epoch 2019, Loss: 0.000002801, Improvement: -0.000001525, Best Loss: 0.000000344 in Epoch 2008
Epoch 2020
Epoch 2020, Loss: 0.000003649, Improvement: 0.000000848, Best Loss: 0.000000344 in Epoch 2008
Epoch 2021
Epoch 2021, Loss: 0.000002512, Improvement: -0.000001137, Best Loss: 0.000000344 in Epoch 2008
Epoch 2022
Epoch 2022, Loss: 0.000004482, Improvement: 0.000001970, Best Loss: 0.000000344 in Epoch 2008
Epoch 2023
Epoch 2023, Loss: 0.000005579, Improvement: 0.000001096, Best Loss: 0.000000344 in Epoch 2008
Epoch 2024
Epoch 2024, Loss: 0.000005963, Improvement: 0.000000384, Best Loss: 0.000000344 in Epoch 2008
Epoch 2025
Epoch 2025, Loss: 0.000006664, Improvement: 0.000000702, Best Loss: 0.000000344 in Epoch 2008
Epoch 2026
Epoch 2026, Loss: 0.000006201, Improvement: -0.000000463, Best Loss: 0.000000344 in Epoch 2008
Epoch 2027
Epoch 2027, Loss: 0.000013753, Improvement: 0.000007552, Best Loss: 0.000000344 in Epoch 2008
Epoch 2028
Epoch 2028, Loss: 0.000010955, Improvement: -0.000002798, Best Loss: 0.000000344 in Epoch 2008
Epoch 2029
Epoch 2029, Loss: 0.000005407, Improvement: -0.000005548, Best Loss: 0.000000344 in Epoch 2008
Epoch 2030
Epoch 2030, Loss: 0.000005206, Improvement: -0.000000201, Best Loss: 0.000000344 in Epoch 2008
Epoch 2031
Epoch 2031, Loss: 0.000002115, Improvement: -0.000003091, Best Loss: 0.000000344 in Epoch 2008
Epoch 2032
Epoch 2032, Loss: 0.000002290, Improvement: 0.000000176, Best Loss: 0.000000344 in Epoch 2008
Epoch 2033
Epoch 2033, Loss: 0.000001916, Improvement: -0.000000375, Best Loss: 0.000000344 in Epoch 2008
Epoch 2034
Epoch 2034, Loss: 0.000002009, Improvement: 0.000000094, Best Loss: 0.000000344 in Epoch 2008
Epoch 2035
Epoch 2035, Loss: 0.000002250, Improvement: 0.000000240, Best Loss: 0.000000344 in Epoch 2008
Epoch 2036
Epoch 2036, Loss: 0.000001192, Improvement: -0.000001058, Best Loss: 0.000000344 in Epoch 2008
Epoch 2037
Epoch 2037, Loss: 0.000000847, Improvement: -0.000000345, Best Loss: 0.000000344 in Epoch 2008
Epoch 2038
Epoch 2038, Loss: 0.000000747, Improvement: -0.000000100, Best Loss: 0.000000344 in Epoch 2008
Epoch 2039
Epoch 2039, Loss: 0.000001008, Improvement: 0.000000261, Best Loss: 0.000000344 in Epoch 2008
Epoch 2040
Epoch 2040, Loss: 0.000001161, Improvement: 0.000000153, Best Loss: 0.000000344 in Epoch 2008
Epoch 2041
Epoch 2041, Loss: 0.000001245, Improvement: 0.000000083, Best Loss: 0.000000344 in Epoch 2008
Epoch 2042
Epoch 2042, Loss: 0.000001029, Improvement: -0.000000216, Best Loss: 0.000000344 in Epoch 2008
Epoch 2043
Epoch 2043, Loss: 0.000001868, Improvement: 0.000000839, Best Loss: 0.000000344 in Epoch 2008
Epoch 2044
Epoch 2044, Loss: 0.000001992, Improvement: 0.000000124, Best Loss: 0.000000344 in Epoch 2008
Epoch 2045
Epoch 2045, Loss: 0.000009738, Improvement: 0.000007747, Best Loss: 0.000000344 in Epoch 2008
Epoch 2046
Epoch 2046, Loss: 0.000016882, Improvement: 0.000007143, Best Loss: 0.000000344 in Epoch 2008
Epoch 2047
Epoch 2047, Loss: 0.000011980, Improvement: -0.000004902, Best Loss: 0.000000344 in Epoch 2008
Epoch 2048
Epoch 2048, Loss: 0.000006136, Improvement: -0.000005844, Best Loss: 0.000000344 in Epoch 2008
Epoch 2049
Epoch 2049, Loss: 0.000004952, Improvement: -0.000001184, Best Loss: 0.000000344 in Epoch 2008
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.000005199, Improvement: 0.000000248, Best Loss: 0.000000344 in Epoch 2008
Epoch 2051
Epoch 2051, Loss: 0.000007985, Improvement: 0.000002786, Best Loss: 0.000000344 in Epoch 2008
Epoch 2052
Epoch 2052, Loss: 0.000003046, Improvement: -0.000004939, Best Loss: 0.000000344 in Epoch 2008
Epoch 2053
Epoch 2053, Loss: 0.000004992, Improvement: 0.000001945, Best Loss: 0.000000344 in Epoch 2008
Epoch 2054
Epoch 2054, Loss: 0.000012642, Improvement: 0.000007650, Best Loss: 0.000000344 in Epoch 2008
Epoch 2055
Epoch 2055, Loss: 0.000012780, Improvement: 0.000000138, Best Loss: 0.000000344 in Epoch 2008
Epoch 2056
Epoch 2056, Loss: 0.000017866, Improvement: 0.000005086, Best Loss: 0.000000344 in Epoch 2008
Epoch 2057
Epoch 2057, Loss: 0.000012987, Improvement: -0.000004879, Best Loss: 0.000000344 in Epoch 2008
Epoch 2058
Epoch 2058, Loss: 0.000009574, Improvement: -0.000003413, Best Loss: 0.000000344 in Epoch 2008
Epoch 2059
Epoch 2059, Loss: 0.000007597, Improvement: -0.000001977, Best Loss: 0.000000344 in Epoch 2008
Epoch 2060
Epoch 2060, Loss: 0.000005627, Improvement: -0.000001970, Best Loss: 0.000000344 in Epoch 2008
Epoch 2061
Epoch 2061, Loss: 0.000003782, Improvement: -0.000001845, Best Loss: 0.000000344 in Epoch 2008
Epoch 2062
Epoch 2062, Loss: 0.000003732, Improvement: -0.000000050, Best Loss: 0.000000344 in Epoch 2008
Epoch 2063
Epoch 2063, Loss: 0.000004151, Improvement: 0.000000420, Best Loss: 0.000000344 in Epoch 2008
Epoch 2064
Epoch 2064, Loss: 0.000004658, Improvement: 0.000000507, Best Loss: 0.000000344 in Epoch 2008
Epoch 2065
Epoch 2065, Loss: 0.000003900, Improvement: -0.000000758, Best Loss: 0.000000344 in Epoch 2008
Epoch 2066
Epoch 2066, Loss: 0.000008013, Improvement: 0.000004113, Best Loss: 0.000000344 in Epoch 2008
Epoch 2067
Epoch 2067, Loss: 0.000008391, Improvement: 0.000000378, Best Loss: 0.000000344 in Epoch 2008
Epoch 2068
Epoch 2068, Loss: 0.000003879, Improvement: -0.000004512, Best Loss: 0.000000344 in Epoch 2008
Epoch 2069
Epoch 2069, Loss: 0.000002966, Improvement: -0.000000913, Best Loss: 0.000000344 in Epoch 2008
Epoch 2070
Epoch 2070, Loss: 0.000003607, Improvement: 0.000000641, Best Loss: 0.000000344 in Epoch 2008
Epoch 2071
Epoch 2071, Loss: 0.000005946, Improvement: 0.000002339, Best Loss: 0.000000344 in Epoch 2008
Epoch 2072
Epoch 2072, Loss: 0.000013556, Improvement: 0.000007610, Best Loss: 0.000000344 in Epoch 2008
Epoch 2073
Epoch 2073, Loss: 0.000009408, Improvement: -0.000004148, Best Loss: 0.000000344 in Epoch 2008
Epoch 2074
Epoch 2074, Loss: 0.000004088, Improvement: -0.000005321, Best Loss: 0.000000344 in Epoch 2008
Epoch 2075
Epoch 2075, Loss: 0.000003992, Improvement: -0.000000095, Best Loss: 0.000000344 in Epoch 2008
Epoch 2076
Epoch 2076, Loss: 0.000003170, Improvement: -0.000000823, Best Loss: 0.000000344 in Epoch 2008
Epoch 2077
Epoch 2077, Loss: 0.000002301, Improvement: -0.000000868, Best Loss: 0.000000344 in Epoch 2008
Epoch 2078
Epoch 2078, Loss: 0.000002274, Improvement: -0.000000027, Best Loss: 0.000000344 in Epoch 2008
Epoch 2079
Epoch 2079, Loss: 0.000001340, Improvement: -0.000000934, Best Loss: 0.000000344 in Epoch 2008
Epoch 2080
Epoch 2080, Loss: 0.000002628, Improvement: 0.000001288, Best Loss: 0.000000344 in Epoch 2008
Epoch 2081
Epoch 2081, Loss: 0.000005206, Improvement: 0.000002578, Best Loss: 0.000000344 in Epoch 2008
Epoch 2082
Epoch 2082, Loss: 0.000003417, Improvement: -0.000001788, Best Loss: 0.000000344 in Epoch 2008
Epoch 2083
Epoch 2083, Loss: 0.000002903, Improvement: -0.000000514, Best Loss: 0.000000344 in Epoch 2008
Epoch 2084
Epoch 2084, Loss: 0.000002991, Improvement: 0.000000088, Best Loss: 0.000000344 in Epoch 2008
Epoch 2085
Epoch 2085, Loss: 0.000002664, Improvement: -0.000000327, Best Loss: 0.000000344 in Epoch 2008
Epoch 2086
Epoch 2086, Loss: 0.000003212, Improvement: 0.000000548, Best Loss: 0.000000344 in Epoch 2008
Epoch 2087
Epoch 2087, Loss: 0.000001531, Improvement: -0.000001681, Best Loss: 0.000000344 in Epoch 2008
Epoch 2088
Epoch 2088, Loss: 0.000001367, Improvement: -0.000000163, Best Loss: 0.000000344 in Epoch 2008
Epoch 2089
Epoch 2089, Loss: 0.000001261, Improvement: -0.000000106, Best Loss: 0.000000344 in Epoch 2008
Epoch 2090
Epoch 2090, Loss: 0.000003389, Improvement: 0.000002127, Best Loss: 0.000000344 in Epoch 2008
Epoch 2091
Epoch 2091, Loss: 0.000004400, Improvement: 0.000001011, Best Loss: 0.000000344 in Epoch 2008
Epoch 2092
Epoch 2092, Loss: 0.000006647, Improvement: 0.000002247, Best Loss: 0.000000344 in Epoch 2008
Epoch 2093
Epoch 2093, Loss: 0.000009366, Improvement: 0.000002719, Best Loss: 0.000000344 in Epoch 2008
Epoch 2094
Epoch 2094, Loss: 0.000007675, Improvement: -0.000001691, Best Loss: 0.000000344 in Epoch 2008
Epoch 2095
Epoch 2095, Loss: 0.000005068, Improvement: -0.000002607, Best Loss: 0.000000344 in Epoch 2008
Epoch 2096
Epoch 2096, Loss: 0.000004283, Improvement: -0.000000785, Best Loss: 0.000000344 in Epoch 2008
Epoch 2097
Epoch 2097, Loss: 0.000003760, Improvement: -0.000000523, Best Loss: 0.000000344 in Epoch 2008
Epoch 2098
Epoch 2098, Loss: 0.000003846, Improvement: 0.000000086, Best Loss: 0.000000344 in Epoch 2008
Epoch 2099
Epoch 2099, Loss: 0.000005112, Improvement: 0.000001266, Best Loss: 0.000000344 in Epoch 2008
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.000006518, Improvement: 0.000001407, Best Loss: 0.000000344 in Epoch 2008
Epoch 2101
Epoch 2101, Loss: 0.000012602, Improvement: 0.000006083, Best Loss: 0.000000344 in Epoch 2008
Epoch 2102
Epoch 2102, Loss: 0.000008484, Improvement: -0.000004118, Best Loss: 0.000000344 in Epoch 2008
Epoch 2103
Epoch 2103, Loss: 0.000004507, Improvement: -0.000003977, Best Loss: 0.000000344 in Epoch 2008
Epoch 2104
Epoch 2104, Loss: 0.000003396, Improvement: -0.000001110, Best Loss: 0.000000344 in Epoch 2008
Epoch 2105
Epoch 2105, Loss: 0.000003658, Improvement: 0.000000261, Best Loss: 0.000000344 in Epoch 2008
Epoch 2106
Epoch 2106, Loss: 0.000007307, Improvement: 0.000003649, Best Loss: 0.000000344 in Epoch 2008
Epoch 2107
Epoch 2107, Loss: 0.000016621, Improvement: 0.000009314, Best Loss: 0.000000344 in Epoch 2008
Epoch 2108
Epoch 2108, Loss: 0.000005356, Improvement: -0.000011264, Best Loss: 0.000000344 in Epoch 2008
Epoch 2109
Epoch 2109, Loss: 0.000002879, Improvement: -0.000002478, Best Loss: 0.000000344 in Epoch 2008
Epoch 2110
Epoch 2110, Loss: 0.000004326, Improvement: 0.000001447, Best Loss: 0.000000344 in Epoch 2008
Epoch 2111
Epoch 2111, Loss: 0.000003426, Improvement: -0.000000900, Best Loss: 0.000000344 in Epoch 2008
Epoch 2112
Epoch 2112, Loss: 0.000003151, Improvement: -0.000000275, Best Loss: 0.000000344 in Epoch 2008
Epoch 2113
Epoch 2113, Loss: 0.000003710, Improvement: 0.000000559, Best Loss: 0.000000344 in Epoch 2008
Epoch 2114
Epoch 2114, Loss: 0.000003319, Improvement: -0.000000392, Best Loss: 0.000000344 in Epoch 2008
Epoch 2115
Epoch 2115, Loss: 0.000005410, Improvement: 0.000002091, Best Loss: 0.000000344 in Epoch 2008
Epoch 2116
Epoch 2116, Loss: 0.000003709, Improvement: -0.000001701, Best Loss: 0.000000344 in Epoch 2008
Epoch 2117
Epoch 2117, Loss: 0.000003857, Improvement: 0.000000149, Best Loss: 0.000000344 in Epoch 2008
Epoch 2118
Epoch 2118, Loss: 0.000003220, Improvement: -0.000000637, Best Loss: 0.000000344 in Epoch 2008
Epoch 2119
Epoch 2119, Loss: 0.000006916, Improvement: 0.000003696, Best Loss: 0.000000344 in Epoch 2008
Epoch 2120
Epoch 2120, Loss: 0.000003502, Improvement: -0.000003415, Best Loss: 0.000000344 in Epoch 2008
Epoch 2121
Epoch 2121, Loss: 0.000007538, Improvement: 0.000004037, Best Loss: 0.000000344 in Epoch 2008
Epoch 2122
Epoch 2122, Loss: 0.000018359, Improvement: 0.000010820, Best Loss: 0.000000344 in Epoch 2008
Epoch 2123
Epoch 2123, Loss: 0.000015927, Improvement: -0.000002431, Best Loss: 0.000000344 in Epoch 2008
Epoch 2124
Epoch 2124, Loss: 0.000021768, Improvement: 0.000005841, Best Loss: 0.000000344 in Epoch 2008
Epoch 2125
Epoch 2125, Loss: 0.000020544, Improvement: -0.000001224, Best Loss: 0.000000344 in Epoch 2008
Epoch 2126
Epoch 2126, Loss: 0.000008783, Improvement: -0.000011761, Best Loss: 0.000000344 in Epoch 2008
Epoch 2127
Epoch 2127, Loss: 0.000008024, Improvement: -0.000000758, Best Loss: 0.000000344 in Epoch 2008
Epoch 2128
Epoch 2128, Loss: 0.000002454, Improvement: -0.000005570, Best Loss: 0.000000344 in Epoch 2008
Epoch 2129
Epoch 2129, Loss: 0.000001054, Improvement: -0.000001400, Best Loss: 0.000000344 in Epoch 2008
Epoch 2130
Epoch 2130, Loss: 0.000001320, Improvement: 0.000000265, Best Loss: 0.000000344 in Epoch 2008
Epoch 2131
Epoch 2131, Loss: 0.000001056, Improvement: -0.000000264, Best Loss: 0.000000344 in Epoch 2008
Epoch 2132
Epoch 2132, Loss: 0.000000692, Improvement: -0.000000364, Best Loss: 0.000000344 in Epoch 2008
Epoch 2133
Epoch 2133, Loss: 0.000000618, Improvement: -0.000000074, Best Loss: 0.000000344 in Epoch 2008
Epoch 2134
Epoch 2134, Loss: 0.000000537, Improvement: -0.000000081, Best Loss: 0.000000344 in Epoch 2008
Epoch 2135
Epoch 2135, Loss: 0.000000553, Improvement: 0.000000017, Best Loss: 0.000000344 in Epoch 2008
Epoch 2136
Epoch 2136, Loss: 0.000000730, Improvement: 0.000000176, Best Loss: 0.000000344 in Epoch 2008
Epoch 2137
Epoch 2137, Loss: 0.000001142, Improvement: 0.000000412, Best Loss: 0.000000344 in Epoch 2008
Epoch 2138
Epoch 2138, Loss: 0.000001048, Improvement: -0.000000094, Best Loss: 0.000000344 in Epoch 2008
Epoch 2139
Epoch 2139, Loss: 0.000003251, Improvement: 0.000002203, Best Loss: 0.000000344 in Epoch 2008
Epoch 2140
Epoch 2140, Loss: 0.000001593, Improvement: -0.000001658, Best Loss: 0.000000344 in Epoch 2008
Epoch 2141
Epoch 2141, Loss: 0.000001280, Improvement: -0.000000313, Best Loss: 0.000000344 in Epoch 2008
Epoch 2142
Epoch 2142, Loss: 0.000001091, Improvement: -0.000000189, Best Loss: 0.000000344 in Epoch 2008
Epoch 2143
Epoch 2143, Loss: 0.000000948, Improvement: -0.000000143, Best Loss: 0.000000344 in Epoch 2008
Epoch 2144
Epoch 2144, Loss: 0.000000945, Improvement: -0.000000003, Best Loss: 0.000000344 in Epoch 2008
Epoch 2145
Epoch 2145, Loss: 0.000002857, Improvement: 0.000001912, Best Loss: 0.000000344 in Epoch 2008
Epoch 2146
Epoch 2146, Loss: 0.000003120, Improvement: 0.000000263, Best Loss: 0.000000344 in Epoch 2008
Epoch 2147
Epoch 2147, Loss: 0.000007446, Improvement: 0.000004326, Best Loss: 0.000000344 in Epoch 2008
Epoch 2148
Epoch 2148, Loss: 0.000005667, Improvement: -0.000001779, Best Loss: 0.000000344 in Epoch 2008
Epoch 2149
Epoch 2149, Loss: 0.000004559, Improvement: -0.000001107, Best Loss: 0.000000344 in Epoch 2008
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.000003419, Improvement: -0.000001141, Best Loss: 0.000000344 in Epoch 2008
Epoch 2151
Epoch 2151, Loss: 0.000009527, Improvement: 0.000006109, Best Loss: 0.000000344 in Epoch 2008
Epoch 2152
Epoch 2152, Loss: 0.000009729, Improvement: 0.000000202, Best Loss: 0.000000344 in Epoch 2008
Epoch 2153
Epoch 2153, Loss: 0.000004621, Improvement: -0.000005109, Best Loss: 0.000000344 in Epoch 2008
Epoch 2154
Epoch 2154, Loss: 0.000002111, Improvement: -0.000002510, Best Loss: 0.000000344 in Epoch 2008
Epoch 2155
Epoch 2155, Loss: 0.000001785, Improvement: -0.000000326, Best Loss: 0.000000344 in Epoch 2008
Epoch 2156
Epoch 2156, Loss: 0.000004251, Improvement: 0.000002466, Best Loss: 0.000000344 in Epoch 2008
Epoch 2157
Epoch 2157, Loss: 0.000004370, Improvement: 0.000000119, Best Loss: 0.000000344 in Epoch 2008
Epoch 2158
Epoch 2158, Loss: 0.000004340, Improvement: -0.000000030, Best Loss: 0.000000344 in Epoch 2008
Epoch 2159
Epoch 2159, Loss: 0.000003884, Improvement: -0.000000456, Best Loss: 0.000000344 in Epoch 2008
Epoch 2160
Epoch 2160, Loss: 0.000007432, Improvement: 0.000003548, Best Loss: 0.000000344 in Epoch 2008
Epoch 2161
Epoch 2161, Loss: 0.000009815, Improvement: 0.000002383, Best Loss: 0.000000344 in Epoch 2008
Epoch 2162
Epoch 2162, Loss: 0.000009251, Improvement: -0.000000565, Best Loss: 0.000000344 in Epoch 2008
Epoch 2163
Epoch 2163, Loss: 0.000009044, Improvement: -0.000000207, Best Loss: 0.000000344 in Epoch 2008
Epoch 2164
Epoch 2164, Loss: 0.000009650, Improvement: 0.000000607, Best Loss: 0.000000344 in Epoch 2008
Epoch 2165
Epoch 2165, Loss: 0.000006937, Improvement: -0.000002713, Best Loss: 0.000000344 in Epoch 2008
Epoch 2166
Epoch 2166, Loss: 0.000007135, Improvement: 0.000000197, Best Loss: 0.000000344 in Epoch 2008
Epoch 2167
Epoch 2167, Loss: 0.000011756, Improvement: 0.000004622, Best Loss: 0.000000344 in Epoch 2008
Epoch 2168
Epoch 2168, Loss: 0.000018189, Improvement: 0.000006432, Best Loss: 0.000000344 in Epoch 2008
Epoch 2169
Epoch 2169, Loss: 0.000017383, Improvement: -0.000000806, Best Loss: 0.000000344 in Epoch 2008
Epoch 2170
Epoch 2170, Loss: 0.000009425, Improvement: -0.000007958, Best Loss: 0.000000344 in Epoch 2008
Epoch 2171
Epoch 2171, Loss: 0.000003372, Improvement: -0.000006052, Best Loss: 0.000000344 in Epoch 2008
Epoch 2172
Epoch 2172, Loss: 0.000002348, Improvement: -0.000001025, Best Loss: 0.000000344 in Epoch 2008
Epoch 2173
Epoch 2173, Loss: 0.000001721, Improvement: -0.000000627, Best Loss: 0.000000344 in Epoch 2008
Epoch 2174
Epoch 2174, Loss: 0.000001622, Improvement: -0.000000099, Best Loss: 0.000000344 in Epoch 2008
Epoch 2175
Epoch 2175, Loss: 0.000009059, Improvement: 0.000007437, Best Loss: 0.000000344 in Epoch 2008
Epoch 2176
Epoch 2176, Loss: 0.000007344, Improvement: -0.000001715, Best Loss: 0.000000344 in Epoch 2008
Epoch 2177
Epoch 2177, Loss: 0.000002925, Improvement: -0.000004419, Best Loss: 0.000000344 in Epoch 2008
Epoch 2178
Epoch 2178, Loss: 0.000001802, Improvement: -0.000001123, Best Loss: 0.000000344 in Epoch 2008
Epoch 2179
Epoch 2179, Loss: 0.000003280, Improvement: 0.000001478, Best Loss: 0.000000344 in Epoch 2008
Epoch 2180
Epoch 2180, Loss: 0.000003232, Improvement: -0.000000048, Best Loss: 0.000000344 in Epoch 2008
Epoch 2181
Epoch 2181, Loss: 0.000002807, Improvement: -0.000000425, Best Loss: 0.000000344 in Epoch 2008
Epoch 2182
Epoch 2182, Loss: 0.000004420, Improvement: 0.000001613, Best Loss: 0.000000344 in Epoch 2008
Epoch 2183
Epoch 2183, Loss: 0.000004165, Improvement: -0.000000255, Best Loss: 0.000000344 in Epoch 2008
Epoch 2184
Epoch 2184, Loss: 0.000003731, Improvement: -0.000000434, Best Loss: 0.000000344 in Epoch 2008
Epoch 2185
Epoch 2185, Loss: 0.000007321, Improvement: 0.000003590, Best Loss: 0.000000344 in Epoch 2008
Epoch 2186
Epoch 2186, Loss: 0.000009666, Improvement: 0.000002345, Best Loss: 0.000000344 in Epoch 2008
Epoch 2187
Epoch 2187, Loss: 0.000010388, Improvement: 0.000000722, Best Loss: 0.000000344 in Epoch 2008
Epoch 2188
Epoch 2188, Loss: 0.000007699, Improvement: -0.000002689, Best Loss: 0.000000344 in Epoch 2008
Epoch 2189
Epoch 2189, Loss: 0.000007110, Improvement: -0.000000589, Best Loss: 0.000000344 in Epoch 2008
Epoch 2190
Epoch 2190, Loss: 0.000004090, Improvement: -0.000003020, Best Loss: 0.000000344 in Epoch 2008
Epoch 2191
Epoch 2191, Loss: 0.000002419, Improvement: -0.000001671, Best Loss: 0.000000344 in Epoch 2008
Epoch 2192
Epoch 2192, Loss: 0.000004066, Improvement: 0.000001647, Best Loss: 0.000000344 in Epoch 2008
Epoch 2193
Epoch 2193, Loss: 0.000004137, Improvement: 0.000000071, Best Loss: 0.000000344 in Epoch 2008
Epoch 2194
Epoch 2194, Loss: 0.000004553, Improvement: 0.000000416, Best Loss: 0.000000344 in Epoch 2008
Epoch 2195
Epoch 2195, Loss: 0.000006268, Improvement: 0.000001716, Best Loss: 0.000000344 in Epoch 2008
Epoch 2196
Epoch 2196, Loss: 0.000004162, Improvement: -0.000002106, Best Loss: 0.000000344 in Epoch 2008
Epoch 2197
Epoch 2197, Loss: 0.000003720, Improvement: -0.000000442, Best Loss: 0.000000344 in Epoch 2008
Epoch 2198
Epoch 2198, Loss: 0.000003614, Improvement: -0.000000106, Best Loss: 0.000000344 in Epoch 2008
Epoch 2199
Epoch 2199, Loss: 0.000003369, Improvement: -0.000000245, Best Loss: 0.000000344 in Epoch 2008
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.000005371, Improvement: 0.000002002, Best Loss: 0.000000344 in Epoch 2008
Epoch 2201
Epoch 2201, Loss: 0.000006324, Improvement: 0.000000953, Best Loss: 0.000000344 in Epoch 2008
Epoch 2202
Epoch 2202, Loss: 0.000003441, Improvement: -0.000002883, Best Loss: 0.000000344 in Epoch 2008
Epoch 2203
Epoch 2203, Loss: 0.000008663, Improvement: 0.000005222, Best Loss: 0.000000344 in Epoch 2008
Epoch 2204
Epoch 2204, Loss: 0.000005861, Improvement: -0.000002802, Best Loss: 0.000000344 in Epoch 2008
Epoch 2205
Epoch 2205, Loss: 0.000008134, Improvement: 0.000002273, Best Loss: 0.000000344 in Epoch 2008
Epoch 2206
Epoch 2206, Loss: 0.000013163, Improvement: 0.000005029, Best Loss: 0.000000344 in Epoch 2008
Epoch 2207
Epoch 2207, Loss: 0.000014488, Improvement: 0.000001326, Best Loss: 0.000000344 in Epoch 2008
Epoch 2208
Epoch 2208, Loss: 0.000010617, Improvement: -0.000003871, Best Loss: 0.000000344 in Epoch 2008
Epoch 2209
Epoch 2209, Loss: 0.000006633, Improvement: -0.000003983, Best Loss: 0.000000344 in Epoch 2008
Epoch 2210
Epoch 2210, Loss: 0.000005683, Improvement: -0.000000950, Best Loss: 0.000000344 in Epoch 2008
Epoch 2211
Epoch 2211, Loss: 0.000003605, Improvement: -0.000002078, Best Loss: 0.000000344 in Epoch 2008
Epoch 2212
Epoch 2212, Loss: 0.000002531, Improvement: -0.000001074, Best Loss: 0.000000344 in Epoch 2008
Epoch 2213
Epoch 2213, Loss: 0.000002256, Improvement: -0.000000275, Best Loss: 0.000000344 in Epoch 2008
Epoch 2214
Epoch 2214, Loss: 0.000002272, Improvement: 0.000000016, Best Loss: 0.000000344 in Epoch 2008
Epoch 2215
Epoch 2215, Loss: 0.000003626, Improvement: 0.000001353, Best Loss: 0.000000344 in Epoch 2008
Epoch 2216
Epoch 2216, Loss: 0.000002243, Improvement: -0.000001383, Best Loss: 0.000000344 in Epoch 2008
Epoch 2217
Epoch 2217, Loss: 0.000001524, Improvement: -0.000000719, Best Loss: 0.000000344 in Epoch 2008
Epoch 2218
Epoch 2218, Loss: 0.000001663, Improvement: 0.000000138, Best Loss: 0.000000344 in Epoch 2008
Epoch 2219
Epoch 2219, Loss: 0.000002274, Improvement: 0.000000611, Best Loss: 0.000000344 in Epoch 2008
Epoch 2220
Epoch 2220, Loss: 0.000001756, Improvement: -0.000000518, Best Loss: 0.000000344 in Epoch 2008
Epoch 2221
Epoch 2221, Loss: 0.000002982, Improvement: 0.000001227, Best Loss: 0.000000344 in Epoch 2008
Epoch 2222
Epoch 2222, Loss: 0.000002884, Improvement: -0.000000098, Best Loss: 0.000000344 in Epoch 2008
Epoch 2223
Epoch 2223, Loss: 0.000002040, Improvement: -0.000000844, Best Loss: 0.000000344 in Epoch 2008
Epoch 2224
Epoch 2224, Loss: 0.000003651, Improvement: 0.000001611, Best Loss: 0.000000344 in Epoch 2008
Epoch 2225
Epoch 2225, Loss: 0.000007621, Improvement: 0.000003969, Best Loss: 0.000000344 in Epoch 2008
Epoch 2226
Epoch 2226, Loss: 0.000005646, Improvement: -0.000001975, Best Loss: 0.000000344 in Epoch 2008
Epoch 2227
Epoch 2227, Loss: 0.000005424, Improvement: -0.000000222, Best Loss: 0.000000344 in Epoch 2008
Epoch 2228
Epoch 2228, Loss: 0.000005277, Improvement: -0.000000147, Best Loss: 0.000000344 in Epoch 2008
Epoch 2229
Epoch 2229, Loss: 0.000004198, Improvement: -0.000001079, Best Loss: 0.000000344 in Epoch 2008
Epoch 2230
Epoch 2230, Loss: 0.000005239, Improvement: 0.000001041, Best Loss: 0.000000344 in Epoch 2008
Epoch 2231
Epoch 2231, Loss: 0.000004265, Improvement: -0.000000975, Best Loss: 0.000000344 in Epoch 2008
Epoch 2232
Epoch 2232, Loss: 0.000006085, Improvement: 0.000001820, Best Loss: 0.000000344 in Epoch 2008
Epoch 2233
Epoch 2233, Loss: 0.000010386, Improvement: 0.000004301, Best Loss: 0.000000344 in Epoch 2008
Epoch 2234
Epoch 2234, Loss: 0.000019303, Improvement: 0.000008918, Best Loss: 0.000000344 in Epoch 2008
Epoch 2235
Epoch 2235, Loss: 0.000025814, Improvement: 0.000006511, Best Loss: 0.000000344 in Epoch 2008
Epoch 2236
Epoch 2236, Loss: 0.000008662, Improvement: -0.000017152, Best Loss: 0.000000344 in Epoch 2008
Epoch 2237
Epoch 2237, Loss: 0.000003766, Improvement: -0.000004897, Best Loss: 0.000000344 in Epoch 2008
Epoch 2238
Epoch 2238, Loss: 0.000002534, Improvement: -0.000001232, Best Loss: 0.000000344 in Epoch 2008
Epoch 2239
Epoch 2239, Loss: 0.000001676, Improvement: -0.000000858, Best Loss: 0.000000344 in Epoch 2008
Epoch 2240
Epoch 2240, Loss: 0.000001269, Improvement: -0.000000407, Best Loss: 0.000000344 in Epoch 2008
Epoch 2241
Epoch 2241, Loss: 0.000001001, Improvement: -0.000000267, Best Loss: 0.000000344 in Epoch 2008
Epoch 2242
Epoch 2242, Loss: 0.000000900, Improvement: -0.000000102, Best Loss: 0.000000344 in Epoch 2008
Epoch 2243
Epoch 2243, Loss: 0.000000727, Improvement: -0.000000172, Best Loss: 0.000000344 in Epoch 2008
Epoch 2244
Epoch 2244, Loss: 0.000000614, Improvement: -0.000000113, Best Loss: 0.000000344 in Epoch 2008
Epoch 2245
Epoch 2245, Loss: 0.000000549, Improvement: -0.000000065, Best Loss: 0.000000344 in Epoch 2008
Epoch 2246
Epoch 2246, Loss: 0.000000518, Improvement: -0.000000031, Best Loss: 0.000000344 in Epoch 2008
Epoch 2247
Epoch 2247, Loss: 0.000000521, Improvement: 0.000000002, Best Loss: 0.000000344 in Epoch 2008
Epoch 2248
Epoch 2248, Loss: 0.000000585, Improvement: 0.000000064, Best Loss: 0.000000344 in Epoch 2008
Epoch 2249
Epoch 2249, Loss: 0.000000681, Improvement: 0.000000096, Best Loss: 0.000000344 in Epoch 2008
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.000000823, Improvement: 0.000000142, Best Loss: 0.000000344 in Epoch 2008
Epoch 2251
Epoch 2251, Loss: 0.000001117, Improvement: 0.000000294, Best Loss: 0.000000344 in Epoch 2008
Epoch 2252
Epoch 2252, Loss: 0.000002262, Improvement: 0.000001145, Best Loss: 0.000000344 in Epoch 2008
Epoch 2253
Epoch 2253, Loss: 0.000002118, Improvement: -0.000000143, Best Loss: 0.000000344 in Epoch 2008
Epoch 2254
Epoch 2254, Loss: 0.000001868, Improvement: -0.000000250, Best Loss: 0.000000344 in Epoch 2008
Epoch 2255
Epoch 2255, Loss: 0.000001910, Improvement: 0.000000042, Best Loss: 0.000000344 in Epoch 2008
Epoch 2256
Epoch 2256, Loss: 0.000001428, Improvement: -0.000000482, Best Loss: 0.000000344 in Epoch 2008
Epoch 2257
Epoch 2257, Loss: 0.000001393, Improvement: -0.000000036, Best Loss: 0.000000344 in Epoch 2008
Epoch 2258
Epoch 2258, Loss: 0.000002599, Improvement: 0.000001206, Best Loss: 0.000000344 in Epoch 2008
Epoch 2259
Epoch 2259, Loss: 0.000003290, Improvement: 0.000000692, Best Loss: 0.000000344 in Epoch 2008
Epoch 2260
Epoch 2260, Loss: 0.000003847, Improvement: 0.000000557, Best Loss: 0.000000344 in Epoch 2008
Epoch 2261
Epoch 2261, Loss: 0.000008518, Improvement: 0.000004671, Best Loss: 0.000000344 in Epoch 2008
Epoch 2262
Epoch 2262, Loss: 0.000010661, Improvement: 0.000002142, Best Loss: 0.000000344 in Epoch 2008
Epoch 2263
Epoch 2263, Loss: 0.000009963, Improvement: -0.000000698, Best Loss: 0.000000344 in Epoch 2008
Epoch 2264
Epoch 2264, Loss: 0.000005912, Improvement: -0.000004051, Best Loss: 0.000000344 in Epoch 2008
Epoch 2265
Epoch 2265, Loss: 0.000003663, Improvement: -0.000002249, Best Loss: 0.000000344 in Epoch 2008
Epoch 2266
Epoch 2266, Loss: 0.000001761, Improvement: -0.000001902, Best Loss: 0.000000344 in Epoch 2008
Epoch 2267
Epoch 2267, Loss: 0.000001425, Improvement: -0.000000335, Best Loss: 0.000000344 in Epoch 2008
Epoch 2268
Epoch 2268, Loss: 0.000001842, Improvement: 0.000000417, Best Loss: 0.000000344 in Epoch 2008
Epoch 2269
Epoch 2269, Loss: 0.000002299, Improvement: 0.000000457, Best Loss: 0.000000344 in Epoch 2008
Epoch 2270
Epoch 2270, Loss: 0.000001560, Improvement: -0.000000739, Best Loss: 0.000000344 in Epoch 2008
Epoch 2271
Epoch 2271, Loss: 0.000001550, Improvement: -0.000000010, Best Loss: 0.000000344 in Epoch 2008
Epoch 2272
Epoch 2272, Loss: 0.000002719, Improvement: 0.000001169, Best Loss: 0.000000344 in Epoch 2008
Epoch 2273
Epoch 2273, Loss: 0.000003491, Improvement: 0.000000772, Best Loss: 0.000000344 in Epoch 2008
Epoch 2274
Epoch 2274, Loss: 0.000003681, Improvement: 0.000000190, Best Loss: 0.000000344 in Epoch 2008
Epoch 2275
Epoch 2275, Loss: 0.000005499, Improvement: 0.000001818, Best Loss: 0.000000344 in Epoch 2008
Epoch 2276
Epoch 2276, Loss: 0.000004627, Improvement: -0.000000873, Best Loss: 0.000000344 in Epoch 2008
Epoch 2277
Epoch 2277, Loss: 0.000002668, Improvement: -0.000001958, Best Loss: 0.000000344 in Epoch 2008
Epoch 2278
Epoch 2278, Loss: 0.000004490, Improvement: 0.000001822, Best Loss: 0.000000344 in Epoch 2008
Epoch 2279
Epoch 2279, Loss: 0.000009778, Improvement: 0.000005288, Best Loss: 0.000000344 in Epoch 2008
Epoch 2280
Epoch 2280, Loss: 0.000011846, Improvement: 0.000002068, Best Loss: 0.000000344 in Epoch 2008
Epoch 2281
Epoch 2281, Loss: 0.000008118, Improvement: -0.000003728, Best Loss: 0.000000344 in Epoch 2008
Epoch 2282
Epoch 2282, Loss: 0.000010153, Improvement: 0.000002035, Best Loss: 0.000000344 in Epoch 2008
Epoch 2283
Epoch 2283, Loss: 0.000005591, Improvement: -0.000004563, Best Loss: 0.000000344 in Epoch 2008
Epoch 2284
Epoch 2284, Loss: 0.000002394, Improvement: -0.000003196, Best Loss: 0.000000344 in Epoch 2008
Epoch 2285
Epoch 2285, Loss: 0.000001638, Improvement: -0.000000757, Best Loss: 0.000000344 in Epoch 2008
Epoch 2286
Epoch 2286, Loss: 0.000001281, Improvement: -0.000000357, Best Loss: 0.000000344 in Epoch 2008
Epoch 2287
Epoch 2287, Loss: 0.000001383, Improvement: 0.000000102, Best Loss: 0.000000344 in Epoch 2008
Epoch 2288
Epoch 2288, Loss: 0.000001237, Improvement: -0.000000146, Best Loss: 0.000000344 in Epoch 2008
Epoch 2289
Epoch 2289, Loss: 0.000001292, Improvement: 0.000000055, Best Loss: 0.000000344 in Epoch 2008
Epoch 2290
Epoch 2290, Loss: 0.000001063, Improvement: -0.000000229, Best Loss: 0.000000344 in Epoch 2008
Epoch 2291
Epoch 2291, Loss: 0.000001916, Improvement: 0.000000853, Best Loss: 0.000000344 in Epoch 2008
Epoch 2292
Epoch 2292, Loss: 0.000011492, Improvement: 0.000009577, Best Loss: 0.000000344 in Epoch 2008
Epoch 2293
Epoch 2293, Loss: 0.000028275, Improvement: 0.000016782, Best Loss: 0.000000344 in Epoch 2008
Epoch 2294
Epoch 2294, Loss: 0.000007839, Improvement: -0.000020436, Best Loss: 0.000000344 in Epoch 2008
Epoch 2295
Epoch 2295, Loss: 0.000004050, Improvement: -0.000003788, Best Loss: 0.000000344 in Epoch 2008
Epoch 2296
Epoch 2296, Loss: 0.000001837, Improvement: -0.000002213, Best Loss: 0.000000344 in Epoch 2008
Epoch 2297
Epoch 2297, Loss: 0.000001278, Improvement: -0.000000559, Best Loss: 0.000000344 in Epoch 2008
Epoch 2298
Epoch 2298, Loss: 0.000001374, Improvement: 0.000000096, Best Loss: 0.000000344 in Epoch 2008
Epoch 2299
Epoch 2299, Loss: 0.000000773, Improvement: -0.000000600, Best Loss: 0.000000344 in Epoch 2008
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.000000648, Improvement: -0.000000125, Best Loss: 0.000000344 in Epoch 2008
Epoch 2301
Epoch 2301, Loss: 0.000000629, Improvement: -0.000000019, Best Loss: 0.000000344 in Epoch 2008
Epoch 2302
Epoch 2302, Loss: 0.000000632, Improvement: 0.000000003, Best Loss: 0.000000344 in Epoch 2008
Epoch 2303
Epoch 2303, Loss: 0.000000642, Improvement: 0.000000010, Best Loss: 0.000000344 in Epoch 2008
Epoch 2304
Epoch 2304, Loss: 0.000000551, Improvement: -0.000000091, Best Loss: 0.000000344 in Epoch 2008
Epoch 2305
Epoch 2305, Loss: 0.000000662, Improvement: 0.000000111, Best Loss: 0.000000344 in Epoch 2008
Epoch 2306
A best model at epoch 2306 has been saved with training error 0.000000328.
Epoch 2306, Loss: 0.000000503, Improvement: -0.000000159, Best Loss: 0.000000328 in Epoch 2306
Epoch 2307
Epoch 2307, Loss: 0.000000900, Improvement: 0.000000397, Best Loss: 0.000000328 in Epoch 2306
Epoch 2308
Epoch 2308, Loss: 0.000000841, Improvement: -0.000000059, Best Loss: 0.000000328 in Epoch 2306
Epoch 2309
Epoch 2309, Loss: 0.000001078, Improvement: 0.000000238, Best Loss: 0.000000328 in Epoch 2306
Epoch 2310
Epoch 2310, Loss: 0.000002765, Improvement: 0.000001687, Best Loss: 0.000000328 in Epoch 2306
Epoch 2311
Epoch 2311, Loss: 0.000008075, Improvement: 0.000005310, Best Loss: 0.000000328 in Epoch 2306
Epoch 2312
Epoch 2312, Loss: 0.000007767, Improvement: -0.000000309, Best Loss: 0.000000328 in Epoch 2306
Epoch 2313
Epoch 2313, Loss: 0.000007404, Improvement: -0.000000363, Best Loss: 0.000000328 in Epoch 2306
Epoch 2314
Epoch 2314, Loss: 0.000005924, Improvement: -0.000001479, Best Loss: 0.000000328 in Epoch 2306
Epoch 2315
Epoch 2315, Loss: 0.000004059, Improvement: -0.000001865, Best Loss: 0.000000328 in Epoch 2306
Epoch 2316
Epoch 2316, Loss: 0.000002457, Improvement: -0.000001602, Best Loss: 0.000000328 in Epoch 2306
Epoch 2317
Epoch 2317, Loss: 0.000007524, Improvement: 0.000005067, Best Loss: 0.000000328 in Epoch 2306
Epoch 2318
Epoch 2318, Loss: 0.000012897, Improvement: 0.000005373, Best Loss: 0.000000328 in Epoch 2306
Epoch 2319
Epoch 2319, Loss: 0.000005921, Improvement: -0.000006976, Best Loss: 0.000000328 in Epoch 2306
Epoch 2320
Epoch 2320, Loss: 0.000007897, Improvement: 0.000001975, Best Loss: 0.000000328 in Epoch 2306
Epoch 2321
Epoch 2321, Loss: 0.000005019, Improvement: -0.000002878, Best Loss: 0.000000328 in Epoch 2306
Epoch 2322
Epoch 2322, Loss: 0.000006588, Improvement: 0.000001569, Best Loss: 0.000000328 in Epoch 2306
Epoch 2323
Epoch 2323, Loss: 0.000005149, Improvement: -0.000001439, Best Loss: 0.000000328 in Epoch 2306
Epoch 2324
Epoch 2324, Loss: 0.000002307, Improvement: -0.000002842, Best Loss: 0.000000328 in Epoch 2306
Epoch 2325
Epoch 2325, Loss: 0.000001298, Improvement: -0.000001009, Best Loss: 0.000000328 in Epoch 2306
Epoch 2326
Epoch 2326, Loss: 0.000001722, Improvement: 0.000000425, Best Loss: 0.000000328 in Epoch 2306
Epoch 2327
Epoch 2327, Loss: 0.000004309, Improvement: 0.000002587, Best Loss: 0.000000328 in Epoch 2306
Epoch 2328
Epoch 2328, Loss: 0.000014772, Improvement: 0.000010463, Best Loss: 0.000000328 in Epoch 2306
Epoch 2329
Epoch 2329, Loss: 0.000004862, Improvement: -0.000009910, Best Loss: 0.000000328 in Epoch 2306
Epoch 2330
Epoch 2330, Loss: 0.000002066, Improvement: -0.000002796, Best Loss: 0.000000328 in Epoch 2306
Epoch 2331
Epoch 2331, Loss: 0.000002187, Improvement: 0.000000121, Best Loss: 0.000000328 in Epoch 2306
Epoch 2332
Epoch 2332, Loss: 0.000001527, Improvement: -0.000000660, Best Loss: 0.000000328 in Epoch 2306
Epoch 2333
Epoch 2333, Loss: 0.000000913, Improvement: -0.000000614, Best Loss: 0.000000328 in Epoch 2306
Epoch 2334
Epoch 2334, Loss: 0.000000870, Improvement: -0.000000043, Best Loss: 0.000000328 in Epoch 2306
Epoch 2335
Epoch 2335, Loss: 0.000001654, Improvement: 0.000000784, Best Loss: 0.000000328 in Epoch 2306
Epoch 2336
Epoch 2336, Loss: 0.000001240, Improvement: -0.000000415, Best Loss: 0.000000328 in Epoch 2306
Epoch 2337
Epoch 2337, Loss: 0.000001980, Improvement: 0.000000740, Best Loss: 0.000000328 in Epoch 2306
Epoch 2338
Epoch 2338, Loss: 0.000001621, Improvement: -0.000000359, Best Loss: 0.000000328 in Epoch 2306
Epoch 2339
Epoch 2339, Loss: 0.000001930, Improvement: 0.000000309, Best Loss: 0.000000328 in Epoch 2306
Epoch 2340
Epoch 2340, Loss: 0.000011038, Improvement: 0.000009108, Best Loss: 0.000000328 in Epoch 2306
Epoch 2341
Epoch 2341, Loss: 0.000008109, Improvement: -0.000002929, Best Loss: 0.000000328 in Epoch 2306
Epoch 2342
Epoch 2342, Loss: 0.000005442, Improvement: -0.000002666, Best Loss: 0.000000328 in Epoch 2306
Epoch 2343
Epoch 2343, Loss: 0.000003810, Improvement: -0.000001632, Best Loss: 0.000000328 in Epoch 2306
Epoch 2344
Epoch 2344, Loss: 0.000003622, Improvement: -0.000000189, Best Loss: 0.000000328 in Epoch 2306
Epoch 2345
Epoch 2345, Loss: 0.000002410, Improvement: -0.000001211, Best Loss: 0.000000328 in Epoch 2306
Epoch 2346
Epoch 2346, Loss: 0.000002713, Improvement: 0.000000303, Best Loss: 0.000000328 in Epoch 2306
Epoch 2347
Epoch 2347, Loss: 0.000007177, Improvement: 0.000004464, Best Loss: 0.000000328 in Epoch 2306
Epoch 2348
Epoch 2348, Loss: 0.000005026, Improvement: -0.000002151, Best Loss: 0.000000328 in Epoch 2306
Epoch 2349
Epoch 2349, Loss: 0.000003457, Improvement: -0.000001570, Best Loss: 0.000000328 in Epoch 2306
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.000002895, Improvement: -0.000000562, Best Loss: 0.000000328 in Epoch 2306
Epoch 2351
Epoch 2351, Loss: 0.000005724, Improvement: 0.000002828, Best Loss: 0.000000328 in Epoch 2306
Epoch 2352
Epoch 2352, Loss: 0.000013409, Improvement: 0.000007686, Best Loss: 0.000000328 in Epoch 2306
Epoch 2353
Epoch 2353, Loss: 0.000020282, Improvement: 0.000006872, Best Loss: 0.000000328 in Epoch 2306
Epoch 2354
Epoch 2354, Loss: 0.000009246, Improvement: -0.000011036, Best Loss: 0.000000328 in Epoch 2306
Epoch 2355
Epoch 2355, Loss: 0.000004896, Improvement: -0.000004349, Best Loss: 0.000000328 in Epoch 2306
Epoch 2356
Epoch 2356, Loss: 0.000002464, Improvement: -0.000002432, Best Loss: 0.000000328 in Epoch 2306
Epoch 2357
Epoch 2357, Loss: 0.000001108, Improvement: -0.000001356, Best Loss: 0.000000328 in Epoch 2306
Epoch 2358
Epoch 2358, Loss: 0.000001072, Improvement: -0.000000036, Best Loss: 0.000000328 in Epoch 2306
Epoch 2359
Epoch 2359, Loss: 0.000001871, Improvement: 0.000000799, Best Loss: 0.000000328 in Epoch 2306
Epoch 2360
Epoch 2360, Loss: 0.000001487, Improvement: -0.000000385, Best Loss: 0.000000328 in Epoch 2306
Epoch 2361
Epoch 2361, Loss: 0.000000840, Improvement: -0.000000646, Best Loss: 0.000000328 in Epoch 2306
Epoch 2362
Epoch 2362, Loss: 0.000000963, Improvement: 0.000000123, Best Loss: 0.000000328 in Epoch 2306
Epoch 2363
Epoch 2363, Loss: 0.000001188, Improvement: 0.000000225, Best Loss: 0.000000328 in Epoch 2306
Epoch 2364
Epoch 2364, Loss: 0.000001404, Improvement: 0.000000216, Best Loss: 0.000000328 in Epoch 2306
Epoch 2365
Epoch 2365, Loss: 0.000001156, Improvement: -0.000000248, Best Loss: 0.000000328 in Epoch 2306
Epoch 2366
Epoch 2366, Loss: 0.000002152, Improvement: 0.000000996, Best Loss: 0.000000328 in Epoch 2306
Epoch 2367
Epoch 2367, Loss: 0.000002705, Improvement: 0.000000553, Best Loss: 0.000000328 in Epoch 2306
Epoch 2368
Epoch 2368, Loss: 0.000002438, Improvement: -0.000000266, Best Loss: 0.000000328 in Epoch 2306
Epoch 2369
Epoch 2369, Loss: 0.000004447, Improvement: 0.000002008, Best Loss: 0.000000328 in Epoch 2306
Epoch 2370
Epoch 2370, Loss: 0.000002837, Improvement: -0.000001609, Best Loss: 0.000000328 in Epoch 2306
Epoch 2371
Epoch 2371, Loss: 0.000003250, Improvement: 0.000000413, Best Loss: 0.000000328 in Epoch 2306
Epoch 2372
Epoch 2372, Loss: 0.000002694, Improvement: -0.000000557, Best Loss: 0.000000328 in Epoch 2306
Epoch 2373
Epoch 2373, Loss: 0.000003302, Improvement: 0.000000608, Best Loss: 0.000000328 in Epoch 2306
Epoch 2374
Epoch 2374, Loss: 0.000004025, Improvement: 0.000000723, Best Loss: 0.000000328 in Epoch 2306
Epoch 2375
Epoch 2375, Loss: 0.000002143, Improvement: -0.000001882, Best Loss: 0.000000328 in Epoch 2306
Epoch 2376
Epoch 2376, Loss: 0.000001938, Improvement: -0.000000206, Best Loss: 0.000000328 in Epoch 2306
Epoch 2377
Epoch 2377, Loss: 0.000003659, Improvement: 0.000001722, Best Loss: 0.000000328 in Epoch 2306
Epoch 2378
Epoch 2378, Loss: 0.000009672, Improvement: 0.000006013, Best Loss: 0.000000328 in Epoch 2306
Epoch 2379
Epoch 2379, Loss: 0.000005351, Improvement: -0.000004321, Best Loss: 0.000000328 in Epoch 2306
Epoch 2380
Epoch 2380, Loss: 0.000007571, Improvement: 0.000002219, Best Loss: 0.000000328 in Epoch 2306
Epoch 2381
Epoch 2381, Loss: 0.000006895, Improvement: -0.000000676, Best Loss: 0.000000328 in Epoch 2306
Epoch 2382
Epoch 2382, Loss: 0.000008153, Improvement: 0.000001259, Best Loss: 0.000000328 in Epoch 2306
Epoch 2383
Epoch 2383, Loss: 0.000007265, Improvement: -0.000000888, Best Loss: 0.000000328 in Epoch 2306
Epoch 2384
Epoch 2384, Loss: 0.000004767, Improvement: -0.000002498, Best Loss: 0.000000328 in Epoch 2306
Epoch 2385
Epoch 2385, Loss: 0.000002879, Improvement: -0.000001888, Best Loss: 0.000000328 in Epoch 2306
Epoch 2386
Epoch 2386, Loss: 0.000002294, Improvement: -0.000000585, Best Loss: 0.000000328 in Epoch 2306
Epoch 2387
Epoch 2387, Loss: 0.000002277, Improvement: -0.000000017, Best Loss: 0.000000328 in Epoch 2306
Epoch 2388
Epoch 2388, Loss: 0.000002627, Improvement: 0.000000349, Best Loss: 0.000000328 in Epoch 2306
Epoch 2389
Epoch 2389, Loss: 0.000007264, Improvement: 0.000004637, Best Loss: 0.000000328 in Epoch 2306
Epoch 2390
Epoch 2390, Loss: 0.000005883, Improvement: -0.000001380, Best Loss: 0.000000328 in Epoch 2306
Epoch 2391
Epoch 2391, Loss: 0.000003741, Improvement: -0.000002143, Best Loss: 0.000000328 in Epoch 2306
Epoch 2392
Epoch 2392, Loss: 0.000002622, Improvement: -0.000001119, Best Loss: 0.000000328 in Epoch 2306
Epoch 2393
Epoch 2393, Loss: 0.000001765, Improvement: -0.000000857, Best Loss: 0.000000328 in Epoch 2306
Epoch 2394
Epoch 2394, Loss: 0.000002435, Improvement: 0.000000670, Best Loss: 0.000000328 in Epoch 2306
Epoch 2395
Epoch 2395, Loss: 0.000001598, Improvement: -0.000000837, Best Loss: 0.000000328 in Epoch 2306
Epoch 2396
Epoch 2396, Loss: 0.000001614, Improvement: 0.000000016, Best Loss: 0.000000328 in Epoch 2306
Epoch 2397
Epoch 2397, Loss: 0.000002390, Improvement: 0.000000776, Best Loss: 0.000000328 in Epoch 2306
Epoch 2398
Epoch 2398, Loss: 0.000005205, Improvement: 0.000002815, Best Loss: 0.000000328 in Epoch 2306
Epoch 2399
Epoch 2399, Loss: 0.000009723, Improvement: 0.000004518, Best Loss: 0.000000328 in Epoch 2306
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.000010425, Improvement: 0.000000702, Best Loss: 0.000000328 in Epoch 2306
Epoch 2401
Epoch 2401, Loss: 0.000006341, Improvement: -0.000004083, Best Loss: 0.000000328 in Epoch 2306
Epoch 2402
Epoch 2402, Loss: 0.000004936, Improvement: -0.000001406, Best Loss: 0.000000328 in Epoch 2306
Epoch 2403
Epoch 2403, Loss: 0.000008074, Improvement: 0.000003139, Best Loss: 0.000000328 in Epoch 2306
Epoch 2404
Epoch 2404, Loss: 0.000006024, Improvement: -0.000002050, Best Loss: 0.000000328 in Epoch 2306
Epoch 2405
Epoch 2405, Loss: 0.000006537, Improvement: 0.000000512, Best Loss: 0.000000328 in Epoch 2306
Epoch 2406
Epoch 2406, Loss: 0.000007998, Improvement: 0.000001461, Best Loss: 0.000000328 in Epoch 2306
Epoch 2407
Epoch 2407, Loss: 0.000003911, Improvement: -0.000004087, Best Loss: 0.000000328 in Epoch 2306
Epoch 2408
Epoch 2408, Loss: 0.000002266, Improvement: -0.000001645, Best Loss: 0.000000328 in Epoch 2306
Epoch 2409
Epoch 2409, Loss: 0.000003551, Improvement: 0.000001284, Best Loss: 0.000000328 in Epoch 2306
Epoch 2410
Epoch 2410, Loss: 0.000004013, Improvement: 0.000000462, Best Loss: 0.000000328 in Epoch 2306
Epoch 2411
Epoch 2411, Loss: 0.000004856, Improvement: 0.000000843, Best Loss: 0.000000328 in Epoch 2306
Epoch 2412
Epoch 2412, Loss: 0.000004882, Improvement: 0.000000027, Best Loss: 0.000000328 in Epoch 2306
Epoch 2413
Epoch 2413, Loss: 0.000002697, Improvement: -0.000002185, Best Loss: 0.000000328 in Epoch 2306
Epoch 2414
Epoch 2414, Loss: 0.000004921, Improvement: 0.000002224, Best Loss: 0.000000328 in Epoch 2306
Epoch 2415
Epoch 2415, Loss: 0.000003194, Improvement: -0.000001727, Best Loss: 0.000000328 in Epoch 2306
Epoch 2416
Epoch 2416, Loss: 0.000003053, Improvement: -0.000000141, Best Loss: 0.000000328 in Epoch 2306
Epoch 2417
Epoch 2417, Loss: 0.000006275, Improvement: 0.000003222, Best Loss: 0.000000328 in Epoch 2306
Epoch 2418
Epoch 2418, Loss: 0.000004591, Improvement: -0.000001684, Best Loss: 0.000000328 in Epoch 2306
Epoch 2419
Epoch 2419, Loss: 0.000003937, Improvement: -0.000000654, Best Loss: 0.000000328 in Epoch 2306
Epoch 2420
Epoch 2420, Loss: 0.000008123, Improvement: 0.000004186, Best Loss: 0.000000328 in Epoch 2306
Epoch 2421
Epoch 2421, Loss: 0.000014915, Improvement: 0.000006791, Best Loss: 0.000000328 in Epoch 2306
Epoch 2422
Epoch 2422, Loss: 0.000016546, Improvement: 0.000001631, Best Loss: 0.000000328 in Epoch 2306
Epoch 2423
Epoch 2423, Loss: 0.000012231, Improvement: -0.000004315, Best Loss: 0.000000328 in Epoch 2306
Epoch 2424
Epoch 2424, Loss: 0.000003935, Improvement: -0.000008296, Best Loss: 0.000000328 in Epoch 2306
Epoch 2425
Epoch 2425, Loss: 0.000002578, Improvement: -0.000001356, Best Loss: 0.000000328 in Epoch 2306
Epoch 2426
Epoch 2426, Loss: 0.000001487, Improvement: -0.000001091, Best Loss: 0.000000328 in Epoch 2306
Epoch 2427
Epoch 2427, Loss: 0.000000785, Improvement: -0.000000702, Best Loss: 0.000000328 in Epoch 2306
Epoch 2428
Epoch 2428, Loss: 0.000000524, Improvement: -0.000000262, Best Loss: 0.000000328 in Epoch 2306
Epoch 2429
A best model at epoch 2429 has been saved with training error 0.000000327.
Epoch 2429, Loss: 0.000000587, Improvement: 0.000000063, Best Loss: 0.000000327 in Epoch 2429
Epoch 2430
Epoch 2430, Loss: 0.000000896, Improvement: 0.000000310, Best Loss: 0.000000327 in Epoch 2429
Epoch 2431
Epoch 2431, Loss: 0.000001427, Improvement: 0.000000531, Best Loss: 0.000000327 in Epoch 2429
Epoch 2432
Epoch 2432, Loss: 0.000002711, Improvement: 0.000001284, Best Loss: 0.000000327 in Epoch 2429
Epoch 2433
Epoch 2433, Loss: 0.000003441, Improvement: 0.000000730, Best Loss: 0.000000327 in Epoch 2429
Epoch 2434
Epoch 2434, Loss: 0.000002819, Improvement: -0.000000622, Best Loss: 0.000000327 in Epoch 2429
Epoch 2435
Epoch 2435, Loss: 0.000001367, Improvement: -0.000001451, Best Loss: 0.000000327 in Epoch 2429
Epoch 2436
Epoch 2436, Loss: 0.000001398, Improvement: 0.000000030, Best Loss: 0.000000327 in Epoch 2429
Epoch 2437
Epoch 2437, Loss: 0.000001141, Improvement: -0.000000257, Best Loss: 0.000000327 in Epoch 2429
Epoch 2438
Epoch 2438, Loss: 0.000001143, Improvement: 0.000000002, Best Loss: 0.000000327 in Epoch 2429
Epoch 2439
Epoch 2439, Loss: 0.000002818, Improvement: 0.000001675, Best Loss: 0.000000327 in Epoch 2429
Epoch 2440
Epoch 2440, Loss: 0.000001086, Improvement: -0.000001733, Best Loss: 0.000000327 in Epoch 2429
Epoch 2441
Epoch 2441, Loss: 0.000000990, Improvement: -0.000000096, Best Loss: 0.000000327 in Epoch 2429
Epoch 2442
Epoch 2442, Loss: 0.000001080, Improvement: 0.000000090, Best Loss: 0.000000327 in Epoch 2429
Epoch 2443
Epoch 2443, Loss: 0.000001331, Improvement: 0.000000252, Best Loss: 0.000000327 in Epoch 2429
Epoch 2444
Epoch 2444, Loss: 0.000000921, Improvement: -0.000000410, Best Loss: 0.000000327 in Epoch 2429
Epoch 2445
Epoch 2445, Loss: 0.000002034, Improvement: 0.000001112, Best Loss: 0.000000327 in Epoch 2429
Epoch 2446
Epoch 2446, Loss: 0.000002470, Improvement: 0.000000437, Best Loss: 0.000000327 in Epoch 2429
Epoch 2447
Epoch 2447, Loss: 0.000002346, Improvement: -0.000000125, Best Loss: 0.000000327 in Epoch 2429
Epoch 2448
Epoch 2448, Loss: 0.000002946, Improvement: 0.000000601, Best Loss: 0.000000327 in Epoch 2429
Epoch 2449
Epoch 2449, Loss: 0.000002342, Improvement: -0.000000604, Best Loss: 0.000000327 in Epoch 2429
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.000006641, Improvement: 0.000004299, Best Loss: 0.000000327 in Epoch 2429
Epoch 2451
Epoch 2451, Loss: 0.000023476, Improvement: 0.000016835, Best Loss: 0.000000327 in Epoch 2429
Epoch 2452
Epoch 2452, Loss: 0.000021545, Improvement: -0.000001931, Best Loss: 0.000000327 in Epoch 2429
Epoch 2453
Epoch 2453, Loss: 0.000009967, Improvement: -0.000011578, Best Loss: 0.000000327 in Epoch 2429
Epoch 2454
Epoch 2454, Loss: 0.000006268, Improvement: -0.000003699, Best Loss: 0.000000327 in Epoch 2429
Epoch 2455
Epoch 2455, Loss: 0.000003425, Improvement: -0.000002843, Best Loss: 0.000000327 in Epoch 2429
Epoch 2456
Epoch 2456, Loss: 0.000001915, Improvement: -0.000001509, Best Loss: 0.000000327 in Epoch 2429
Epoch 2457
Epoch 2457, Loss: 0.000000844, Improvement: -0.000001071, Best Loss: 0.000000327 in Epoch 2429
Epoch 2458
Epoch 2458, Loss: 0.000000747, Improvement: -0.000000097, Best Loss: 0.000000327 in Epoch 2429
Epoch 2459
Epoch 2459, Loss: 0.000000632, Improvement: -0.000000115, Best Loss: 0.000000327 in Epoch 2429
Epoch 2460
Epoch 2460, Loss: 0.000000543, Improvement: -0.000000089, Best Loss: 0.000000327 in Epoch 2429
Epoch 2461
Epoch 2461, Loss: 0.000000495, Improvement: -0.000000048, Best Loss: 0.000000327 in Epoch 2429
Epoch 2462
A best model at epoch 2462 has been saved with training error 0.000000307.
Epoch 2462, Loss: 0.000000446, Improvement: -0.000000049, Best Loss: 0.000000307 in Epoch 2462
Epoch 2463
Epoch 2463, Loss: 0.000000453, Improvement: 0.000000006, Best Loss: 0.000000307 in Epoch 2462
Epoch 2464
Epoch 2464, Loss: 0.000000460, Improvement: 0.000000007, Best Loss: 0.000000307 in Epoch 2462
Epoch 2465
Epoch 2465, Loss: 0.000000480, Improvement: 0.000000020, Best Loss: 0.000000307 in Epoch 2462
Epoch 2466
Epoch 2466, Loss: 0.000000492, Improvement: 0.000000012, Best Loss: 0.000000307 in Epoch 2462
Epoch 2467
Epoch 2467, Loss: 0.000000541, Improvement: 0.000000049, Best Loss: 0.000000307 in Epoch 2462
Epoch 2468
Epoch 2468, Loss: 0.000000479, Improvement: -0.000000062, Best Loss: 0.000000307 in Epoch 2462
Epoch 2469
Epoch 2469, Loss: 0.000000444, Improvement: -0.000000035, Best Loss: 0.000000307 in Epoch 2462
Epoch 2470
Epoch 2470, Loss: 0.000000587, Improvement: 0.000000142, Best Loss: 0.000000307 in Epoch 2462
Epoch 2471
Epoch 2471, Loss: 0.000001036, Improvement: 0.000000449, Best Loss: 0.000000307 in Epoch 2462
Epoch 2472
Epoch 2472, Loss: 0.000001271, Improvement: 0.000000235, Best Loss: 0.000000307 in Epoch 2462
Epoch 2473
Epoch 2473, Loss: 0.000001025, Improvement: -0.000000246, Best Loss: 0.000000307 in Epoch 2462
Epoch 2474
Epoch 2474, Loss: 0.000000928, Improvement: -0.000000097, Best Loss: 0.000000307 in Epoch 2462
Epoch 2475
Epoch 2475, Loss: 0.000001010, Improvement: 0.000000082, Best Loss: 0.000000307 in Epoch 2462
Epoch 2476
Epoch 2476, Loss: 0.000001198, Improvement: 0.000000188, Best Loss: 0.000000307 in Epoch 2462
Epoch 2477
Epoch 2477, Loss: 0.000000855, Improvement: -0.000000343, Best Loss: 0.000000307 in Epoch 2462
Epoch 2478
Epoch 2478, Loss: 0.000000766, Improvement: -0.000000089, Best Loss: 0.000000307 in Epoch 2462
Epoch 2479
Epoch 2479, Loss: 0.000000917, Improvement: 0.000000152, Best Loss: 0.000000307 in Epoch 2462
Epoch 2480
Epoch 2480, Loss: 0.000001148, Improvement: 0.000000230, Best Loss: 0.000000307 in Epoch 2462
Epoch 2481
Epoch 2481, Loss: 0.000001744, Improvement: 0.000000596, Best Loss: 0.000000307 in Epoch 2462
Epoch 2482
Epoch 2482, Loss: 0.000003963, Improvement: 0.000002219, Best Loss: 0.000000307 in Epoch 2462
Epoch 2483
Epoch 2483, Loss: 0.000005494, Improvement: 0.000001532, Best Loss: 0.000000307 in Epoch 2462
Epoch 2484
Epoch 2484, Loss: 0.000008990, Improvement: 0.000003495, Best Loss: 0.000000307 in Epoch 2462
Epoch 2485
Epoch 2485, Loss: 0.000010478, Improvement: 0.000001489, Best Loss: 0.000000307 in Epoch 2462
Epoch 2486
Epoch 2486, Loss: 0.000005374, Improvement: -0.000005104, Best Loss: 0.000000307 in Epoch 2462
Epoch 2487
Epoch 2487, Loss: 0.000007213, Improvement: 0.000001839, Best Loss: 0.000000307 in Epoch 2462
Epoch 2488
Epoch 2488, Loss: 0.000006624, Improvement: -0.000000589, Best Loss: 0.000000307 in Epoch 2462
Epoch 2489
Epoch 2489, Loss: 0.000007128, Improvement: 0.000000504, Best Loss: 0.000000307 in Epoch 2462
Epoch 2490
Epoch 2490, Loss: 0.000007101, Improvement: -0.000000028, Best Loss: 0.000000307 in Epoch 2462
Epoch 2491
Epoch 2491, Loss: 0.000008568, Improvement: 0.000001467, Best Loss: 0.000000307 in Epoch 2462
Epoch 2492
Epoch 2492, Loss: 0.000010097, Improvement: 0.000001529, Best Loss: 0.000000307 in Epoch 2462
Epoch 2493
Epoch 2493, Loss: 0.000006521, Improvement: -0.000003576, Best Loss: 0.000000307 in Epoch 2462
Epoch 2494
Epoch 2494, Loss: 0.000008037, Improvement: 0.000001516, Best Loss: 0.000000307 in Epoch 2462
Epoch 2495
Epoch 2495, Loss: 0.000005096, Improvement: -0.000002941, Best Loss: 0.000000307 in Epoch 2462
Epoch 2496
Epoch 2496, Loss: 0.000007791, Improvement: 0.000002695, Best Loss: 0.000000307 in Epoch 2462
Epoch 2497
Epoch 2497, Loss: 0.000009030, Improvement: 0.000001239, Best Loss: 0.000000307 in Epoch 2462
Epoch 2498
Epoch 2498, Loss: 0.000003519, Improvement: -0.000005511, Best Loss: 0.000000307 in Epoch 2462
Epoch 2499
Epoch 2499, Loss: 0.000016791, Improvement: 0.000013272, Best Loss: 0.000000307 in Epoch 2462
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.000024098, Improvement: 0.000007307, Best Loss: 0.000000307 in Epoch 2462
Epoch 2501
Epoch 2501, Loss: 0.000013676, Improvement: -0.000010423, Best Loss: 0.000000307 in Epoch 2462
Epoch 2502
Epoch 2502, Loss: 0.000006696, Improvement: -0.000006980, Best Loss: 0.000000307 in Epoch 2462
Epoch 2503
Epoch 2503, Loss: 0.000003767, Improvement: -0.000002929, Best Loss: 0.000000307 in Epoch 2462
Epoch 2504
Epoch 2504, Loss: 0.000002154, Improvement: -0.000001612, Best Loss: 0.000000307 in Epoch 2462
Epoch 2505
Epoch 2505, Loss: 0.000003344, Improvement: 0.000001190, Best Loss: 0.000000307 in Epoch 2462
Epoch 2506
Epoch 2506, Loss: 0.000001653, Improvement: -0.000001691, Best Loss: 0.000000307 in Epoch 2462
Epoch 2507
Epoch 2507, Loss: 0.000000948, Improvement: -0.000000705, Best Loss: 0.000000307 in Epoch 2462
Epoch 2508
Epoch 2508, Loss: 0.000000992, Improvement: 0.000000044, Best Loss: 0.000000307 in Epoch 2462
Epoch 2509
Epoch 2509, Loss: 0.000000681, Improvement: -0.000000310, Best Loss: 0.000000307 in Epoch 2462
Epoch 2510
Epoch 2510, Loss: 0.000000519, Improvement: -0.000000163, Best Loss: 0.000000307 in Epoch 2462
Epoch 2511
Epoch 2511, Loss: 0.000000859, Improvement: 0.000000340, Best Loss: 0.000000307 in Epoch 2462
Epoch 2512
Epoch 2512, Loss: 0.000001526, Improvement: 0.000000668, Best Loss: 0.000000307 in Epoch 2462
Epoch 2513
Epoch 2513, Loss: 0.000001113, Improvement: -0.000000413, Best Loss: 0.000000307 in Epoch 2462
Epoch 2514
Epoch 2514, Loss: 0.000000751, Improvement: -0.000000362, Best Loss: 0.000000307 in Epoch 2462
Epoch 2515
Epoch 2515, Loss: 0.000001918, Improvement: 0.000001167, Best Loss: 0.000000307 in Epoch 2462
Epoch 2516
Epoch 2516, Loss: 0.000004501, Improvement: 0.000002583, Best Loss: 0.000000307 in Epoch 2462
Epoch 2517
Epoch 2517, Loss: 0.000003901, Improvement: -0.000000599, Best Loss: 0.000000307 in Epoch 2462
Epoch 2518
Epoch 2518, Loss: 0.000002136, Improvement: -0.000001766, Best Loss: 0.000000307 in Epoch 2462
Epoch 2519
Epoch 2519, Loss: 0.000002190, Improvement: 0.000000055, Best Loss: 0.000000307 in Epoch 2462
Epoch 2520
Epoch 2520, Loss: 0.000002049, Improvement: -0.000000142, Best Loss: 0.000000307 in Epoch 2462
Epoch 2521
Epoch 2521, Loss: 0.000003777, Improvement: 0.000001728, Best Loss: 0.000000307 in Epoch 2462
Epoch 2522
Epoch 2522, Loss: 0.000002079, Improvement: -0.000001698, Best Loss: 0.000000307 in Epoch 2462
Epoch 2523
Epoch 2523, Loss: 0.000001025, Improvement: -0.000001053, Best Loss: 0.000000307 in Epoch 2462
Epoch 2524
Epoch 2524, Loss: 0.000001186, Improvement: 0.000000160, Best Loss: 0.000000307 in Epoch 2462
Epoch 2525
Epoch 2525, Loss: 0.000001265, Improvement: 0.000000079, Best Loss: 0.000000307 in Epoch 2462
Epoch 2526
Epoch 2526, Loss: 0.000001242, Improvement: -0.000000022, Best Loss: 0.000000307 in Epoch 2462
Epoch 2527
Epoch 2527, Loss: 0.000001905, Improvement: 0.000000663, Best Loss: 0.000000307 in Epoch 2462
Epoch 2528
Epoch 2528, Loss: 0.000003185, Improvement: 0.000001280, Best Loss: 0.000000307 in Epoch 2462
Epoch 2529
Epoch 2529, Loss: 0.000003839, Improvement: 0.000000653, Best Loss: 0.000000307 in Epoch 2462
Epoch 2530
Epoch 2530, Loss: 0.000004095, Improvement: 0.000000257, Best Loss: 0.000000307 in Epoch 2462
Epoch 2531
Epoch 2531, Loss: 0.000004754, Improvement: 0.000000658, Best Loss: 0.000000307 in Epoch 2462
Epoch 2532
Epoch 2532, Loss: 0.000005226, Improvement: 0.000000472, Best Loss: 0.000000307 in Epoch 2462
Epoch 2533
Epoch 2533, Loss: 0.000013380, Improvement: 0.000008154, Best Loss: 0.000000307 in Epoch 2462
Epoch 2534
Epoch 2534, Loss: 0.000021583, Improvement: 0.000008203, Best Loss: 0.000000307 in Epoch 2462
Epoch 2535
Epoch 2535, Loss: 0.000011106, Improvement: -0.000010477, Best Loss: 0.000000307 in Epoch 2462
Epoch 2536
Epoch 2536, Loss: 0.000004002, Improvement: -0.000007104, Best Loss: 0.000000307 in Epoch 2462
Epoch 2537
Epoch 2537, Loss: 0.000002299, Improvement: -0.000001703, Best Loss: 0.000000307 in Epoch 2462
Epoch 2538
Epoch 2538, Loss: 0.000001501, Improvement: -0.000000798, Best Loss: 0.000000307 in Epoch 2462
Epoch 2539
Epoch 2539, Loss: 0.000001128, Improvement: -0.000000373, Best Loss: 0.000000307 in Epoch 2462
Epoch 2540
Epoch 2540, Loss: 0.000000698, Improvement: -0.000000429, Best Loss: 0.000000307 in Epoch 2462
Epoch 2541
Epoch 2541, Loss: 0.000000507, Improvement: -0.000000192, Best Loss: 0.000000307 in Epoch 2462
Epoch 2542
Epoch 2542, Loss: 0.000000628, Improvement: 0.000000121, Best Loss: 0.000000307 in Epoch 2462
Epoch 2543
Epoch 2543, Loss: 0.000000458, Improvement: -0.000000170, Best Loss: 0.000000307 in Epoch 2462
Epoch 2544
A best model at epoch 2544 has been saved with training error 0.000000306.
Epoch 2544, Loss: 0.000000490, Improvement: 0.000000032, Best Loss: 0.000000306 in Epoch 2544
Epoch 2545
Epoch 2545, Loss: 0.000000470, Improvement: -0.000000020, Best Loss: 0.000000306 in Epoch 2544
Epoch 2546
A best model at epoch 2546 has been saved with training error 0.000000280.
Epoch 2546, Loss: 0.000000542, Improvement: 0.000000071, Best Loss: 0.000000280 in Epoch 2546
Epoch 2547
Epoch 2547, Loss: 0.000000631, Improvement: 0.000000090, Best Loss: 0.000000280 in Epoch 2546
Epoch 2548
Epoch 2548, Loss: 0.000000604, Improvement: -0.000000027, Best Loss: 0.000000280 in Epoch 2546
Epoch 2549
Epoch 2549, Loss: 0.000000500, Improvement: -0.000000104, Best Loss: 0.000000280 in Epoch 2546
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.000000710, Improvement: 0.000000210, Best Loss: 0.000000280 in Epoch 2546
Epoch 2551
Epoch 2551, Loss: 0.000000540, Improvement: -0.000000170, Best Loss: 0.000000280 in Epoch 2546
Epoch 2552
Epoch 2552, Loss: 0.000000575, Improvement: 0.000000035, Best Loss: 0.000000280 in Epoch 2546
Epoch 2553
Epoch 2553, Loss: 0.000000709, Improvement: 0.000000134, Best Loss: 0.000000280 in Epoch 2546
Epoch 2554
Epoch 2554, Loss: 0.000001437, Improvement: 0.000000728, Best Loss: 0.000000280 in Epoch 2546
Epoch 2555
Epoch 2555, Loss: 0.000001464, Improvement: 0.000000027, Best Loss: 0.000000280 in Epoch 2546
Epoch 2556
Epoch 2556, Loss: 0.000001806, Improvement: 0.000000342, Best Loss: 0.000000280 in Epoch 2546
Epoch 2557
Epoch 2557, Loss: 0.000002270, Improvement: 0.000000464, Best Loss: 0.000000280 in Epoch 2546
Epoch 2558
Epoch 2558, Loss: 0.000002955, Improvement: 0.000000685, Best Loss: 0.000000280 in Epoch 2546
Epoch 2559
Epoch 2559, Loss: 0.000003351, Improvement: 0.000000396, Best Loss: 0.000000280 in Epoch 2546
Epoch 2560
Epoch 2560, Loss: 0.000002958, Improvement: -0.000000392, Best Loss: 0.000000280 in Epoch 2546
Epoch 2561
Epoch 2561, Loss: 0.000006248, Improvement: 0.000003290, Best Loss: 0.000000280 in Epoch 2546
Epoch 2562
Epoch 2562, Loss: 0.000006969, Improvement: 0.000000721, Best Loss: 0.000000280 in Epoch 2546
Epoch 2563
Epoch 2563, Loss: 0.000005023, Improvement: -0.000001947, Best Loss: 0.000000280 in Epoch 2546
Epoch 2564
Epoch 2564, Loss: 0.000002347, Improvement: -0.000002675, Best Loss: 0.000000280 in Epoch 2546
Epoch 2565
Epoch 2565, Loss: 0.000005557, Improvement: 0.000003209, Best Loss: 0.000000280 in Epoch 2546
Epoch 2566
Epoch 2566, Loss: 0.000010163, Improvement: 0.000004606, Best Loss: 0.000000280 in Epoch 2546
Epoch 2567
Epoch 2567, Loss: 0.000004992, Improvement: -0.000005170, Best Loss: 0.000000280 in Epoch 2546
Epoch 2568
Epoch 2568, Loss: 0.000004056, Improvement: -0.000000936, Best Loss: 0.000000280 in Epoch 2546
Epoch 2569
Epoch 2569, Loss: 0.000005562, Improvement: 0.000001505, Best Loss: 0.000000280 in Epoch 2546
Epoch 2570
Epoch 2570, Loss: 0.000008219, Improvement: 0.000002657, Best Loss: 0.000000280 in Epoch 2546
Epoch 2571
Epoch 2571, Loss: 0.000012872, Improvement: 0.000004653, Best Loss: 0.000000280 in Epoch 2546
Epoch 2572
Epoch 2572, Loss: 0.000009045, Improvement: -0.000003828, Best Loss: 0.000000280 in Epoch 2546
Epoch 2573
Epoch 2573, Loss: 0.000022660, Improvement: 0.000013616, Best Loss: 0.000000280 in Epoch 2546
Epoch 2574
Epoch 2574, Loss: 0.000011266, Improvement: -0.000011394, Best Loss: 0.000000280 in Epoch 2546
Epoch 2575
Epoch 2575, Loss: 0.000003569, Improvement: -0.000007697, Best Loss: 0.000000280 in Epoch 2546
Epoch 2576
Epoch 2576, Loss: 0.000001740, Improvement: -0.000001830, Best Loss: 0.000000280 in Epoch 2546
Epoch 2577
Epoch 2577, Loss: 0.000000961, Improvement: -0.000000779, Best Loss: 0.000000280 in Epoch 2546
Epoch 2578
Epoch 2578, Loss: 0.000001260, Improvement: 0.000000299, Best Loss: 0.000000280 in Epoch 2546
Epoch 2579
Epoch 2579, Loss: 0.000001605, Improvement: 0.000000345, Best Loss: 0.000000280 in Epoch 2546
Epoch 2580
Epoch 2580, Loss: 0.000001113, Improvement: -0.000000492, Best Loss: 0.000000280 in Epoch 2546
Epoch 2581
Epoch 2581, Loss: 0.000000764, Improvement: -0.000000350, Best Loss: 0.000000280 in Epoch 2546
Epoch 2582
Epoch 2582, Loss: 0.000000565, Improvement: -0.000000199, Best Loss: 0.000000280 in Epoch 2546
Epoch 2583
Epoch 2583, Loss: 0.000000490, Improvement: -0.000000075, Best Loss: 0.000000280 in Epoch 2546
Epoch 2584
Epoch 2584, Loss: 0.000000463, Improvement: -0.000000027, Best Loss: 0.000000280 in Epoch 2546
Epoch 2585
Epoch 2585, Loss: 0.000000571, Improvement: 0.000000108, Best Loss: 0.000000280 in Epoch 2546
Epoch 2586
Epoch 2586, Loss: 0.000000558, Improvement: -0.000000013, Best Loss: 0.000000280 in Epoch 2546
Epoch 2587
Epoch 2587, Loss: 0.000001144, Improvement: 0.000000586, Best Loss: 0.000000280 in Epoch 2546
Epoch 2588
Epoch 2588, Loss: 0.000001922, Improvement: 0.000000778, Best Loss: 0.000000280 in Epoch 2546
Epoch 2589
Epoch 2589, Loss: 0.000001473, Improvement: -0.000000449, Best Loss: 0.000000280 in Epoch 2546
Epoch 2590
Epoch 2590, Loss: 0.000001503, Improvement: 0.000000030, Best Loss: 0.000000280 in Epoch 2546
Epoch 2591
Epoch 2591, Loss: 0.000000961, Improvement: -0.000000542, Best Loss: 0.000000280 in Epoch 2546
Epoch 2592
Epoch 2592, Loss: 0.000000710, Improvement: -0.000000251, Best Loss: 0.000000280 in Epoch 2546
Epoch 2593
Epoch 2593, Loss: 0.000000649, Improvement: -0.000000061, Best Loss: 0.000000280 in Epoch 2546
Epoch 2594
Epoch 2594, Loss: 0.000001016, Improvement: 0.000000367, Best Loss: 0.000000280 in Epoch 2546
Epoch 2595
Epoch 2595, Loss: 0.000001322, Improvement: 0.000000306, Best Loss: 0.000000280 in Epoch 2546
Epoch 2596
Epoch 2596, Loss: 0.000001031, Improvement: -0.000000291, Best Loss: 0.000000280 in Epoch 2546
Epoch 2597
Epoch 2597, Loss: 0.000000711, Improvement: -0.000000320, Best Loss: 0.000000280 in Epoch 2546
Epoch 2598
Epoch 2598, Loss: 0.000000570, Improvement: -0.000000141, Best Loss: 0.000000280 in Epoch 2546
Epoch 2599
Epoch 2599, Loss: 0.000000828, Improvement: 0.000000258, Best Loss: 0.000000280 in Epoch 2546
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.000001311, Improvement: 0.000000483, Best Loss: 0.000000280 in Epoch 2546
Epoch 2601
Epoch 2601, Loss: 0.000000934, Improvement: -0.000000377, Best Loss: 0.000000280 in Epoch 2546
Epoch 2602
Epoch 2602, Loss: 0.000001205, Improvement: 0.000000271, Best Loss: 0.000000280 in Epoch 2546
Epoch 2603
Epoch 2603, Loss: 0.000004480, Improvement: 0.000003275, Best Loss: 0.000000280 in Epoch 2546
Epoch 2604
Epoch 2604, Loss: 0.000009482, Improvement: 0.000005002, Best Loss: 0.000000280 in Epoch 2546
Epoch 2605
Epoch 2605, Loss: 0.000005135, Improvement: -0.000004346, Best Loss: 0.000000280 in Epoch 2546
Epoch 2606
Epoch 2606, Loss: 0.000006435, Improvement: 0.000001300, Best Loss: 0.000000280 in Epoch 2546
Epoch 2607
Epoch 2607, Loss: 0.000004573, Improvement: -0.000001862, Best Loss: 0.000000280 in Epoch 2546
Epoch 2608
Epoch 2608, Loss: 0.000002147, Improvement: -0.000002425, Best Loss: 0.000000280 in Epoch 2546
Epoch 2609
Epoch 2609, Loss: 0.000001357, Improvement: -0.000000791, Best Loss: 0.000000280 in Epoch 2546
Epoch 2610
Epoch 2610, Loss: 0.000001168, Improvement: -0.000000189, Best Loss: 0.000000280 in Epoch 2546
Epoch 2611
Epoch 2611, Loss: 0.000001430, Improvement: 0.000000262, Best Loss: 0.000000280 in Epoch 2546
Epoch 2612
Epoch 2612, Loss: 0.000002539, Improvement: 0.000001110, Best Loss: 0.000000280 in Epoch 2546
Epoch 2613
Epoch 2613, Loss: 0.000003179, Improvement: 0.000000640, Best Loss: 0.000000280 in Epoch 2546
Epoch 2614
Epoch 2614, Loss: 0.000003502, Improvement: 0.000000323, Best Loss: 0.000000280 in Epoch 2546
Epoch 2615
Epoch 2615, Loss: 0.000003000, Improvement: -0.000000502, Best Loss: 0.000000280 in Epoch 2546
Epoch 2616
Epoch 2616, Loss: 0.000003833, Improvement: 0.000000833, Best Loss: 0.000000280 in Epoch 2546
Epoch 2617
Epoch 2617, Loss: 0.000006022, Improvement: 0.000002188, Best Loss: 0.000000280 in Epoch 2546
Epoch 2618
Epoch 2618, Loss: 0.000008037, Improvement: 0.000002015, Best Loss: 0.000000280 in Epoch 2546
Epoch 2619
Epoch 2619, Loss: 0.000009437, Improvement: 0.000001401, Best Loss: 0.000000280 in Epoch 2546
Epoch 2620
Epoch 2620, Loss: 0.000017111, Improvement: 0.000007674, Best Loss: 0.000000280 in Epoch 2546
Epoch 2621
Epoch 2621, Loss: 0.000011540, Improvement: -0.000005571, Best Loss: 0.000000280 in Epoch 2546
Epoch 2622
Epoch 2622, Loss: 0.000016354, Improvement: 0.000004814, Best Loss: 0.000000280 in Epoch 2546
Epoch 2623
Epoch 2623, Loss: 0.000011471, Improvement: -0.000004882, Best Loss: 0.000000280 in Epoch 2546
Epoch 2624
Epoch 2624, Loss: 0.000006015, Improvement: -0.000005457, Best Loss: 0.000000280 in Epoch 2546
Epoch 2625
Epoch 2625, Loss: 0.000004052, Improvement: -0.000001963, Best Loss: 0.000000280 in Epoch 2546
Epoch 2626
Epoch 2626, Loss: 0.000002064, Improvement: -0.000001988, Best Loss: 0.000000280 in Epoch 2546
Epoch 2627
Epoch 2627, Loss: 0.000001221, Improvement: -0.000000843, Best Loss: 0.000000280 in Epoch 2546
Epoch 2628
Epoch 2628, Loss: 0.000000903, Improvement: -0.000000318, Best Loss: 0.000000280 in Epoch 2546
Epoch 2629
Epoch 2629, Loss: 0.000000764, Improvement: -0.000000139, Best Loss: 0.000000280 in Epoch 2546
Epoch 2630
Epoch 2630, Loss: 0.000000897, Improvement: 0.000000132, Best Loss: 0.000000280 in Epoch 2546
Epoch 2631
Epoch 2631, Loss: 0.000001053, Improvement: 0.000000157, Best Loss: 0.000000280 in Epoch 2546
Epoch 2632
Epoch 2632, Loss: 0.000000727, Improvement: -0.000000326, Best Loss: 0.000000280 in Epoch 2546
Epoch 2633
Epoch 2633, Loss: 0.000000882, Improvement: 0.000000155, Best Loss: 0.000000280 in Epoch 2546
Epoch 2634
Epoch 2634, Loss: 0.000001301, Improvement: 0.000000419, Best Loss: 0.000000280 in Epoch 2546
Epoch 2635
Epoch 2635, Loss: 0.000001309, Improvement: 0.000000008, Best Loss: 0.000000280 in Epoch 2546
Epoch 2636
Epoch 2636, Loss: 0.000001915, Improvement: 0.000000606, Best Loss: 0.000000280 in Epoch 2546
Epoch 2637
Epoch 2637, Loss: 0.000001650, Improvement: -0.000000265, Best Loss: 0.000000280 in Epoch 2546
Epoch 2638
Epoch 2638, Loss: 0.000000942, Improvement: -0.000000708, Best Loss: 0.000000280 in Epoch 2546
Epoch 2639
Epoch 2639, Loss: 0.000001204, Improvement: 0.000000262, Best Loss: 0.000000280 in Epoch 2546
Epoch 2640
Epoch 2640, Loss: 0.000001806, Improvement: 0.000000602, Best Loss: 0.000000280 in Epoch 2546
Epoch 2641
Epoch 2641, Loss: 0.000001716, Improvement: -0.000000091, Best Loss: 0.000000280 in Epoch 2546
Epoch 2642
Epoch 2642, Loss: 0.000001280, Improvement: -0.000000436, Best Loss: 0.000000280 in Epoch 2546
Epoch 2643
Epoch 2643, Loss: 0.000001059, Improvement: -0.000000221, Best Loss: 0.000000280 in Epoch 2546
Epoch 2644
Epoch 2644, Loss: 0.000001309, Improvement: 0.000000250, Best Loss: 0.000000280 in Epoch 2546
Epoch 2645
Epoch 2645, Loss: 0.000002466, Improvement: 0.000001157, Best Loss: 0.000000280 in Epoch 2546
Epoch 2646
Epoch 2646, Loss: 0.000005438, Improvement: 0.000002972, Best Loss: 0.000000280 in Epoch 2546
Epoch 2647
Epoch 2647, Loss: 0.000006368, Improvement: 0.000000929, Best Loss: 0.000000280 in Epoch 2546
Epoch 2648
Epoch 2648, Loss: 0.000014381, Improvement: 0.000008014, Best Loss: 0.000000280 in Epoch 2546
Epoch 2649
Epoch 2649, Loss: 0.000018512, Improvement: 0.000004130, Best Loss: 0.000000280 in Epoch 2546
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.000007435, Improvement: -0.000011077, Best Loss: 0.000000280 in Epoch 2546
Epoch 2651
Epoch 2651, Loss: 0.000004159, Improvement: -0.000003276, Best Loss: 0.000000280 in Epoch 2546
Epoch 2652
Epoch 2652, Loss: 0.000004004, Improvement: -0.000000154, Best Loss: 0.000000280 in Epoch 2546
Epoch 2653
Epoch 2653, Loss: 0.000002379, Improvement: -0.000001625, Best Loss: 0.000000280 in Epoch 2546
Epoch 2654
Epoch 2654, Loss: 0.000001403, Improvement: -0.000000976, Best Loss: 0.000000280 in Epoch 2546
Epoch 2655
Epoch 2655, Loss: 0.000001030, Improvement: -0.000000373, Best Loss: 0.000000280 in Epoch 2546
Epoch 2656
Epoch 2656, Loss: 0.000000820, Improvement: -0.000000210, Best Loss: 0.000000280 in Epoch 2546
Epoch 2657
Epoch 2657, Loss: 0.000000595, Improvement: -0.000000225, Best Loss: 0.000000280 in Epoch 2546
Epoch 2658
Epoch 2658, Loss: 0.000000468, Improvement: -0.000000127, Best Loss: 0.000000280 in Epoch 2546
Epoch 2659
Epoch 2659, Loss: 0.000000515, Improvement: 0.000000047, Best Loss: 0.000000280 in Epoch 2546
Epoch 2660
A best model at epoch 2660 has been saved with training error 0.000000274.
Epoch 2660, Loss: 0.000000469, Improvement: -0.000000046, Best Loss: 0.000000274 in Epoch 2660
Epoch 2661
Epoch 2661, Loss: 0.000000561, Improvement: 0.000000092, Best Loss: 0.000000274 in Epoch 2660
Epoch 2662
Epoch 2662, Loss: 0.000000678, Improvement: 0.000000116, Best Loss: 0.000000274 in Epoch 2660
Epoch 2663
Epoch 2663, Loss: 0.000001303, Improvement: 0.000000625, Best Loss: 0.000000274 in Epoch 2660
Epoch 2664
Epoch 2664, Loss: 0.000001364, Improvement: 0.000000061, Best Loss: 0.000000274 in Epoch 2660
Epoch 2665
Epoch 2665, Loss: 0.000001100, Improvement: -0.000000263, Best Loss: 0.000000274 in Epoch 2660
Epoch 2666
Epoch 2666, Loss: 0.000001023, Improvement: -0.000000077, Best Loss: 0.000000274 in Epoch 2660
Epoch 2667
Epoch 2667, Loss: 0.000000595, Improvement: -0.000000428, Best Loss: 0.000000274 in Epoch 2660
Epoch 2668
Epoch 2668, Loss: 0.000000759, Improvement: 0.000000164, Best Loss: 0.000000274 in Epoch 2660
Epoch 2669
Epoch 2669, Loss: 0.000000997, Improvement: 0.000000238, Best Loss: 0.000000274 in Epoch 2660
Epoch 2670
Epoch 2670, Loss: 0.000001638, Improvement: 0.000000640, Best Loss: 0.000000274 in Epoch 2660
Epoch 2671
Epoch 2671, Loss: 0.000002230, Improvement: 0.000000592, Best Loss: 0.000000274 in Epoch 2660
Epoch 2672
Epoch 2672, Loss: 0.000003582, Improvement: 0.000001352, Best Loss: 0.000000274 in Epoch 2660
Epoch 2673
Epoch 2673, Loss: 0.000006966, Improvement: 0.000003384, Best Loss: 0.000000274 in Epoch 2660
Epoch 2674
Epoch 2674, Loss: 0.000007041, Improvement: 0.000000076, Best Loss: 0.000000274 in Epoch 2660
Epoch 2675
Epoch 2675, Loss: 0.000006544, Improvement: -0.000000497, Best Loss: 0.000000274 in Epoch 2660
Epoch 2676
Epoch 2676, Loss: 0.000003612, Improvement: -0.000002932, Best Loss: 0.000000274 in Epoch 2660
Epoch 2677
Epoch 2677, Loss: 0.000004823, Improvement: 0.000001211, Best Loss: 0.000000274 in Epoch 2660
Epoch 2678
Epoch 2678, Loss: 0.000011007, Improvement: 0.000006184, Best Loss: 0.000000274 in Epoch 2660
Epoch 2679
Epoch 2679, Loss: 0.000016742, Improvement: 0.000005735, Best Loss: 0.000000274 in Epoch 2660
Epoch 2680
Epoch 2680, Loss: 0.000004167, Improvement: -0.000012574, Best Loss: 0.000000274 in Epoch 2660
Epoch 2681
Epoch 2681, Loss: 0.000002640, Improvement: -0.000001528, Best Loss: 0.000000274 in Epoch 2660
Epoch 2682
Epoch 2682, Loss: 0.000000860, Improvement: -0.000001780, Best Loss: 0.000000274 in Epoch 2660
Epoch 2683
Epoch 2683, Loss: 0.000000849, Improvement: -0.000000011, Best Loss: 0.000000274 in Epoch 2660
Epoch 2684
Epoch 2684, Loss: 0.000000628, Improvement: -0.000000221, Best Loss: 0.000000274 in Epoch 2660
Epoch 2685
Epoch 2685, Loss: 0.000000762, Improvement: 0.000000134, Best Loss: 0.000000274 in Epoch 2660
Epoch 2686
Epoch 2686, Loss: 0.000000920, Improvement: 0.000000158, Best Loss: 0.000000274 in Epoch 2660
Epoch 2687
Epoch 2687, Loss: 0.000000871, Improvement: -0.000000049, Best Loss: 0.000000274 in Epoch 2660
Epoch 2688
Epoch 2688, Loss: 0.000000600, Improvement: -0.000000271, Best Loss: 0.000000274 in Epoch 2660
Epoch 2689
A best model at epoch 2689 has been saved with training error 0.000000266.
Epoch 2689, Loss: 0.000000611, Improvement: 0.000000011, Best Loss: 0.000000266 in Epoch 2689
Epoch 2690
Epoch 2690, Loss: 0.000000641, Improvement: 0.000000031, Best Loss: 0.000000266 in Epoch 2689
Epoch 2691
Epoch 2691, Loss: 0.000001213, Improvement: 0.000000572, Best Loss: 0.000000266 in Epoch 2689
Epoch 2692
Epoch 2692, Loss: 0.000000935, Improvement: -0.000000278, Best Loss: 0.000000266 in Epoch 2689
Epoch 2693
Epoch 2693, Loss: 0.000000756, Improvement: -0.000000179, Best Loss: 0.000000266 in Epoch 2689
Epoch 2694
Epoch 2694, Loss: 0.000000697, Improvement: -0.000000058, Best Loss: 0.000000266 in Epoch 2689
Epoch 2695
Epoch 2695, Loss: 0.000001268, Improvement: 0.000000571, Best Loss: 0.000000266 in Epoch 2689
Epoch 2696
Epoch 2696, Loss: 0.000004348, Improvement: 0.000003080, Best Loss: 0.000000266 in Epoch 2689
Epoch 2697
Epoch 2697, Loss: 0.000026857, Improvement: 0.000022509, Best Loss: 0.000000266 in Epoch 2689
Epoch 2698
Epoch 2698, Loss: 0.000018961, Improvement: -0.000007895, Best Loss: 0.000000266 in Epoch 2689
Epoch 2699
Epoch 2699, Loss: 0.000006769, Improvement: -0.000012192, Best Loss: 0.000000266 in Epoch 2689
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.000002962, Improvement: -0.000003807, Best Loss: 0.000000266 in Epoch 2689
Epoch 2701
Epoch 2701, Loss: 0.000001367, Improvement: -0.000001595, Best Loss: 0.000000266 in Epoch 2689
Epoch 2702
Epoch 2702, Loss: 0.000000772, Improvement: -0.000000595, Best Loss: 0.000000266 in Epoch 2689
Epoch 2703
Epoch 2703, Loss: 0.000000653, Improvement: -0.000000119, Best Loss: 0.000000266 in Epoch 2689
Epoch 2704
Epoch 2704, Loss: 0.000000492, Improvement: -0.000000161, Best Loss: 0.000000266 in Epoch 2689
Epoch 2705
Epoch 2705, Loss: 0.000000374, Improvement: -0.000000117, Best Loss: 0.000000266 in Epoch 2689
Epoch 2706
Epoch 2706, Loss: 0.000000416, Improvement: 0.000000041, Best Loss: 0.000000266 in Epoch 2689
Epoch 2707
A best model at epoch 2707 has been saved with training error 0.000000248.
Epoch 2707, Loss: 0.000000344, Improvement: -0.000000072, Best Loss: 0.000000248 in Epoch 2707
Epoch 2708
A best model at epoch 2708 has been saved with training error 0.000000235.
Epoch 2708, Loss: 0.000000331, Improvement: -0.000000013, Best Loss: 0.000000235 in Epoch 2708
Epoch 2709
A best model at epoch 2709 has been saved with training error 0.000000230.
Epoch 2709, Loss: 0.000000348, Improvement: 0.000000017, Best Loss: 0.000000230 in Epoch 2709
Epoch 2710
Epoch 2710, Loss: 0.000000381, Improvement: 0.000000033, Best Loss: 0.000000230 in Epoch 2709
Epoch 2711
Epoch 2711, Loss: 0.000000432, Improvement: 0.000000051, Best Loss: 0.000000230 in Epoch 2709
Epoch 2712
Epoch 2712, Loss: 0.000000464, Improvement: 0.000000031, Best Loss: 0.000000230 in Epoch 2709
Epoch 2713
Epoch 2713, Loss: 0.000000498, Improvement: 0.000000034, Best Loss: 0.000000230 in Epoch 2709
Epoch 2714
Epoch 2714, Loss: 0.000000387, Improvement: -0.000000111, Best Loss: 0.000000230 in Epoch 2709
Epoch 2715
Epoch 2715, Loss: 0.000000389, Improvement: 0.000000002, Best Loss: 0.000000230 in Epoch 2709
Epoch 2716
Epoch 2716, Loss: 0.000000409, Improvement: 0.000000020, Best Loss: 0.000000230 in Epoch 2709
Epoch 2717
Epoch 2717, Loss: 0.000000501, Improvement: 0.000000092, Best Loss: 0.000000230 in Epoch 2709
Epoch 2718
Epoch 2718, Loss: 0.000000642, Improvement: 0.000000140, Best Loss: 0.000000230 in Epoch 2709
Epoch 2719
Epoch 2719, Loss: 0.000001330, Improvement: 0.000000688, Best Loss: 0.000000230 in Epoch 2709
Epoch 2720
Epoch 2720, Loss: 0.000001423, Improvement: 0.000000094, Best Loss: 0.000000230 in Epoch 2709
Epoch 2721
Epoch 2721, Loss: 0.000001881, Improvement: 0.000000458, Best Loss: 0.000000230 in Epoch 2709
Epoch 2722
Epoch 2722, Loss: 0.000002991, Improvement: 0.000001110, Best Loss: 0.000000230 in Epoch 2709
Epoch 2723
Epoch 2723, Loss: 0.000005438, Improvement: 0.000002447, Best Loss: 0.000000230 in Epoch 2709
Epoch 2724
Epoch 2724, Loss: 0.000006663, Improvement: 0.000001225, Best Loss: 0.000000230 in Epoch 2709
Epoch 2725
Epoch 2725, Loss: 0.000019246, Improvement: 0.000012583, Best Loss: 0.000000230 in Epoch 2709
Epoch 2726
Epoch 2726, Loss: 0.000010314, Improvement: -0.000008932, Best Loss: 0.000000230 in Epoch 2709
Epoch 2727
Epoch 2727, Loss: 0.000002070, Improvement: -0.000008244, Best Loss: 0.000000230 in Epoch 2709
Epoch 2728
Epoch 2728, Loss: 0.000000936, Improvement: -0.000001135, Best Loss: 0.000000230 in Epoch 2709
Epoch 2729
Epoch 2729, Loss: 0.000000571, Improvement: -0.000000365, Best Loss: 0.000000230 in Epoch 2709
Epoch 2730
Epoch 2730, Loss: 0.000000489, Improvement: -0.000000082, Best Loss: 0.000000230 in Epoch 2709
Epoch 2731
Epoch 2731, Loss: 0.000000422, Improvement: -0.000000067, Best Loss: 0.000000230 in Epoch 2709
Epoch 2732
Epoch 2732, Loss: 0.000000428, Improvement: 0.000000007, Best Loss: 0.000000230 in Epoch 2709
Epoch 2733
Epoch 2733, Loss: 0.000000351, Improvement: -0.000000077, Best Loss: 0.000000230 in Epoch 2709
Epoch 2734
Epoch 2734, Loss: 0.000000459, Improvement: 0.000000108, Best Loss: 0.000000230 in Epoch 2709
Epoch 2735
Epoch 2735, Loss: 0.000000440, Improvement: -0.000000019, Best Loss: 0.000000230 in Epoch 2709
Epoch 2736
Epoch 2736, Loss: 0.000000591, Improvement: 0.000000151, Best Loss: 0.000000230 in Epoch 2709
Epoch 2737
Epoch 2737, Loss: 0.000000388, Improvement: -0.000000204, Best Loss: 0.000000230 in Epoch 2709
Epoch 2738
Epoch 2738, Loss: 0.000000374, Improvement: -0.000000014, Best Loss: 0.000000230 in Epoch 2709
Epoch 2739
Epoch 2739, Loss: 0.000000361, Improvement: -0.000000013, Best Loss: 0.000000230 in Epoch 2709
Epoch 2740
Epoch 2740, Loss: 0.000000385, Improvement: 0.000000024, Best Loss: 0.000000230 in Epoch 2709
Epoch 2741
Epoch 2741, Loss: 0.000000400, Improvement: 0.000000015, Best Loss: 0.000000230 in Epoch 2709
Epoch 2742
Epoch 2742, Loss: 0.000000406, Improvement: 0.000000006, Best Loss: 0.000000230 in Epoch 2709
Epoch 2743
Epoch 2743, Loss: 0.000000347, Improvement: -0.000000059, Best Loss: 0.000000230 in Epoch 2709
Epoch 2744
Epoch 2744, Loss: 0.000000310, Improvement: -0.000000037, Best Loss: 0.000000230 in Epoch 2709
Epoch 2745
A best model at epoch 2745 has been saved with training error 0.000000205.
A best model at epoch 2745 has been saved with training error 0.000000197.
Epoch 2745, Loss: 0.000000275, Improvement: -0.000000035, Best Loss: 0.000000197 in Epoch 2745
Epoch 2746
Epoch 2746, Loss: 0.000000297, Improvement: 0.000000022, Best Loss: 0.000000197 in Epoch 2745
Epoch 2747
Epoch 2747, Loss: 0.000000296, Improvement: -0.000000001, Best Loss: 0.000000197 in Epoch 2745
Epoch 2748
Epoch 2748, Loss: 0.000000286, Improvement: -0.000000010, Best Loss: 0.000000197 in Epoch 2745
Epoch 2749
Epoch 2749, Loss: 0.000000363, Improvement: 0.000000077, Best Loss: 0.000000197 in Epoch 2745
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.000000494, Improvement: 0.000000131, Best Loss: 0.000000197 in Epoch 2745
Epoch 2751
Epoch 2751, Loss: 0.000000713, Improvement: 0.000000219, Best Loss: 0.000000197 in Epoch 2745
Epoch 2752
Epoch 2752, Loss: 0.000001928, Improvement: 0.000001215, Best Loss: 0.000000197 in Epoch 2745
Epoch 2753
Epoch 2753, Loss: 0.000005448, Improvement: 0.000003520, Best Loss: 0.000000197 in Epoch 2745
Epoch 2754
Epoch 2754, Loss: 0.000023640, Improvement: 0.000018192, Best Loss: 0.000000197 in Epoch 2745
Epoch 2755
Epoch 2755, Loss: 0.000008523, Improvement: -0.000015118, Best Loss: 0.000000197 in Epoch 2745
Epoch 2756
Epoch 2756, Loss: 0.000003271, Improvement: -0.000005252, Best Loss: 0.000000197 in Epoch 2745
Epoch 2757
Epoch 2757, Loss: 0.000002330, Improvement: -0.000000940, Best Loss: 0.000000197 in Epoch 2745
Epoch 2758
Epoch 2758, Loss: 0.000001805, Improvement: -0.000000525, Best Loss: 0.000000197 in Epoch 2745
Epoch 2759
Epoch 2759, Loss: 0.000002168, Improvement: 0.000000363, Best Loss: 0.000000197 in Epoch 2745
Epoch 2760
Epoch 2760, Loss: 0.000002038, Improvement: -0.000000130, Best Loss: 0.000000197 in Epoch 2745
Epoch 2761
Epoch 2761, Loss: 0.000000916, Improvement: -0.000001122, Best Loss: 0.000000197 in Epoch 2745
Epoch 2762
Epoch 2762, Loss: 0.000000759, Improvement: -0.000000157, Best Loss: 0.000000197 in Epoch 2745
Epoch 2763
Epoch 2763, Loss: 0.000000550, Improvement: -0.000000209, Best Loss: 0.000000197 in Epoch 2745
Epoch 2764
Epoch 2764, Loss: 0.000000677, Improvement: 0.000000127, Best Loss: 0.000000197 in Epoch 2745
Epoch 2765
Epoch 2765, Loss: 0.000000669, Improvement: -0.000000008, Best Loss: 0.000000197 in Epoch 2745
Epoch 2766
Epoch 2766, Loss: 0.000000684, Improvement: 0.000000015, Best Loss: 0.000000197 in Epoch 2745
Epoch 2767
Epoch 2767, Loss: 0.000001001, Improvement: 0.000000317, Best Loss: 0.000000197 in Epoch 2745
Epoch 2768
Epoch 2768, Loss: 0.000001580, Improvement: 0.000000579, Best Loss: 0.000000197 in Epoch 2745
Epoch 2769
Epoch 2769, Loss: 0.000001200, Improvement: -0.000000380, Best Loss: 0.000000197 in Epoch 2745
Epoch 2770
Epoch 2770, Loss: 0.000000850, Improvement: -0.000000350, Best Loss: 0.000000197 in Epoch 2745
Epoch 2771
Epoch 2771, Loss: 0.000000985, Improvement: 0.000000134, Best Loss: 0.000000197 in Epoch 2745
Epoch 2772
Epoch 2772, Loss: 0.000000745, Improvement: -0.000000239, Best Loss: 0.000000197 in Epoch 2745
Epoch 2773
Epoch 2773, Loss: 0.000000674, Improvement: -0.000000072, Best Loss: 0.000000197 in Epoch 2745
Epoch 2774
Epoch 2774, Loss: 0.000000699, Improvement: 0.000000025, Best Loss: 0.000000197 in Epoch 2745
Epoch 2775
Epoch 2775, Loss: 0.000000541, Improvement: -0.000000158, Best Loss: 0.000000197 in Epoch 2745
Epoch 2776
Epoch 2776, Loss: 0.000000481, Improvement: -0.000000060, Best Loss: 0.000000197 in Epoch 2745
Epoch 2777
Epoch 2777, Loss: 0.000001001, Improvement: 0.000000521, Best Loss: 0.000000197 in Epoch 2745
Epoch 2778
Epoch 2778, Loss: 0.000001702, Improvement: 0.000000701, Best Loss: 0.000000197 in Epoch 2745
Epoch 2779
Epoch 2779, Loss: 0.000001309, Improvement: -0.000000393, Best Loss: 0.000000197 in Epoch 2745
Epoch 2780
Epoch 2780, Loss: 0.000002146, Improvement: 0.000000837, Best Loss: 0.000000197 in Epoch 2745
Epoch 2781
Epoch 2781, Loss: 0.000001861, Improvement: -0.000000285, Best Loss: 0.000000197 in Epoch 2745
Epoch 2782
Epoch 2782, Loss: 0.000004171, Improvement: 0.000002310, Best Loss: 0.000000197 in Epoch 2745
Epoch 2783
Epoch 2783, Loss: 0.000003616, Improvement: -0.000000555, Best Loss: 0.000000197 in Epoch 2745
Epoch 2784
Epoch 2784, Loss: 0.000003558, Improvement: -0.000000058, Best Loss: 0.000000197 in Epoch 2745
Epoch 2785
Epoch 2785, Loss: 0.000003347, Improvement: -0.000000211, Best Loss: 0.000000197 in Epoch 2745
Epoch 2786
Epoch 2786, Loss: 0.000002178, Improvement: -0.000001170, Best Loss: 0.000000197 in Epoch 2745
Epoch 2787
Epoch 2787, Loss: 0.000001949, Improvement: -0.000000229, Best Loss: 0.000000197 in Epoch 2745
Epoch 2788
Epoch 2788, Loss: 0.000005177, Improvement: 0.000003228, Best Loss: 0.000000197 in Epoch 2745
Epoch 2789
Epoch 2789, Loss: 0.000006873, Improvement: 0.000001697, Best Loss: 0.000000197 in Epoch 2745
Epoch 2790
Epoch 2790, Loss: 0.000005050, Improvement: -0.000001824, Best Loss: 0.000000197 in Epoch 2745
Epoch 2791
Epoch 2791, Loss: 0.000008018, Improvement: 0.000002968, Best Loss: 0.000000197 in Epoch 2745
Epoch 2792
Epoch 2792, Loss: 0.000003372, Improvement: -0.000004646, Best Loss: 0.000000197 in Epoch 2745
Epoch 2793
Epoch 2793, Loss: 0.000004431, Improvement: 0.000001059, Best Loss: 0.000000197 in Epoch 2745
Epoch 2794
Epoch 2794, Loss: 0.000008005, Improvement: 0.000003574, Best Loss: 0.000000197 in Epoch 2745
Epoch 2795
Epoch 2795, Loss: 0.000009279, Improvement: 0.000001274, Best Loss: 0.000000197 in Epoch 2745
Epoch 2796
Epoch 2796, Loss: 0.000011222, Improvement: 0.000001943, Best Loss: 0.000000197 in Epoch 2745
Epoch 2797
Epoch 2797, Loss: 0.000015852, Improvement: 0.000004630, Best Loss: 0.000000197 in Epoch 2745
Epoch 2798
Epoch 2798, Loss: 0.000013423, Improvement: -0.000002429, Best Loss: 0.000000197 in Epoch 2745
Epoch 2799
Epoch 2799, Loss: 0.000007124, Improvement: -0.000006299, Best Loss: 0.000000197 in Epoch 2745
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.000005305, Improvement: -0.000001819, Best Loss: 0.000000197 in Epoch 2745
Epoch 2801
Epoch 2801, Loss: 0.000002591, Improvement: -0.000002714, Best Loss: 0.000000197 in Epoch 2745
Epoch 2802
Epoch 2802, Loss: 0.000001525, Improvement: -0.000001065, Best Loss: 0.000000197 in Epoch 2745
Epoch 2803
Epoch 2803, Loss: 0.000005169, Improvement: 0.000003643, Best Loss: 0.000000197 in Epoch 2745
Epoch 2804
Epoch 2804, Loss: 0.000003629, Improvement: -0.000001540, Best Loss: 0.000000197 in Epoch 2745
Epoch 2805
Epoch 2805, Loss: 0.000005598, Improvement: 0.000001969, Best Loss: 0.000000197 in Epoch 2745
Epoch 2806
Epoch 2806, Loss: 0.000002688, Improvement: -0.000002910, Best Loss: 0.000000197 in Epoch 2745
Epoch 2807
Epoch 2807, Loss: 0.000003757, Improvement: 0.000001069, Best Loss: 0.000000197 in Epoch 2745
Epoch 2808
Epoch 2808, Loss: 0.000004934, Improvement: 0.000001177, Best Loss: 0.000000197 in Epoch 2745
Epoch 2809
Epoch 2809, Loss: 0.000003983, Improvement: -0.000000951, Best Loss: 0.000000197 in Epoch 2745
Epoch 2810
Epoch 2810, Loss: 0.000001267, Improvement: -0.000002716, Best Loss: 0.000000197 in Epoch 2745
Epoch 2811
Epoch 2811, Loss: 0.000001452, Improvement: 0.000000186, Best Loss: 0.000000197 in Epoch 2745
Epoch 2812
Epoch 2812, Loss: 0.000003218, Improvement: 0.000001766, Best Loss: 0.000000197 in Epoch 2745
Epoch 2813
Epoch 2813, Loss: 0.000001728, Improvement: -0.000001491, Best Loss: 0.000000197 in Epoch 2745
Epoch 2814
Epoch 2814, Loss: 0.000002775, Improvement: 0.000001047, Best Loss: 0.000000197 in Epoch 2745
Epoch 2815
Epoch 2815, Loss: 0.000002336, Improvement: -0.000000439, Best Loss: 0.000000197 in Epoch 2745
Epoch 2816
Epoch 2816, Loss: 0.000004329, Improvement: 0.000001993, Best Loss: 0.000000197 in Epoch 2745
Epoch 2817
Epoch 2817, Loss: 0.000005443, Improvement: 0.000001114, Best Loss: 0.000000197 in Epoch 2745
Epoch 2818
Epoch 2818, Loss: 0.000012670, Improvement: 0.000007228, Best Loss: 0.000000197 in Epoch 2745
Epoch 2819
Epoch 2819, Loss: 0.000005199, Improvement: -0.000007472, Best Loss: 0.000000197 in Epoch 2745
Epoch 2820
Epoch 2820, Loss: 0.000002614, Improvement: -0.000002585, Best Loss: 0.000000197 in Epoch 2745
Epoch 2821
Epoch 2821, Loss: 0.000001878, Improvement: -0.000000735, Best Loss: 0.000000197 in Epoch 2745
Epoch 2822
Epoch 2822, Loss: 0.000001474, Improvement: -0.000000405, Best Loss: 0.000000197 in Epoch 2745
Epoch 2823
Epoch 2823, Loss: 0.000001893, Improvement: 0.000000419, Best Loss: 0.000000197 in Epoch 2745
Epoch 2824
Epoch 2824, Loss: 0.000002871, Improvement: 0.000000978, Best Loss: 0.000000197 in Epoch 2745
Epoch 2825
Epoch 2825, Loss: 0.000001584, Improvement: -0.000001287, Best Loss: 0.000000197 in Epoch 2745
Epoch 2826
Epoch 2826, Loss: 0.000000829, Improvement: -0.000000755, Best Loss: 0.000000197 in Epoch 2745
Epoch 2827
Epoch 2827, Loss: 0.000001382, Improvement: 0.000000553, Best Loss: 0.000000197 in Epoch 2745
Epoch 2828
Epoch 2828, Loss: 0.000001971, Improvement: 0.000000589, Best Loss: 0.000000197 in Epoch 2745
Epoch 2829
Epoch 2829, Loss: 0.000001839, Improvement: -0.000000132, Best Loss: 0.000000197 in Epoch 2745
Epoch 2830
Epoch 2830, Loss: 0.000001621, Improvement: -0.000000218, Best Loss: 0.000000197 in Epoch 2745
Epoch 2831
Epoch 2831, Loss: 0.000002562, Improvement: 0.000000941, Best Loss: 0.000000197 in Epoch 2745
Epoch 2832
Epoch 2832, Loss: 0.000004064, Improvement: 0.000001502, Best Loss: 0.000000197 in Epoch 2745
Epoch 2833
Epoch 2833, Loss: 0.000002362, Improvement: -0.000001702, Best Loss: 0.000000197 in Epoch 2745
Epoch 2834
Epoch 2834, Loss: 0.000001574, Improvement: -0.000000788, Best Loss: 0.000000197 in Epoch 2745
Epoch 2835
Epoch 2835, Loss: 0.000002217, Improvement: 0.000000643, Best Loss: 0.000000197 in Epoch 2745
Epoch 2836
Epoch 2836, Loss: 0.000002048, Improvement: -0.000000169, Best Loss: 0.000000197 in Epoch 2745
Epoch 2837
Epoch 2837, Loss: 0.000003896, Improvement: 0.000001849, Best Loss: 0.000000197 in Epoch 2745
Epoch 2838
Epoch 2838, Loss: 0.000006031, Improvement: 0.000002134, Best Loss: 0.000000197 in Epoch 2745
Epoch 2839
Epoch 2839, Loss: 0.000008835, Improvement: 0.000002805, Best Loss: 0.000000197 in Epoch 2745
Epoch 2840
Epoch 2840, Loss: 0.000011254, Improvement: 0.000002418, Best Loss: 0.000000197 in Epoch 2745
Epoch 2841
Epoch 2841, Loss: 0.000008004, Improvement: -0.000003250, Best Loss: 0.000000197 in Epoch 2745
Epoch 2842
Epoch 2842, Loss: 0.000004304, Improvement: -0.000003700, Best Loss: 0.000000197 in Epoch 2745
Epoch 2843
Epoch 2843, Loss: 0.000003221, Improvement: -0.000001083, Best Loss: 0.000000197 in Epoch 2745
Epoch 2844
Epoch 2844, Loss: 0.000003468, Improvement: 0.000000247, Best Loss: 0.000000197 in Epoch 2745
Epoch 2845
Epoch 2845, Loss: 0.000002054, Improvement: -0.000001414, Best Loss: 0.000000197 in Epoch 2745
Epoch 2846
Epoch 2846, Loss: 0.000003546, Improvement: 0.000001492, Best Loss: 0.000000197 in Epoch 2745
Epoch 2847
Epoch 2847, Loss: 0.000001879, Improvement: -0.000001667, Best Loss: 0.000000197 in Epoch 2745
Epoch 2848
Epoch 2848, Loss: 0.000001872, Improvement: -0.000000007, Best Loss: 0.000000197 in Epoch 2745
Epoch 2849
Epoch 2849, Loss: 0.000001727, Improvement: -0.000000145, Best Loss: 0.000000197 in Epoch 2745
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.000001473, Improvement: -0.000000254, Best Loss: 0.000000197 in Epoch 2745
Epoch 2851
Epoch 2851, Loss: 0.000002016, Improvement: 0.000000543, Best Loss: 0.000000197 in Epoch 2745
Epoch 2852
Epoch 2852, Loss: 0.000003576, Improvement: 0.000001560, Best Loss: 0.000000197 in Epoch 2745
Epoch 2853
Epoch 2853, Loss: 0.000007125, Improvement: 0.000003550, Best Loss: 0.000000197 in Epoch 2745
Epoch 2854
Epoch 2854, Loss: 0.000006208, Improvement: -0.000000917, Best Loss: 0.000000197 in Epoch 2745
Epoch 2855
Epoch 2855, Loss: 0.000003821, Improvement: -0.000002388, Best Loss: 0.000000197 in Epoch 2745
Epoch 2856
Epoch 2856, Loss: 0.000001429, Improvement: -0.000002392, Best Loss: 0.000000197 in Epoch 2745
Epoch 2857
Epoch 2857, Loss: 0.000001078, Improvement: -0.000000351, Best Loss: 0.000000197 in Epoch 2745
Epoch 2858
Epoch 2858, Loss: 0.000000908, Improvement: -0.000000170, Best Loss: 0.000000197 in Epoch 2745
Epoch 2859
Epoch 2859, Loss: 0.000001666, Improvement: 0.000000757, Best Loss: 0.000000197 in Epoch 2745
Epoch 2860
Epoch 2860, Loss: 0.000004743, Improvement: 0.000003077, Best Loss: 0.000000197 in Epoch 2745
Epoch 2861
Epoch 2861, Loss: 0.000004615, Improvement: -0.000000128, Best Loss: 0.000000197 in Epoch 2745
Epoch 2862
Epoch 2862, Loss: 0.000003459, Improvement: -0.000001156, Best Loss: 0.000000197 in Epoch 2745
Epoch 2863
Epoch 2863, Loss: 0.000003028, Improvement: -0.000000431, Best Loss: 0.000000197 in Epoch 2745
Epoch 2864
Epoch 2864, Loss: 0.000008662, Improvement: 0.000005635, Best Loss: 0.000000197 in Epoch 2745
Epoch 2865
Epoch 2865, Loss: 0.000008344, Improvement: -0.000000318, Best Loss: 0.000000197 in Epoch 2745
Epoch 2866
Epoch 2866, Loss: 0.000004859, Improvement: -0.000003485, Best Loss: 0.000000197 in Epoch 2745
Epoch 2867
Epoch 2867, Loss: 0.000009905, Improvement: 0.000005046, Best Loss: 0.000000197 in Epoch 2745
Epoch 2868
Epoch 2868, Loss: 0.000011200, Improvement: 0.000001295, Best Loss: 0.000000197 in Epoch 2745
Epoch 2869
Epoch 2869, Loss: 0.000008462, Improvement: -0.000002738, Best Loss: 0.000000197 in Epoch 2745
Epoch 2870
Epoch 2870, Loss: 0.000004063, Improvement: -0.000004399, Best Loss: 0.000000197 in Epoch 2745
Epoch 2871
Epoch 2871, Loss: 0.000003486, Improvement: -0.000000576, Best Loss: 0.000000197 in Epoch 2745
Epoch 2872
Epoch 2872, Loss: 0.000001580, Improvement: -0.000001906, Best Loss: 0.000000197 in Epoch 2745
Epoch 2873
Epoch 2873, Loss: 0.000001529, Improvement: -0.000000051, Best Loss: 0.000000197 in Epoch 2745
Epoch 2874
Epoch 2874, Loss: 0.000001024, Improvement: -0.000000506, Best Loss: 0.000000197 in Epoch 2745
Epoch 2875
Epoch 2875, Loss: 0.000000816, Improvement: -0.000000207, Best Loss: 0.000000197 in Epoch 2745
Epoch 2876
Epoch 2876, Loss: 0.000000501, Improvement: -0.000000316, Best Loss: 0.000000197 in Epoch 2745
Epoch 2877
Epoch 2877, Loss: 0.000000785, Improvement: 0.000000284, Best Loss: 0.000000197 in Epoch 2745
Epoch 2878
Epoch 2878, Loss: 0.000000890, Improvement: 0.000000106, Best Loss: 0.000000197 in Epoch 2745
Epoch 2879
Epoch 2879, Loss: 0.000000563, Improvement: -0.000000328, Best Loss: 0.000000197 in Epoch 2745
Epoch 2880
Epoch 2880, Loss: 0.000000717, Improvement: 0.000000154, Best Loss: 0.000000197 in Epoch 2745
Epoch 2881
Epoch 2881, Loss: 0.000001137, Improvement: 0.000000420, Best Loss: 0.000000197 in Epoch 2745
Epoch 2882
Epoch 2882, Loss: 0.000001829, Improvement: 0.000000692, Best Loss: 0.000000197 in Epoch 2745
Epoch 2883
Epoch 2883, Loss: 0.000001772, Improvement: -0.000000057, Best Loss: 0.000000197 in Epoch 2745
Epoch 2884
Epoch 2884, Loss: 0.000001856, Improvement: 0.000000084, Best Loss: 0.000000197 in Epoch 2745
Epoch 2885
Epoch 2885, Loss: 0.000004338, Improvement: 0.000002482, Best Loss: 0.000000197 in Epoch 2745
Epoch 2886
Epoch 2886, Loss: 0.000010146, Improvement: 0.000005808, Best Loss: 0.000000197 in Epoch 2745
Epoch 2887
Epoch 2887, Loss: 0.000009179, Improvement: -0.000000967, Best Loss: 0.000000197 in Epoch 2745
Epoch 2888
Epoch 2888, Loss: 0.000008696, Improvement: -0.000000483, Best Loss: 0.000000197 in Epoch 2745
Epoch 2889
Epoch 2889, Loss: 0.000009699, Improvement: 0.000001002, Best Loss: 0.000000197 in Epoch 2745
Epoch 2890
Epoch 2890, Loss: 0.000006206, Improvement: -0.000003493, Best Loss: 0.000000197 in Epoch 2745
Epoch 2891
Epoch 2891, Loss: 0.000002562, Improvement: -0.000003644, Best Loss: 0.000000197 in Epoch 2745
Epoch 2892
Epoch 2892, Loss: 0.000008532, Improvement: 0.000005969, Best Loss: 0.000000197 in Epoch 2745
Epoch 2893
Epoch 2893, Loss: 0.000013050, Improvement: 0.000004518, Best Loss: 0.000000197 in Epoch 2745
Epoch 2894
Epoch 2894, Loss: 0.000006060, Improvement: -0.000006990, Best Loss: 0.000000197 in Epoch 2745
Epoch 2895
Epoch 2895, Loss: 0.000003135, Improvement: -0.000002926, Best Loss: 0.000000197 in Epoch 2745
Epoch 2896
Epoch 2896, Loss: 0.000003101, Improvement: -0.000000033, Best Loss: 0.000000197 in Epoch 2745
Epoch 2897
Epoch 2897, Loss: 0.000001886, Improvement: -0.000001216, Best Loss: 0.000000197 in Epoch 2745
Epoch 2898
Epoch 2898, Loss: 0.000001426, Improvement: -0.000000459, Best Loss: 0.000000197 in Epoch 2745
Epoch 2899
Epoch 2899, Loss: 0.000001175, Improvement: -0.000000251, Best Loss: 0.000000197 in Epoch 2745
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.000001977, Improvement: 0.000000801, Best Loss: 0.000000197 in Epoch 2745
Epoch 2901
Epoch 2901, Loss: 0.000004690, Improvement: 0.000002713, Best Loss: 0.000000197 in Epoch 2745
Epoch 2902
Epoch 2902, Loss: 0.000003831, Improvement: -0.000000858, Best Loss: 0.000000197 in Epoch 2745
Epoch 2903
Epoch 2903, Loss: 0.000001520, Improvement: -0.000002312, Best Loss: 0.000000197 in Epoch 2745
Epoch 2904
Epoch 2904, Loss: 0.000000870, Improvement: -0.000000650, Best Loss: 0.000000197 in Epoch 2745
Epoch 2905
Epoch 2905, Loss: 0.000000781, Improvement: -0.000000089, Best Loss: 0.000000197 in Epoch 2745
Epoch 2906
Epoch 2906, Loss: 0.000001356, Improvement: 0.000000575, Best Loss: 0.000000197 in Epoch 2745
Epoch 2907
Epoch 2907, Loss: 0.000001151, Improvement: -0.000000206, Best Loss: 0.000000197 in Epoch 2745
Epoch 2908
Epoch 2908, Loss: 0.000000768, Improvement: -0.000000383, Best Loss: 0.000000197 in Epoch 2745
Epoch 2909
Epoch 2909, Loss: 0.000001362, Improvement: 0.000000594, Best Loss: 0.000000197 in Epoch 2745
Epoch 2910
Epoch 2910, Loss: 0.000001124, Improvement: -0.000000238, Best Loss: 0.000000197 in Epoch 2745
Epoch 2911
Epoch 2911, Loss: 0.000000704, Improvement: -0.000000420, Best Loss: 0.000000197 in Epoch 2745
Epoch 2912
Epoch 2912, Loss: 0.000001445, Improvement: 0.000000741, Best Loss: 0.000000197 in Epoch 2745
Epoch 2913
Epoch 2913, Loss: 0.000001670, Improvement: 0.000000226, Best Loss: 0.000000197 in Epoch 2745
Epoch 2914
Epoch 2914, Loss: 0.000001548, Improvement: -0.000000123, Best Loss: 0.000000197 in Epoch 2745
Epoch 2915
Epoch 2915, Loss: 0.000001992, Improvement: 0.000000444, Best Loss: 0.000000197 in Epoch 2745
Epoch 2916
Epoch 2916, Loss: 0.000002893, Improvement: 0.000000900, Best Loss: 0.000000197 in Epoch 2745
Epoch 2917
Epoch 2917, Loss: 0.000004055, Improvement: 0.000001162, Best Loss: 0.000000197 in Epoch 2745
Epoch 2918
Epoch 2918, Loss: 0.000004024, Improvement: -0.000000031, Best Loss: 0.000000197 in Epoch 2745
Epoch 2919
Epoch 2919, Loss: 0.000005377, Improvement: 0.000001353, Best Loss: 0.000000197 in Epoch 2745
Epoch 2920
Epoch 2920, Loss: 0.000004824, Improvement: -0.000000553, Best Loss: 0.000000197 in Epoch 2745
Epoch 2921
Epoch 2921, Loss: 0.000003394, Improvement: -0.000001430, Best Loss: 0.000000197 in Epoch 2745
Epoch 2922
Epoch 2922, Loss: 0.000002977, Improvement: -0.000000417, Best Loss: 0.000000197 in Epoch 2745
Epoch 2923
Epoch 2923, Loss: 0.000005223, Improvement: 0.000002246, Best Loss: 0.000000197 in Epoch 2745
Epoch 2924
Epoch 2924, Loss: 0.000005232, Improvement: 0.000000009, Best Loss: 0.000000197 in Epoch 2745
Epoch 2925
Epoch 2925, Loss: 0.000007368, Improvement: 0.000002135, Best Loss: 0.000000197 in Epoch 2745
Epoch 2926
Epoch 2926, Loss: 0.000005433, Improvement: -0.000001935, Best Loss: 0.000000197 in Epoch 2745
Epoch 2927
Epoch 2927, Loss: 0.000010323, Improvement: 0.000004890, Best Loss: 0.000000197 in Epoch 2745
Epoch 2928
Epoch 2928, Loss: 0.000007118, Improvement: -0.000003205, Best Loss: 0.000000197 in Epoch 2745
Epoch 2929
Epoch 2929, Loss: 0.000007557, Improvement: 0.000000439, Best Loss: 0.000000197 in Epoch 2745
Epoch 2930
Epoch 2930, Loss: 0.000006510, Improvement: -0.000001047, Best Loss: 0.000000197 in Epoch 2745
Epoch 2931
Epoch 2931, Loss: 0.000003879, Improvement: -0.000002631, Best Loss: 0.000000197 in Epoch 2745
Epoch 2932
Epoch 2932, Loss: 0.000003072, Improvement: -0.000000806, Best Loss: 0.000000197 in Epoch 2745
Epoch 2933
Epoch 2933, Loss: 0.000001490, Improvement: -0.000001582, Best Loss: 0.000000197 in Epoch 2745
Epoch 2934
Epoch 2934, Loss: 0.000001329, Improvement: -0.000000161, Best Loss: 0.000000197 in Epoch 2745
Epoch 2935
Epoch 2935, Loss: 0.000001096, Improvement: -0.000000233, Best Loss: 0.000000197 in Epoch 2745
Epoch 2936
Epoch 2936, Loss: 0.000000616, Improvement: -0.000000480, Best Loss: 0.000000197 in Epoch 2745
Epoch 2937
Epoch 2937, Loss: 0.000000513, Improvement: -0.000000103, Best Loss: 0.000000197 in Epoch 2745
Epoch 2938
Epoch 2938, Loss: 0.000000579, Improvement: 0.000000066, Best Loss: 0.000000197 in Epoch 2745
Epoch 2939
Epoch 2939, Loss: 0.000000650, Improvement: 0.000000072, Best Loss: 0.000000197 in Epoch 2745
Epoch 2940
Epoch 2940, Loss: 0.000001178, Improvement: 0.000000528, Best Loss: 0.000000197 in Epoch 2745
Epoch 2941
Epoch 2941, Loss: 0.000001724, Improvement: 0.000000545, Best Loss: 0.000000197 in Epoch 2745
Epoch 2942
Epoch 2942, Loss: 0.000001930, Improvement: 0.000000207, Best Loss: 0.000000197 in Epoch 2745
Epoch 2943
Epoch 2943, Loss: 0.000002732, Improvement: 0.000000802, Best Loss: 0.000000197 in Epoch 2745
Epoch 2944
Epoch 2944, Loss: 0.000004228, Improvement: 0.000001496, Best Loss: 0.000000197 in Epoch 2745
Epoch 2945
Epoch 2945, Loss: 0.000005418, Improvement: 0.000001190, Best Loss: 0.000000197 in Epoch 2745
Epoch 2946
Epoch 2946, Loss: 0.000012892, Improvement: 0.000007474, Best Loss: 0.000000197 in Epoch 2745
Epoch 2947
Epoch 2947, Loss: 0.000008955, Improvement: -0.000003937, Best Loss: 0.000000197 in Epoch 2745
Epoch 2948
Epoch 2948, Loss: 0.000006120, Improvement: -0.000002835, Best Loss: 0.000000197 in Epoch 2745
Epoch 2949
Epoch 2949, Loss: 0.000008495, Improvement: 0.000002375, Best Loss: 0.000000197 in Epoch 2745
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.000012004, Improvement: 0.000003509, Best Loss: 0.000000197 in Epoch 2745
Epoch 2951
Epoch 2951, Loss: 0.000013038, Improvement: 0.000001035, Best Loss: 0.000000197 in Epoch 2745
Epoch 2952
Epoch 2952, Loss: 0.000006788, Improvement: -0.000006251, Best Loss: 0.000000197 in Epoch 2745
Epoch 2953
Epoch 2953, Loss: 0.000002248, Improvement: -0.000004539, Best Loss: 0.000000197 in Epoch 2745
Epoch 2954
Epoch 2954, Loss: 0.000000916, Improvement: -0.000001332, Best Loss: 0.000000197 in Epoch 2745
Epoch 2955
Epoch 2955, Loss: 0.000000607, Improvement: -0.000000309, Best Loss: 0.000000197 in Epoch 2745
Epoch 2956
Epoch 2956, Loss: 0.000000481, Improvement: -0.000000127, Best Loss: 0.000000197 in Epoch 2745
Epoch 2957
Epoch 2957, Loss: 0.000000483, Improvement: 0.000000002, Best Loss: 0.000000197 in Epoch 2745
Epoch 2958
Epoch 2958, Loss: 0.000000393, Improvement: -0.000000090, Best Loss: 0.000000197 in Epoch 2745
Epoch 2959
Epoch 2959, Loss: 0.000000455, Improvement: 0.000000062, Best Loss: 0.000000197 in Epoch 2745
Epoch 2960
Epoch 2960, Loss: 0.000000415, Improvement: -0.000000040, Best Loss: 0.000000197 in Epoch 2745
Epoch 2961
Epoch 2961, Loss: 0.000000406, Improvement: -0.000000008, Best Loss: 0.000000197 in Epoch 2745
Epoch 2962
Epoch 2962, Loss: 0.000000435, Improvement: 0.000000029, Best Loss: 0.000000197 in Epoch 2745
Epoch 2963
Epoch 2963, Loss: 0.000000550, Improvement: 0.000000115, Best Loss: 0.000000197 in Epoch 2745
Epoch 2964
Epoch 2964, Loss: 0.000000456, Improvement: -0.000000094, Best Loss: 0.000000197 in Epoch 2745
Epoch 2965
Epoch 2965, Loss: 0.000000558, Improvement: 0.000000103, Best Loss: 0.000000197 in Epoch 2745
Epoch 2966
Epoch 2966, Loss: 0.000000590, Improvement: 0.000000032, Best Loss: 0.000000197 in Epoch 2745
Epoch 2967
Epoch 2967, Loss: 0.000000770, Improvement: 0.000000180, Best Loss: 0.000000197 in Epoch 2745
Epoch 2968
Epoch 2968, Loss: 0.000001161, Improvement: 0.000000391, Best Loss: 0.000000197 in Epoch 2745
Epoch 2969
Epoch 2969, Loss: 0.000000808, Improvement: -0.000000353, Best Loss: 0.000000197 in Epoch 2745
Epoch 2970
Epoch 2970, Loss: 0.000000894, Improvement: 0.000000085, Best Loss: 0.000000197 in Epoch 2745
Epoch 2971
Epoch 2971, Loss: 0.000000946, Improvement: 0.000000052, Best Loss: 0.000000197 in Epoch 2745
Epoch 2972
Epoch 2972, Loss: 0.000001191, Improvement: 0.000000245, Best Loss: 0.000000197 in Epoch 2745
Epoch 2973
Epoch 2973, Loss: 0.000000975, Improvement: -0.000000217, Best Loss: 0.000000197 in Epoch 2745
Epoch 2974
Epoch 2974, Loss: 0.000001060, Improvement: 0.000000085, Best Loss: 0.000000197 in Epoch 2745
Epoch 2975
Epoch 2975, Loss: 0.000001830, Improvement: 0.000000770, Best Loss: 0.000000197 in Epoch 2745
Epoch 2976
Epoch 2976, Loss: 0.000001565, Improvement: -0.000000266, Best Loss: 0.000000197 in Epoch 2745
Epoch 2977
Epoch 2977, Loss: 0.000001319, Improvement: -0.000000246, Best Loss: 0.000000197 in Epoch 2745
Epoch 2978
Epoch 2978, Loss: 0.000001681, Improvement: 0.000000362, Best Loss: 0.000000197 in Epoch 2745
Epoch 2979
Epoch 2979, Loss: 0.000002103, Improvement: 0.000000422, Best Loss: 0.000000197 in Epoch 2745
Epoch 2980
Epoch 2980, Loss: 0.000001770, Improvement: -0.000000333, Best Loss: 0.000000197 in Epoch 2745
Epoch 2981
Epoch 2981, Loss: 0.000003543, Improvement: 0.000001773, Best Loss: 0.000000197 in Epoch 2745
Epoch 2982
Epoch 2982, Loss: 0.000002132, Improvement: -0.000001411, Best Loss: 0.000000197 in Epoch 2745
Epoch 2983
Epoch 2983, Loss: 0.000001621, Improvement: -0.000000511, Best Loss: 0.000000197 in Epoch 2745
Epoch 2984
Epoch 2984, Loss: 0.000001349, Improvement: -0.000000272, Best Loss: 0.000000197 in Epoch 2745
Epoch 2985
Epoch 2985, Loss: 0.000001214, Improvement: -0.000000135, Best Loss: 0.000000197 in Epoch 2745
Epoch 2986
Epoch 2986, Loss: 0.000003709, Improvement: 0.000002495, Best Loss: 0.000000197 in Epoch 2745
Epoch 2987
Epoch 2987, Loss: 0.000007683, Improvement: 0.000003975, Best Loss: 0.000000197 in Epoch 2745
Epoch 2988
Epoch 2988, Loss: 0.000006072, Improvement: -0.000001611, Best Loss: 0.000000197 in Epoch 2745
Epoch 2989
Epoch 2989, Loss: 0.000013469, Improvement: 0.000007397, Best Loss: 0.000000197 in Epoch 2745
Epoch 2990
Epoch 2990, Loss: 0.000005816, Improvement: -0.000007653, Best Loss: 0.000000197 in Epoch 2745
Epoch 2991
Epoch 2991, Loss: 0.000003313, Improvement: -0.000002504, Best Loss: 0.000000197 in Epoch 2745
Epoch 2992
Epoch 2992, Loss: 0.000001999, Improvement: -0.000001314, Best Loss: 0.000000197 in Epoch 2745
Epoch 2993
Epoch 2993, Loss: 0.000001816, Improvement: -0.000000182, Best Loss: 0.000000197 in Epoch 2745
Epoch 2994
Epoch 2994, Loss: 0.000001824, Improvement: 0.000000008, Best Loss: 0.000000197 in Epoch 2745
Epoch 2995
Epoch 2995, Loss: 0.000003023, Improvement: 0.000001199, Best Loss: 0.000000197 in Epoch 2745
Epoch 2996
Epoch 2996, Loss: 0.000002925, Improvement: -0.000000098, Best Loss: 0.000000197 in Epoch 2745
Epoch 2997
Epoch 2997, Loss: 0.000001274, Improvement: -0.000001651, Best Loss: 0.000000197 in Epoch 2745
Epoch 2998
Epoch 2998, Loss: 0.000000738, Improvement: -0.000000536, Best Loss: 0.000000197 in Epoch 2745
Epoch 2999
Epoch 2999, Loss: 0.000000455, Improvement: -0.000000283, Best Loss: 0.000000197 in Epoch 2745
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.000000442, Improvement: -0.000000014, Best Loss: 0.000000197 in Epoch 2745
[1.63319676e-02 1.08004160e-02 1.03519030e-02 ... 7.38256121e-07
 4.55443788e-07 4.41532571e-07]
