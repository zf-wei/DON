{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7581882a095f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:43.779748Z",
     "start_time": "2024-11-26T00:54:41.663241Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.expanduser(\"~/DON\")))\n",
    "\n",
    "'''\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"DeepONet with configurable parameters.\")\n",
    "parser.add_argument('--problem', type=str, default=\"heat\", help='Problem to solve')\n",
    "parser.add_argument('--var', type=int, default=0, help='Variant of DeepONet')\n",
    "parser.add_argument('--visc', type=float, default=0.0001, help='Viscosity')\n",
    "parser.add_argument('--struct', type=int, default=1, help='Structure of DeepONet')\n",
    "parser.add_argument('--sensor', type=int, default=50, help='Number of sensors')\n",
    "parser.add_argument('--boundary_parameter', type=float, default=0, help='Weight parameter for boundary conditions')\n",
    "parser.add_argument('--initial_parameter', type=float, default=0, help='Weight parameter for initial conditions')\n",
    "parser.add_argument('--train_batch_size', type=int, default=8000, help='Train Batch size')\n",
    "parser.add_argument('--test_batch_size', type=int, default=2000, help='Train Batch size')\n",
    "# 解析命令行参数\n",
    "args = parser.parse_args()\n",
    "problem = args.problem\n",
    "var = args.var\n",
    "visc = args.visc\n",
    "struct = args.struct\n",
    "n_points = args.sensor\n",
    "boundary_parameter = args.boundary_parameter\n",
    "initial_parameter = args.initial_parameter\n",
    "train_batch_size = args.train_batch_size\n",
    "test_batch_size = args.test_batch_size\n",
    "'''\n",
    "problem = \"burgers\"\n",
    "var = 6\n",
    "visc = 0.0001\n",
    "struct = 1\n",
    "n_points = 101\n",
    "boundary_parameter = 0\n",
    "initial_parameter = 0\n",
    "train_batch_size = 8000\n",
    "test_batch_size = 2000\n",
    "\n",
    "epochs = 2\n",
    "## 需要修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e92850a88f7af6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:43.872299Z",
     "start_time": "2024-11-26T00:54:43.813311Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we define the configurable parameters for the DeepONet\n",
    "\n",
    "time_limit = 1\n",
    "time_step = 0.01\n",
    "\n",
    "if problem==\"heat\":\n",
    "    time_start = time_step\n",
    "    total_time_steps = int(time_limit/time_step)\n",
    "    from utilities.tools import get_cell_centers\n",
    "    evaluating_points = get_cell_centers(n_points=n_points)\n",
    "elif problem==\"burgers\":\n",
    "    time_start = 0\n",
    "    total_time_steps = (int(time_limit/time_step)+1)\n",
    "    evaluating_points = np.linspace(0, 1, n_points)\n",
    "\n",
    "evaluating_points = np.around(evaluating_points, decimals=2)\n",
    "\n",
    "total_sample = 500\n",
    "border = int(total_sample * 4 / 5) # 设置训练集和测试集的边界\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "branch_input_dim = n_points  # Number of points to represent the original function\n",
    "trunk_input_dim = 2     # Coordinate where we evaluate the transformed function\n",
    "\n",
    "# Define the dictionary mapping struct values to neural network structures\n",
    "if var!=6:\n",
    "    structures = {\n",
    "        1: {'hidden_dims': [100, 100, 100, 100], 'output_dim': 50},\n",
    "        2: {'hidden_dims': [200, 200, 200, 200], 'output_dim': 50}\n",
    "    }\n",
    "\n",
    "    # Get the configuration based on the struct value\n",
    "    config = structures.get(struct, {'hidden_dims': [], 'output_dim': 0})\n",
    "\n",
    "    hidden_dims = config['hidden_dims']\n",
    "    output_dim = config['output_dim']\n",
    "elif var==6:\n",
    "    structure_params = {\n",
    "        1: (4, 4, 100, 50),\n",
    "        2: (4, 4, 200, 50),\n",
    "    }\n",
    "    if struct in structure_params:\n",
    "        branch_depth, trunk_depth, hidden_dim, output_dim = structure_params[struct]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid structure type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b174778098c640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:43.960926Z",
     "start_time": "2024-11-26T00:54:43.956316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of y_tensor is torch.Size([10201, 2]).\n",
      "The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.\n",
      "The zero coordinate of y_expanded is time and the first coordinate is space.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we import the function to get the cell centers of a 1D mesh.\n",
    "# Also, we set up the spatial and temporal grid points for the training and testing datasets.\n",
    "# This is the so-called y_expanded tensor.\n",
    "time_steps = np.arange(time_start, time_limit+time_step, time_step)\n",
    "time_steps = np.around(time_steps, decimals=2)\n",
    "\n",
    "Y1, Y2 = np.meshgrid(evaluating_points, time_steps)  # 第一个变量进行行展开，第二个变量进行列展开\n",
    "\n",
    "y = np.column_stack([Y2.ravel(),Y1.ravel()]) \n",
    "# 先将 Y2 和 Y1 进行展开，然后将展开后的两个向量进行列合并\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "print(f\"The dimension of y_tensor is {y_tensor.shape}.\")\n",
    "y_expanded = y_tensor.unsqueeze(0).expand(total_sample, -1, -1)\n",
    "print(f\"The dimension of y_expanded is {y_expanded.shape} after expanding.\")\n",
    "print(\"The zero coordinate of y_expanded is time and the first coordinate is space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5207f665c8910c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:44.054933Z",
     "start_time": "2024-11-26T00:54:44.043183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the initial conditions are: (500, 101)\n",
      "The dimensions of the solutions are: (500, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we load the initial conditions and solutions from the saved files.\n",
    "\n",
    "# Define the directory where you want to save the file\n",
    "from pathlib import Path\n",
    "# Get the current directory\n",
    "current_dir = Path.cwd()\n",
    "#data_directory = os.path.join(current_dir.parent, 'data')\n",
    "## 需要修改\n",
    "data_directory = os.path.join(current_dir, 'data')\n",
    "initials_name = f'{problem}_initials_{visc}_{len(evaluating_points)}.npy'\n",
    "solutions_name = f'{problem}_solutions_{visc}_{len(evaluating_points)}.npy'\n",
    "\n",
    "# Define the file paths\n",
    "initials_path = os.path.join(data_directory, initials_name)\n",
    "solutions_path = os.path.join(data_directory, solutions_name)\n",
    "\n",
    "# Load the data\n",
    "initials = np.load(initials_path)\n",
    "solutions = np.load(solutions_path)\n",
    "\n",
    "print(f\"The dimensions of the initial conditions are: {initials.shape}\")\n",
    "print(f\"The dimensions of the solutions are: {solutions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263385cef0858257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:44.234Z",
     "start_time": "2024-11-26T00:54:44.132139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of u_tensor is torch.Size([500, 101]).\n",
      "The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we arrange the initial conditions into the desired format for training the DeepONet.\n",
    "# This is the so-called u_expanded tensor.\n",
    "u_tensor = torch.tensor(initials, dtype=torch.float)\n",
    "print(f\"The dimension of u_tensor is {u_tensor.shape}.\")\n",
    "\n",
    "u_expanded = u_tensor.unsqueeze(1) # u_expanded: tensor[total_sample, 1, n_points]\n",
    "u_expanded = u_expanded.expand(-1, total_time_steps * n_points, -1) # u_expanded: tensor[total_sample, total_time_steps*n_points, n_points]\n",
    "print(f\"The dimension of u_expanded is {u_expanded.shape} after expanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e93725c26537d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:44.283796Z",
     "start_time": "2024-11-26T00:54:44.274772Z"
    }
   },
   "outputs": [],
   "source": [
    "# I have a tensor of shape (total_sample, n_points) representing the initial conditions. In this cell, I wanted to expand it to (total_sample, total_time_steps*n_points) by repeating the initial conditions for each time step.\n",
    "\n",
    "# Assuming u_tensor is the tensor of shape (total_sample, n_points)\n",
    "# Expand the tensor to (total_sample, total_time_steps*n_points)\n",
    "u_corresponding = u_tensor.repeat(1, total_time_steps)\n",
    "u_corresponding = u_corresponding.unsqueeze(2) # This is the so-called corresponding initial value\n",
    "\n",
    "# Take the spatial coordinate of the y_expanded tensor\n",
    "y_space = y_expanded[:, :, 1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addb4d7d40a31a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:44.356924Z",
     "start_time": "2024-11-26T00:54:44.354139Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we modify the input of the DeepONet based on the variant chosen.\n",
    "# We also update the input dimensions of the DeepONet based on the variant chosen.\n",
    "if var==2 or var==3:\n",
    "    y_expanded = torch.cat((y_expanded, u_corresponding), dim=-1)\n",
    "elif var==4:\n",
    "    y_expanded = torch.cat((y_expanded, u_expanded), dim=-1)\n",
    "\n",
    "if var== 1 or var==3 or var==4:\n",
    "    u_expanded = torch.cat((u_expanded, y_space), dim=-1)\n",
    "\n",
    "var_mapping = {\n",
    "    1: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim},\n",
    "    2: {'var_branch_input_dim': branch_input_dim, 'var_trunk_input_dim': trunk_input_dim + 1},\n",
    "    3: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim + 1},\n",
    "    4: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim + branch_input_dim}\n",
    "}\n",
    "\n",
    "if var in var_mapping:\n",
    "    branch_input_dim = var_mapping[var]['var_branch_input_dim']\n",
    "    trunk_input_dim = var_mapping[var]['var_trunk_input_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8da03c7d319f1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:44.456588Z",
     "start_time": "2024-11-26T00:54:44.430231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded solution dataset has dimension (500, 101, 101),\n",
      "\t while the arranged linearized dataset has dimension (500, 10201).\n",
      "The dimension of s_tensor is torch.Size([500, 10201]).\n",
      "The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we arrange the solutions into the desired format for training the DeepONet.\n",
    "# This is the so-called s_expanded tensor.\n",
    "\n",
    "solutions_linear = np.zeros((total_sample, total_time_steps*n_points))\n",
    "\n",
    "for i in range(total_sample):\n",
    "    solutions_linear[i] = solutions[i].flatten()\n",
    "\n",
    "# solutions is a 3D array of shape (total_sample, total_time_steps, n_points)\n",
    "print(f\"The loaded solution dataset has dimension {solutions.shape},\\n\\t while the arranged linearized dataset has dimension {solutions_linear.shape}.\")\n",
    "\n",
    "s_tensor  = torch.tensor(solutions_linear, dtype=torch.float) # s_tensor: tensor[total_sample, total_time_steps*n_points]\n",
    "s_expanded  = s_tensor.unsqueeze(2) # s_expanded: tensor[total_sample, total_time_steps*n_points, 1]\n",
    "\n",
    "print(f\"The dimension of s_tensor is {s_tensor.shape}.\")\n",
    "print(f\"The dimension of s_expanded is {s_expanded.shape} after expanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93069c846cf6a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:45.112225Z",
     "start_time": "2024-11-26T00:54:44.523274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 4080400 samples, while the train_loader has 817 batches.\n",
      "The training dataset has 1020100 samples, while the train_loader has 205 batches.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we create the training and testing datasets and dataloader for the DeepONet.\n",
    "\n",
    "u_train = u_expanded[:border]\n",
    "y_train = y_expanded[:border]\n",
    "s_train = s_expanded[:border]\n",
    "\n",
    "u_test = u_expanded[border:]\n",
    "y_test = y_expanded[border:]\n",
    "s_test = s_expanded[border:]\n",
    "\n",
    "u_train_combined = u_train.reshape(-1, u_train.shape[-1])\n",
    "y_train_combined = y_train.reshape(-1, y_train.shape[-1])\n",
    "s_train_combined = s_train.reshape(-1, s_train.shape[-1])\n",
    "\n",
    "u_test_combined = u_test.reshape(-1, u_test.shape[-1])\n",
    "y_test_combined = y_test.reshape(-1, y_test.shape[-1])\n",
    "s_test_combined = s_test.reshape(-1, s_test.shape[-1])\n",
    "\n",
    "from utilities.tools import CustomDataset as CustomDataset\n",
    "\n",
    "train_set = CustomDataset(u_train_combined, y_train_combined, s_train_combined)\n",
    "test_set = CustomDataset(u_test_combined, y_test_combined, s_test_combined)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "print(f\"The training dataset has {len(train_set)} samples, while the train_loader has {len(train_loader)} batches.\")\n",
    "print(f\"The training dataset has {len(test_set)} samples, while the train_loader has {len(test_loader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2eb4da6aa21f586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:54:46.228481Z",
     "start_time": "2024-11-26T00:54:45.156248Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we import and set up the model and load the trained parameters\n",
    "# The loss function is also imported here.\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if var!=6:\n",
    "    from utilities.DeepONets import DeepONet\n",
    "    model = DeepONet(branch_input_dim, trunk_input_dim, hidden_dims, output_dim).to(device)\n",
    "elif var==6:\n",
    "    from utilities.DeepONets import ModifiedDeepONet\n",
    "    model = ModifiedDeepONet(branch_input_dim, branch_depth, trunk_input_dim, trunk_depth, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from utilities.loss_fns import loss_fn_1d_combined as loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc153cac14e1864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:56:27.844217Z",
     "start_time": "2024-11-26T00:54:46.284075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at epoch 1 has been saved with training error 0.05217382684350.\n",
      "A best model at epoch 1 has been saved with training error 0.04060793295503.\n",
      "A best model at epoch 1 has been saved with training error 0.04002447426319.\n",
      "A best model at epoch 1 has been saved with training error 0.03806798905134.\n",
      "A best model at epoch 1 has been saved with training error 0.03572792559862.\n",
      "A best model at epoch 1 has been saved with training error 0.03524262830615.\n",
      "A best model at epoch 1 has been saved with training error 0.03511505946517.\n",
      "A best model at epoch 1 has been saved with training error 0.03502191603184.\n",
      "A best model at epoch 1 has been saved with training error 0.03480844199657.\n",
      "A best model at epoch 1 has been saved with training error 0.03363770619035.\n",
      "A best model at epoch 1 has been saved with training error 0.03174154832959.\n",
      "A best model at epoch 1 has been saved with training error 0.03128217160702.\n",
      "A best model at epoch 1 has been saved with training error 0.03076159581542.\n",
      "A best model at epoch 1 has been saved with training error 0.02848455868661.\n",
      "A best model at epoch 1 has been saved with training error 0.02800911292434.\n",
      "A best model at epoch 1 has been saved with training error 0.02742314524949.\n",
      "A best model at epoch 1 has been saved with training error 0.02688203193247.\n",
      "A best model at epoch 1 has been saved with training error 0.02634398825467.\n",
      "A best model at epoch 1 has been saved with training error 0.02438488230109.\n",
      "A best model at epoch 1 has been saved with training error 0.02227652445436.\n",
      "A best model at epoch 1 has been saved with training error 0.02196339145303.\n",
      "A best model at epoch 1 has been saved with training error 0.02126987278461.\n",
      "A best model at epoch 1 has been saved with training error 0.02120688930154.\n",
      "A best model at epoch 1 has been saved with training error 0.02002484351397.\n",
      "A best model at epoch 1 has been saved with training error 0.02001968771219.\n",
      "A best model at epoch 1 has been saved with training error 0.01962261088192.\n",
      "A best model at epoch 1 has been saved with training error 0.01850228570402.\n",
      "A best model at epoch 1 has been saved with training error 0.01784988865256.\n",
      "A best model at epoch 1 has been saved with training error 0.01769652776420.\n",
      "A best model at epoch 1 has been saved with training error 0.01658490672708.\n",
      "A best model at epoch 1 has been saved with training error 0.01606105081737.\n",
      "A best model at epoch 1 has been saved with training error 0.01581817679107.\n",
      "A best model at epoch 1 has been saved with training error 0.01563115604222.\n",
      "A best model at epoch 1 has been saved with training error 0.01502232812345.\n",
      "A best model at epoch 1 has been saved with training error 0.01452752854675.\n",
      "A best model at epoch 1 has been saved with training error 0.01372706331313.\n",
      "A best model at epoch 1 has been saved with training error 0.01353076752275.\n",
      "A best model at epoch 1 has been saved with training error 0.01330461166799.\n",
      "A best model at epoch 1 has been saved with training error 0.01262544095516.\n",
      "A best model at epoch 1 has been saved with training error 0.01257055252790.\n",
      "A best model at epoch 1 has been saved with training error 0.01239415630698.\n",
      "A best model at epoch 1 has been saved with training error 0.01176216453314.\n",
      "A best model at epoch 1 has been saved with training error 0.01098343078047.\n",
      "A best model at epoch 1 has been saved with training error 0.01094443164766.\n",
      "A best model at epoch 1 has been saved with training error 0.01034706085920.\n",
      "A best model at epoch 1 has been saved with training error 0.01017075870186.\n",
      "A best model at epoch 1 has been saved with training error 0.00976254325360.\n",
      "A best model at epoch 1 has been saved with training error 0.00967170111835.\n",
      "A best model at epoch 1 has been saved with training error 0.00938756484538.\n",
      "A best model at epoch 1 has been saved with training error 0.00921236258000.\n",
      "A best model at epoch 1 has been saved with training error 0.00906264875084.\n",
      "A best model at epoch 1 has been saved with training error 0.00871332269162.\n",
      "A best model at epoch 1 has been saved with training error 0.00844724196941.\n",
      "A best model at epoch 1 has been saved with training error 0.00838520843536.\n",
      "A best model at epoch 1 has been saved with training error 0.00827639456838.\n",
      "A best model at epoch 1 has been saved with training error 0.00728504406288.\n",
      "A best model at epoch 1 has been saved with training error 0.00724424608052.\n",
      "A best model at epoch 1 has been saved with training error 0.00718615949154.\n",
      "A best model at epoch 1 has been saved with training error 0.00698174070567.\n",
      "A best model at epoch 1 has been saved with training error 0.00654233247042.\n",
      "A best model at epoch 1 has been saved with training error 0.00654123863205.\n",
      "A best model at epoch 1 has been saved with training error 0.00632929056883.\n",
      "A best model at epoch 1 has been saved with training error 0.00609047105536.\n",
      "A best model at epoch 1 has been saved with training error 0.00593313202262.\n",
      "A best model at epoch 1 has been saved with training error 0.00588839408010.\n",
      "A best model at epoch 1 has been saved with training error 0.00545439077541.\n",
      "A best model at epoch 1 has been saved with training error 0.00507974298671.\n",
      "A best model at epoch 1 has been saved with training error 0.00500414986163.\n",
      "A best model at epoch 1 has been saved with training error 0.00495840935037.\n",
      "A best model at epoch 1 has been saved with training error 0.00485374964774.\n",
      "A best model at epoch 1 has been saved with training error 0.00462462101132.\n",
      "A best model at epoch 1 has been saved with training error 0.00461699673906.\n",
      "A best model at epoch 1 has been saved with training error 0.00457151746377.\n",
      "A best model at epoch 1 has been saved with training error 0.00449843751267.\n",
      "A best model at epoch 1 has been saved with training error 0.00398721499369.\n",
      "A best model at epoch 1 has been saved with training error 0.00382889132015.\n",
      "A best model at epoch 1 has been saved with training error 0.00378174101934.\n",
      "A best model at epoch 1 has been saved with training error 0.00369829777628.\n",
      "A best model at epoch 1 has been saved with training error 0.00349418260157.\n",
      "A best model at epoch 1 has been saved with training error 0.00343705341220.\n",
      "A best model at epoch 1 has been saved with training error 0.00332772405818.\n",
      "A best model at epoch 1 has been saved with training error 0.00308365514502.\n",
      "A best model at epoch 1 has been saved with training error 0.00297141144983.\n",
      "A best model at epoch 1 has been saved with training error 0.00283089675941.\n",
      "A best model at epoch 1 has been saved with training error 0.00272885337472.\n",
      "A best model at epoch 1 has been saved with training error 0.00271015730686.\n",
      "A best model at epoch 1 has been saved with training error 0.00268928892910.\n",
      "A best model at epoch 1 has been saved with training error 0.00264213024639.\n",
      "A best model at epoch 1 has been saved with training error 0.00257911882363.\n",
      "A best model at epoch 1 has been saved with training error 0.00246790703386.\n",
      "A best model at epoch 1 has been saved with training error 0.00246135960333.\n",
      "A best model at epoch 1 has been saved with training error 0.00233977637254.\n",
      "A best model at epoch 1 has been saved with training error 0.00226075202227.\n",
      "A best model at epoch 1 has been saved with training error 0.00222980836406.\n",
      "A best model at epoch 1 has been saved with training error 0.00187969685066.\n",
      "A best model at epoch 1 has been saved with training error 0.00186183035839.\n",
      "A best model at epoch 1 has been saved with training error 0.00183724903036.\n",
      "A best model at epoch 1 has been saved with training error 0.00181824818719.\n",
      "A best model at epoch 1 has been saved with training error 0.00181145244278.\n",
      "A best model at epoch 1 has been saved with training error 0.00178670021705.\n",
      "A best model at epoch 1 has been saved with training error 0.00166379637085.\n",
      "A best model at epoch 1 has been saved with training error 0.00160429568496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00643186861639, Improvement: 0.00643186861639, Best Loss: 0.00091881066328 in Epoch 1, Time: 51.34 seconds\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at epoch 1 has been saved with training error 0.00091881066328.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.00160196621764, Improvement: -0.00482990239874, Best Loss: 0.00091881066328 in Epoch 1, Time: 50.21 seconds\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "error_list = []\n",
    "time_list = []\n",
    "err_best = float('inf')\n",
    "err_prev = 0\n",
    "best_epoch = 0\n",
    "model_best = model.state_dict().copy()\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    print(f\"Epoch {epoch+1}\") \n",
    "    err = []\n",
    "    for input1_batch, input2_batch, target_batch in train_loader:\n",
    "        input1_batch = input1_batch.to(device)\n",
    "        input2_batch = input2_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1_batch, input2_batch)\n",
    "        loss = loss_fn(outputs, target_batch, boundary_parameter, initial_parameter, total_time_steps, n_points)\n",
    "        err.append(loss.item())\n",
    "        if loss.item()<err_best:\n",
    "            err_best = loss.item()\n",
    "            best_epoch = epoch\n",
    "            model_best = model.state_dict().copy()\n",
    "            model_params_best = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-best.pth\"\n",
    "            torch.save(model_best, model_params_best)\n",
    "            print(f\"A best model at epoch {epoch+1} has been saved with training error {err_best:.14f}.\", file=sys.stderr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del input1_batch, input2_batch, outputs, loss\n",
    "        torch.cuda.empty_cache()  # 释放当前批次的缓存\n",
    "    error_list.append(err)\n",
    "    err_curr = np.mean(err)\n",
    "    epoch_time = time.time() - start_time  # Calculate the elapsed time\n",
    "    time_list.append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {err_curr:.14f}, Improvement: {err_curr - err_prev:.14f}, Best Loss: {err_best:.14f} in Epoch {best_epoch+1}, Time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    err_prev = err_curr\n",
    "    if epoch%50==49:\n",
    "        # 保存损失值和模型\n",
    "        training_error_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-final.npy\"\n",
    "        model_params_final = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-final.pth\"\n",
    "        np.save(training_error_list, np.array(error_list))\n",
    "        torch.save(model.state_dict(), model_params_final)\n",
    "        print(f\"Model saving checkpoint: the model trained after epoch {epoch+1} has been saved with the training errors.\", file=sys.stderr)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828a564ec2bd2525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:56:27.961106Z",
     "start_time": "2024-11-26T00:56:27.958788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00643187 0.00160197]\n"
     ]
    }
   ],
   "source": [
    "errs = np.array(error_list)\n",
    "\n",
    "print(np.mean(errs,axis=1))\n",
    "\n",
    "## 需要修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df781e2b09174ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:56:28.048541Z",
     "start_time": "2024-11-26T00:56:28.028013Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存损失值和模型\n",
    "training_error_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-final.npy\"\n",
    "training_time_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-final.time\"\n",
    "model_params_final = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-final.pth\"\n",
    "\n",
    "np.save(training_error_list, np.array(error_list))\n",
    "np.save(training_time_list, np.array(time_list))\n",
    "torch.save(model.state_dict(), model_params_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2acf165b01f82a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T00:56:28.104265Z",
     "start_time": "2024-11-26T00:56:28.101414Z"
    }
   },
   "outputs": [],
   "source": [
    "# # In this cell, we compute training and testing loss for the model\n",
    "def compute_loss(model, data_loader, device, description=\"Computing loss\"):\n",
    "    \"\"\"\n",
    "    Compute the loss for a given dataset using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - data_loader: DataLoader for the dataset (train or test).\n",
    "    - device: Device to run the computations on (CPU or GPU).\n",
    "    - description: Description for the tqdm progress bar.\n",
    "\n",
    "    Returns:\n",
    "    - average_loss: The average loss over the dataset.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input1_batch, input2_batch, target_batch in data_loader:\n",
    "            input1_batch = input1_batch.to(device)\n",
    "            input2_batch = input2_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            outputs = model(input1_batch, input2_batch)\n",
    "            loss = loss_fn(outputs, target_batch, boundary_parameter, initial_parameter, total_time_steps, n_points)\n",
    "            total_loss += loss.item()\n",
    "            del input1_batch, input2_batch, target_batch, outputs\n",
    "            torch.cuda.empty_cache()  # Release cache for the current batch\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f70f1aeac644ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:03:50.288808Z",
     "start_time": "2024-11-26T01:02:48.416662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65453/3610576477.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_params_best, map_location=torch.device(device)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the best parameters, the training loss is 0.00195049163797, while the testing loss is 0.00231439382120.\n"
     ]
    }
   ],
   "source": [
    "training_loss_best = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{train_batch_size}-best.loss\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_params_best, map_location=torch.device(device), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "training_loss = compute_loss(model, train_loader, device, description=\"Computing training loss\")\n",
    "testing_loss = compute_loss(model, test_loader, device, description=\"Computing testing loss\")\n",
    "\n",
    "print(f\"With the best parameters, the training loss is {training_loss:.14f}, while the testing loss is {testing_loss:.14f}.\")\n",
    "\n",
    "with open(training_loss_best, 'w') as f:\n",
    "    f.write(\"With the best parameters, the training and testing losses for the model are as follows:\\n\")\n",
    "    f.write(f\"Train loss: {training_loss}\\n\")\n",
    "    f.write(f\"Test loss: {testing_loss}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
