The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00026330066612.
A best model at epoch 1 has been saved with training error 0.00006139582547.
A best model at epoch 1 has been saved with training error 0.00005928619066.
Epoch 1, Loss: 0.00109509876056, Improvement: 0.00109509876056, Best Loss: 0.00005928619066 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.00003907254722.
Epoch 2, Loss: 0.00007725937630, Improvement: -0.00101783938426, Best Loss: 0.00003907254722 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.00003143704089.
Epoch 3, Loss: 0.00005486875671, Improvement: -0.00002239061960, Best Loss: 0.00003143704089 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.00002505338671.
Epoch 4, Loss: 0.00005007652990, Improvement: -0.00000479222681, Best Loss: 0.00002505338671 in Epoch 4
Epoch 5
Epoch 5, Loss: 0.00004904285979, Improvement: -0.00000103367011, Best Loss: 0.00002505338671 in Epoch 4
Epoch 6
Epoch 6, Loss: 0.00004829630398, Improvement: -0.00000074655582, Best Loss: 0.00002505338671 in Epoch 4
Epoch 7
Epoch 7, Loss: 0.00004742529136, Improvement: -0.00000087101262, Best Loss: 0.00002505338671 in Epoch 4
Epoch 8
Epoch 8, Loss: 0.00004645819390, Improvement: -0.00000096709746, Best Loss: 0.00002505338671 in Epoch 4
Epoch 9
Epoch 9, Loss: 0.00004526209023, Improvement: -0.00000119610368, Best Loss: 0.00002505338671 in Epoch 4
Epoch 10
Epoch 10, Loss: 0.00004388292618, Improvement: -0.00000137916404, Best Loss: 0.00002505338671 in Epoch 4
Epoch 11
Epoch 11, Loss: 0.00004226541541, Improvement: -0.00000161751077, Best Loss: 0.00002505338671 in Epoch 4
Epoch 12
Epoch 12, Loss: 0.00004038803218, Improvement: -0.00000187738324, Best Loss: 0.00002505338671 in Epoch 4
Epoch 13
A best model at epoch 13 has been saved with training error 0.00002289452277.
Epoch 13, Loss: 0.00003852428717, Improvement: -0.00000186374500, Best Loss: 0.00002289452277 in Epoch 13
Epoch 14
A best model at epoch 14 has been saved with training error 0.00002261798909.
Epoch 14, Loss: 0.00003688163315, Improvement: -0.00000164265402, Best Loss: 0.00002261798909 in Epoch 14
Epoch 15
A best model at epoch 15 has been saved with training error 0.00001691888428.
Epoch 15, Loss: 0.00003573039694, Improvement: -0.00000115123621, Best Loss: 0.00001691888428 in Epoch 15
Epoch 16
Epoch 16, Loss: 0.00003502213758, Improvement: -0.00000070825936, Best Loss: 0.00001691888428 in Epoch 15
Epoch 17
Epoch 17, Loss: 0.00003450536724, Improvement: -0.00000051677034, Best Loss: 0.00001691888428 in Epoch 15
Epoch 18
Epoch 18, Loss: 0.00003410133795, Improvement: -0.00000040402929, Best Loss: 0.00001691888428 in Epoch 15
Epoch 19
Epoch 19, Loss: 0.00003374790640, Improvement: -0.00000035343155, Best Loss: 0.00001691888428 in Epoch 15
Epoch 20
Epoch 20, Loss: 0.00003342135005, Improvement: -0.00000032655635, Best Loss: 0.00001691888428 in Epoch 15
Epoch 21
Epoch 21, Loss: 0.00003309362210, Improvement: -0.00000032772796, Best Loss: 0.00001691888428 in Epoch 15
Epoch 22
Epoch 22, Loss: 0.00003275627014, Improvement: -0.00000033735196, Best Loss: 0.00001691888428 in Epoch 15
Epoch 23
Epoch 23, Loss: 0.00003239738935, Improvement: -0.00000035888079, Best Loss: 0.00001691888428 in Epoch 15
Epoch 24
Epoch 24, Loss: 0.00003199755456, Improvement: -0.00000039983479, Best Loss: 0.00001691888428 in Epoch 15
Epoch 25
Epoch 25, Loss: 0.00003156691564, Improvement: -0.00000043063892, Best Loss: 0.00001691888428 in Epoch 15
Epoch 26
Epoch 26, Loss: 0.00003108241817, Improvement: -0.00000048449747, Best Loss: 0.00001691888428 in Epoch 15
Epoch 27
Epoch 27, Loss: 0.00003055330826, Improvement: -0.00000052910991, Best Loss: 0.00001691888428 in Epoch 15
Epoch 28
Epoch 28, Loss: 0.00002996114627, Improvement: -0.00000059216200, Best Loss: 0.00001691888428 in Epoch 15
Epoch 29
A best model at epoch 29 has been saved with training error 0.00001364513901.
Epoch 29, Loss: 0.00002929627294, Improvement: -0.00000066487332, Best Loss: 0.00001364513901 in Epoch 29
Epoch 30
Epoch 30, Loss: 0.00002855810153, Improvement: -0.00000073817141, Best Loss: 0.00001364513901 in Epoch 29
Epoch 31
Epoch 31, Loss: 0.00002771537220, Improvement: -0.00000084272933, Best Loss: 0.00001364513901 in Epoch 29
Epoch 32
Epoch 32, Loss: 0.00002675621608, Improvement: -0.00000095915611, Best Loss: 0.00001364513901 in Epoch 29
Epoch 33
Epoch 33, Loss: 0.00002565243258, Improvement: -0.00000110378351, Best Loss: 0.00001364513901 in Epoch 29
Epoch 34
Epoch 34, Loss: 0.00002437036082, Improvement: -0.00000128207175, Best Loss: 0.00001364513901 in Epoch 29
Epoch 35
A best model at epoch 35 has been saved with training error 0.00001333159071.
Epoch 35, Loss: 0.00002293959710, Improvement: -0.00000143076372, Best Loss: 0.00001333159071 in Epoch 35
Epoch 36
A best model at epoch 36 has been saved with training error 0.00001225303913.
Epoch 36, Loss: 0.00002127664502, Improvement: -0.00000166295208, Best Loss: 0.00001225303913 in Epoch 36
Epoch 37
A best model at epoch 37 has been saved with training error 0.00001114415227.
Epoch 37, Loss: 0.00001933550061, Improvement: -0.00000194114441, Best Loss: 0.00001114415227 in Epoch 37
Epoch 38
A best model at epoch 38 has been saved with training error 0.00000879189338.
Epoch 38, Loss: 0.00001717183541, Improvement: -0.00000216366520, Best Loss: 0.00000879189338 in Epoch 38
Epoch 39
A best model at epoch 39 has been saved with training error 0.00000847360934.
Epoch 39, Loss: 0.00001460809053, Improvement: -0.00000256374487, Best Loss: 0.00000847360934 in Epoch 39
Epoch 40
Epoch 40, Loss: 0.00001202974622, Improvement: -0.00000257834431, Best Loss: 0.00000847360934 in Epoch 39
Epoch 41
A best model at epoch 41 has been saved with training error 0.00000833411468.
A best model at epoch 41 has been saved with training error 0.00000688672526.
A best model at epoch 41 has been saved with training error 0.00000686475369.
A best model at epoch 41 has been saved with training error 0.00000545595731.
Epoch 41, Loss: 0.00000934580605, Improvement: -0.00000268394017, Best Loss: 0.00000545595731 in Epoch 41
Epoch 42
A best model at epoch 42 has been saved with training error 0.00000456439057.
Epoch 42, Loss: 0.00000682400307, Improvement: -0.00000252180298, Best Loss: 0.00000456439057 in Epoch 42
Epoch 43
A best model at epoch 43 has been saved with training error 0.00000409415816.
A best model at epoch 43 has been saved with training error 0.00000385662679.
A best model at epoch 43 has been saved with training error 0.00000371583860.
A best model at epoch 43 has been saved with training error 0.00000250139260.
Epoch 43, Loss: 0.00000478786191, Improvement: -0.00000203614117, Best Loss: 0.00000250139260 in Epoch 43
Epoch 44
A best model at epoch 44 has been saved with training error 0.00000228204908.
Epoch 44, Loss: 0.00000332970947, Improvement: -0.00000145815244, Best Loss: 0.00000228204908 in Epoch 44
Epoch 45
A best model at epoch 45 has been saved with training error 0.00000206061986.
A best model at epoch 45 has been saved with training error 0.00000181464418.
Epoch 45, Loss: 0.00000248357800, Improvement: -0.00000084613146, Best Loss: 0.00000181464418 in Epoch 45
Epoch 46
A best model at epoch 46 has been saved with training error 0.00000178572839.
A best model at epoch 46 has been saved with training error 0.00000173000308.
A best model at epoch 46 has been saved with training error 0.00000149871198.
Epoch 46, Loss: 0.00000206366773, Improvement: -0.00000041991027, Best Loss: 0.00000149871198 in Epoch 46
Epoch 47
A best model at epoch 47 has been saved with training error 0.00000133112655.
A best model at epoch 47 has been saved with training error 0.00000124752239.
Epoch 47, Loss: 0.00000178827303, Improvement: -0.00000027539470, Best Loss: 0.00000124752239 in Epoch 47
Epoch 48
A best model at epoch 48 has been saved with training error 0.00000106044661.
Epoch 48, Loss: 0.00000152981571, Improvement: -0.00000025845732, Best Loss: 0.00000106044661 in Epoch 48
Epoch 49
A best model at epoch 49 has been saved with training error 0.00000104893127.
Epoch 49, Loss: 0.00000141025004, Improvement: -0.00000011956567, Best Loss: 0.00000104893127 in Epoch 49
Epoch 50
A best model at epoch 50 has been saved with training error 0.00000094600318.
A best model at epoch 50 has been saved with training error 0.00000087530486.
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00000132383577, Improvement: -0.00000008641427, Best Loss: 0.00000087530486 in Epoch 50
Epoch 51
A best model at epoch 51 has been saved with training error 0.00000080928146.
Epoch 51, Loss: 0.00000121154040, Improvement: -0.00000011229537, Best Loss: 0.00000080928146 in Epoch 51
Epoch 52
A best model at epoch 52 has been saved with training error 0.00000078584623.
Epoch 52, Loss: 0.00000115023250, Improvement: -0.00000006130790, Best Loss: 0.00000078584623 in Epoch 52
Epoch 53
Epoch 53, Loss: 0.00000111389008, Improvement: -0.00000003634242, Best Loss: 0.00000078584623 in Epoch 52
Epoch 54
Epoch 54, Loss: 0.00000107332554, Improvement: -0.00000004056454, Best Loss: 0.00000078584623 in Epoch 52
Epoch 55
A best model at epoch 55 has been saved with training error 0.00000071259763.
A best model at epoch 55 has been saved with training error 0.00000067295286.
Epoch 55, Loss: 0.00000103791267, Improvement: -0.00000003541287, Best Loss: 0.00000067295286 in Epoch 55
Epoch 56
Epoch 56, Loss: 0.00000100578952, Improvement: -0.00000003212315, Best Loss: 0.00000067295286 in Epoch 55
Epoch 57
Epoch 57, Loss: 0.00000098324787, Improvement: -0.00000002254165, Best Loss: 0.00000067295286 in Epoch 55
Epoch 58
A best model at epoch 58 has been saved with training error 0.00000059813812.
Epoch 58, Loss: 0.00000095661946, Improvement: -0.00000002662841, Best Loss: 0.00000059813812 in Epoch 58
Epoch 59
Epoch 59, Loss: 0.00000093731130, Improvement: -0.00000001930816, Best Loss: 0.00000059813812 in Epoch 58
Epoch 60
Epoch 60, Loss: 0.00000092685497, Improvement: -0.00000001045633, Best Loss: 0.00000059813812 in Epoch 58
Epoch 61
Epoch 61, Loss: 0.00000090916288, Improvement: -0.00000001769210, Best Loss: 0.00000059813812 in Epoch 58
Epoch 62
Epoch 62, Loss: 0.00000089457024, Improvement: -0.00000001459263, Best Loss: 0.00000059813812 in Epoch 58
Epoch 63
Epoch 63, Loss: 0.00000089318411, Improvement: -0.00000000138614, Best Loss: 0.00000059813812 in Epoch 58
Epoch 64
Epoch 64, Loss: 0.00000087851116, Improvement: -0.00000001467295, Best Loss: 0.00000059813812 in Epoch 58
Epoch 65
Epoch 65, Loss: 0.00000086369608, Improvement: -0.00000001481507, Best Loss: 0.00000059813812 in Epoch 58
Epoch 66
Epoch 66, Loss: 0.00000085360651, Improvement: -0.00000001008958, Best Loss: 0.00000059813812 in Epoch 58
Epoch 67
Epoch 67, Loss: 0.00000085256079, Improvement: -0.00000000104572, Best Loss: 0.00000059813812 in Epoch 58
Epoch 68
A best model at epoch 68 has been saved with training error 0.00000057249190.
Epoch 68, Loss: 0.00000083522178, Improvement: -0.00000001733900, Best Loss: 0.00000057249190 in Epoch 68
Epoch 69
Epoch 69, Loss: 0.00000082448901, Improvement: -0.00000001073278, Best Loss: 0.00000057249190 in Epoch 68
Epoch 70
Epoch 70, Loss: 0.00000081773539, Improvement: -0.00000000675361, Best Loss: 0.00000057249190 in Epoch 68
Epoch 71
Epoch 71, Loss: 0.00000081108669, Improvement: -0.00000000664870, Best Loss: 0.00000057249190 in Epoch 68
Epoch 72
Epoch 72, Loss: 0.00000080785715, Improvement: -0.00000000322954, Best Loss: 0.00000057249190 in Epoch 68
Epoch 73
A best model at epoch 73 has been saved with training error 0.00000055816400.
Epoch 73, Loss: 0.00000079672991, Improvement: -0.00000001112724, Best Loss: 0.00000055816400 in Epoch 73
Epoch 74
A best model at epoch 74 has been saved with training error 0.00000044371703.
Epoch 74, Loss: 0.00000078999901, Improvement: -0.00000000673090, Best Loss: 0.00000044371703 in Epoch 74
Epoch 75
Epoch 75, Loss: 0.00000077904186, Improvement: -0.00000001095715, Best Loss: 0.00000044371703 in Epoch 74
Epoch 76
Epoch 76, Loss: 0.00000077116463, Improvement: -0.00000000787722, Best Loss: 0.00000044371703 in Epoch 74
Epoch 77
Epoch 77, Loss: 0.00000076599738, Improvement: -0.00000000516725, Best Loss: 0.00000044371703 in Epoch 74
Epoch 78
Epoch 78, Loss: 0.00000075263796, Improvement: -0.00000001335941, Best Loss: 0.00000044371703 in Epoch 74
Epoch 79
Epoch 79, Loss: 0.00000075046853, Improvement: -0.00000000216943, Best Loss: 0.00000044371703 in Epoch 74
Epoch 80
Epoch 80, Loss: 0.00000074166985, Improvement: -0.00000000879868, Best Loss: 0.00000044371703 in Epoch 74
Epoch 81
Epoch 81, Loss: 0.00000073655595, Improvement: -0.00000000511390, Best Loss: 0.00000044371703 in Epoch 74
Epoch 82
Epoch 82, Loss: 0.00000072533434, Improvement: -0.00000001122161, Best Loss: 0.00000044371703 in Epoch 74
Epoch 83
Epoch 83, Loss: 0.00000072389665, Improvement: -0.00000000143768, Best Loss: 0.00000044371703 in Epoch 74
Epoch 84
Epoch 84, Loss: 0.00000071534831, Improvement: -0.00000000854834, Best Loss: 0.00000044371703 in Epoch 74
Epoch 85
Epoch 85, Loss: 0.00000072462160, Improvement: 0.00000000927329, Best Loss: 0.00000044371703 in Epoch 74
Epoch 86
Epoch 86, Loss: 0.00000071622663, Improvement: -0.00000000839497, Best Loss: 0.00000044371703 in Epoch 74
Epoch 87
Epoch 87, Loss: 0.00000072422201, Improvement: 0.00000000799538, Best Loss: 0.00000044371703 in Epoch 74
Epoch 88
Epoch 88, Loss: 0.00000069141878, Improvement: -0.00000003280323, Best Loss: 0.00000044371703 in Epoch 74
Epoch 89
A best model at epoch 89 has been saved with training error 0.00000040402836.
Epoch 89, Loss: 0.00000067625215, Improvement: -0.00000001516663, Best Loss: 0.00000040402836 in Epoch 89
Epoch 90
Epoch 90, Loss: 0.00000070531482, Improvement: 0.00000002906268, Best Loss: 0.00000040402836 in Epoch 89
Epoch 91
Epoch 91, Loss: 0.00000070819731, Improvement: 0.00000000288248, Best Loss: 0.00000040402836 in Epoch 89
Epoch 92
Epoch 92, Loss: 0.00000065765116, Improvement: -0.00000005054615, Best Loss: 0.00000040402836 in Epoch 89
Epoch 93
Epoch 93, Loss: 0.00000064653592, Improvement: -0.00000001111524, Best Loss: 0.00000040402836 in Epoch 89
Epoch 94
Epoch 94, Loss: 0.00000065665162, Improvement: 0.00000001011570, Best Loss: 0.00000040402836 in Epoch 89
Epoch 95
Epoch 95, Loss: 0.00000064226207, Improvement: -0.00000001438955, Best Loss: 0.00000040402836 in Epoch 89
Epoch 96
Epoch 96, Loss: 0.00000072101047, Improvement: 0.00000007874840, Best Loss: 0.00000040402836 in Epoch 89
Epoch 97
Epoch 97, Loss: 0.00000065676087, Improvement: -0.00000006424959, Best Loss: 0.00000040402836 in Epoch 89
Epoch 98
Epoch 98, Loss: 0.00000061949630, Improvement: -0.00000003726458, Best Loss: 0.00000040402836 in Epoch 89
Epoch 99
Epoch 99, Loss: 0.00000061608330, Improvement: -0.00000000341300, Best Loss: 0.00000040402836 in Epoch 89
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00000079164285, Improvement: 0.00000017555955, Best Loss: 0.00000040402836 in Epoch 89
Epoch 101
Epoch 101, Loss: 0.00000060723980, Improvement: -0.00000018440305, Best Loss: 0.00000040402836 in Epoch 89
Epoch 102
Epoch 102, Loss: 0.00000061251433, Improvement: 0.00000000527453, Best Loss: 0.00000040402836 in Epoch 89
Epoch 103
Epoch 103, Loss: 0.00000064746263, Improvement: 0.00000003494830, Best Loss: 0.00000040402836 in Epoch 89
Epoch 104
Epoch 104, Loss: 0.00000082703377, Improvement: 0.00000017957114, Best Loss: 0.00000040402836 in Epoch 89
Epoch 105
Epoch 105, Loss: 0.00000059605755, Improvement: -0.00000023097622, Best Loss: 0.00000040402836 in Epoch 89
Epoch 106
Epoch 106, Loss: 0.00000055268961, Improvement: -0.00000004336795, Best Loss: 0.00000040402836 in Epoch 89
Epoch 107
A best model at epoch 107 has been saved with training error 0.00000038538209.
Epoch 107, Loss: 0.00000082165856, Improvement: 0.00000026896895, Best Loss: 0.00000038538209 in Epoch 107
Epoch 108
Epoch 108, Loss: 0.00000068392524, Improvement: -0.00000013773332, Best Loss: 0.00000038538209 in Epoch 107
Epoch 109
Epoch 109, Loss: 0.00000059335205, Improvement: -0.00000009057319, Best Loss: 0.00000038538209 in Epoch 107
Epoch 110
A best model at epoch 110 has been saved with training error 0.00000038039326.
Epoch 110, Loss: 0.00000055667776, Improvement: -0.00000003667429, Best Loss: 0.00000038039326 in Epoch 110
Epoch 111
Epoch 111, Loss: 0.00000075532896, Improvement: 0.00000019865120, Best Loss: 0.00000038039326 in Epoch 110
Epoch 112
Epoch 112, Loss: 0.00000092913498, Improvement: 0.00000017380602, Best Loss: 0.00000038039326 in Epoch 110
Epoch 113
Epoch 113, Loss: 0.00000056734674, Improvement: -0.00000036178823, Best Loss: 0.00000038039326 in Epoch 110
Epoch 114
Epoch 114, Loss: 0.00000051761957, Improvement: -0.00000004972718, Best Loss: 0.00000038039326 in Epoch 110
Epoch 115
A best model at epoch 115 has been saved with training error 0.00000034124119.
Epoch 115, Loss: 0.00000049589435, Improvement: -0.00000002172522, Best Loss: 0.00000034124119 in Epoch 115
Epoch 116
Epoch 116, Loss: 0.00000051561912, Improvement: 0.00000001972477, Best Loss: 0.00000034124119 in Epoch 115
Epoch 117
Epoch 117, Loss: 0.00000117348368, Improvement: 0.00000065786456, Best Loss: 0.00000034124119 in Epoch 115
Epoch 118
A best model at epoch 118 has been saved with training error 0.00000031816396.
Epoch 118, Loss: 0.00000058564425, Improvement: -0.00000058783943, Best Loss: 0.00000031816396 in Epoch 118
Epoch 119
Epoch 119, Loss: 0.00000051107710, Improvement: -0.00000007456714, Best Loss: 0.00000031816396 in Epoch 118
Epoch 120
Epoch 120, Loss: 0.00000053870579, Improvement: 0.00000002762869, Best Loss: 0.00000031816396 in Epoch 118
Epoch 121
A best model at epoch 121 has been saved with training error 0.00000030810864.
Epoch 121, Loss: 0.00000046611601, Improvement: -0.00000007258978, Best Loss: 0.00000030810864 in Epoch 121
Epoch 122
Epoch 122, Loss: 0.00000100048153, Improvement: 0.00000053436551, Best Loss: 0.00000030810864 in Epoch 121
Epoch 123
Epoch 123, Loss: 0.00000092876377, Improvement: -0.00000007171776, Best Loss: 0.00000030810864 in Epoch 121
Epoch 124
A best model at epoch 124 has been saved with training error 0.00000025717489.
Epoch 124, Loss: 0.00000050559511, Improvement: -0.00000042316866, Best Loss: 0.00000025717489 in Epoch 124
Epoch 125
Epoch 125, Loss: 0.00000045410293, Improvement: -0.00000005149218, Best Loss: 0.00000025717489 in Epoch 124
Epoch 126
Epoch 126, Loss: 0.00000044478365, Improvement: -0.00000000931928, Best Loss: 0.00000025717489 in Epoch 124
Epoch 127
Epoch 127, Loss: 0.00000043851695, Improvement: -0.00000000626669, Best Loss: 0.00000025717489 in Epoch 124
Epoch 128
Epoch 128, Loss: 0.00000042955431, Improvement: -0.00000000896264, Best Loss: 0.00000025717489 in Epoch 124
Epoch 129
Epoch 129, Loss: 0.00000041655894, Improvement: -0.00000001299537, Best Loss: 0.00000025717489 in Epoch 124
Epoch 130
Epoch 130, Loss: 0.00000041243765, Improvement: -0.00000000412129, Best Loss: 0.00000025717489 in Epoch 124
Epoch 131
Epoch 131, Loss: 0.00000059078293, Improvement: 0.00000017834528, Best Loss: 0.00000025717489 in Epoch 124
Epoch 132
Epoch 132, Loss: 0.00000158856519, Improvement: 0.00000099778226, Best Loss: 0.00000025717489 in Epoch 124
Epoch 133
Epoch 133, Loss: 0.00000052145809, Improvement: -0.00000106710711, Best Loss: 0.00000025717489 in Epoch 124
Epoch 134
Epoch 134, Loss: 0.00000042749053, Improvement: -0.00000009396756, Best Loss: 0.00000025717489 in Epoch 124
Epoch 135
Epoch 135, Loss: 0.00000039494152, Improvement: -0.00000003254901, Best Loss: 0.00000025717489 in Epoch 124
Epoch 136
Epoch 136, Loss: 0.00000038163588, Improvement: -0.00000001330563, Best Loss: 0.00000025717489 in Epoch 124
Epoch 137
Epoch 137, Loss: 0.00000037597404, Improvement: -0.00000000566185, Best Loss: 0.00000025717489 in Epoch 124
Epoch 138
Epoch 138, Loss: 0.00000037054943, Improvement: -0.00000000542460, Best Loss: 0.00000025717489 in Epoch 124
Epoch 139
Epoch 139, Loss: 0.00000038928133, Improvement: 0.00000001873190, Best Loss: 0.00000025717489 in Epoch 124
Epoch 140
Epoch 140, Loss: 0.00000036251653, Improvement: -0.00000002676480, Best Loss: 0.00000025717489 in Epoch 124
Epoch 141
Epoch 141, Loss: 0.00000173724534, Improvement: 0.00000137472881, Best Loss: 0.00000025717489 in Epoch 124
Epoch 142
Epoch 142, Loss: 0.00000077753431, Improvement: -0.00000095971103, Best Loss: 0.00000025717489 in Epoch 124
Epoch 143
Epoch 143, Loss: 0.00000040550589, Improvement: -0.00000037202842, Best Loss: 0.00000025717489 in Epoch 124
Epoch 144
Epoch 144, Loss: 0.00000035828250, Improvement: -0.00000004722339, Best Loss: 0.00000025717489 in Epoch 124
Epoch 145
Epoch 145, Loss: 0.00000034859034, Improvement: -0.00000000969216, Best Loss: 0.00000025717489 in Epoch 124
Epoch 146
Epoch 146, Loss: 0.00000034249214, Improvement: -0.00000000609820, Best Loss: 0.00000025717489 in Epoch 124
Epoch 147
Epoch 147, Loss: 0.00000034659551, Improvement: 0.00000000410336, Best Loss: 0.00000025717489 in Epoch 124
Epoch 148
Epoch 148, Loss: 0.00000035541958, Improvement: 0.00000000882407, Best Loss: 0.00000025717489 in Epoch 124
Epoch 149
Epoch 149, Loss: 0.00000034778325, Improvement: -0.00000000763633, Best Loss: 0.00000025717489 in Epoch 124
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00000049986801, Improvement: 0.00000015208476, Best Loss: 0.00000025717489 in Epoch 124
Epoch 151
Epoch 151, Loss: 0.00000186757891, Improvement: 0.00000136771090, Best Loss: 0.00000025717489 in Epoch 124
Epoch 152
Epoch 152, Loss: 0.00000055319841, Improvement: -0.00000131438050, Best Loss: 0.00000025717489 in Epoch 124
Epoch 153
A best model at epoch 153 has been saved with training error 0.00000025380439.
A best model at epoch 153 has been saved with training error 0.00000025170638.
Epoch 153, Loss: 0.00000033728037, Improvement: -0.00000021591804, Best Loss: 0.00000025170638 in Epoch 153
Epoch 154
A best model at epoch 154 has been saved with training error 0.00000025053271.
A best model at epoch 154 has been saved with training error 0.00000021646383.
Epoch 154, Loss: 0.00000033946225, Improvement: 0.00000000218189, Best Loss: 0.00000021646383 in Epoch 154
Epoch 155
Epoch 155, Loss: 0.00000032335015, Improvement: -0.00000001611210, Best Loss: 0.00000021646383 in Epoch 154
Epoch 156
Epoch 156, Loss: 0.00000141342313, Improvement: 0.00000109007298, Best Loss: 0.00000021646383 in Epoch 154
Epoch 157
Epoch 157, Loss: 0.00000130205936, Improvement: -0.00000011136377, Best Loss: 0.00000021646383 in Epoch 154
Epoch 158
Epoch 158, Loss: 0.00000048826086, Improvement: -0.00000081379850, Best Loss: 0.00000021646383 in Epoch 154
Epoch 159
Epoch 159, Loss: 0.00000036679766, Improvement: -0.00000012146319, Best Loss: 0.00000021646383 in Epoch 154
Epoch 160
Epoch 160, Loss: 0.00000030703402, Improvement: -0.00000005976364, Best Loss: 0.00000021646383 in Epoch 154
Epoch 161
Epoch 161, Loss: 0.00000029496768, Improvement: -0.00000001206634, Best Loss: 0.00000021646383 in Epoch 154
Epoch 162
Epoch 162, Loss: 0.00000029087138, Improvement: -0.00000000409630, Best Loss: 0.00000021646383 in Epoch 154
Epoch 163
Epoch 163, Loss: 0.00000029593123, Improvement: 0.00000000505986, Best Loss: 0.00000021646383 in Epoch 154
Epoch 164
A best model at epoch 164 has been saved with training error 0.00000021496361.
Epoch 164, Loss: 0.00000028803424, Improvement: -0.00000000789700, Best Loss: 0.00000021496361 in Epoch 164
Epoch 165
Epoch 165, Loss: 0.00000087646562, Improvement: 0.00000058843139, Best Loss: 0.00000021496361 in Epoch 164
Epoch 166
Epoch 166, Loss: 0.00000190676269, Improvement: 0.00000103029706, Best Loss: 0.00000021496361 in Epoch 164
Epoch 167
Epoch 167, Loss: 0.00000048383382, Improvement: -0.00000142292886, Best Loss: 0.00000021496361 in Epoch 164
Epoch 168
Epoch 168, Loss: 0.00000029755473, Improvement: -0.00000018627909, Best Loss: 0.00000021496361 in Epoch 164
Epoch 169
A best model at epoch 169 has been saved with training error 0.00000019837307.
Epoch 169, Loss: 0.00000027924208, Improvement: -0.00000001831265, Best Loss: 0.00000019837307 in Epoch 169
Epoch 170
Epoch 170, Loss: 0.00000028179818, Improvement: 0.00000000255611, Best Loss: 0.00000019837307 in Epoch 169
Epoch 171
Epoch 171, Loss: 0.00000026918017, Improvement: -0.00000001261801, Best Loss: 0.00000019837307 in Epoch 169
Epoch 172
Epoch 172, Loss: 0.00000028081962, Improvement: 0.00000001163945, Best Loss: 0.00000019837307 in Epoch 169
Epoch 173
Epoch 173, Loss: 0.00000186707311, Improvement: 0.00000158625348, Best Loss: 0.00000019837307 in Epoch 169
Epoch 174
Epoch 174, Loss: 0.00000102871156, Improvement: -0.00000083836155, Best Loss: 0.00000019837307 in Epoch 169
Epoch 175
Epoch 175, Loss: 0.00000042328820, Improvement: -0.00000060542337, Best Loss: 0.00000019837307 in Epoch 169
Epoch 176
Epoch 176, Loss: 0.00000029639562, Improvement: -0.00000012689258, Best Loss: 0.00000019837307 in Epoch 169
Epoch 177
Epoch 177, Loss: 0.00000028409594, Improvement: -0.00000001229968, Best Loss: 0.00000019837307 in Epoch 169
Epoch 178
Epoch 178, Loss: 0.00000027514134, Improvement: -0.00000000895460, Best Loss: 0.00000019837307 in Epoch 169
Epoch 179
A best model at epoch 179 has been saved with training error 0.00000018938213.
Epoch 179, Loss: 0.00000026541318, Improvement: -0.00000000972816, Best Loss: 0.00000018938213 in Epoch 179
Epoch 180
Epoch 180, Loss: 0.00000177267761, Improvement: 0.00000150726444, Best Loss: 0.00000018938213 in Epoch 179
Epoch 181
Epoch 181, Loss: 0.00000100604253, Improvement: -0.00000076663508, Best Loss: 0.00000018938213 in Epoch 179
Epoch 182
Epoch 182, Loss: 0.00000037220460, Improvement: -0.00000063383793, Best Loss: 0.00000018938213 in Epoch 179
Epoch 183
Epoch 183, Loss: 0.00000026727504, Improvement: -0.00000010492956, Best Loss: 0.00000018938213 in Epoch 179
Epoch 184
A best model at epoch 184 has been saved with training error 0.00000018744171.
Epoch 184, Loss: 0.00000025762134, Improvement: -0.00000000965371, Best Loss: 0.00000018744171 in Epoch 184
Epoch 185
A best model at epoch 185 has been saved with training error 0.00000018296454.
Epoch 185, Loss: 0.00000025075860, Improvement: -0.00000000686274, Best Loss: 0.00000018296454 in Epoch 185
Epoch 186
Epoch 186, Loss: 0.00000024968172, Improvement: -0.00000000107689, Best Loss: 0.00000018296454 in Epoch 185
Epoch 187
Epoch 187, Loss: 0.00000026278889, Improvement: 0.00000001310718, Best Loss: 0.00000018296454 in Epoch 185
Epoch 188
Epoch 188, Loss: 0.00000164113849, Improvement: 0.00000137834960, Best Loss: 0.00000018296454 in Epoch 185
Epoch 189
Epoch 189, Loss: 0.00000127897666, Improvement: -0.00000036216183, Best Loss: 0.00000018296454 in Epoch 185
Epoch 190
A best model at epoch 190 has been saved with training error 0.00000016486005.
Epoch 190, Loss: 0.00000037065714, Improvement: -0.00000090831952, Best Loss: 0.00000016486005 in Epoch 190
Epoch 191
Epoch 191, Loss: 0.00000026101882, Improvement: -0.00000010963832, Best Loss: 0.00000016486005 in Epoch 190
Epoch 192
Epoch 192, Loss: 0.00000024988300, Improvement: -0.00000001113582, Best Loss: 0.00000016486005 in Epoch 190
Epoch 193
Epoch 193, Loss: 0.00000023973779, Improvement: -0.00000001014521, Best Loss: 0.00000016486005 in Epoch 190
Epoch 194
Epoch 194, Loss: 0.00000024011651, Improvement: 0.00000000037873, Best Loss: 0.00000016486005 in Epoch 190
Epoch 195
Epoch 195, Loss: 0.00000106088927, Improvement: 0.00000082077276, Best Loss: 0.00000016486005 in Epoch 190
Epoch 196
Epoch 196, Loss: 0.00000148805035, Improvement: 0.00000042716108, Best Loss: 0.00000016486005 in Epoch 190
Epoch 197
Epoch 197, Loss: 0.00000071590542, Improvement: -0.00000077214493, Best Loss: 0.00000016486005 in Epoch 190
Epoch 198
Epoch 198, Loss: 0.00000041439227, Improvement: -0.00000030151315, Best Loss: 0.00000016486005 in Epoch 190
Epoch 199
Epoch 199, Loss: 0.00000084516866, Improvement: 0.00000043077639, Best Loss: 0.00000016486005 in Epoch 190
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00000185719035, Improvement: 0.00000101202169, Best Loss: 0.00000016486005 in Epoch 190
Epoch 201
Epoch 201, Loss: 0.00000116082894, Improvement: -0.00000069636141, Best Loss: 0.00000016486005 in Epoch 190
Epoch 202
Epoch 202, Loss: 0.00000056031166, Improvement: -0.00000060051728, Best Loss: 0.00000016486005 in Epoch 190
Epoch 203
Epoch 203, Loss: 0.00000037357189, Improvement: -0.00000018673977, Best Loss: 0.00000016486005 in Epoch 190
Epoch 204
Epoch 204, Loss: 0.00000124520944, Improvement: 0.00000087163755, Best Loss: 0.00000016486005 in Epoch 190
Epoch 205
Epoch 205, Loss: 0.00000162618878, Improvement: 0.00000038097934, Best Loss: 0.00000016486005 in Epoch 190
Epoch 206
Epoch 206, Loss: 0.00000072513031, Improvement: -0.00000090105847, Best Loss: 0.00000016486005 in Epoch 190
Epoch 207
Epoch 207, Loss: 0.00000030636519, Improvement: -0.00000041876512, Best Loss: 0.00000016486005 in Epoch 190
Epoch 208
Epoch 208, Loss: 0.00000024643839, Improvement: -0.00000005992680, Best Loss: 0.00000016486005 in Epoch 190
Epoch 209
Epoch 209, Loss: 0.00000023712381, Improvement: -0.00000000931458, Best Loss: 0.00000016486005 in Epoch 190
Epoch 210
A best model at epoch 210 has been saved with training error 0.00000014735558.
Epoch 210, Loss: 0.00000023055669, Improvement: -0.00000000656712, Best Loss: 0.00000014735558 in Epoch 210
Epoch 211
Epoch 211, Loss: 0.00000022701625, Improvement: -0.00000000354044, Best Loss: 0.00000014735558 in Epoch 210
Epoch 212
Epoch 212, Loss: 0.00000033217544, Improvement: 0.00000010515919, Best Loss: 0.00000014735558 in Epoch 210
Epoch 213
Epoch 213, Loss: 0.00000090893450, Improvement: 0.00000057675906, Best Loss: 0.00000014735558 in Epoch 210
Epoch 214
Epoch 214, Loss: 0.00000170526014, Improvement: 0.00000079632564, Best Loss: 0.00000014735558 in Epoch 210
Epoch 215
Epoch 215, Loss: 0.00000040125547, Improvement: -0.00000130400467, Best Loss: 0.00000014735558 in Epoch 210
Epoch 216
Epoch 216, Loss: 0.00000026050367, Improvement: -0.00000014075180, Best Loss: 0.00000014735558 in Epoch 210
Epoch 217
Epoch 217, Loss: 0.00000023533533, Improvement: -0.00000002516833, Best Loss: 0.00000014735558 in Epoch 210
Epoch 218
Epoch 218, Loss: 0.00000041125639, Improvement: 0.00000017592106, Best Loss: 0.00000014735558 in Epoch 210
Epoch 219
Epoch 219, Loss: 0.00000166368368, Improvement: 0.00000125242729, Best Loss: 0.00000014735558 in Epoch 210
Epoch 220
Epoch 220, Loss: 0.00000037701570, Improvement: -0.00000128666798, Best Loss: 0.00000014735558 in Epoch 210
Epoch 221
Epoch 221, Loss: 0.00000025934474, Improvement: -0.00000011767096, Best Loss: 0.00000014735558 in Epoch 210
Epoch 222
Epoch 222, Loss: 0.00000059045913, Improvement: 0.00000033111439, Best Loss: 0.00000014735558 in Epoch 210
Epoch 223
Epoch 223, Loss: 0.00000140562664, Improvement: 0.00000081516751, Best Loss: 0.00000014735558 in Epoch 210
Epoch 224
Epoch 224, Loss: 0.00000075700905, Improvement: -0.00000064861759, Best Loss: 0.00000014735558 in Epoch 210
Epoch 225
Epoch 225, Loss: 0.00000030711535, Improvement: -0.00000044989370, Best Loss: 0.00000014735558 in Epoch 210
Epoch 226
Epoch 226, Loss: 0.00000036376607, Improvement: 0.00000005665072, Best Loss: 0.00000014735558 in Epoch 210
Epoch 227
Epoch 227, Loss: 0.00000026927596, Improvement: -0.00000009449011, Best Loss: 0.00000014735558 in Epoch 210
Epoch 228
Epoch 228, Loss: 0.00000025007244, Improvement: -0.00000001920352, Best Loss: 0.00000014735558 in Epoch 210
Epoch 229
Epoch 229, Loss: 0.00000083375990, Improvement: 0.00000058368746, Best Loss: 0.00000014735558 in Epoch 210
Epoch 230
Epoch 230, Loss: 0.00000156989324, Improvement: 0.00000073613333, Best Loss: 0.00000014735558 in Epoch 210
Epoch 231
Epoch 231, Loss: 0.00000051940313, Improvement: -0.00000105049010, Best Loss: 0.00000014735558 in Epoch 210
Epoch 232
Epoch 232, Loss: 0.00000024367701, Improvement: -0.00000027572612, Best Loss: 0.00000014735558 in Epoch 210
Epoch 233
Epoch 233, Loss: 0.00000023056681, Improvement: -0.00000001311021, Best Loss: 0.00000014735558 in Epoch 210
Epoch 234
Epoch 234, Loss: 0.00000022384997, Improvement: -0.00000000671684, Best Loss: 0.00000014735558 in Epoch 210
Epoch 235
Epoch 235, Loss: 0.00000022712256, Improvement: 0.00000000327259, Best Loss: 0.00000014735558 in Epoch 210
Epoch 236
Epoch 236, Loss: 0.00000023624037, Improvement: 0.00000000911781, Best Loss: 0.00000014735558 in Epoch 210
Epoch 237
Epoch 237, Loss: 0.00000144747761, Improvement: 0.00000121123724, Best Loss: 0.00000014735558 in Epoch 210
Epoch 238
Epoch 238, Loss: 0.00000147665412, Improvement: 0.00000002917651, Best Loss: 0.00000014735558 in Epoch 210
Epoch 239
Epoch 239, Loss: 0.00000051539336, Improvement: -0.00000096126076, Best Loss: 0.00000014735558 in Epoch 210
Epoch 240
Epoch 240, Loss: 0.00000035784001, Improvement: -0.00000015755336, Best Loss: 0.00000014735558 in Epoch 210
Epoch 241
Epoch 241, Loss: 0.00000027573619, Improvement: -0.00000008210382, Best Loss: 0.00000014735558 in Epoch 210
Epoch 242
Epoch 242, Loss: 0.00000053469926, Improvement: 0.00000025896307, Best Loss: 0.00000014735558 in Epoch 210
Epoch 243
Epoch 243, Loss: 0.00000141037054, Improvement: 0.00000087567128, Best Loss: 0.00000014735558 in Epoch 210
Epoch 244
Epoch 244, Loss: 0.00000069814042, Improvement: -0.00000071223012, Best Loss: 0.00000014735558 in Epoch 210
Epoch 245
Epoch 245, Loss: 0.00000028390626, Improvement: -0.00000041423417, Best Loss: 0.00000014735558 in Epoch 210
Epoch 246
A best model at epoch 246 has been saved with training error 0.00000014203206.
Epoch 246, Loss: 0.00000021822587, Improvement: -0.00000006568038, Best Loss: 0.00000014203206 in Epoch 246
Epoch 247
Epoch 247, Loss: 0.00000021001189, Improvement: -0.00000000821398, Best Loss: 0.00000014203206 in Epoch 246
Epoch 248
Epoch 248, Loss: 0.00000020799223, Improvement: -0.00000000201966, Best Loss: 0.00000014203206 in Epoch 246
Epoch 249
A best model at epoch 249 has been saved with training error 0.00000012867915.
Epoch 249, Loss: 0.00000020696903, Improvement: -0.00000000102320, Best Loss: 0.00000012867915 in Epoch 249
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000020599145, Improvement: -0.00000000097758, Best Loss: 0.00000012867915 in Epoch 249
Epoch 251
Epoch 251, Loss: 0.00000020506340, Improvement: -0.00000000092805, Best Loss: 0.00000012867915 in Epoch 249
Epoch 252
Epoch 252, Loss: 0.00000020557123, Improvement: 0.00000000050783, Best Loss: 0.00000012867915 in Epoch 249
Epoch 253
Epoch 253, Loss: 0.00000020419761, Improvement: -0.00000000137361, Best Loss: 0.00000012867915 in Epoch 249
Epoch 254
Epoch 254, Loss: 0.00000020304931, Improvement: -0.00000000114830, Best Loss: 0.00000012867915 in Epoch 249
Epoch 255
Epoch 255, Loss: 0.00000026380254, Improvement: 0.00000006075323, Best Loss: 0.00000012867915 in Epoch 249
Epoch 256
Epoch 256, Loss: 0.00000155536081, Improvement: 0.00000129155826, Best Loss: 0.00000012867915 in Epoch 249
Epoch 257
Epoch 257, Loss: 0.00000040983225, Improvement: -0.00000114552855, Best Loss: 0.00000012867915 in Epoch 249
Epoch 258
Epoch 258, Loss: 0.00000023265228, Improvement: -0.00000017717998, Best Loss: 0.00000012867915 in Epoch 249
Epoch 259
Epoch 259, Loss: 0.00000020939392, Improvement: -0.00000002325836, Best Loss: 0.00000012867915 in Epoch 249
Epoch 260
Epoch 260, Loss: 0.00000020360363, Improvement: -0.00000000579029, Best Loss: 0.00000012867915 in Epoch 249
Epoch 261
Epoch 261, Loss: 0.00000020093994, Improvement: -0.00000000266369, Best Loss: 0.00000012867915 in Epoch 249
Epoch 262
Epoch 262, Loss: 0.00000020080389, Improvement: -0.00000000013605, Best Loss: 0.00000012867915 in Epoch 249
Epoch 263
Epoch 263, Loss: 0.00000019999899, Improvement: -0.00000000080490, Best Loss: 0.00000012867915 in Epoch 249
Epoch 264
Epoch 264, Loss: 0.00000019945420, Improvement: -0.00000000054479, Best Loss: 0.00000012867915 in Epoch 249
Epoch 265
Epoch 265, Loss: 0.00000019907723, Improvement: -0.00000000037697, Best Loss: 0.00000012867915 in Epoch 249
Epoch 266
Epoch 266, Loss: 0.00000037500792, Improvement: 0.00000017593069, Best Loss: 0.00000012867915 in Epoch 249
Epoch 267
Epoch 267, Loss: 0.00000175353856, Improvement: 0.00000137853064, Best Loss: 0.00000012867915 in Epoch 249
Epoch 268
Epoch 268, Loss: 0.00000065892187, Improvement: -0.00000109461669, Best Loss: 0.00000012867915 in Epoch 249
Epoch 269
Epoch 269, Loss: 0.00000039346694, Improvement: -0.00000026545493, Best Loss: 0.00000012867915 in Epoch 249
Epoch 270
Epoch 270, Loss: 0.00000025049889, Improvement: -0.00000014296805, Best Loss: 0.00000012867915 in Epoch 249
Epoch 271
Epoch 271, Loss: 0.00000050860121, Improvement: 0.00000025810232, Best Loss: 0.00000012867915 in Epoch 249
Epoch 272
Epoch 272, Loss: 0.00000134010055, Improvement: 0.00000083149934, Best Loss: 0.00000012867915 in Epoch 249
Epoch 273
Epoch 273, Loss: 0.00000062311466, Improvement: -0.00000071698589, Best Loss: 0.00000012867915 in Epoch 249
Epoch 274
Epoch 274, Loss: 0.00000025769799, Improvement: -0.00000036541668, Best Loss: 0.00000012867915 in Epoch 249
Epoch 275
Epoch 275, Loss: 0.00000020805171, Improvement: -0.00000004964628, Best Loss: 0.00000012867915 in Epoch 249
Epoch 276
Epoch 276, Loss: 0.00000020176063, Improvement: -0.00000000629108, Best Loss: 0.00000012867915 in Epoch 249
Epoch 277
Epoch 277, Loss: 0.00000019887293, Improvement: -0.00000000288770, Best Loss: 0.00000012867915 in Epoch 249
Epoch 278
Epoch 278, Loss: 0.00000019683012, Improvement: -0.00000000204281, Best Loss: 0.00000012867915 in Epoch 249
Epoch 279
A best model at epoch 279 has been saved with training error 0.00000010699380.
Epoch 279, Loss: 0.00000019620705, Improvement: -0.00000000062307, Best Loss: 0.00000010699380 in Epoch 279
Epoch 280
Epoch 280, Loss: 0.00000019585119, Improvement: -0.00000000035586, Best Loss: 0.00000010699380 in Epoch 279
Epoch 281
Epoch 281, Loss: 0.00000019433658, Improvement: -0.00000000151461, Best Loss: 0.00000010699380 in Epoch 279
Epoch 282
Epoch 282, Loss: 0.00000019582930, Improvement: 0.00000000149271, Best Loss: 0.00000010699380 in Epoch 279
Epoch 283
Epoch 283, Loss: 0.00000019420022, Improvement: -0.00000000162907, Best Loss: 0.00000010699380 in Epoch 279
Epoch 284
Epoch 284, Loss: 0.00000019545828, Improvement: 0.00000000125806, Best Loss: 0.00000010699380 in Epoch 279
Epoch 285
Epoch 285, Loss: 0.00000019325124, Improvement: -0.00000000220704, Best Loss: 0.00000010699380 in Epoch 279
Epoch 286
Epoch 286, Loss: 0.00000099025319, Improvement: 0.00000079700195, Best Loss: 0.00000010699380 in Epoch 279
Epoch 287
Epoch 287, Loss: 0.00000068265089, Improvement: -0.00000030760230, Best Loss: 0.00000010699380 in Epoch 279
Epoch 288
Epoch 288, Loss: 0.00000028930356, Improvement: -0.00000039334733, Best Loss: 0.00000010699380 in Epoch 279
Epoch 289
Epoch 289, Loss: 0.00000021334980, Improvement: -0.00000007595376, Best Loss: 0.00000010699380 in Epoch 279
Epoch 290
Epoch 290, Loss: 0.00000020097377, Improvement: -0.00000001237603, Best Loss: 0.00000010699380 in Epoch 279
Epoch 291
Epoch 291, Loss: 0.00000019496750, Improvement: -0.00000000600627, Best Loss: 0.00000010699380 in Epoch 279
Epoch 292
Epoch 292, Loss: 0.00000020196567, Improvement: 0.00000000699816, Best Loss: 0.00000010699380 in Epoch 279
Epoch 293
Epoch 293, Loss: 0.00000019981340, Improvement: -0.00000000215227, Best Loss: 0.00000010699380 in Epoch 279
Epoch 294
Epoch 294, Loss: 0.00000030698668, Improvement: 0.00000010717328, Best Loss: 0.00000010699380 in Epoch 279
Epoch 295
Epoch 295, Loss: 0.00000083854785, Improvement: 0.00000053156116, Best Loss: 0.00000010699380 in Epoch 279
Epoch 296
Epoch 296, Loss: 0.00000109915836, Improvement: 0.00000026061051, Best Loss: 0.00000010699380 in Epoch 279
Epoch 297
Epoch 297, Loss: 0.00000053109063, Improvement: -0.00000056806772, Best Loss: 0.00000010699380 in Epoch 279
Epoch 298
Epoch 298, Loss: 0.00000024226465, Improvement: -0.00000028882598, Best Loss: 0.00000010699380 in Epoch 279
Epoch 299
Epoch 299, Loss: 0.00000019850652, Improvement: -0.00000004375813, Best Loss: 0.00000010699380 in Epoch 279
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000019411302, Improvement: -0.00000000439351, Best Loss: 0.00000010699380 in Epoch 279
Epoch 301
Epoch 301, Loss: 0.00000019268265, Improvement: -0.00000000143037, Best Loss: 0.00000010699380 in Epoch 279
Epoch 302
Epoch 302, Loss: 0.00000019391463, Improvement: 0.00000000123199, Best Loss: 0.00000010699380 in Epoch 279
Epoch 303
A best model at epoch 303 has been saved with training error 0.00000009509390.
Epoch 303, Loss: 0.00000019239479, Improvement: -0.00000000151984, Best Loss: 0.00000009509390 in Epoch 303
Epoch 304
Epoch 304, Loss: 0.00000019094014, Improvement: -0.00000000145465, Best Loss: 0.00000009509390 in Epoch 303
Epoch 305
Epoch 305, Loss: 0.00000031174840, Improvement: 0.00000012080826, Best Loss: 0.00000009509390 in Epoch 303
Epoch 306
Epoch 306, Loss: 0.00000152286465, Improvement: 0.00000121111625, Best Loss: 0.00000009509390 in Epoch 303
Epoch 307
Epoch 307, Loss: 0.00000047190760, Improvement: -0.00000105095705, Best Loss: 0.00000009509390 in Epoch 303
Epoch 308
Epoch 308, Loss: 0.00000023346430, Improvement: -0.00000023844330, Best Loss: 0.00000009509390 in Epoch 303
Epoch 309
Epoch 309, Loss: 0.00000019654651, Improvement: -0.00000003691778, Best Loss: 0.00000009509390 in Epoch 303
Epoch 310
Epoch 310, Loss: 0.00000019393962, Improvement: -0.00000000260689, Best Loss: 0.00000009509390 in Epoch 303
Epoch 311
Epoch 311, Loss: 0.00000019152637, Improvement: -0.00000000241325, Best Loss: 0.00000009509390 in Epoch 303
Epoch 312
Epoch 312, Loss: 0.00000018943157, Improvement: -0.00000000209481, Best Loss: 0.00000009509390 in Epoch 303
Epoch 313
Epoch 313, Loss: 0.00000018919132, Improvement: -0.00000000024024, Best Loss: 0.00000009509390 in Epoch 303
Epoch 314
Epoch 314, Loss: 0.00000018950086, Improvement: 0.00000000030954, Best Loss: 0.00000009509390 in Epoch 303
Epoch 315
Epoch 315, Loss: 0.00000018993770, Improvement: 0.00000000043683, Best Loss: 0.00000009509390 in Epoch 303
Epoch 316
Epoch 316, Loss: 0.00000019140770, Improvement: 0.00000000147000, Best Loss: 0.00000009509390 in Epoch 303
Epoch 317
Epoch 317, Loss: 0.00000030593719, Improvement: 0.00000011452950, Best Loss: 0.00000009509390 in Epoch 303
Epoch 318
Epoch 318, Loss: 0.00000125758176, Improvement: 0.00000095164457, Best Loss: 0.00000009509390 in Epoch 303
Epoch 319
Epoch 319, Loss: 0.00000044227570, Improvement: -0.00000081530606, Best Loss: 0.00000009509390 in Epoch 303
Epoch 320
Epoch 320, Loss: 0.00000022231906, Improvement: -0.00000021995664, Best Loss: 0.00000009509390 in Epoch 303
Epoch 321
Epoch 321, Loss: 0.00000019423044, Improvement: -0.00000002808862, Best Loss: 0.00000009509390 in Epoch 303
Epoch 322
Epoch 322, Loss: 0.00000018788615, Improvement: -0.00000000634429, Best Loss: 0.00000009509390 in Epoch 303
Epoch 323
Epoch 323, Loss: 0.00000018743073, Improvement: -0.00000000045542, Best Loss: 0.00000009509390 in Epoch 303
Epoch 324
Epoch 324, Loss: 0.00000018705784, Improvement: -0.00000000037289, Best Loss: 0.00000009509390 in Epoch 303
Epoch 325
Epoch 325, Loss: 0.00000018786001, Improvement: 0.00000000080217, Best Loss: 0.00000009509390 in Epoch 303
Epoch 326
Epoch 326, Loss: 0.00000018649722, Improvement: -0.00000000136279, Best Loss: 0.00000009509390 in Epoch 303
Epoch 327
Epoch 327, Loss: 0.00000018957144, Improvement: 0.00000000307422, Best Loss: 0.00000009509390 in Epoch 303
Epoch 328
Epoch 328, Loss: 0.00000066032933, Improvement: 0.00000047075788, Best Loss: 0.00000009509390 in Epoch 303
Epoch 329
Epoch 329, Loss: 0.00000103107974, Improvement: 0.00000037075041, Best Loss: 0.00000009509390 in Epoch 303
Epoch 330
Epoch 330, Loss: 0.00000030951491, Improvement: -0.00000072156483, Best Loss: 0.00000009509390 in Epoch 303
Epoch 331
Epoch 331, Loss: 0.00000020193814, Improvement: -0.00000010757677, Best Loss: 0.00000009509390 in Epoch 303
Epoch 332
Epoch 332, Loss: 0.00000018741121, Improvement: -0.00000001452693, Best Loss: 0.00000009509390 in Epoch 303
Epoch 333
Epoch 333, Loss: 0.00000018630069, Improvement: -0.00000000111052, Best Loss: 0.00000009509390 in Epoch 303
Epoch 334
Epoch 334, Loss: 0.00000018509931, Improvement: -0.00000000120137, Best Loss: 0.00000009509390 in Epoch 303
Epoch 335
Epoch 335, Loss: 0.00000018548662, Improvement: 0.00000000038731, Best Loss: 0.00000009509390 in Epoch 303
Epoch 336
Epoch 336, Loss: 0.00000018450496, Improvement: -0.00000000098166, Best Loss: 0.00000009509390 in Epoch 303
Epoch 337
Epoch 337, Loss: 0.00000018420207, Improvement: -0.00000000030289, Best Loss: 0.00000009509390 in Epoch 303
Epoch 338
Epoch 338, Loss: 0.00000018763646, Improvement: 0.00000000343439, Best Loss: 0.00000009509390 in Epoch 303
Epoch 339
Epoch 339, Loss: 0.00000018520890, Improvement: -0.00000000242755, Best Loss: 0.00000009509390 in Epoch 303
Epoch 340
Epoch 340, Loss: 0.00000019812020, Improvement: 0.00000001291129, Best Loss: 0.00000009509390 in Epoch 303
Epoch 341
Epoch 341, Loss: 0.00000095063320, Improvement: 0.00000075251301, Best Loss: 0.00000009509390 in Epoch 303
Epoch 342
Epoch 342, Loss: 0.00000038946975, Improvement: -0.00000056116345, Best Loss: 0.00000009509390 in Epoch 303
Epoch 343
Epoch 343, Loss: 0.00000027111372, Improvement: -0.00000011835603, Best Loss: 0.00000009509390 in Epoch 303
Epoch 344
Epoch 344, Loss: 0.00000019316225, Improvement: -0.00000007795147, Best Loss: 0.00000009509390 in Epoch 303
Epoch 345
Epoch 345, Loss: 0.00000018708781, Improvement: -0.00000000607444, Best Loss: 0.00000009509390 in Epoch 303
Epoch 346
Epoch 346, Loss: 0.00000018542900, Improvement: -0.00000000165881, Best Loss: 0.00000009509390 in Epoch 303
Epoch 347
Epoch 347, Loss: 0.00000018698909, Improvement: 0.00000000156009, Best Loss: 0.00000009509390 in Epoch 303
Epoch 348
Epoch 348, Loss: 0.00000018391725, Improvement: -0.00000000307184, Best Loss: 0.00000009509390 in Epoch 303
Epoch 349
Epoch 349, Loss: 0.00000046457731, Improvement: 0.00000028066006, Best Loss: 0.00000009509390 in Epoch 303
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000034030507, Improvement: -0.00000012427225, Best Loss: 0.00000009509390 in Epoch 303
Epoch 351
Epoch 351, Loss: 0.00000061181882, Improvement: 0.00000027151376, Best Loss: 0.00000009509390 in Epoch 303
Epoch 352
Epoch 352, Loss: 0.00000092969651, Improvement: 0.00000031787769, Best Loss: 0.00000009509390 in Epoch 303
Epoch 353
Epoch 353, Loss: 0.00000037124196, Improvement: -0.00000055845455, Best Loss: 0.00000009509390 in Epoch 303
Epoch 354
Epoch 354, Loss: 0.00000023625128, Improvement: -0.00000013499067, Best Loss: 0.00000009509390 in Epoch 303
Epoch 355
Epoch 355, Loss: 0.00000019431377, Improvement: -0.00000004193751, Best Loss: 0.00000009509390 in Epoch 303
Epoch 356
Epoch 356, Loss: 0.00000018632553, Improvement: -0.00000000798824, Best Loss: 0.00000009509390 in Epoch 303
Epoch 357
Epoch 357, Loss: 0.00000018560574, Improvement: -0.00000000071979, Best Loss: 0.00000009509390 in Epoch 303
Epoch 358
Epoch 358, Loss: 0.00000018262318, Improvement: -0.00000000298255, Best Loss: 0.00000009509390 in Epoch 303
Epoch 359
Epoch 359, Loss: 0.00000055932294, Improvement: 0.00000037669975, Best Loss: 0.00000009509390 in Epoch 303
Epoch 360
Epoch 360, Loss: 0.00000086188207, Improvement: 0.00000030255913, Best Loss: 0.00000009509390 in Epoch 303
Epoch 361
Epoch 361, Loss: 0.00000028104071, Improvement: -0.00000058084136, Best Loss: 0.00000009509390 in Epoch 303
Epoch 362
Epoch 362, Loss: 0.00000020800778, Improvement: -0.00000007303293, Best Loss: 0.00000009509390 in Epoch 303
Epoch 363
Epoch 363, Loss: 0.00000018997349, Improvement: -0.00000001803429, Best Loss: 0.00000009509390 in Epoch 303
Epoch 364
Epoch 364, Loss: 0.00000018642905, Improvement: -0.00000000354444, Best Loss: 0.00000009509390 in Epoch 303
Epoch 365
Epoch 365, Loss: 0.00000018435710, Improvement: -0.00000000207196, Best Loss: 0.00000009509390 in Epoch 303
Epoch 366
Epoch 366, Loss: 0.00000018276027, Improvement: -0.00000000159682, Best Loss: 0.00000009509390 in Epoch 303
Epoch 367
Epoch 367, Loss: 0.00000018244074, Improvement: -0.00000000031953, Best Loss: 0.00000009509390 in Epoch 303
Epoch 368
Epoch 368, Loss: 0.00000018182028, Improvement: -0.00000000062046, Best Loss: 0.00000009509390 in Epoch 303
Epoch 369
Epoch 369, Loss: 0.00000018135519, Improvement: -0.00000000046509, Best Loss: 0.00000009509390 in Epoch 303
Epoch 370
Epoch 370, Loss: 0.00000018166541, Improvement: 0.00000000031022, Best Loss: 0.00000009509390 in Epoch 303
Epoch 371
Epoch 371, Loss: 0.00000018196964, Improvement: 0.00000000030423, Best Loss: 0.00000009509390 in Epoch 303
Epoch 372
Epoch 372, Loss: 0.00000018865077, Improvement: 0.00000000668112, Best Loss: 0.00000009509390 in Epoch 303
Epoch 373
Epoch 373, Loss: 0.00000045959172, Improvement: 0.00000027094095, Best Loss: 0.00000009509390 in Epoch 303
Epoch 374
Epoch 374, Loss: 0.00000077831430, Improvement: 0.00000031872258, Best Loss: 0.00000009509390 in Epoch 303
Epoch 375
Epoch 375, Loss: 0.00000068347273, Improvement: -0.00000009484157, Best Loss: 0.00000009509390 in Epoch 303
Epoch 376
Epoch 376, Loss: 0.00000026310127, Improvement: -0.00000042037146, Best Loss: 0.00000009509390 in Epoch 303
Epoch 377
Epoch 377, Loss: 0.00000019097004, Improvement: -0.00000007213122, Best Loss: 0.00000009509390 in Epoch 303
Epoch 378
Epoch 378, Loss: 0.00000018301160, Improvement: -0.00000000795844, Best Loss: 0.00000009509390 in Epoch 303
Epoch 379
Epoch 379, Loss: 0.00000018222051, Improvement: -0.00000000079109, Best Loss: 0.00000009509390 in Epoch 303
Epoch 380
Epoch 380, Loss: 0.00000018468019, Improvement: 0.00000000245968, Best Loss: 0.00000009509390 in Epoch 303
Epoch 381
Epoch 381, Loss: 0.00000019127942, Improvement: 0.00000000659923, Best Loss: 0.00000009509390 in Epoch 303
Epoch 382
Epoch 382, Loss: 0.00000077111540, Improvement: 0.00000057983598, Best Loss: 0.00000009509390 in Epoch 303
Epoch 383
Epoch 383, Loss: 0.00000060121121, Improvement: -0.00000016990419, Best Loss: 0.00000009509390 in Epoch 303
Epoch 384
Epoch 384, Loss: 0.00000025810145, Improvement: -0.00000034310976, Best Loss: 0.00000009509390 in Epoch 303
Epoch 385
Epoch 385, Loss: 0.00000019249107, Improvement: -0.00000006561038, Best Loss: 0.00000009509390 in Epoch 303
Epoch 386
Epoch 386, Loss: 0.00000018393272, Improvement: -0.00000000855835, Best Loss: 0.00000009509390 in Epoch 303
Epoch 387
Epoch 387, Loss: 0.00000018138997, Improvement: -0.00000000254275, Best Loss: 0.00000009509390 in Epoch 303
Epoch 388
Epoch 388, Loss: 0.00000018383478, Improvement: 0.00000000244482, Best Loss: 0.00000009509390 in Epoch 303
Epoch 389
Epoch 389, Loss: 0.00000018502024, Improvement: 0.00000000118546, Best Loss: 0.00000009509390 in Epoch 303
Epoch 390
Epoch 390, Loss: 0.00000018231791, Improvement: -0.00000000270233, Best Loss: 0.00000009509390 in Epoch 303
Epoch 391
Epoch 391, Loss: 0.00000018041597, Improvement: -0.00000000190194, Best Loss: 0.00000009509390 in Epoch 303
Epoch 392
Epoch 392, Loss: 0.00000017902672, Improvement: -0.00000000138925, Best Loss: 0.00000009509390 in Epoch 303
Epoch 393
Epoch 393, Loss: 0.00000018641640, Improvement: 0.00000000738968, Best Loss: 0.00000009509390 in Epoch 303
Epoch 394
Epoch 394, Loss: 0.00000018455741, Improvement: -0.00000000185899, Best Loss: 0.00000009509390 in Epoch 303
Epoch 395
Epoch 395, Loss: 0.00000018272580, Improvement: -0.00000000183161, Best Loss: 0.00000009509390 in Epoch 303
Epoch 396
Epoch 396, Loss: 0.00000018404533, Improvement: 0.00000000131953, Best Loss: 0.00000009509390 in Epoch 303
Epoch 397
Epoch 397, Loss: 0.00000045294708, Improvement: 0.00000026890175, Best Loss: 0.00000009509390 in Epoch 303
Epoch 398
Epoch 398, Loss: 0.00000050416550, Improvement: 0.00000005121842, Best Loss: 0.00000009509390 in Epoch 303
Epoch 399
Epoch 399, Loss: 0.00000020310820, Improvement: -0.00000030105729, Best Loss: 0.00000009509390 in Epoch 303
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000018165511, Improvement: -0.00000002145309, Best Loss: 0.00000009509390 in Epoch 303
Epoch 401
Epoch 401, Loss: 0.00000018022727, Improvement: -0.00000000142785, Best Loss: 0.00000009509390 in Epoch 303
Epoch 402
Epoch 402, Loss: 0.00000018135185, Improvement: 0.00000000112459, Best Loss: 0.00000009509390 in Epoch 303
Epoch 403
Epoch 403, Loss: 0.00000018339000, Improvement: 0.00000000203814, Best Loss: 0.00000009509390 in Epoch 303
Epoch 404
Epoch 404, Loss: 0.00000031540181, Improvement: 0.00000013201182, Best Loss: 0.00000009509390 in Epoch 303
Epoch 405
Epoch 405, Loss: 0.00000055997541, Improvement: 0.00000024457360, Best Loss: 0.00000009509390 in Epoch 303
Epoch 406
Epoch 406, Loss: 0.00000039388462, Improvement: -0.00000016609079, Best Loss: 0.00000009509390 in Epoch 303
Epoch 407
Epoch 407, Loss: 0.00000089274318, Improvement: 0.00000049885855, Best Loss: 0.00000009509390 in Epoch 303
Epoch 408
Epoch 408, Loss: 0.00000049433073, Improvement: -0.00000039841244, Best Loss: 0.00000009509390 in Epoch 303
Epoch 409
Epoch 409, Loss: 0.00000023019945, Improvement: -0.00000026413129, Best Loss: 0.00000009509390 in Epoch 303
Epoch 410
Epoch 410, Loss: 0.00000018401633, Improvement: -0.00000004618312, Best Loss: 0.00000009509390 in Epoch 303
Epoch 411
Epoch 411, Loss: 0.00000018078802, Improvement: -0.00000000322831, Best Loss: 0.00000009509390 in Epoch 303
Epoch 412
Epoch 412, Loss: 0.00000018031248, Improvement: -0.00000000047554, Best Loss: 0.00000009509390 in Epoch 303
Epoch 413
Epoch 413, Loss: 0.00000017907041, Improvement: -0.00000000124207, Best Loss: 0.00000009509390 in Epoch 303
Epoch 414
Epoch 414, Loss: 0.00000017739747, Improvement: -0.00000000167293, Best Loss: 0.00000009509390 in Epoch 303
Epoch 415
Epoch 415, Loss: 0.00000017901484, Improvement: 0.00000000161736, Best Loss: 0.00000009509390 in Epoch 303
Epoch 416
Epoch 416, Loss: 0.00000017933315, Improvement: 0.00000000031831, Best Loss: 0.00000009509390 in Epoch 303
Epoch 417
Epoch 417, Loss: 0.00000017792543, Improvement: -0.00000000140772, Best Loss: 0.00000009509390 in Epoch 303
Epoch 418
Epoch 418, Loss: 0.00000017979221, Improvement: 0.00000000186679, Best Loss: 0.00000009509390 in Epoch 303
Epoch 419
Epoch 419, Loss: 0.00000017914528, Improvement: -0.00000000064693, Best Loss: 0.00000009509390 in Epoch 303
Epoch 420
Epoch 420, Loss: 0.00000018269275, Improvement: 0.00000000354747, Best Loss: 0.00000009509390 in Epoch 303
Epoch 421
Epoch 421, Loss: 0.00000019116176, Improvement: 0.00000000846901, Best Loss: 0.00000009509390 in Epoch 303
Epoch 422
Epoch 422, Loss: 0.00000047386116, Improvement: 0.00000028269939, Best Loss: 0.00000009509390 in Epoch 303
Epoch 423
Epoch 423, Loss: 0.00000073679307, Improvement: 0.00000026293191, Best Loss: 0.00000009509390 in Epoch 303
Epoch 424
Epoch 424, Loss: 0.00000024362544, Improvement: -0.00000049316763, Best Loss: 0.00000009509390 in Epoch 303
Epoch 425
Epoch 425, Loss: 0.00000018936836, Improvement: -0.00000005425707, Best Loss: 0.00000009509390 in Epoch 303
Epoch 426
Epoch 426, Loss: 0.00000017990757, Improvement: -0.00000000946079, Best Loss: 0.00000009509390 in Epoch 303
Epoch 427
Epoch 427, Loss: 0.00000017822615, Improvement: -0.00000000168142, Best Loss: 0.00000009509390 in Epoch 303
Epoch 428
Epoch 428, Loss: 0.00000017689770, Improvement: -0.00000000132845, Best Loss: 0.00000009509390 in Epoch 303
Epoch 429
Epoch 429, Loss: 0.00000017673886, Improvement: -0.00000000015885, Best Loss: 0.00000009509390 in Epoch 303
Epoch 430
Epoch 430, Loss: 0.00000017718231, Improvement: 0.00000000044345, Best Loss: 0.00000009509390 in Epoch 303
Epoch 431
Epoch 431, Loss: 0.00000017614562, Improvement: -0.00000000103669, Best Loss: 0.00000009509390 in Epoch 303
Epoch 432
Epoch 432, Loss: 0.00000017547209, Improvement: -0.00000000067353, Best Loss: 0.00000009509390 in Epoch 303
Epoch 433
Epoch 433, Loss: 0.00000017551999, Improvement: 0.00000000004790, Best Loss: 0.00000009509390 in Epoch 303
Epoch 434
Epoch 434, Loss: 0.00000017794288, Improvement: 0.00000000242288, Best Loss: 0.00000009509390 in Epoch 303
Epoch 435
Epoch 435, Loss: 0.00000017522718, Improvement: -0.00000000271570, Best Loss: 0.00000009509390 in Epoch 303
Epoch 436
Epoch 436, Loss: 0.00000017492732, Improvement: -0.00000000029986, Best Loss: 0.00000009509390 in Epoch 303
Epoch 437
Epoch 437, Loss: 0.00000017396905, Improvement: -0.00000000095827, Best Loss: 0.00000009509390 in Epoch 303
Epoch 438
Epoch 438, Loss: 0.00000035345648, Improvement: 0.00000017948743, Best Loss: 0.00000009509390 in Epoch 303
Epoch 439
Epoch 439, Loss: 0.00000022675663, Improvement: -0.00000012669985, Best Loss: 0.00000009509390 in Epoch 303
Epoch 440
Epoch 440, Loss: 0.00000021307740, Improvement: -0.00000001367923, Best Loss: 0.00000009509390 in Epoch 303
Epoch 441
Epoch 441, Loss: 0.00000021928803, Improvement: 0.00000000621063, Best Loss: 0.00000009509390 in Epoch 303
Epoch 442
Epoch 442, Loss: 0.00000025519473, Improvement: 0.00000003590670, Best Loss: 0.00000009509390 in Epoch 303
Epoch 443
Epoch 443, Loss: 0.00000082521177, Improvement: 0.00000057001704, Best Loss: 0.00000009509390 in Epoch 303
Epoch 444
Epoch 444, Loss: 0.00000026637019, Improvement: -0.00000055884158, Best Loss: 0.00000009509390 in Epoch 303
Epoch 445
Epoch 445, Loss: 0.00000018283120, Improvement: -0.00000008353898, Best Loss: 0.00000009509390 in Epoch 303
Epoch 446
Epoch 446, Loss: 0.00000018016717, Improvement: -0.00000000266403, Best Loss: 0.00000009509390 in Epoch 303
Epoch 447
Epoch 447, Loss: 0.00000017953088, Improvement: -0.00000000063629, Best Loss: 0.00000009509390 in Epoch 303
Epoch 448
Epoch 448, Loss: 0.00000017781846, Improvement: -0.00000000171242, Best Loss: 0.00000009509390 in Epoch 303
Epoch 449
Epoch 449, Loss: 0.00000017888468, Improvement: 0.00000000106622, Best Loss: 0.00000009509390 in Epoch 303
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000017735798, Improvement: -0.00000000152670, Best Loss: 0.00000009509390 in Epoch 303
Epoch 451
Epoch 451, Loss: 0.00000017658127, Improvement: -0.00000000077671, Best Loss: 0.00000009509390 in Epoch 303
Epoch 452
Epoch 452, Loss: 0.00000018047442, Improvement: 0.00000000389315, Best Loss: 0.00000009509390 in Epoch 303
Epoch 453
Epoch 453, Loss: 0.00000054558454, Improvement: 0.00000036511013, Best Loss: 0.00000009509390 in Epoch 303
Epoch 454
Epoch 454, Loss: 0.00000035433719, Improvement: -0.00000019124735, Best Loss: 0.00000009509390 in Epoch 303
Epoch 455
Epoch 455, Loss: 0.00000034580442, Improvement: -0.00000000853277, Best Loss: 0.00000009509390 in Epoch 303
Epoch 456
Epoch 456, Loss: 0.00000022977482, Improvement: -0.00000011602960, Best Loss: 0.00000009509390 in Epoch 303
Epoch 457
Epoch 457, Loss: 0.00000018220635, Improvement: -0.00000004756847, Best Loss: 0.00000009509390 in Epoch 303
Epoch 458
Epoch 458, Loss: 0.00000017591227, Improvement: -0.00000000629409, Best Loss: 0.00000009509390 in Epoch 303
Epoch 459
Epoch 459, Loss: 0.00000017631615, Improvement: 0.00000000040388, Best Loss: 0.00000009509390 in Epoch 303
Epoch 460
Epoch 460, Loss: 0.00000017885150, Improvement: 0.00000000253535, Best Loss: 0.00000009509390 in Epoch 303
Epoch 461
Epoch 461, Loss: 0.00000019942951, Improvement: 0.00000002057801, Best Loss: 0.00000009509390 in Epoch 303
Epoch 462
Epoch 462, Loss: 0.00000073057288, Improvement: 0.00000053114337, Best Loss: 0.00000009509390 in Epoch 303
Epoch 463
Epoch 463, Loss: 0.00000034691171, Improvement: -0.00000038366117, Best Loss: 0.00000009509390 in Epoch 303
Epoch 464
Epoch 464, Loss: 0.00000020778477, Improvement: -0.00000013912694, Best Loss: 0.00000009509390 in Epoch 303
Epoch 465
Epoch 465, Loss: 0.00000018364866, Improvement: -0.00000002413611, Best Loss: 0.00000009509390 in Epoch 303
Epoch 466
Epoch 466, Loss: 0.00000018487597, Improvement: 0.00000000122731, Best Loss: 0.00000009509390 in Epoch 303
Epoch 467
Epoch 467, Loss: 0.00000017718469, Improvement: -0.00000000769128, Best Loss: 0.00000009509390 in Epoch 303
Epoch 468
Epoch 468, Loss: 0.00000017474280, Improvement: -0.00000000244189, Best Loss: 0.00000009509390 in Epoch 303
Epoch 469
Epoch 469, Loss: 0.00000017446119, Improvement: -0.00000000028161, Best Loss: 0.00000009509390 in Epoch 303
Epoch 470
Epoch 470, Loss: 0.00000017488248, Improvement: 0.00000000042129, Best Loss: 0.00000009509390 in Epoch 303
Epoch 471
Epoch 471, Loss: 0.00000017621934, Improvement: 0.00000000133685, Best Loss: 0.00000009509390 in Epoch 303
Epoch 472
Epoch 472, Loss: 0.00000017949275, Improvement: 0.00000000327341, Best Loss: 0.00000009509390 in Epoch 303
Epoch 473
Epoch 473, Loss: 0.00000019399125, Improvement: 0.00000001449850, Best Loss: 0.00000009509390 in Epoch 303
Epoch 474
Epoch 474, Loss: 0.00000018982752, Improvement: -0.00000000416373, Best Loss: 0.00000009509390 in Epoch 303
Epoch 475
Epoch 475, Loss: 0.00000023882670, Improvement: 0.00000004899917, Best Loss: 0.00000009509390 in Epoch 303
Epoch 476
Epoch 476, Loss: 0.00000019032626, Improvement: -0.00000004850044, Best Loss: 0.00000009509390 in Epoch 303
Epoch 477
Epoch 477, Loss: 0.00000018645255, Improvement: -0.00000000387371, Best Loss: 0.00000009509390 in Epoch 303
Epoch 478
Epoch 478, Loss: 0.00000067966896, Improvement: 0.00000049321641, Best Loss: 0.00000009509390 in Epoch 303
Epoch 479
Epoch 479, Loss: 0.00000027738325, Improvement: -0.00000040228571, Best Loss: 0.00000009509390 in Epoch 303
Epoch 480
Epoch 480, Loss: 0.00000018871121, Improvement: -0.00000008867204, Best Loss: 0.00000009509390 in Epoch 303
Epoch 481
Epoch 481, Loss: 0.00000017799815, Improvement: -0.00000001071306, Best Loss: 0.00000009509390 in Epoch 303
Epoch 482
Epoch 482, Loss: 0.00000017296644, Improvement: -0.00000000503171, Best Loss: 0.00000009509390 in Epoch 303
Epoch 483
Epoch 483, Loss: 0.00000017254095, Improvement: -0.00000000042549, Best Loss: 0.00000009509390 in Epoch 303
Epoch 484
Epoch 484, Loss: 0.00000017393277, Improvement: 0.00000000139182, Best Loss: 0.00000009509390 in Epoch 303
Epoch 485
Epoch 485, Loss: 0.00000017383301, Improvement: -0.00000000009976, Best Loss: 0.00000009509390 in Epoch 303
Epoch 486
Epoch 486, Loss: 0.00000017269350, Improvement: -0.00000000113951, Best Loss: 0.00000009509390 in Epoch 303
Epoch 487
Epoch 487, Loss: 0.00000017277910, Improvement: 0.00000000008560, Best Loss: 0.00000009509390 in Epoch 303
Epoch 488
Epoch 488, Loss: 0.00000018691785, Improvement: 0.00000001413875, Best Loss: 0.00000009509390 in Epoch 303
Epoch 489
Epoch 489, Loss: 0.00000039950276, Improvement: 0.00000021258491, Best Loss: 0.00000009509390 in Epoch 303
Epoch 490
Epoch 490, Loss: 0.00000068103349, Improvement: 0.00000028153073, Best Loss: 0.00000009509390 in Epoch 303
Epoch 491
Epoch 491, Loss: 0.00000023302890, Improvement: -0.00000044800459, Best Loss: 0.00000009509390 in Epoch 303
Epoch 492
Epoch 492, Loss: 0.00000019027490, Improvement: -0.00000004275401, Best Loss: 0.00000009509390 in Epoch 303
Epoch 493
Epoch 493, Loss: 0.00000018450981, Improvement: -0.00000000576508, Best Loss: 0.00000009509390 in Epoch 303
Epoch 494
Epoch 494, Loss: 0.00000017584852, Improvement: -0.00000000866129, Best Loss: 0.00000009509390 in Epoch 303
Epoch 495
Epoch 495, Loss: 0.00000017372967, Improvement: -0.00000000211885, Best Loss: 0.00000009509390 in Epoch 303
Epoch 496
Epoch 496, Loss: 0.00000017167483, Improvement: -0.00000000205484, Best Loss: 0.00000009509390 in Epoch 303
Epoch 497
Epoch 497, Loss: 0.00000017134829, Improvement: -0.00000000032654, Best Loss: 0.00000009509390 in Epoch 303
Epoch 498
Epoch 498, Loss: 0.00000017131730, Improvement: -0.00000000003098, Best Loss: 0.00000009509390 in Epoch 303
Epoch 499
Epoch 499, Loss: 0.00000017112497, Improvement: -0.00000000019233, Best Loss: 0.00000009509390 in Epoch 303
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000016979438, Improvement: -0.00000000133059, Best Loss: 0.00000009509390 in Epoch 303
Epoch 501
Epoch 501, Loss: 0.00000017239465, Improvement: 0.00000000260027, Best Loss: 0.00000009509390 in Epoch 303
Epoch 502
Epoch 502, Loss: 0.00000017011145, Improvement: -0.00000000228320, Best Loss: 0.00000009509390 in Epoch 303
Epoch 503
Epoch 503, Loss: 0.00000017324646, Improvement: 0.00000000313501, Best Loss: 0.00000009509390 in Epoch 303
Epoch 504
Epoch 504, Loss: 0.00000041953488, Improvement: 0.00000024628842, Best Loss: 0.00000009509390 in Epoch 303
Epoch 505
Epoch 505, Loss: 0.00000053546942, Improvement: 0.00000011593454, Best Loss: 0.00000009509390 in Epoch 303
Epoch 506
Epoch 506, Loss: 0.00000020075048, Improvement: -0.00000033471894, Best Loss: 0.00000009509390 in Epoch 303
Epoch 507
Epoch 507, Loss: 0.00000017453601, Improvement: -0.00000002621447, Best Loss: 0.00000009509390 in Epoch 303
Epoch 508
Epoch 508, Loss: 0.00000017042839, Improvement: -0.00000000410762, Best Loss: 0.00000009509390 in Epoch 303
Epoch 509
Epoch 509, Loss: 0.00000017024529, Improvement: -0.00000000018310, Best Loss: 0.00000009509390 in Epoch 303
Epoch 510
Epoch 510, Loss: 0.00000016973397, Improvement: -0.00000000051132, Best Loss: 0.00000009509390 in Epoch 303
Epoch 511
Epoch 511, Loss: 0.00000016969616, Improvement: -0.00000000003781, Best Loss: 0.00000009509390 in Epoch 303
Epoch 512
Epoch 512, Loss: 0.00000016875312, Improvement: -0.00000000094304, Best Loss: 0.00000009509390 in Epoch 303
Epoch 513
Epoch 513, Loss: 0.00000016829573, Improvement: -0.00000000045739, Best Loss: 0.00000009509390 in Epoch 303
Epoch 514
Epoch 514, Loss: 0.00000017135846, Improvement: 0.00000000306273, Best Loss: 0.00000009509390 in Epoch 303
Epoch 515
Epoch 515, Loss: 0.00000020718997, Improvement: 0.00000003583151, Best Loss: 0.00000009509390 in Epoch 303
Epoch 516
Epoch 516, Loss: 0.00000027295059, Improvement: 0.00000006576062, Best Loss: 0.00000009509390 in Epoch 303
Epoch 517
Epoch 517, Loss: 0.00000072186750, Improvement: 0.00000044891691, Best Loss: 0.00000009509390 in Epoch 303
Epoch 518
Epoch 518, Loss: 0.00000056793151, Improvement: -0.00000015393599, Best Loss: 0.00000009509390 in Epoch 303
Epoch 519
Epoch 519, Loss: 0.00000019609605, Improvement: -0.00000037183546, Best Loss: 0.00000009509390 in Epoch 303
Epoch 520
Epoch 520, Loss: 0.00000017513908, Improvement: -0.00000002095698, Best Loss: 0.00000009509390 in Epoch 303
Epoch 521
Epoch 521, Loss: 0.00000017028441, Improvement: -0.00000000485466, Best Loss: 0.00000009509390 in Epoch 303
Epoch 522
Epoch 522, Loss: 0.00000016943114, Improvement: -0.00000000085328, Best Loss: 0.00000009509390 in Epoch 303
Epoch 523
Epoch 523, Loss: 0.00000016853513, Improvement: -0.00000000089601, Best Loss: 0.00000009509390 in Epoch 303
Epoch 524
Epoch 524, Loss: 0.00000016870078, Improvement: 0.00000000016565, Best Loss: 0.00000009509390 in Epoch 303
Epoch 525
Epoch 525, Loss: 0.00000016875162, Improvement: 0.00000000005085, Best Loss: 0.00000009509390 in Epoch 303
Epoch 526
Epoch 526, Loss: 0.00000016854351, Improvement: -0.00000000020811, Best Loss: 0.00000009509390 in Epoch 303
Epoch 527
Epoch 527, Loss: 0.00000016768228, Improvement: -0.00000000086123, Best Loss: 0.00000009509390 in Epoch 303
Epoch 528
Epoch 528, Loss: 0.00000016735440, Improvement: -0.00000000032789, Best Loss: 0.00000009509390 in Epoch 303
Epoch 529
Epoch 529, Loss: 0.00000016987645, Improvement: 0.00000000252205, Best Loss: 0.00000009509390 in Epoch 303
Epoch 530
Epoch 530, Loss: 0.00000017716494, Improvement: 0.00000000728849, Best Loss: 0.00000009509390 in Epoch 303
Epoch 531
Epoch 531, Loss: 0.00000016789232, Improvement: -0.00000000927262, Best Loss: 0.00000009509390 in Epoch 303
Epoch 532
Epoch 532, Loss: 0.00000016719230, Improvement: -0.00000000070001, Best Loss: 0.00000009509390 in Epoch 303
Epoch 533
Epoch 533, Loss: 0.00000016701844, Improvement: -0.00000000017386, Best Loss: 0.00000009509390 in Epoch 303
Epoch 534
Epoch 534, Loss: 0.00000016628130, Improvement: -0.00000000073715, Best Loss: 0.00000009509390 in Epoch 303
Epoch 535
Epoch 535, Loss: 0.00000016741278, Improvement: 0.00000000113148, Best Loss: 0.00000009509390 in Epoch 303
Epoch 536
Epoch 536, Loss: 0.00000016947516, Improvement: 0.00000000206238, Best Loss: 0.00000009509390 in Epoch 303
Epoch 537
Epoch 537, Loss: 0.00000016828999, Improvement: -0.00000000118517, Best Loss: 0.00000009509390 in Epoch 303
Epoch 538
Epoch 538, Loss: 0.00000016705684, Improvement: -0.00000000123315, Best Loss: 0.00000009509390 in Epoch 303
Epoch 539
Epoch 539, Loss: 0.00000042483724, Improvement: 0.00000025778040, Best Loss: 0.00000009509390 in Epoch 303
Epoch 540
Epoch 540, Loss: 0.00000031557236, Improvement: -0.00000010926488, Best Loss: 0.00000009509390 in Epoch 303
Epoch 541
Epoch 541, Loss: 0.00000081464552, Improvement: 0.00000049907316, Best Loss: 0.00000009509390 in Epoch 303
Epoch 542
Epoch 542, Loss: 0.00000029887328, Improvement: -0.00000051577224, Best Loss: 0.00000009509390 in Epoch 303
Epoch 543
Epoch 543, Loss: 0.00000019308283, Improvement: -0.00000010579045, Best Loss: 0.00000009509390 in Epoch 303
Epoch 544
Epoch 544, Loss: 0.00000016986009, Improvement: -0.00000002322274, Best Loss: 0.00000009509390 in Epoch 303
Epoch 545
Epoch 545, Loss: 0.00000016944298, Improvement: -0.00000000041711, Best Loss: 0.00000009509390 in Epoch 303
Epoch 546
Epoch 546, Loss: 0.00000016759856, Improvement: -0.00000000184443, Best Loss: 0.00000009509390 in Epoch 303
Epoch 547
Epoch 547, Loss: 0.00000016804972, Improvement: 0.00000000045117, Best Loss: 0.00000009509390 in Epoch 303
Epoch 548
Epoch 548, Loss: 0.00000016871008, Improvement: 0.00000000066035, Best Loss: 0.00000009509390 in Epoch 303
Epoch 549
Epoch 549, Loss: 0.00000016713835, Improvement: -0.00000000157172, Best Loss: 0.00000009509390 in Epoch 303
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000016896389, Improvement: 0.00000000182554, Best Loss: 0.00000009509390 in Epoch 303
Epoch 551
Epoch 551, Loss: 0.00000016794670, Improvement: -0.00000000101719, Best Loss: 0.00000009509390 in Epoch 303
Epoch 552
Epoch 552, Loss: 0.00000016662445, Improvement: -0.00000000132225, Best Loss: 0.00000009509390 in Epoch 303
Epoch 553
Epoch 553, Loss: 0.00000016561628, Improvement: -0.00000000100817, Best Loss: 0.00000009509390 in Epoch 303
Epoch 554
Epoch 554, Loss: 0.00000016726313, Improvement: 0.00000000164686, Best Loss: 0.00000009509390 in Epoch 303
Epoch 555
Epoch 555, Loss: 0.00000016915787, Improvement: 0.00000000189474, Best Loss: 0.00000009509390 in Epoch 303
Epoch 556
Epoch 556, Loss: 0.00000016573608, Improvement: -0.00000000342180, Best Loss: 0.00000009509390 in Epoch 303
Epoch 557
Epoch 557, Loss: 0.00000016457359, Improvement: -0.00000000116248, Best Loss: 0.00000009509390 in Epoch 303
Epoch 558
Epoch 558, Loss: 0.00000016536546, Improvement: 0.00000000079186, Best Loss: 0.00000009509390 in Epoch 303
Epoch 559
Epoch 559, Loss: 0.00000016518982, Improvement: -0.00000000017564, Best Loss: 0.00000009509390 in Epoch 303
Epoch 560
Epoch 560, Loss: 0.00000016675863, Improvement: 0.00000000156881, Best Loss: 0.00000009509390 in Epoch 303
Epoch 561
Epoch 561, Loss: 0.00000017060407, Improvement: 0.00000000384544, Best Loss: 0.00000009509390 in Epoch 303
Epoch 562
Epoch 562, Loss: 0.00000020238197, Improvement: 0.00000003177790, Best Loss: 0.00000009509390 in Epoch 303
Epoch 563
Epoch 563, Loss: 0.00000017597413, Improvement: -0.00000002640784, Best Loss: 0.00000009509390 in Epoch 303
Epoch 564
Epoch 564, Loss: 0.00000016740424, Improvement: -0.00000000856989, Best Loss: 0.00000009509390 in Epoch 303
Epoch 565
Epoch 565, Loss: 0.00000016903451, Improvement: 0.00000000163027, Best Loss: 0.00000009509390 in Epoch 303
Epoch 566
Epoch 566, Loss: 0.00000017252074, Improvement: 0.00000000348623, Best Loss: 0.00000009509390 in Epoch 303
Epoch 567
Epoch 567, Loss: 0.00000064236738, Improvement: 0.00000046984664, Best Loss: 0.00000009509390 in Epoch 303
Epoch 568
Epoch 568, Loss: 0.00000055850279, Improvement: -0.00000008386459, Best Loss: 0.00000009509390 in Epoch 303
Epoch 569
Epoch 569, Loss: 0.00000021184859, Improvement: -0.00000034665420, Best Loss: 0.00000009509390 in Epoch 303
Epoch 570
Epoch 570, Loss: 0.00000017609476, Improvement: -0.00000003575382, Best Loss: 0.00000009509390 in Epoch 303
Epoch 571
Epoch 571, Loss: 0.00000016855485, Improvement: -0.00000000753991, Best Loss: 0.00000009509390 in Epoch 303
Epoch 572
A best model at epoch 572 has been saved with training error 0.00000009184015.
Epoch 572, Loss: 0.00000016621915, Improvement: -0.00000000233570, Best Loss: 0.00000009184015 in Epoch 572
Epoch 573
Epoch 573, Loss: 0.00000016624992, Improvement: 0.00000000003077, Best Loss: 0.00000009184015 in Epoch 572
Epoch 574
Epoch 574, Loss: 0.00000016566756, Improvement: -0.00000000058235, Best Loss: 0.00000009184015 in Epoch 572
Epoch 575
Epoch 575, Loss: 0.00000016604772, Improvement: 0.00000000038016, Best Loss: 0.00000009184015 in Epoch 572
Epoch 576
Epoch 576, Loss: 0.00000016518203, Improvement: -0.00000000086569, Best Loss: 0.00000009184015 in Epoch 572
Epoch 577
Epoch 577, Loss: 0.00000016506757, Improvement: -0.00000000011446, Best Loss: 0.00000009184015 in Epoch 572
Epoch 578
Epoch 578, Loss: 0.00000016632731, Improvement: 0.00000000125974, Best Loss: 0.00000009184015 in Epoch 572
Epoch 579
Epoch 579, Loss: 0.00000016604297, Improvement: -0.00000000028434, Best Loss: 0.00000009184015 in Epoch 572
Epoch 580
Epoch 580, Loss: 0.00000016525286, Improvement: -0.00000000079011, Best Loss: 0.00000009184015 in Epoch 572
Epoch 581
Epoch 581, Loss: 0.00000016816989, Improvement: 0.00000000291703, Best Loss: 0.00000009184015 in Epoch 572
Epoch 582
Epoch 582, Loss: 0.00000016696275, Improvement: -0.00000000120714, Best Loss: 0.00000009184015 in Epoch 572
Epoch 583
Epoch 583, Loss: 0.00000016492342, Improvement: -0.00000000203933, Best Loss: 0.00000009184015 in Epoch 572
Epoch 584
Epoch 584, Loss: 0.00000016456449, Improvement: -0.00000000035893, Best Loss: 0.00000009184015 in Epoch 572
Epoch 585
Epoch 585, Loss: 0.00000016705457, Improvement: 0.00000000249008, Best Loss: 0.00000009184015 in Epoch 572
Epoch 586
Epoch 586, Loss: 0.00000016451771, Improvement: -0.00000000253686, Best Loss: 0.00000009184015 in Epoch 572
Epoch 587
Epoch 587, Loss: 0.00000016566834, Improvement: 0.00000000115063, Best Loss: 0.00000009184015 in Epoch 572
Epoch 588
Epoch 588, Loss: 0.00000016937966, Improvement: 0.00000000371132, Best Loss: 0.00000009184015 in Epoch 572
Epoch 589
Epoch 589, Loss: 0.00000016542492, Improvement: -0.00000000395474, Best Loss: 0.00000009184015 in Epoch 572
Epoch 590
Epoch 590, Loss: 0.00000016866324, Improvement: 0.00000000323831, Best Loss: 0.00000009184015 in Epoch 572
Epoch 591
Epoch 591, Loss: 0.00000018595338, Improvement: 0.00000001729014, Best Loss: 0.00000009184015 in Epoch 572
Epoch 592
Epoch 592, Loss: 0.00000017499715, Improvement: -0.00000001095623, Best Loss: 0.00000009184015 in Epoch 572
Epoch 593
Epoch 593, Loss: 0.00000017211793, Improvement: -0.00000000287921, Best Loss: 0.00000009184015 in Epoch 572
Epoch 594
Epoch 594, Loss: 0.00000020810776, Improvement: 0.00000003598983, Best Loss: 0.00000009184015 in Epoch 572
Epoch 595
Epoch 595, Loss: 0.00000032736520, Improvement: 0.00000011925744, Best Loss: 0.00000009184015 in Epoch 572
Epoch 596
Epoch 596, Loss: 0.00000030949638, Improvement: -0.00000001786881, Best Loss: 0.00000009184015 in Epoch 572
Epoch 597
Epoch 597, Loss: 0.00000036205375, Improvement: 0.00000005255737, Best Loss: 0.00000009184015 in Epoch 572
Epoch 598
Epoch 598, Loss: 0.00000042784472, Improvement: 0.00000006579097, Best Loss: 0.00000009184015 in Epoch 572
Epoch 599
Epoch 599, Loss: 0.00000021100337, Improvement: -0.00000021684135, Best Loss: 0.00000009184015 in Epoch 572
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000017067633, Improvement: -0.00000004032704, Best Loss: 0.00000009184015 in Epoch 572
Epoch 601
Epoch 601, Loss: 0.00000016690795, Improvement: -0.00000000376838, Best Loss: 0.00000009184015 in Epoch 572
Epoch 602
Epoch 602, Loss: 0.00000016485969, Improvement: -0.00000000204826, Best Loss: 0.00000009184015 in Epoch 572
Epoch 603
Epoch 603, Loss: 0.00000016414298, Improvement: -0.00000000071671, Best Loss: 0.00000009184015 in Epoch 572
Epoch 604
Epoch 604, Loss: 0.00000016659959, Improvement: 0.00000000245661, Best Loss: 0.00000009184015 in Epoch 572
Epoch 605
Epoch 605, Loss: 0.00000016544670, Improvement: -0.00000000115288, Best Loss: 0.00000009184015 in Epoch 572
Epoch 606
Epoch 606, Loss: 0.00000016974652, Improvement: 0.00000000429982, Best Loss: 0.00000009184015 in Epoch 572
Epoch 607
Epoch 607, Loss: 0.00000016886590, Improvement: -0.00000000088062, Best Loss: 0.00000009184015 in Epoch 572
Epoch 608
Epoch 608, Loss: 0.00000018326357, Improvement: 0.00000001439767, Best Loss: 0.00000009184015 in Epoch 572
Epoch 609
Epoch 609, Loss: 0.00000034433689, Improvement: 0.00000016107332, Best Loss: 0.00000009184015 in Epoch 572
Epoch 610
Epoch 610, Loss: 0.00000042380523, Improvement: 0.00000007946834, Best Loss: 0.00000009184015 in Epoch 572
Epoch 611
Epoch 611, Loss: 0.00000019064015, Improvement: -0.00000023316508, Best Loss: 0.00000009184015 in Epoch 572
Epoch 612
Epoch 612, Loss: 0.00000016931927, Improvement: -0.00000002132088, Best Loss: 0.00000009184015 in Epoch 572
Epoch 613
Epoch 613, Loss: 0.00000016469428, Improvement: -0.00000000462499, Best Loss: 0.00000009184015 in Epoch 572
Epoch 614
Epoch 614, Loss: 0.00000016512719, Improvement: 0.00000000043291, Best Loss: 0.00000009184015 in Epoch 572
Epoch 615
Epoch 615, Loss: 0.00000016413916, Improvement: -0.00000000098803, Best Loss: 0.00000009184015 in Epoch 572
Epoch 616
Epoch 616, Loss: 0.00000016418804, Improvement: 0.00000000004889, Best Loss: 0.00000009184015 in Epoch 572
Epoch 617
Epoch 617, Loss: 0.00000016393254, Improvement: -0.00000000025550, Best Loss: 0.00000009184015 in Epoch 572
Epoch 618
Epoch 618, Loss: 0.00000017588627, Improvement: 0.00000001195373, Best Loss: 0.00000009184015 in Epoch 572
Epoch 619
Epoch 619, Loss: 0.00000044075552, Improvement: 0.00000026486925, Best Loss: 0.00000009184015 in Epoch 572
Epoch 620
Epoch 620, Loss: 0.00000058585855, Improvement: 0.00000014510304, Best Loss: 0.00000009184015 in Epoch 572
Epoch 621
Epoch 621, Loss: 0.00000038026193, Improvement: -0.00000020559662, Best Loss: 0.00000009184015 in Epoch 572
Epoch 622
Epoch 622, Loss: 0.00000019995954, Improvement: -0.00000018030240, Best Loss: 0.00000009184015 in Epoch 572
Epoch 623
Epoch 623, Loss: 0.00000017441434, Improvement: -0.00000002554519, Best Loss: 0.00000009184015 in Epoch 572
Epoch 624
A best model at epoch 624 has been saved with training error 0.00000009092277.
Epoch 624, Loss: 0.00000016757681, Improvement: -0.00000000683754, Best Loss: 0.00000009092277 in Epoch 624
Epoch 625
Epoch 625, Loss: 0.00000016574459, Improvement: -0.00000000183222, Best Loss: 0.00000009092277 in Epoch 624
Epoch 626
Epoch 626, Loss: 0.00000016590320, Improvement: 0.00000000015861, Best Loss: 0.00000009092277 in Epoch 624
Epoch 627
Epoch 627, Loss: 0.00000016858625, Improvement: 0.00000000268304, Best Loss: 0.00000009092277 in Epoch 624
Epoch 628
Epoch 628, Loss: 0.00000016625809, Improvement: -0.00000000232816, Best Loss: 0.00000009092277 in Epoch 624
Epoch 629
Epoch 629, Loss: 0.00000016763069, Improvement: 0.00000000137260, Best Loss: 0.00000009092277 in Epoch 624
Epoch 630
Epoch 630, Loss: 0.00000016516163, Improvement: -0.00000000246906, Best Loss: 0.00000009092277 in Epoch 624
Epoch 631
Epoch 631, Loss: 0.00000016693855, Improvement: 0.00000000177692, Best Loss: 0.00000009092277 in Epoch 624
Epoch 632
Epoch 632, Loss: 0.00000017120566, Improvement: 0.00000000426711, Best Loss: 0.00000009092277 in Epoch 624
Epoch 633
Epoch 633, Loss: 0.00000017603224, Improvement: 0.00000000482659, Best Loss: 0.00000009092277 in Epoch 624
Epoch 634
Epoch 634, Loss: 0.00000026504838, Improvement: 0.00000008901614, Best Loss: 0.00000009092277 in Epoch 624
Epoch 635
Epoch 635, Loss: 0.00000021713442, Improvement: -0.00000004791396, Best Loss: 0.00000009092277 in Epoch 624
Epoch 636
Epoch 636, Loss: 0.00000018466155, Improvement: -0.00000003247287, Best Loss: 0.00000009092277 in Epoch 624
Epoch 637
Epoch 637, Loss: 0.00000036327015, Improvement: 0.00000017860860, Best Loss: 0.00000009092277 in Epoch 624
Epoch 638
Epoch 638, Loss: 0.00000049370837, Improvement: 0.00000013043822, Best Loss: 0.00000009092277 in Epoch 624
Epoch 639
Epoch 639, Loss: 0.00000023665572, Improvement: -0.00000025705265, Best Loss: 0.00000009092277 in Epoch 624
Epoch 640
Epoch 640, Loss: 0.00000017786706, Improvement: -0.00000005878866, Best Loss: 0.00000009092277 in Epoch 624
Epoch 641
Epoch 641, Loss: 0.00000017826059, Improvement: 0.00000000039353, Best Loss: 0.00000009092277 in Epoch 624
Epoch 642
Epoch 642, Loss: 0.00000018899179, Improvement: 0.00000001073120, Best Loss: 0.00000009092277 in Epoch 624
Epoch 643
Epoch 643, Loss: 0.00000017370511, Improvement: -0.00000001528669, Best Loss: 0.00000009092277 in Epoch 624
Epoch 644
Epoch 644, Loss: 0.00000016573804, Improvement: -0.00000000796707, Best Loss: 0.00000009092277 in Epoch 624
Epoch 645
Epoch 645, Loss: 0.00000016498029, Improvement: -0.00000000075774, Best Loss: 0.00000009092277 in Epoch 624
Epoch 646
Epoch 646, Loss: 0.00000016625376, Improvement: 0.00000000127347, Best Loss: 0.00000009092277 in Epoch 624
Epoch 647
Epoch 647, Loss: 0.00000016693235, Improvement: 0.00000000067859, Best Loss: 0.00000009092277 in Epoch 624
Epoch 648
Epoch 648, Loss: 0.00000016577116, Improvement: -0.00000000116119, Best Loss: 0.00000009092277 in Epoch 624
Epoch 649
Epoch 649, Loss: 0.00000016582205, Improvement: 0.00000000005088, Best Loss: 0.00000009092277 in Epoch 624
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000016586835, Improvement: 0.00000000004630, Best Loss: 0.00000009092277 in Epoch 624
Epoch 651
Epoch 651, Loss: 0.00000016467738, Improvement: -0.00000000119096, Best Loss: 0.00000009092277 in Epoch 624
Epoch 652
Epoch 652, Loss: 0.00000018668787, Improvement: 0.00000002201048, Best Loss: 0.00000009092277 in Epoch 624
Epoch 653
Epoch 653, Loss: 0.00000020721539, Improvement: 0.00000002052752, Best Loss: 0.00000009092277 in Epoch 624
Epoch 654
Epoch 654, Loss: 0.00000018703030, Improvement: -0.00000002018509, Best Loss: 0.00000009092277 in Epoch 624
Epoch 655
Epoch 655, Loss: 0.00000037490023, Improvement: 0.00000018786993, Best Loss: 0.00000009092277 in Epoch 624
Epoch 656
Epoch 656, Loss: 0.00000063254677, Improvement: 0.00000025764654, Best Loss: 0.00000009092277 in Epoch 624
Epoch 657
Epoch 657, Loss: 0.00000026511615, Improvement: -0.00000036743063, Best Loss: 0.00000009092277 in Epoch 624
Epoch 658
Epoch 658, Loss: 0.00000018932605, Improvement: -0.00000007579010, Best Loss: 0.00000009092277 in Epoch 624
Epoch 659
Epoch 659, Loss: 0.00000017075352, Improvement: -0.00000001857253, Best Loss: 0.00000009092277 in Epoch 624
Epoch 660
Epoch 660, Loss: 0.00000016702735, Improvement: -0.00000000372616, Best Loss: 0.00000009092277 in Epoch 624
Epoch 661
Epoch 661, Loss: 0.00000016838577, Improvement: 0.00000000135842, Best Loss: 0.00000009092277 in Epoch 624
Epoch 662
Epoch 662, Loss: 0.00000016638310, Improvement: -0.00000000200267, Best Loss: 0.00000009092277 in Epoch 624
Epoch 663
Epoch 663, Loss: 0.00000016613047, Improvement: -0.00000000025262, Best Loss: 0.00000009092277 in Epoch 624
Epoch 664
Epoch 664, Loss: 0.00000016639310, Improvement: 0.00000000026262, Best Loss: 0.00000009092277 in Epoch 624
Epoch 665
Epoch 665, Loss: 0.00000016590223, Improvement: -0.00000000049087, Best Loss: 0.00000009092277 in Epoch 624
Epoch 666
Epoch 666, Loss: 0.00000016412296, Improvement: -0.00000000177927, Best Loss: 0.00000009092277 in Epoch 624
Epoch 667
Epoch 667, Loss: 0.00000016527214, Improvement: 0.00000000114919, Best Loss: 0.00000009092277 in Epoch 624
Epoch 668
Epoch 668, Loss: 0.00000016729262, Improvement: 0.00000000202048, Best Loss: 0.00000009092277 in Epoch 624
Epoch 669
Epoch 669, Loss: 0.00000017626362, Improvement: 0.00000000897100, Best Loss: 0.00000009092277 in Epoch 624
Epoch 670
Epoch 670, Loss: 0.00000019149936, Improvement: 0.00000001523574, Best Loss: 0.00000009092277 in Epoch 624
Epoch 671
Epoch 671, Loss: 0.00000044048617, Improvement: 0.00000024898681, Best Loss: 0.00000009092277 in Epoch 624
Epoch 672
Epoch 672, Loss: 0.00000040409388, Improvement: -0.00000003639229, Best Loss: 0.00000009092277 in Epoch 624
Epoch 673
Epoch 673, Loss: 0.00000019639477, Improvement: -0.00000020769911, Best Loss: 0.00000009092277 in Epoch 624
Epoch 674
Epoch 674, Loss: 0.00000016773457, Improvement: -0.00000002866020, Best Loss: 0.00000009092277 in Epoch 624
Epoch 675
Epoch 675, Loss: 0.00000016772311, Improvement: -0.00000000001146, Best Loss: 0.00000009092277 in Epoch 624
Epoch 676
Epoch 676, Loss: 0.00000016782359, Improvement: 0.00000000010048, Best Loss: 0.00000009092277 in Epoch 624
Epoch 677
Epoch 677, Loss: 0.00000016347513, Improvement: -0.00000000434846, Best Loss: 0.00000009092277 in Epoch 624
Epoch 678
Epoch 678, Loss: 0.00000016217131, Improvement: -0.00000000130382, Best Loss: 0.00000009092277 in Epoch 624
Epoch 679
Epoch 679, Loss: 0.00000016468153, Improvement: 0.00000000251022, Best Loss: 0.00000009092277 in Epoch 624
Epoch 680
Epoch 680, Loss: 0.00000016363722, Improvement: -0.00000000104431, Best Loss: 0.00000009092277 in Epoch 624
Epoch 681
Epoch 681, Loss: 0.00000016643713, Improvement: 0.00000000279991, Best Loss: 0.00000009092277 in Epoch 624
Epoch 682
Epoch 682, Loss: 0.00000016485504, Improvement: -0.00000000158209, Best Loss: 0.00000009092277 in Epoch 624
Epoch 683
Epoch 683, Loss: 0.00000016385119, Improvement: -0.00000000100385, Best Loss: 0.00000009092277 in Epoch 624
Epoch 684
Epoch 684, Loss: 0.00000016844326, Improvement: 0.00000000459207, Best Loss: 0.00000009092277 in Epoch 624
Epoch 685
Epoch 685, Loss: 0.00000016696021, Improvement: -0.00000000148304, Best Loss: 0.00000009092277 in Epoch 624
Epoch 686
Epoch 686, Loss: 0.00000017128192, Improvement: 0.00000000432171, Best Loss: 0.00000009092277 in Epoch 624
Epoch 687
Epoch 687, Loss: 0.00000018395526, Improvement: 0.00000001267334, Best Loss: 0.00000009092277 in Epoch 624
Epoch 688
Epoch 688, Loss: 0.00000018799091, Improvement: 0.00000000403565, Best Loss: 0.00000009092277 in Epoch 624
Epoch 689
Epoch 689, Loss: 0.00000018519507, Improvement: -0.00000000279584, Best Loss: 0.00000009092277 in Epoch 624
Epoch 690
Epoch 690, Loss: 0.00000016995560, Improvement: -0.00000001523947, Best Loss: 0.00000009092277 in Epoch 624
Epoch 691
Epoch 691, Loss: 0.00000018389632, Improvement: 0.00000001394072, Best Loss: 0.00000009092277 in Epoch 624
Epoch 692
Epoch 692, Loss: 0.00000019162248, Improvement: 0.00000000772616, Best Loss: 0.00000009092277 in Epoch 624
Epoch 693
Epoch 693, Loss: 0.00000043147917, Improvement: 0.00000023985669, Best Loss: 0.00000009092277 in Epoch 624
Epoch 694
Epoch 694, Loss: 0.00000027242009, Improvement: -0.00000015905908, Best Loss: 0.00000009092277 in Epoch 624
Epoch 695
Epoch 695, Loss: 0.00000018903233, Improvement: -0.00000008338776, Best Loss: 0.00000009092277 in Epoch 624
Epoch 696
Epoch 696, Loss: 0.00000016519435, Improvement: -0.00000002383799, Best Loss: 0.00000009092277 in Epoch 624
Epoch 697
Epoch 697, Loss: 0.00000016398090, Improvement: -0.00000000121345, Best Loss: 0.00000009092277 in Epoch 624
Epoch 698
Epoch 698, Loss: 0.00000016333149, Improvement: -0.00000000064940, Best Loss: 0.00000009092277 in Epoch 624
Epoch 699
Epoch 699, Loss: 0.00000016625193, Improvement: 0.00000000292044, Best Loss: 0.00000009092277 in Epoch 624
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000016643917, Improvement: 0.00000000018724, Best Loss: 0.00000009092277 in Epoch 624
Epoch 701
Epoch 701, Loss: 0.00000016742859, Improvement: 0.00000000098942, Best Loss: 0.00000009092277 in Epoch 624
Epoch 702
Epoch 702, Loss: 0.00000016616661, Improvement: -0.00000000126198, Best Loss: 0.00000009092277 in Epoch 624
Epoch 703
Epoch 703, Loss: 0.00000017283375, Improvement: 0.00000000666714, Best Loss: 0.00000009092277 in Epoch 624
Epoch 704
Epoch 704, Loss: 0.00000031325586, Improvement: 0.00000014042211, Best Loss: 0.00000009092277 in Epoch 624
Epoch 705
Epoch 705, Loss: 0.00000058039285, Improvement: 0.00000026713699, Best Loss: 0.00000009092277 in Epoch 624
Epoch 706
Epoch 706, Loss: 0.00000037449183, Improvement: -0.00000020590102, Best Loss: 0.00000009092277 in Epoch 624
Epoch 707
Epoch 707, Loss: 0.00000020965645, Improvement: -0.00000016483538, Best Loss: 0.00000009092277 in Epoch 624
Epoch 708
Epoch 708, Loss: 0.00000016893445, Improvement: -0.00000004072200, Best Loss: 0.00000009092277 in Epoch 624
Epoch 709
Epoch 709, Loss: 0.00000016286703, Improvement: -0.00000000606743, Best Loss: 0.00000009092277 in Epoch 624
Epoch 710
Epoch 710, Loss: 0.00000016278713, Improvement: -0.00000000007990, Best Loss: 0.00000009092277 in Epoch 624
Epoch 711
Epoch 711, Loss: 0.00000016127304, Improvement: -0.00000000151409, Best Loss: 0.00000009092277 in Epoch 624
Epoch 712
Epoch 712, Loss: 0.00000016193659, Improvement: 0.00000000066355, Best Loss: 0.00000009092277 in Epoch 624
Epoch 713
Epoch 713, Loss: 0.00000016486423, Improvement: 0.00000000292764, Best Loss: 0.00000009092277 in Epoch 624
Epoch 714
Epoch 714, Loss: 0.00000016639285, Improvement: 0.00000000152862, Best Loss: 0.00000009092277 in Epoch 624
Epoch 715
Epoch 715, Loss: 0.00000016188055, Improvement: -0.00000000451230, Best Loss: 0.00000009092277 in Epoch 624
Epoch 716
Epoch 716, Loss: 0.00000016287378, Improvement: 0.00000000099323, Best Loss: 0.00000009092277 in Epoch 624
Epoch 717
Epoch 717, Loss: 0.00000016349639, Improvement: 0.00000000062262, Best Loss: 0.00000009092277 in Epoch 624
Epoch 718
Epoch 718, Loss: 0.00000016452337, Improvement: 0.00000000102697, Best Loss: 0.00000009092277 in Epoch 624
Epoch 719
Epoch 719, Loss: 0.00000016759337, Improvement: 0.00000000307000, Best Loss: 0.00000009092277 in Epoch 624
Epoch 720
Epoch 720, Loss: 0.00000017088425, Improvement: 0.00000000329088, Best Loss: 0.00000009092277 in Epoch 624
Epoch 721
Epoch 721, Loss: 0.00000022389130, Improvement: 0.00000005300705, Best Loss: 0.00000009092277 in Epoch 624
Epoch 722
Epoch 722, Loss: 0.00000018229159, Improvement: -0.00000004159971, Best Loss: 0.00000009092277 in Epoch 624
Epoch 723
Epoch 723, Loss: 0.00000020276685, Improvement: 0.00000002047526, Best Loss: 0.00000009092277 in Epoch 624
Epoch 724
Epoch 724, Loss: 0.00000031623151, Improvement: 0.00000011346465, Best Loss: 0.00000009092277 in Epoch 624
Epoch 725
Epoch 725, Loss: 0.00000051485541, Improvement: 0.00000019862390, Best Loss: 0.00000009092277 in Epoch 624
Epoch 726
Epoch 726, Loss: 0.00000027708589, Improvement: -0.00000023776951, Best Loss: 0.00000009092277 in Epoch 624
Epoch 727
Epoch 727, Loss: 0.00000018779349, Improvement: -0.00000008929240, Best Loss: 0.00000009092277 in Epoch 624
Epoch 728
Epoch 728, Loss: 0.00000018062660, Improvement: -0.00000000716689, Best Loss: 0.00000009092277 in Epoch 624
Epoch 729
Epoch 729, Loss: 0.00000016379031, Improvement: -0.00000001683629, Best Loss: 0.00000009092277 in Epoch 624
Epoch 730
A best model at epoch 730 has been saved with training error 0.00000008941115.
Epoch 730, Loss: 0.00000016536288, Improvement: 0.00000000157256, Best Loss: 0.00000008941115 in Epoch 730
Epoch 731
Epoch 731, Loss: 0.00000016660539, Improvement: 0.00000000124251, Best Loss: 0.00000008941115 in Epoch 730
Epoch 732
Epoch 732, Loss: 0.00000016337618, Improvement: -0.00000000322920, Best Loss: 0.00000008941115 in Epoch 730
Epoch 733
Epoch 733, Loss: 0.00000016305255, Improvement: -0.00000000032364, Best Loss: 0.00000008941115 in Epoch 730
Epoch 734
Epoch 734, Loss: 0.00000016504916, Improvement: 0.00000000199661, Best Loss: 0.00000008941115 in Epoch 730
Epoch 735
Epoch 735, Loss: 0.00000018021105, Improvement: 0.00000001516189, Best Loss: 0.00000008941115 in Epoch 730
Epoch 736
Epoch 736, Loss: 0.00000019134338, Improvement: 0.00000001113233, Best Loss: 0.00000008941115 in Epoch 730
Epoch 737
Epoch 737, Loss: 0.00000017246766, Improvement: -0.00000001887572, Best Loss: 0.00000008941115 in Epoch 730
Epoch 738
Epoch 738, Loss: 0.00000018886940, Improvement: 0.00000001640174, Best Loss: 0.00000008941115 in Epoch 730
Epoch 739
Epoch 739, Loss: 0.00000021284979, Improvement: 0.00000002398039, Best Loss: 0.00000008941115 in Epoch 730
Epoch 740
Epoch 740, Loss: 0.00000050818096, Improvement: 0.00000029533117, Best Loss: 0.00000008941115 in Epoch 730
Epoch 741
Epoch 741, Loss: 0.00000023431233, Improvement: -0.00000027386862, Best Loss: 0.00000008941115 in Epoch 730
Epoch 742
Epoch 742, Loss: 0.00000018401530, Improvement: -0.00000005029703, Best Loss: 0.00000008941115 in Epoch 730
Epoch 743
Epoch 743, Loss: 0.00000016490869, Improvement: -0.00000001910661, Best Loss: 0.00000008941115 in Epoch 730
Epoch 744
Epoch 744, Loss: 0.00000016427534, Improvement: -0.00000000063335, Best Loss: 0.00000008941115 in Epoch 730
Epoch 745
Epoch 745, Loss: 0.00000016419968, Improvement: -0.00000000007566, Best Loss: 0.00000008941115 in Epoch 730
Epoch 746
Epoch 746, Loss: 0.00000016909567, Improvement: 0.00000000489599, Best Loss: 0.00000008941115 in Epoch 730
Epoch 747
A best model at epoch 747 has been saved with training error 0.00000008885957.
Epoch 747, Loss: 0.00000016837019, Improvement: -0.00000000072547, Best Loss: 0.00000008885957 in Epoch 747
Epoch 748
Epoch 748, Loss: 0.00000016246218, Improvement: -0.00000000590801, Best Loss: 0.00000008885957 in Epoch 747
Epoch 749
Epoch 749, Loss: 0.00000016407859, Improvement: 0.00000000161640, Best Loss: 0.00000008885957 in Epoch 747
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000016457854, Improvement: 0.00000000049995, Best Loss: 0.00000008885957 in Epoch 747
Epoch 751
Epoch 751, Loss: 0.00000016416034, Improvement: -0.00000000041820, Best Loss: 0.00000008885957 in Epoch 747
Epoch 752
Epoch 752, Loss: 0.00000016515847, Improvement: 0.00000000099813, Best Loss: 0.00000008885957 in Epoch 747
Epoch 753
Epoch 753, Loss: 0.00000030964670, Improvement: 0.00000014448823, Best Loss: 0.00000008885957 in Epoch 747
Epoch 754
Epoch 754, Loss: 0.00000024874819, Improvement: -0.00000006089851, Best Loss: 0.00000008885957 in Epoch 747
Epoch 755
Epoch 755, Loss: 0.00000021318921, Improvement: -0.00000003555898, Best Loss: 0.00000008885957 in Epoch 747
Epoch 756
Epoch 756, Loss: 0.00000021541483, Improvement: 0.00000000222563, Best Loss: 0.00000008885957 in Epoch 747
Epoch 757
Epoch 757, Loss: 0.00000020495532, Improvement: -0.00000001045952, Best Loss: 0.00000008885957 in Epoch 747
Epoch 758
Epoch 758, Loss: 0.00000017739120, Improvement: -0.00000002756411, Best Loss: 0.00000008885957 in Epoch 747
Epoch 759
Epoch 759, Loss: 0.00000017839664, Improvement: 0.00000000100544, Best Loss: 0.00000008885957 in Epoch 747
Epoch 760
Epoch 760, Loss: 0.00000020168167, Improvement: 0.00000002328503, Best Loss: 0.00000008885957 in Epoch 747
Epoch 761
Epoch 761, Loss: 0.00000030131065, Improvement: 0.00000009962898, Best Loss: 0.00000008885957 in Epoch 747
Epoch 762
Epoch 762, Loss: 0.00000049179089, Improvement: 0.00000019048024, Best Loss: 0.00000008885957 in Epoch 747
Epoch 763
Epoch 763, Loss: 0.00000030925787, Improvement: -0.00000018253302, Best Loss: 0.00000008885957 in Epoch 747
Epoch 764
Epoch 764, Loss: 0.00000018802422, Improvement: -0.00000012123364, Best Loss: 0.00000008885957 in Epoch 747
Epoch 765
Epoch 765, Loss: 0.00000016750661, Improvement: -0.00000002051761, Best Loss: 0.00000008885957 in Epoch 747
Epoch 766
Epoch 766, Loss: 0.00000017975099, Improvement: 0.00000001224438, Best Loss: 0.00000008885957 in Epoch 747
Epoch 767
Epoch 767, Loss: 0.00000017675536, Improvement: -0.00000000299564, Best Loss: 0.00000008885957 in Epoch 747
Epoch 768
Epoch 768, Loss: 0.00000017205896, Improvement: -0.00000000469640, Best Loss: 0.00000008885957 in Epoch 747
Epoch 769
Epoch 769, Loss: 0.00000016922980, Improvement: -0.00000000282916, Best Loss: 0.00000008885957 in Epoch 747
Epoch 770
Epoch 770, Loss: 0.00000016356157, Improvement: -0.00000000566823, Best Loss: 0.00000008885957 in Epoch 747
Epoch 771
Epoch 771, Loss: 0.00000016395032, Improvement: 0.00000000038875, Best Loss: 0.00000008885957 in Epoch 747
Epoch 772
Epoch 772, Loss: 0.00000016494950, Improvement: 0.00000000099918, Best Loss: 0.00000008885957 in Epoch 747
Epoch 773
Epoch 773, Loss: 0.00000019588810, Improvement: 0.00000003093860, Best Loss: 0.00000008885957 in Epoch 747
Epoch 774
Epoch 774, Loss: 0.00000040443693, Improvement: 0.00000020854882, Best Loss: 0.00000008885957 in Epoch 747
Epoch 775
Epoch 775, Loss: 0.00000027713033, Improvement: -0.00000012730660, Best Loss: 0.00000008885957 in Epoch 747
Epoch 776
Epoch 776, Loss: 0.00000019044088, Improvement: -0.00000008668945, Best Loss: 0.00000008885957 in Epoch 747
Epoch 777
Epoch 777, Loss: 0.00000016138779, Improvement: -0.00000002905309, Best Loss: 0.00000008885957 in Epoch 747
Epoch 778
A best model at epoch 778 has been saved with training error 0.00000008455923.
Epoch 778, Loss: 0.00000015978332, Improvement: -0.00000000160447, Best Loss: 0.00000008455923 in Epoch 778
Epoch 779
Epoch 779, Loss: 0.00000016031532, Improvement: 0.00000000053200, Best Loss: 0.00000008455923 in Epoch 778
Epoch 780
Epoch 780, Loss: 0.00000016365026, Improvement: 0.00000000333494, Best Loss: 0.00000008455923 in Epoch 778
Epoch 781
Epoch 781, Loss: 0.00000016386198, Improvement: 0.00000000021172, Best Loss: 0.00000008455923 in Epoch 778
Epoch 782
Epoch 782, Loss: 0.00000020123674, Improvement: 0.00000003737476, Best Loss: 0.00000008455923 in Epoch 778
Epoch 783
Epoch 783, Loss: 0.00000031469047, Improvement: 0.00000011345373, Best Loss: 0.00000008455923 in Epoch 778
Epoch 784
Epoch 784, Loss: 0.00000028718190, Improvement: -0.00000002750857, Best Loss: 0.00000008455923 in Epoch 778
Epoch 785
Epoch 785, Loss: 0.00000024792697, Improvement: -0.00000003925493, Best Loss: 0.00000008455923 in Epoch 778
Epoch 786
Epoch 786, Loss: 0.00000044296747, Improvement: 0.00000019504050, Best Loss: 0.00000008455923 in Epoch 778
Epoch 787
Epoch 787, Loss: 0.00000024033724, Improvement: -0.00000020263023, Best Loss: 0.00000008455923 in Epoch 778
Epoch 788
Epoch 788, Loss: 0.00000016639419, Improvement: -0.00000007394305, Best Loss: 0.00000008455923 in Epoch 778
Epoch 789
Epoch 789, Loss: 0.00000016291373, Improvement: -0.00000000348045, Best Loss: 0.00000008455923 in Epoch 778
Epoch 790
Epoch 790, Loss: 0.00000016351374, Improvement: 0.00000000060001, Best Loss: 0.00000008455923 in Epoch 778
Epoch 791
Epoch 791, Loss: 0.00000016603516, Improvement: 0.00000000252142, Best Loss: 0.00000008455923 in Epoch 778
Epoch 792
Epoch 792, Loss: 0.00000016170518, Improvement: -0.00000000432998, Best Loss: 0.00000008455923 in Epoch 778
Epoch 793
Epoch 793, Loss: 0.00000015947512, Improvement: -0.00000000223006, Best Loss: 0.00000008455923 in Epoch 778
Epoch 794
Epoch 794, Loss: 0.00000015896257, Improvement: -0.00000000051255, Best Loss: 0.00000008455923 in Epoch 778
Epoch 795
Epoch 795, Loss: 0.00000016255618, Improvement: 0.00000000359362, Best Loss: 0.00000008455923 in Epoch 778
Epoch 796
A best model at epoch 796 has been saved with training error 0.00000008408916.
Epoch 796, Loss: 0.00000016211476, Improvement: -0.00000000044143, Best Loss: 0.00000008408916 in Epoch 796
Epoch 797
Epoch 797, Loss: 0.00000016058672, Improvement: -0.00000000152803, Best Loss: 0.00000008408916 in Epoch 796
Epoch 798
Epoch 798, Loss: 0.00000016038410, Improvement: -0.00000000020262, Best Loss: 0.00000008408916 in Epoch 796
Epoch 799
Epoch 799, Loss: 0.00000016443465, Improvement: 0.00000000405055, Best Loss: 0.00000008408916 in Epoch 796
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000018301113, Improvement: 0.00000001857648, Best Loss: 0.00000008408916 in Epoch 796
Epoch 801
Epoch 801, Loss: 0.00000020833549, Improvement: 0.00000002532435, Best Loss: 0.00000008408916 in Epoch 796
Epoch 802
Epoch 802, Loss: 0.00000039621740, Improvement: 0.00000018788191, Best Loss: 0.00000008408916 in Epoch 796
Epoch 803
Epoch 803, Loss: 0.00000034831593, Improvement: -0.00000004790147, Best Loss: 0.00000008408916 in Epoch 796
Epoch 804
Epoch 804, Loss: 0.00000018775888, Improvement: -0.00000016055706, Best Loss: 0.00000008408916 in Epoch 796
Epoch 805
Epoch 805, Loss: 0.00000016754615, Improvement: -0.00000002021272, Best Loss: 0.00000008408916 in Epoch 796
Epoch 806
Epoch 806, Loss: 0.00000016242742, Improvement: -0.00000000511874, Best Loss: 0.00000008408916 in Epoch 796
Epoch 807
Epoch 807, Loss: 0.00000016027745, Improvement: -0.00000000214996, Best Loss: 0.00000008408916 in Epoch 796
Epoch 808
Epoch 808, Loss: 0.00000015892667, Improvement: -0.00000000135078, Best Loss: 0.00000008408916 in Epoch 796
Epoch 809
Epoch 809, Loss: 0.00000015882576, Improvement: -0.00000000010092, Best Loss: 0.00000008408916 in Epoch 796
Epoch 810
Epoch 810, Loss: 0.00000015966709, Improvement: 0.00000000084133, Best Loss: 0.00000008408916 in Epoch 796
Epoch 811
Epoch 811, Loss: 0.00000016203872, Improvement: 0.00000000237162, Best Loss: 0.00000008408916 in Epoch 796
Epoch 812
Epoch 812, Loss: 0.00000016208181, Improvement: 0.00000000004310, Best Loss: 0.00000008408916 in Epoch 796
Epoch 813
Epoch 813, Loss: 0.00000016200032, Improvement: -0.00000000008149, Best Loss: 0.00000008408916 in Epoch 796
Epoch 814
Epoch 814, Loss: 0.00000015970166, Improvement: -0.00000000229866, Best Loss: 0.00000008408916 in Epoch 796
Epoch 815
Epoch 815, Loss: 0.00000016950163, Improvement: 0.00000000979997, Best Loss: 0.00000008408916 in Epoch 796
Epoch 816
Epoch 816, Loss: 0.00000036240535, Improvement: 0.00000019290372, Best Loss: 0.00000008408916 in Epoch 796
Epoch 817
Epoch 817, Loss: 0.00000034477331, Improvement: -0.00000001763204, Best Loss: 0.00000008408916 in Epoch 796
Epoch 818
Epoch 818, Loss: 0.00000018690749, Improvement: -0.00000015786582, Best Loss: 0.00000008408916 in Epoch 796
Epoch 819
Epoch 819, Loss: 0.00000018184710, Improvement: -0.00000000506039, Best Loss: 0.00000008408916 in Epoch 796
Epoch 820
Epoch 820, Loss: 0.00000016280504, Improvement: -0.00000001904206, Best Loss: 0.00000008408916 in Epoch 796
Epoch 821
Epoch 821, Loss: 0.00000015777079, Improvement: -0.00000000503426, Best Loss: 0.00000008408916 in Epoch 796
Epoch 822
Epoch 822, Loss: 0.00000016136693, Improvement: 0.00000000359615, Best Loss: 0.00000008408916 in Epoch 796
Epoch 823
Epoch 823, Loss: 0.00000018991465, Improvement: 0.00000002854772, Best Loss: 0.00000008408916 in Epoch 796
Epoch 824
Epoch 824, Loss: 0.00000020157662, Improvement: 0.00000001166198, Best Loss: 0.00000008408916 in Epoch 796
Epoch 825
Epoch 825, Loss: 0.00000035171741, Improvement: 0.00000015014078, Best Loss: 0.00000008408916 in Epoch 796
Epoch 826
Epoch 826, Loss: 0.00000041535930, Improvement: 0.00000006364189, Best Loss: 0.00000008408916 in Epoch 796
Epoch 827
Epoch 827, Loss: 0.00000021047922, Improvement: -0.00000020488008, Best Loss: 0.00000008408916 in Epoch 796
Epoch 828
Epoch 828, Loss: 0.00000017240225, Improvement: -0.00000003807697, Best Loss: 0.00000008408916 in Epoch 796
Epoch 829
Epoch 829, Loss: 0.00000016290498, Improvement: -0.00000000949727, Best Loss: 0.00000008408916 in Epoch 796
Epoch 830
Epoch 830, Loss: 0.00000015814366, Improvement: -0.00000000476132, Best Loss: 0.00000008408916 in Epoch 796
Epoch 831
Epoch 831, Loss: 0.00000015940627, Improvement: 0.00000000126261, Best Loss: 0.00000008408916 in Epoch 796
Epoch 832
Epoch 832, Loss: 0.00000015745261, Improvement: -0.00000000195366, Best Loss: 0.00000008408916 in Epoch 796
Epoch 833
Epoch 833, Loss: 0.00000015625286, Improvement: -0.00000000119975, Best Loss: 0.00000008408916 in Epoch 796
Epoch 834
Epoch 834, Loss: 0.00000015654390, Improvement: 0.00000000029104, Best Loss: 0.00000008408916 in Epoch 796
Epoch 835
Epoch 835, Loss: 0.00000015907493, Improvement: 0.00000000253103, Best Loss: 0.00000008408916 in Epoch 796
Epoch 836
Epoch 836, Loss: 0.00000015708206, Improvement: -0.00000000199287, Best Loss: 0.00000008408916 in Epoch 796
Epoch 837
Epoch 837, Loss: 0.00000015941281, Improvement: 0.00000000233075, Best Loss: 0.00000008408916 in Epoch 796
Epoch 838
Epoch 838, Loss: 0.00000016744581, Improvement: 0.00000000803300, Best Loss: 0.00000008408916 in Epoch 796
Epoch 839
Epoch 839, Loss: 0.00000021172507, Improvement: 0.00000004427926, Best Loss: 0.00000008408916 in Epoch 796
Epoch 840
Epoch 840, Loss: 0.00000017390367, Improvement: -0.00000003782139, Best Loss: 0.00000008408916 in Epoch 796
Epoch 841
Epoch 841, Loss: 0.00000016508141, Improvement: -0.00000000882226, Best Loss: 0.00000008408916 in Epoch 796
Epoch 842
Epoch 842, Loss: 0.00000017601651, Improvement: 0.00000001093510, Best Loss: 0.00000008408916 in Epoch 796
Epoch 843
Epoch 843, Loss: 0.00000018170164, Improvement: 0.00000000568513, Best Loss: 0.00000008408916 in Epoch 796
Epoch 844
Epoch 844, Loss: 0.00000027838980, Improvement: 0.00000009668816, Best Loss: 0.00000008408916 in Epoch 796
Epoch 845
Epoch 845, Loss: 0.00000039203194, Improvement: 0.00000011364213, Best Loss: 0.00000008408916 in Epoch 796
Epoch 846
Epoch 846, Loss: 0.00000019746237, Improvement: -0.00000019456957, Best Loss: 0.00000008408916 in Epoch 796
Epoch 847
Epoch 847, Loss: 0.00000016869017, Improvement: -0.00000002877220, Best Loss: 0.00000008408916 in Epoch 796
Epoch 848
Epoch 848, Loss: 0.00000016218264, Improvement: -0.00000000650753, Best Loss: 0.00000008408916 in Epoch 796
Epoch 849
Epoch 849, Loss: 0.00000015933519, Improvement: -0.00000000284745, Best Loss: 0.00000008408916 in Epoch 796
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000015842769, Improvement: -0.00000000090750, Best Loss: 0.00000008408916 in Epoch 796
Epoch 851
Epoch 851, Loss: 0.00000015562306, Improvement: -0.00000000280462, Best Loss: 0.00000008408916 in Epoch 796
Epoch 852
Epoch 852, Loss: 0.00000015843906, Improvement: 0.00000000281600, Best Loss: 0.00000008408916 in Epoch 796
Epoch 853
Epoch 853, Loss: 0.00000015928101, Improvement: 0.00000000084195, Best Loss: 0.00000008408916 in Epoch 796
Epoch 854
Epoch 854, Loss: 0.00000016259984, Improvement: 0.00000000331884, Best Loss: 0.00000008408916 in Epoch 796
Epoch 855
Epoch 855, Loss: 0.00000016551479, Improvement: 0.00000000291495, Best Loss: 0.00000008408916 in Epoch 796
Epoch 856
Epoch 856, Loss: 0.00000015705646, Improvement: -0.00000000845833, Best Loss: 0.00000008408916 in Epoch 796
Epoch 857
Epoch 857, Loss: 0.00000015907486, Improvement: 0.00000000201839, Best Loss: 0.00000008408916 in Epoch 796
Epoch 858
Epoch 858, Loss: 0.00000016395382, Improvement: 0.00000000487897, Best Loss: 0.00000008408916 in Epoch 796
Epoch 859
Epoch 859, Loss: 0.00000018464367, Improvement: 0.00000002068985, Best Loss: 0.00000008408916 in Epoch 796
Epoch 860
Epoch 860, Loss: 0.00000017022030, Improvement: -0.00000001442337, Best Loss: 0.00000008408916 in Epoch 796
Epoch 861
Epoch 861, Loss: 0.00000030812894, Improvement: 0.00000013790864, Best Loss: 0.00000008408916 in Epoch 796
Epoch 862
Epoch 862, Loss: 0.00000042265654, Improvement: 0.00000011452760, Best Loss: 0.00000008408916 in Epoch 796
Epoch 863
Epoch 863, Loss: 0.00000020423885, Improvement: -0.00000021841769, Best Loss: 0.00000008408916 in Epoch 796
Epoch 864
Epoch 864, Loss: 0.00000016599119, Improvement: -0.00000003824766, Best Loss: 0.00000008408916 in Epoch 796
Epoch 865
Epoch 865, Loss: 0.00000015691817, Improvement: -0.00000000907302, Best Loss: 0.00000008408916 in Epoch 796
Epoch 866
Epoch 866, Loss: 0.00000015568444, Improvement: -0.00000000123373, Best Loss: 0.00000008408916 in Epoch 796
Epoch 867
Epoch 867, Loss: 0.00000015833999, Improvement: 0.00000000265555, Best Loss: 0.00000008408916 in Epoch 796
Epoch 868
Epoch 868, Loss: 0.00000015372202, Improvement: -0.00000000461797, Best Loss: 0.00000008408916 in Epoch 796
Epoch 869
Epoch 869, Loss: 0.00000015361943, Improvement: -0.00000000010259, Best Loss: 0.00000008408916 in Epoch 796
Epoch 870
Epoch 870, Loss: 0.00000015278427, Improvement: -0.00000000083516, Best Loss: 0.00000008408916 in Epoch 796
Epoch 871
Epoch 871, Loss: 0.00000015726167, Improvement: 0.00000000447740, Best Loss: 0.00000008408916 in Epoch 796
Epoch 872
Epoch 872, Loss: 0.00000016559884, Improvement: 0.00000000833718, Best Loss: 0.00000008408916 in Epoch 796
Epoch 873
Epoch 873, Loss: 0.00000016295769, Improvement: -0.00000000264115, Best Loss: 0.00000008408916 in Epoch 796
Epoch 874
Epoch 874, Loss: 0.00000018575104, Improvement: 0.00000002279335, Best Loss: 0.00000008408916 in Epoch 796
Epoch 875
Epoch 875, Loss: 0.00000015648148, Improvement: -0.00000002926956, Best Loss: 0.00000008408916 in Epoch 796
Epoch 876
Epoch 876, Loss: 0.00000033426569, Improvement: 0.00000017778422, Best Loss: 0.00000008408916 in Epoch 796
Epoch 877
Epoch 877, Loss: 0.00000051823714, Improvement: 0.00000018397145, Best Loss: 0.00000008408916 in Epoch 796
Epoch 878
Epoch 878, Loss: 0.00000021030444, Improvement: -0.00000030793270, Best Loss: 0.00000008408916 in Epoch 796
Epoch 879
Epoch 879, Loss: 0.00000015319368, Improvement: -0.00000005711076, Best Loss: 0.00000008408916 in Epoch 796
Epoch 880
Epoch 880, Loss: 0.00000015098189, Improvement: -0.00000000221179, Best Loss: 0.00000008408916 in Epoch 796
Epoch 881
Epoch 881, Loss: 0.00000014618156, Improvement: -0.00000000480034, Best Loss: 0.00000008408916 in Epoch 796
Epoch 882
Epoch 882, Loss: 0.00000014779708, Improvement: 0.00000000161553, Best Loss: 0.00000008408916 in Epoch 796
Epoch 883
Epoch 883, Loss: 0.00000014571176, Improvement: -0.00000000208532, Best Loss: 0.00000008408916 in Epoch 796
Epoch 884
Epoch 884, Loss: 0.00000014867231, Improvement: 0.00000000296056, Best Loss: 0.00000008408916 in Epoch 796
Epoch 885
Epoch 885, Loss: 0.00000014507611, Improvement: -0.00000000359621, Best Loss: 0.00000008408916 in Epoch 796
Epoch 886
Epoch 886, Loss: 0.00000014306266, Improvement: -0.00000000201345, Best Loss: 0.00000008408916 in Epoch 796
Epoch 887
Epoch 887, Loss: 0.00000014369081, Improvement: 0.00000000062815, Best Loss: 0.00000008408916 in Epoch 796
Epoch 888
Epoch 888, Loss: 0.00000014058281, Improvement: -0.00000000310800, Best Loss: 0.00000008408916 in Epoch 796
Epoch 889
Epoch 889, Loss: 0.00000014019792, Improvement: -0.00000000038489, Best Loss: 0.00000008408916 in Epoch 796
Epoch 890
Epoch 890, Loss: 0.00000013893962, Improvement: -0.00000000125830, Best Loss: 0.00000008408916 in Epoch 796
Epoch 891
Epoch 891, Loss: 0.00000014083111, Improvement: 0.00000000189149, Best Loss: 0.00000008408916 in Epoch 796
Epoch 892
Epoch 892, Loss: 0.00000015949137, Improvement: 0.00000001866027, Best Loss: 0.00000008408916 in Epoch 796
Epoch 893
Epoch 893, Loss: 0.00000015425765, Improvement: -0.00000000523372, Best Loss: 0.00000008408916 in Epoch 796
Epoch 894
Epoch 894, Loss: 0.00000014776555, Improvement: -0.00000000649210, Best Loss: 0.00000008408916 in Epoch 796
Epoch 895
Epoch 895, Loss: 0.00000014282687, Improvement: -0.00000000493868, Best Loss: 0.00000008408916 in Epoch 796
Epoch 896
Epoch 896, Loss: 0.00000016770617, Improvement: 0.00000002487930, Best Loss: 0.00000008408916 in Epoch 796
Epoch 897
Epoch 897, Loss: 0.00000016741578, Improvement: -0.00000000029038, Best Loss: 0.00000008408916 in Epoch 796
Epoch 898
Epoch 898, Loss: 0.00000014267865, Improvement: -0.00000002473713, Best Loss: 0.00000008408916 in Epoch 796
Epoch 899
Epoch 899, Loss: 0.00000013843582, Improvement: -0.00000000424283, Best Loss: 0.00000008408916 in Epoch 796
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000014478581, Improvement: 0.00000000634999, Best Loss: 0.00000008408916 in Epoch 796
Epoch 901
Epoch 901, Loss: 0.00000046916367, Improvement: 0.00000032437786, Best Loss: 0.00000008408916 in Epoch 796
Epoch 902
Epoch 902, Loss: 0.00000039791636, Improvement: -0.00000007124731, Best Loss: 0.00000008408916 in Epoch 796
Epoch 903
Epoch 903, Loss: 0.00000019217472, Improvement: -0.00000020574164, Best Loss: 0.00000008408916 in Epoch 796
Epoch 904
Epoch 904, Loss: 0.00000015536089, Improvement: -0.00000003681384, Best Loss: 0.00000008408916 in Epoch 796
Epoch 905
Epoch 905, Loss: 0.00000014757006, Improvement: -0.00000000779083, Best Loss: 0.00000008408916 in Epoch 796
Epoch 906
Epoch 906, Loss: 0.00000014234373, Improvement: -0.00000000522632, Best Loss: 0.00000008408916 in Epoch 796
Epoch 907
Epoch 907, Loss: 0.00000014117201, Improvement: -0.00000000117172, Best Loss: 0.00000008408916 in Epoch 796
Epoch 908
Epoch 908, Loss: 0.00000014114552, Improvement: -0.00000000002649, Best Loss: 0.00000008408916 in Epoch 796
Epoch 909
Epoch 909, Loss: 0.00000014019740, Improvement: -0.00000000094813, Best Loss: 0.00000008408916 in Epoch 796
Epoch 910
Epoch 910, Loss: 0.00000013807955, Improvement: -0.00000000211784, Best Loss: 0.00000008408916 in Epoch 796
Epoch 911
Epoch 911, Loss: 0.00000013849124, Improvement: 0.00000000041168, Best Loss: 0.00000008408916 in Epoch 796
Epoch 912
Epoch 912, Loss: 0.00000013632658, Improvement: -0.00000000216465, Best Loss: 0.00000008408916 in Epoch 796
Epoch 913
Epoch 913, Loss: 0.00000013587689, Improvement: -0.00000000044969, Best Loss: 0.00000008408916 in Epoch 796
Epoch 914
Epoch 914, Loss: 0.00000014006435, Improvement: 0.00000000418745, Best Loss: 0.00000008408916 in Epoch 796
Epoch 915
Epoch 915, Loss: 0.00000013438956, Improvement: -0.00000000567479, Best Loss: 0.00000008408916 in Epoch 796
Epoch 916
Epoch 916, Loss: 0.00000013327049, Improvement: -0.00000000111907, Best Loss: 0.00000008408916 in Epoch 796
Epoch 917
Epoch 917, Loss: 0.00000013026098, Improvement: -0.00000000300950, Best Loss: 0.00000008408916 in Epoch 796
Epoch 918
Epoch 918, Loss: 0.00000013363315, Improvement: 0.00000000337216, Best Loss: 0.00000008408916 in Epoch 796
Epoch 919
Epoch 919, Loss: 0.00000013943250, Improvement: 0.00000000579936, Best Loss: 0.00000008408916 in Epoch 796
Epoch 920
Epoch 920, Loss: 0.00000013314073, Improvement: -0.00000000629177, Best Loss: 0.00000008408916 in Epoch 796
Epoch 921
Epoch 921, Loss: 0.00000013136666, Improvement: -0.00000000177407, Best Loss: 0.00000008408916 in Epoch 796
Epoch 922
Epoch 922, Loss: 0.00000013606073, Improvement: 0.00000000469407, Best Loss: 0.00000008408916 in Epoch 796
Epoch 923
Epoch 923, Loss: 0.00000013853988, Improvement: 0.00000000247914, Best Loss: 0.00000008408916 in Epoch 796
Epoch 924
Epoch 924, Loss: 0.00000013773261, Improvement: -0.00000000080727, Best Loss: 0.00000008408916 in Epoch 796
Epoch 925
A best model at epoch 925 has been saved with training error 0.00000008378726.
Epoch 925, Loss: 0.00000012747815, Improvement: -0.00000001025446, Best Loss: 0.00000008378726 in Epoch 925
Epoch 926
Epoch 926, Loss: 0.00000013007572, Improvement: 0.00000000259757, Best Loss: 0.00000008378726 in Epoch 925
Epoch 927
Epoch 927, Loss: 0.00000012435625, Improvement: -0.00000000571947, Best Loss: 0.00000008378726 in Epoch 925
Epoch 928
Epoch 928, Loss: 0.00000013351410, Improvement: 0.00000000915785, Best Loss: 0.00000008378726 in Epoch 925
Epoch 929
Epoch 929, Loss: 0.00000014503772, Improvement: 0.00000001152363, Best Loss: 0.00000008378726 in Epoch 925
Epoch 930
Epoch 930, Loss: 0.00000015774228, Improvement: 0.00000001270456, Best Loss: 0.00000008378726 in Epoch 925
Epoch 931
Epoch 931, Loss: 0.00000017646038, Improvement: 0.00000001871810, Best Loss: 0.00000008378726 in Epoch 925
Epoch 932
Epoch 932, Loss: 0.00000015259913, Improvement: -0.00000002386124, Best Loss: 0.00000008378726 in Epoch 925
Epoch 933
Epoch 933, Loss: 0.00000013954591, Improvement: -0.00000001305322, Best Loss: 0.00000008378726 in Epoch 925
Epoch 934
Epoch 934, Loss: 0.00000016012079, Improvement: 0.00000002057488, Best Loss: 0.00000008378726 in Epoch 925
Epoch 935
Epoch 935, Loss: 0.00000013705414, Improvement: -0.00000002306665, Best Loss: 0.00000008378726 in Epoch 925
Epoch 936
Epoch 936, Loss: 0.00000015257014, Improvement: 0.00000001551600, Best Loss: 0.00000008378726 in Epoch 925
Epoch 937
Epoch 937, Loss: 0.00000040140222, Improvement: 0.00000024883208, Best Loss: 0.00000008378726 in Epoch 925
Epoch 938
Epoch 938, Loss: 0.00000027214964, Improvement: -0.00000012925258, Best Loss: 0.00000008378726 in Epoch 925
Epoch 939
Epoch 939, Loss: 0.00000019463035, Improvement: -0.00000007751929, Best Loss: 0.00000008378726 in Epoch 925
Epoch 940
Epoch 940, Loss: 0.00000014062417, Improvement: -0.00000005400618, Best Loss: 0.00000008378726 in Epoch 925
Epoch 941
Epoch 941, Loss: 0.00000013131984, Improvement: -0.00000000930432, Best Loss: 0.00000008378726 in Epoch 925
Epoch 942
Epoch 942, Loss: 0.00000012892736, Improvement: -0.00000000239248, Best Loss: 0.00000008378726 in Epoch 925
Epoch 943
A best model at epoch 943 has been saved with training error 0.00000007786883.
Epoch 943, Loss: 0.00000012866033, Improvement: -0.00000000026703, Best Loss: 0.00000007786883 in Epoch 943
Epoch 944
Epoch 944, Loss: 0.00000012935584, Improvement: 0.00000000069551, Best Loss: 0.00000007786883 in Epoch 943
Epoch 945
Epoch 945, Loss: 0.00000012991459, Improvement: 0.00000000055875, Best Loss: 0.00000007786883 in Epoch 943
Epoch 946
Epoch 946, Loss: 0.00000012061882, Improvement: -0.00000000929577, Best Loss: 0.00000007786883 in Epoch 943
Epoch 947
Epoch 947, Loss: 0.00000014351218, Improvement: 0.00000002289337, Best Loss: 0.00000007786883 in Epoch 943
Epoch 948
Epoch 948, Loss: 0.00000013326665, Improvement: -0.00000001024554, Best Loss: 0.00000007786883 in Epoch 943
Epoch 949
Epoch 949, Loss: 0.00000024591955, Improvement: 0.00000011265291, Best Loss: 0.00000007786883 in Epoch 943
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000031039612, Improvement: 0.00000006447657, Best Loss: 0.00000007786883 in Epoch 943
Epoch 951
Epoch 951, Loss: 0.00000019046947, Improvement: -0.00000011992666, Best Loss: 0.00000007786883 in Epoch 943
Epoch 952
Epoch 952, Loss: 0.00000017581730, Improvement: -0.00000001465217, Best Loss: 0.00000007786883 in Epoch 943
Epoch 953
Epoch 953, Loss: 0.00000013322748, Improvement: -0.00000004258982, Best Loss: 0.00000007786883 in Epoch 943
Epoch 954
Epoch 954, Loss: 0.00000013585013, Improvement: 0.00000000262264, Best Loss: 0.00000007786883 in Epoch 943
Epoch 955
Epoch 955, Loss: 0.00000013341140, Improvement: -0.00000000243873, Best Loss: 0.00000007786883 in Epoch 943
Epoch 956
Epoch 956, Loss: 0.00000016695475, Improvement: 0.00000003354336, Best Loss: 0.00000007786883 in Epoch 943
Epoch 957
Epoch 957, Loss: 0.00000016672000, Improvement: -0.00000000023476, Best Loss: 0.00000007786883 in Epoch 943
Epoch 958
Epoch 958, Loss: 0.00000015363782, Improvement: -0.00000001308217, Best Loss: 0.00000007786883 in Epoch 943
Epoch 959
Epoch 959, Loss: 0.00000018116834, Improvement: 0.00000002753052, Best Loss: 0.00000007786883 in Epoch 943
Epoch 960
Epoch 960, Loss: 0.00000019366753, Improvement: 0.00000001249919, Best Loss: 0.00000007786883 in Epoch 943
Epoch 961
Epoch 961, Loss: 0.00000035311299, Improvement: 0.00000015944546, Best Loss: 0.00000007786883 in Epoch 943
Epoch 962
Epoch 962, Loss: 0.00000017310159, Improvement: -0.00000018001140, Best Loss: 0.00000007786883 in Epoch 943
Epoch 963
Epoch 963, Loss: 0.00000014840543, Improvement: -0.00000002469616, Best Loss: 0.00000007786883 in Epoch 943
Epoch 964
Epoch 964, Loss: 0.00000013886001, Improvement: -0.00000000954542, Best Loss: 0.00000007786883 in Epoch 943
Epoch 965
Epoch 965, Loss: 0.00000012803088, Improvement: -0.00000001082913, Best Loss: 0.00000007786883 in Epoch 943
Epoch 966
Epoch 966, Loss: 0.00000014049545, Improvement: 0.00000001246458, Best Loss: 0.00000007786883 in Epoch 943
Epoch 967
Epoch 967, Loss: 0.00000015958115, Improvement: 0.00000001908570, Best Loss: 0.00000007786883 in Epoch 943
Epoch 968
Epoch 968, Loss: 0.00000019013599, Improvement: 0.00000003055484, Best Loss: 0.00000007786883 in Epoch 943
Epoch 969
Epoch 969, Loss: 0.00000024538101, Improvement: 0.00000005524502, Best Loss: 0.00000007786883 in Epoch 943
Epoch 970
Epoch 970, Loss: 0.00000014179153, Improvement: -0.00000010358948, Best Loss: 0.00000007786883 in Epoch 943
Epoch 971
Epoch 971, Loss: 0.00000012015888, Improvement: -0.00000002163264, Best Loss: 0.00000007786883 in Epoch 943
Epoch 972
Epoch 972, Loss: 0.00000012193885, Improvement: 0.00000000177996, Best Loss: 0.00000007786883 in Epoch 943
Epoch 973
Epoch 973, Loss: 0.00000014647616, Improvement: 0.00000002453731, Best Loss: 0.00000007786883 in Epoch 943
Epoch 974
Epoch 974, Loss: 0.00000023412891, Improvement: 0.00000008765275, Best Loss: 0.00000007786883 in Epoch 943
Epoch 975
Epoch 975, Loss: 0.00000021565224, Improvement: -0.00000001847667, Best Loss: 0.00000007786883 in Epoch 943
Epoch 976
Epoch 976, Loss: 0.00000022626120, Improvement: 0.00000001060896, Best Loss: 0.00000007786883 in Epoch 943
Epoch 977
Epoch 977, Loss: 0.00000022384065, Improvement: -0.00000000242055, Best Loss: 0.00000007786883 in Epoch 943
Epoch 978
Epoch 978, Loss: 0.00000030590407, Improvement: 0.00000008206342, Best Loss: 0.00000007786883 in Epoch 943
Epoch 979
Epoch 979, Loss: 0.00000016519595, Improvement: -0.00000014070812, Best Loss: 0.00000007786883 in Epoch 943
Epoch 980
Epoch 980, Loss: 0.00000016584838, Improvement: 0.00000000065243, Best Loss: 0.00000007786883 in Epoch 943
Epoch 981
Epoch 981, Loss: 0.00000014562649, Improvement: -0.00000002022189, Best Loss: 0.00000007786883 in Epoch 943
Epoch 982
Epoch 982, Loss: 0.00000014034266, Improvement: -0.00000000528383, Best Loss: 0.00000007786883 in Epoch 943
Epoch 983
Epoch 983, Loss: 0.00000013875986, Improvement: -0.00000000158280, Best Loss: 0.00000007786883 in Epoch 943
Epoch 984
Epoch 984, Loss: 0.00000021165786, Improvement: 0.00000007289800, Best Loss: 0.00000007786883 in Epoch 943
Epoch 985
Epoch 985, Loss: 0.00000022761174, Improvement: 0.00000001595388, Best Loss: 0.00000007786883 in Epoch 943
Epoch 986
Epoch 986, Loss: 0.00000022526086, Improvement: -0.00000000235088, Best Loss: 0.00000007786883 in Epoch 943
Epoch 987
Epoch 987, Loss: 0.00000014147881, Improvement: -0.00000008378205, Best Loss: 0.00000007786883 in Epoch 943
Epoch 988
A best model at epoch 988 has been saved with training error 0.00000006520661.
Epoch 988, Loss: 0.00000014042767, Improvement: -0.00000000105114, Best Loss: 0.00000006520661 in Epoch 988
Epoch 989
Epoch 989, Loss: 0.00000011863988, Improvement: -0.00000002178779, Best Loss: 0.00000006520661 in Epoch 988
Epoch 990
Epoch 990, Loss: 0.00000011768923, Improvement: -0.00000000095065, Best Loss: 0.00000006520661 in Epoch 988
Epoch 991
Epoch 991, Loss: 0.00000011749309, Improvement: -0.00000000019614, Best Loss: 0.00000006520661 in Epoch 988
Epoch 992
Epoch 992, Loss: 0.00000017238667, Improvement: 0.00000005489359, Best Loss: 0.00000006520661 in Epoch 988
Epoch 993
Epoch 993, Loss: 0.00000018189148, Improvement: 0.00000000950481, Best Loss: 0.00000006520661 in Epoch 988
Epoch 994
Epoch 994, Loss: 0.00000027829626, Improvement: 0.00000009640477, Best Loss: 0.00000006520661 in Epoch 988
Epoch 995
Epoch 995, Loss: 0.00000036402250, Improvement: 0.00000008572624, Best Loss: 0.00000006520661 in Epoch 988
Epoch 996
Epoch 996, Loss: 0.00000018424707, Improvement: -0.00000017977544, Best Loss: 0.00000006520661 in Epoch 988
Epoch 997
Epoch 997, Loss: 0.00000012837924, Improvement: -0.00000005586782, Best Loss: 0.00000006520661 in Epoch 988
Epoch 998
Epoch 998, Loss: 0.00000011850773, Improvement: -0.00000000987152, Best Loss: 0.00000006520661 in Epoch 988
Epoch 999
Epoch 999, Loss: 0.00000011798417, Improvement: -0.00000000052355, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000011277103, Improvement: -0.00000000521314, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1001
Epoch 1001, Loss: 0.00000011277505, Improvement: 0.00000000000402, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1002
Epoch 1002, Loss: 0.00000011341013, Improvement: 0.00000000063508, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1003
Epoch 1003, Loss: 0.00000013181078, Improvement: 0.00000001840065, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1004
Epoch 1004, Loss: 0.00000023569649, Improvement: 0.00000010388571, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1005
Epoch 1005, Loss: 0.00000032258808, Improvement: 0.00000008689160, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1006
Epoch 1006, Loss: 0.00000018085902, Improvement: -0.00000014172906, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1007
Epoch 1007, Loss: 0.00000015168525, Improvement: -0.00000002917377, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1008
Epoch 1008, Loss: 0.00000014517660, Improvement: -0.00000000650865, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1009
Epoch 1009, Loss: 0.00000015506591, Improvement: 0.00000000988931, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1010
Epoch 1010, Loss: 0.00000015094390, Improvement: -0.00000000412200, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1011
Epoch 1011, Loss: 0.00000013072264, Improvement: -0.00000002022126, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1012
Epoch 1012, Loss: 0.00000014754602, Improvement: 0.00000001682338, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1013
Epoch 1013, Loss: 0.00000012913352, Improvement: -0.00000001841250, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1014
Epoch 1014, Loss: 0.00000012054921, Improvement: -0.00000000858431, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1015
Epoch 1015, Loss: 0.00000011876882, Improvement: -0.00000000178039, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1016
Epoch 1016, Loss: 0.00000012105426, Improvement: 0.00000000228544, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1017
Epoch 1017, Loss: 0.00000012349521, Improvement: 0.00000000244095, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1018
Epoch 1018, Loss: 0.00000016541627, Improvement: 0.00000004192106, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1019
Epoch 1019, Loss: 0.00000015536178, Improvement: -0.00000001005449, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1020
Epoch 1020, Loss: 0.00000042710627, Improvement: 0.00000027174449, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1021
Epoch 1021, Loss: 0.00000027713328, Improvement: -0.00000014997300, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1022
Epoch 1022, Loss: 0.00000015990464, Improvement: -0.00000011722864, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1023
Epoch 1023, Loss: 0.00000013371341, Improvement: -0.00000002619123, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1024
Epoch 1024, Loss: 0.00000012578978, Improvement: -0.00000000792363, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1025
Epoch 1025, Loss: 0.00000012283281, Improvement: -0.00000000295697, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1026
Epoch 1026, Loss: 0.00000012147182, Improvement: -0.00000000136099, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1027
Epoch 1027, Loss: 0.00000011476535, Improvement: -0.00000000670647, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1028
Epoch 1028, Loss: 0.00000011225176, Improvement: -0.00000000251359, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1029
Epoch 1029, Loss: 0.00000011131459, Improvement: -0.00000000093716, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1030
Epoch 1030, Loss: 0.00000012500046, Improvement: 0.00000001368587, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1031
Epoch 1031, Loss: 0.00000014729978, Improvement: 0.00000002229932, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1032
Epoch 1032, Loss: 0.00000013917363, Improvement: -0.00000000812615, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1033
Epoch 1033, Loss: 0.00000011166876, Improvement: -0.00000002750487, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1034
Epoch 1034, Loss: 0.00000011001852, Improvement: -0.00000000165024, Best Loss: 0.00000006520661 in Epoch 988
Epoch 1035
A best model at epoch 1035 has been saved with training error 0.00000006248611.
Epoch 1035, Loss: 0.00000010826592, Improvement: -0.00000000175260, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1036
Epoch 1036, Loss: 0.00000011073475, Improvement: 0.00000000246883, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1037
Epoch 1037, Loss: 0.00000011611454, Improvement: 0.00000000537979, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1038
Epoch 1038, Loss: 0.00000012096175, Improvement: 0.00000000484721, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1039
Epoch 1039, Loss: 0.00000015310260, Improvement: 0.00000003214085, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1040
Epoch 1040, Loss: 0.00000021160540, Improvement: 0.00000005850280, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1041
Epoch 1041, Loss: 0.00000018866079, Improvement: -0.00000002294461, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1042
Epoch 1042, Loss: 0.00000016326112, Improvement: -0.00000002539967, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1043
Epoch 1043, Loss: 0.00000014709942, Improvement: -0.00000001616169, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1044
Epoch 1044, Loss: 0.00000013847003, Improvement: -0.00000000862940, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1045
Epoch 1045, Loss: 0.00000025472358, Improvement: 0.00000011625355, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1046
Epoch 1046, Loss: 0.00000018025852, Improvement: -0.00000007446506, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1047
Epoch 1047, Loss: 0.00000015152967, Improvement: -0.00000002872885, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1048
Epoch 1048, Loss: 0.00000012885490, Improvement: -0.00000002267477, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1049
Epoch 1049, Loss: 0.00000013466635, Improvement: 0.00000000581145, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000014613197, Improvement: 0.00000001146562, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1051
Epoch 1051, Loss: 0.00000017177337, Improvement: 0.00000002564140, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1052
Epoch 1052, Loss: 0.00000013203434, Improvement: -0.00000003973902, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1053
Epoch 1053, Loss: 0.00000010816036, Improvement: -0.00000002387398, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1054
Epoch 1054, Loss: 0.00000011016664, Improvement: 0.00000000200628, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1055
Epoch 1055, Loss: 0.00000013959092, Improvement: 0.00000002942429, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1056
Epoch 1056, Loss: 0.00000013555716, Improvement: -0.00000000403377, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1057
Epoch 1057, Loss: 0.00000018795057, Improvement: 0.00000005239341, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1058
Epoch 1058, Loss: 0.00000019682831, Improvement: 0.00000000887774, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1059
Epoch 1059, Loss: 0.00000012579296, Improvement: -0.00000007103535, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1060
Epoch 1060, Loss: 0.00000011790390, Improvement: -0.00000000788905, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1061
Epoch 1061, Loss: 0.00000011285898, Improvement: -0.00000000504492, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1062
Epoch 1062, Loss: 0.00000018839102, Improvement: 0.00000007553204, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1063
Epoch 1063, Loss: 0.00000021635933, Improvement: 0.00000002796831, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1064
Epoch 1064, Loss: 0.00000025171093, Improvement: 0.00000003535160, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1065
Epoch 1065, Loss: 0.00000021237159, Improvement: -0.00000003933935, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1066
Epoch 1066, Loss: 0.00000012547122, Improvement: -0.00000008690037, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1067
Epoch 1067, Loss: 0.00000012072431, Improvement: -0.00000000474691, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1068
Epoch 1068, Loss: 0.00000011067335, Improvement: -0.00000001005096, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1069
Epoch 1069, Loss: 0.00000010357847, Improvement: -0.00000000709488, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1070
Epoch 1070, Loss: 0.00000010828491, Improvement: 0.00000000470644, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1071
Epoch 1071, Loss: 0.00000011076577, Improvement: 0.00000000248086, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1072
Epoch 1072, Loss: 0.00000018087612, Improvement: 0.00000007011035, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1073
Epoch 1073, Loss: 0.00000029358890, Improvement: 0.00000011271278, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1074
Epoch 1074, Loss: 0.00000012909658, Improvement: -0.00000016449232, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1075
Epoch 1075, Loss: 0.00000011892764, Improvement: -0.00000001016894, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1076
Epoch 1076, Loss: 0.00000010759260, Improvement: -0.00000001133503, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1077
Epoch 1077, Loss: 0.00000010913282, Improvement: 0.00000000154022, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1078
Epoch 1078, Loss: 0.00000011113903, Improvement: 0.00000000200621, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1079
Epoch 1079, Loss: 0.00000011061383, Improvement: -0.00000000052520, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1080
Epoch 1080, Loss: 0.00000014751413, Improvement: 0.00000003690031, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1081
Epoch 1081, Loss: 0.00000018035450, Improvement: 0.00000003284037, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1082
Epoch 1082, Loss: 0.00000015517957, Improvement: -0.00000002517493, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1083
Epoch 1083, Loss: 0.00000012829152, Improvement: -0.00000002688804, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1084
Epoch 1084, Loss: 0.00000012404559, Improvement: -0.00000000424593, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1085
Epoch 1085, Loss: 0.00000012495648, Improvement: 0.00000000091089, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1086
Epoch 1086, Loss: 0.00000014460083, Improvement: 0.00000001964435, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1087
Epoch 1087, Loss: 0.00000017615122, Improvement: 0.00000003155040, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1088
Epoch 1088, Loss: 0.00000019925567, Improvement: 0.00000002310445, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1089
Epoch 1089, Loss: 0.00000015370614, Improvement: -0.00000004554953, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1090
Epoch 1090, Loss: 0.00000015336653, Improvement: -0.00000000033961, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1091
Epoch 1091, Loss: 0.00000013411447, Improvement: -0.00000001925206, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1092
Epoch 1092, Loss: 0.00000010895539, Improvement: -0.00000002515908, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1093
Epoch 1093, Loss: 0.00000012602441, Improvement: 0.00000001706902, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1094
Epoch 1094, Loss: 0.00000013965377, Improvement: 0.00000001362936, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1095
Epoch 1095, Loss: 0.00000014974455, Improvement: 0.00000001009077, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1096
Epoch 1096, Loss: 0.00000023118301, Improvement: 0.00000008143847, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1097
Epoch 1097, Loss: 0.00000014086036, Improvement: -0.00000009032265, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1098
Epoch 1098, Loss: 0.00000013796293, Improvement: -0.00000000289743, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1099
Epoch 1099, Loss: 0.00000019668414, Improvement: 0.00000005872120, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000014258683, Improvement: -0.00000005409730, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1101
Epoch 1101, Loss: 0.00000013270412, Improvement: -0.00000000988271, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1102
Epoch 1102, Loss: 0.00000016468517, Improvement: 0.00000003198105, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1103
Epoch 1103, Loss: 0.00000020100882, Improvement: 0.00000003632365, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1104
Epoch 1104, Loss: 0.00000016387721, Improvement: -0.00000003713162, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1105
Epoch 1105, Loss: 0.00000016195628, Improvement: -0.00000000192093, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1106
Epoch 1106, Loss: 0.00000013128796, Improvement: -0.00000003066832, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1107
Epoch 1107, Loss: 0.00000015309954, Improvement: 0.00000002181158, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1108
Epoch 1108, Loss: 0.00000013210022, Improvement: -0.00000002099932, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1109
Epoch 1109, Loss: 0.00000012341139, Improvement: -0.00000000868884, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1110
Epoch 1110, Loss: 0.00000021061179, Improvement: 0.00000008720040, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1111
Epoch 1111, Loss: 0.00000021598085, Improvement: 0.00000000536907, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1112
Epoch 1112, Loss: 0.00000032682264, Improvement: 0.00000011084179, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1113
Epoch 1113, Loss: 0.00000012787503, Improvement: -0.00000019894761, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1114
Epoch 1114, Loss: 0.00000011816611, Improvement: -0.00000000970892, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1115
Epoch 1115, Loss: 0.00000011532816, Improvement: -0.00000000283796, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1116
Epoch 1116, Loss: 0.00000011029528, Improvement: -0.00000000503287, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1117
Epoch 1117, Loss: 0.00000010542017, Improvement: -0.00000000487512, Best Loss: 0.00000006248611 in Epoch 1035
Epoch 1118
A best model at epoch 1118 has been saved with training error 0.00000006152845.
Epoch 1118, Loss: 0.00000010127590, Improvement: -0.00000000414427, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1119
Epoch 1119, Loss: 0.00000010075918, Improvement: -0.00000000051672, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1120
Epoch 1120, Loss: 0.00000010185201, Improvement: 0.00000000109283, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1121
Epoch 1121, Loss: 0.00000009965979, Improvement: -0.00000000219222, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1122
Epoch 1122, Loss: 0.00000010046190, Improvement: 0.00000000080211, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1123
Epoch 1123, Loss: 0.00000010565093, Improvement: 0.00000000518903, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1124
Epoch 1124, Loss: 0.00000010573119, Improvement: 0.00000000008027, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1125
Epoch 1125, Loss: 0.00000011409254, Improvement: 0.00000000836134, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1126
Epoch 1126, Loss: 0.00000011153058, Improvement: -0.00000000256196, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1127
Epoch 1127, Loss: 0.00000011010108, Improvement: -0.00000000142950, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1128
Epoch 1128, Loss: 0.00000013597310, Improvement: 0.00000002587202, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1129
Epoch 1129, Loss: 0.00000013811617, Improvement: 0.00000000214307, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1130
Epoch 1130, Loss: 0.00000012058762, Improvement: -0.00000001752855, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1131
Epoch 1131, Loss: 0.00000011739612, Improvement: -0.00000000319150, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1132
Epoch 1132, Loss: 0.00000017016717, Improvement: 0.00000005277105, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1133
Epoch 1133, Loss: 0.00000010948484, Improvement: -0.00000006068233, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1134
Epoch 1134, Loss: 0.00000012639863, Improvement: 0.00000001691379, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1135
Epoch 1135, Loss: 0.00000018033287, Improvement: 0.00000005393424, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1136
Epoch 1136, Loss: 0.00000016889665, Improvement: -0.00000001143621, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1137
Epoch 1137, Loss: 0.00000011416683, Improvement: -0.00000005472983, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1138
Epoch 1138, Loss: 0.00000011189286, Improvement: -0.00000000227397, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1139
Epoch 1139, Loss: 0.00000011019742, Improvement: -0.00000000169544, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1140
Epoch 1140, Loss: 0.00000011647921, Improvement: 0.00000000628179, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1141
Epoch 1141, Loss: 0.00000012133588, Improvement: 0.00000000485667, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1142
Epoch 1142, Loss: 0.00000020079898, Improvement: 0.00000007946310, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1143
Epoch 1143, Loss: 0.00000015312522, Improvement: -0.00000004767376, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1144
Epoch 1144, Loss: 0.00000020582538, Improvement: 0.00000005270016, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1145
Epoch 1145, Loss: 0.00000015029836, Improvement: -0.00000005552702, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1146
Epoch 1146, Loss: 0.00000012045726, Improvement: -0.00000002984110, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1147
Epoch 1147, Loss: 0.00000013027106, Improvement: 0.00000000981380, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1148
Epoch 1148, Loss: 0.00000011439452, Improvement: -0.00000001587654, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1149
Epoch 1149, Loss: 0.00000010691463, Improvement: -0.00000000747989, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000013100137, Improvement: 0.00000002408674, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1151
Epoch 1151, Loss: 0.00000011582512, Improvement: -0.00000001517624, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1152
Epoch 1152, Loss: 0.00000011936481, Improvement: 0.00000000353969, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1153
Epoch 1153, Loss: 0.00000028199070, Improvement: 0.00000016262589, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1154
Epoch 1154, Loss: 0.00000029239310, Improvement: 0.00000001040240, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1155
Epoch 1155, Loss: 0.00000015651290, Improvement: -0.00000013588021, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1156
Epoch 1156, Loss: 0.00000014286048, Improvement: -0.00000001365242, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1157
Epoch 1157, Loss: 0.00000012951584, Improvement: -0.00000001334464, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1158
Epoch 1158, Loss: 0.00000011389227, Improvement: -0.00000001562357, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1159
Epoch 1159, Loss: 0.00000010457772, Improvement: -0.00000000931455, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1160
Epoch 1160, Loss: 0.00000012170412, Improvement: 0.00000001712640, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1161
Epoch 1161, Loss: 0.00000015735142, Improvement: 0.00000003564730, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1162
Epoch 1162, Loss: 0.00000013099805, Improvement: -0.00000002635337, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1163
Epoch 1163, Loss: 0.00000012578687, Improvement: -0.00000000521119, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1164
Epoch 1164, Loss: 0.00000015719119, Improvement: 0.00000003140432, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1165
Epoch 1165, Loss: 0.00000015963781, Improvement: 0.00000000244662, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1166
Epoch 1166, Loss: 0.00000013649475, Improvement: -0.00000002314306, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1167
Epoch 1167, Loss: 0.00000014859393, Improvement: 0.00000001209918, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1168
Epoch 1168, Loss: 0.00000013382641, Improvement: -0.00000001476752, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1169
Epoch 1169, Loss: 0.00000014592231, Improvement: 0.00000001209590, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1170
Epoch 1170, Loss: 0.00000011202478, Improvement: -0.00000003389753, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1171
Epoch 1171, Loss: 0.00000009769336, Improvement: -0.00000001433142, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1172
Epoch 1172, Loss: 0.00000010237794, Improvement: 0.00000000468458, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1173
Epoch 1173, Loss: 0.00000011243555, Improvement: 0.00000001005761, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1174
Epoch 1174, Loss: 0.00000014830106, Improvement: 0.00000003586551, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1175
Epoch 1175, Loss: 0.00000014460357, Improvement: -0.00000000369749, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1176
Epoch 1176, Loss: 0.00000018447558, Improvement: 0.00000003987201, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1177
Epoch 1177, Loss: 0.00000019427591, Improvement: 0.00000000980033, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1178
Epoch 1178, Loss: 0.00000012403519, Improvement: -0.00000007024072, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1179
Epoch 1179, Loss: 0.00000012948868, Improvement: 0.00000000545349, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1180
Epoch 1180, Loss: 0.00000011798373, Improvement: -0.00000001150495, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1181
Epoch 1181, Loss: 0.00000011142584, Improvement: -0.00000000655789, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1182
Epoch 1182, Loss: 0.00000011044069, Improvement: -0.00000000098514, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1183
Epoch 1183, Loss: 0.00000010688194, Improvement: -0.00000000355876, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1184
Epoch 1184, Loss: 0.00000009879786, Improvement: -0.00000000808408, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1185
Epoch 1185, Loss: 0.00000010225709, Improvement: 0.00000000345923, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1186
Epoch 1186, Loss: 0.00000009665999, Improvement: -0.00000000559710, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1187
Epoch 1187, Loss: 0.00000016082473, Improvement: 0.00000006416474, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1188
Epoch 1188, Loss: 0.00000024095263, Improvement: 0.00000008012790, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1189
Epoch 1189, Loss: 0.00000015120857, Improvement: -0.00000008974406, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1190
Epoch 1190, Loss: 0.00000012061703, Improvement: -0.00000003059154, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1191
Epoch 1191, Loss: 0.00000010855584, Improvement: -0.00000001206119, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1192
Epoch 1192, Loss: 0.00000010555556, Improvement: -0.00000000300028, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1193
Epoch 1193, Loss: 0.00000009880296, Improvement: -0.00000000675260, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1194
Epoch 1194, Loss: 0.00000011285152, Improvement: 0.00000001404856, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1195
Epoch 1195, Loss: 0.00000016827939, Improvement: 0.00000005542787, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1196
Epoch 1196, Loss: 0.00000017198263, Improvement: 0.00000000370324, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1197
Epoch 1197, Loss: 0.00000010600104, Improvement: -0.00000006598158, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1198
Epoch 1198, Loss: 0.00000009514973, Improvement: -0.00000001085132, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1199
Epoch 1199, Loss: 0.00000009561262, Improvement: 0.00000000046289, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000009493639, Improvement: -0.00000000067623, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1201
Epoch 1201, Loss: 0.00000009442438, Improvement: -0.00000000051201, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1202
Epoch 1202, Loss: 0.00000009212507, Improvement: -0.00000000229931, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1203
Epoch 1203, Loss: 0.00000009203071, Improvement: -0.00000000009436, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1204
Epoch 1204, Loss: 0.00000009637585, Improvement: 0.00000000434514, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1205
Epoch 1205, Loss: 0.00000009284726, Improvement: -0.00000000352860, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1206
Epoch 1206, Loss: 0.00000009777648, Improvement: 0.00000000492922, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1207
Epoch 1207, Loss: 0.00000013242585, Improvement: 0.00000003464937, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1208
Epoch 1208, Loss: 0.00000011343767, Improvement: -0.00000001898818, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1209
Epoch 1209, Loss: 0.00000009909387, Improvement: -0.00000001434380, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1210
Epoch 1210, Loss: 0.00000010065983, Improvement: 0.00000000156595, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1211
Epoch 1211, Loss: 0.00000010797597, Improvement: 0.00000000731614, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1212
Epoch 1212, Loss: 0.00000013469769, Improvement: 0.00000002672172, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1213
Epoch 1213, Loss: 0.00000017830254, Improvement: 0.00000004360485, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1214
Epoch 1214, Loss: 0.00000016619690, Improvement: -0.00000001210564, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1215
Epoch 1215, Loss: 0.00000028090643, Improvement: 0.00000011470953, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1216
Epoch 1216, Loss: 0.00000022683437, Improvement: -0.00000005407206, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1217
Epoch 1217, Loss: 0.00000016130917, Improvement: -0.00000006552520, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1218
Epoch 1218, Loss: 0.00000010683793, Improvement: -0.00000005447124, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1219
Epoch 1219, Loss: 0.00000010495690, Improvement: -0.00000000188103, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1220
Epoch 1220, Loss: 0.00000010248841, Improvement: -0.00000000246849, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1221
Epoch 1221, Loss: 0.00000009256089, Improvement: -0.00000000992752, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1222
Epoch 1222, Loss: 0.00000009245694, Improvement: -0.00000000010395, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1223
Epoch 1223, Loss: 0.00000009031027, Improvement: -0.00000000214667, Best Loss: 0.00000006152845 in Epoch 1118
Epoch 1224
A best model at epoch 1224 has been saved with training error 0.00000005270565.
Epoch 1224, Loss: 0.00000009662670, Improvement: 0.00000000631643, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1225
Epoch 1225, Loss: 0.00000009411298, Improvement: -0.00000000251372, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1226
Epoch 1226, Loss: 0.00000009201474, Improvement: -0.00000000209824, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1227
Epoch 1227, Loss: 0.00000009137733, Improvement: -0.00000000063741, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1228
Epoch 1228, Loss: 0.00000009342026, Improvement: 0.00000000204293, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1229
Epoch 1229, Loss: 0.00000009284932, Improvement: -0.00000000057093, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1230
Epoch 1230, Loss: 0.00000009871617, Improvement: 0.00000000586684, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1231
Epoch 1231, Loss: 0.00000011968480, Improvement: 0.00000002096864, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1232
Epoch 1232, Loss: 0.00000012009275, Improvement: 0.00000000040795, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1233
Epoch 1233, Loss: 0.00000013717498, Improvement: 0.00000001708223, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1234
Epoch 1234, Loss: 0.00000010244100, Improvement: -0.00000003473398, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1235
Epoch 1235, Loss: 0.00000009675116, Improvement: -0.00000000568984, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1236
Epoch 1236, Loss: 0.00000009111132, Improvement: -0.00000000563984, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1237
Epoch 1237, Loss: 0.00000013360832, Improvement: 0.00000004249700, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1238
Epoch 1238, Loss: 0.00000014843884, Improvement: 0.00000001483052, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1239
Epoch 1239, Loss: 0.00000013369379, Improvement: -0.00000001474505, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1240
Epoch 1240, Loss: 0.00000010267092, Improvement: -0.00000003102286, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1241
Epoch 1241, Loss: 0.00000009814483, Improvement: -0.00000000452610, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1242
Epoch 1242, Loss: 0.00000010285053, Improvement: 0.00000000470571, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1243
Epoch 1243, Loss: 0.00000013530638, Improvement: 0.00000003245585, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1244
Epoch 1244, Loss: 0.00000013803523, Improvement: 0.00000000272885, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1245
Epoch 1245, Loss: 0.00000022818764, Improvement: 0.00000009015242, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1246
Epoch 1246, Loss: 0.00000020897091, Improvement: -0.00000001921673, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1247
Epoch 1247, Loss: 0.00000011025018, Improvement: -0.00000009872073, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1248
Epoch 1248, Loss: 0.00000009773256, Improvement: -0.00000001251762, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1249
Epoch 1249, Loss: 0.00000008917482, Improvement: -0.00000000855774, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000009208072, Improvement: 0.00000000290590, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1251
Epoch 1251, Loss: 0.00000015012836, Improvement: 0.00000005804764, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1252
Epoch 1252, Loss: 0.00000016963385, Improvement: 0.00000001950548, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1253
Epoch 1253, Loss: 0.00000011281998, Improvement: -0.00000005681387, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1254
Epoch 1254, Loss: 0.00000010732353, Improvement: -0.00000000549645, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1255
Epoch 1255, Loss: 0.00000013928842, Improvement: 0.00000003196489, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1256
Epoch 1256, Loss: 0.00000015763947, Improvement: 0.00000001835105, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1257
Epoch 1257, Loss: 0.00000009776227, Improvement: -0.00000005987720, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1258
Epoch 1258, Loss: 0.00000010298831, Improvement: 0.00000000522604, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1259
Epoch 1259, Loss: 0.00000008964512, Improvement: -0.00000001334319, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1260
Epoch 1260, Loss: 0.00000008982261, Improvement: 0.00000000017749, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1261
Epoch 1261, Loss: 0.00000009460734, Improvement: 0.00000000478473, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1262
Epoch 1262, Loss: 0.00000022386592, Improvement: 0.00000012925858, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1263
Epoch 1263, Loss: 0.00000018750987, Improvement: -0.00000003635605, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1264
Epoch 1264, Loss: 0.00000013492217, Improvement: -0.00000005258770, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1265
Epoch 1265, Loss: 0.00000011485669, Improvement: -0.00000002006547, Best Loss: 0.00000005270565 in Epoch 1224
Epoch 1266
A best model at epoch 1266 has been saved with training error 0.00000005189728.
Epoch 1266, Loss: 0.00000009185261, Improvement: -0.00000002300408, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1267
Epoch 1267, Loss: 0.00000008904249, Improvement: -0.00000000281013, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1268
Epoch 1268, Loss: 0.00000009016989, Improvement: 0.00000000112741, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1269
Epoch 1269, Loss: 0.00000009093384, Improvement: 0.00000000076395, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1270
Epoch 1270, Loss: 0.00000008858742, Improvement: -0.00000000234642, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1271
Epoch 1271, Loss: 0.00000011603449, Improvement: 0.00000002744707, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1272
Epoch 1272, Loss: 0.00000010478748, Improvement: -0.00000001124701, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1273
Epoch 1273, Loss: 0.00000009601954, Improvement: -0.00000000876794, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1274
Epoch 1274, Loss: 0.00000011079804, Improvement: 0.00000001477850, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1275
Epoch 1275, Loss: 0.00000019234062, Improvement: 0.00000008154258, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1276
Epoch 1276, Loss: 0.00000014470200, Improvement: -0.00000004763861, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1277
Epoch 1277, Loss: 0.00000009979629, Improvement: -0.00000004490571, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1278
Epoch 1278, Loss: 0.00000012364029, Improvement: 0.00000002384400, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1279
Epoch 1279, Loss: 0.00000011032903, Improvement: -0.00000001331125, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1280
Epoch 1280, Loss: 0.00000021243687, Improvement: 0.00000010210784, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1281
Epoch 1281, Loss: 0.00000015531540, Improvement: -0.00000005712147, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1282
Epoch 1282, Loss: 0.00000010671641, Improvement: -0.00000004859899, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1283
Epoch 1283, Loss: 0.00000008984481, Improvement: -0.00000001687160, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1284
Epoch 1284, Loss: 0.00000008734709, Improvement: -0.00000000249773, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1285
Epoch 1285, Loss: 0.00000009433473, Improvement: 0.00000000698764, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1286
Epoch 1286, Loss: 0.00000010120001, Improvement: 0.00000000686528, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1287
Epoch 1287, Loss: 0.00000009307619, Improvement: -0.00000000812382, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1288
Epoch 1288, Loss: 0.00000009052793, Improvement: -0.00000000254826, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1289
Epoch 1289, Loss: 0.00000010281183, Improvement: 0.00000001228389, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1290
Epoch 1290, Loss: 0.00000009347571, Improvement: -0.00000000933612, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1291
Epoch 1291, Loss: 0.00000014279640, Improvement: 0.00000004932069, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1292
Epoch 1292, Loss: 0.00000011362082, Improvement: -0.00000002917558, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1293
Epoch 1293, Loss: 0.00000012077781, Improvement: 0.00000000715699, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1294
Epoch 1294, Loss: 0.00000015411956, Improvement: 0.00000003334175, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1295
Epoch 1295, Loss: 0.00000012262530, Improvement: -0.00000003149426, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1296
Epoch 1296, Loss: 0.00000013169773, Improvement: 0.00000000907243, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1297
Epoch 1297, Loss: 0.00000015625505, Improvement: 0.00000002455733, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1298
Epoch 1298, Loss: 0.00000039894486, Improvement: 0.00000024268980, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1299
Epoch 1299, Loss: 0.00000015581382, Improvement: -0.00000024313103, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000011729842, Improvement: -0.00000003851540, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1301
Epoch 1301, Loss: 0.00000009631618, Improvement: -0.00000002098224, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1302
Epoch 1302, Loss: 0.00000008653462, Improvement: -0.00000000978156, Best Loss: 0.00000005189728 in Epoch 1266
Epoch 1303
A best model at epoch 1303 has been saved with training error 0.00000005145099.
Epoch 1303, Loss: 0.00000008297116, Improvement: -0.00000000356346, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1304
Epoch 1304, Loss: 0.00000008124397, Improvement: -0.00000000172719, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1305
Epoch 1305, Loss: 0.00000008121156, Improvement: -0.00000000003241, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1306
Epoch 1306, Loss: 0.00000008309381, Improvement: 0.00000000188225, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1307
Epoch 1307, Loss: 0.00000007956016, Improvement: -0.00000000353365, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1308
Epoch 1308, Loss: 0.00000007977078, Improvement: 0.00000000021062, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1309
Epoch 1309, Loss: 0.00000007946711, Improvement: -0.00000000030367, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1310
Epoch 1310, Loss: 0.00000008272841, Improvement: 0.00000000326130, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1311
Epoch 1311, Loss: 0.00000009022255, Improvement: 0.00000000749414, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1312
Epoch 1312, Loss: 0.00000008192598, Improvement: -0.00000000829657, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1313
Epoch 1313, Loss: 0.00000007874767, Improvement: -0.00000000317831, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1314
Epoch 1314, Loss: 0.00000008687614, Improvement: 0.00000000812847, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1315
Epoch 1315, Loss: 0.00000008842788, Improvement: 0.00000000155174, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1316
Epoch 1316, Loss: 0.00000008506300, Improvement: -0.00000000336488, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1317
Epoch 1317, Loss: 0.00000008981469, Improvement: 0.00000000475169, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1318
Epoch 1318, Loss: 0.00000011615237, Improvement: 0.00000002633768, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1319
Epoch 1319, Loss: 0.00000009716745, Improvement: -0.00000001898492, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1320
Epoch 1320, Loss: 0.00000008570284, Improvement: -0.00000001146461, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1321
Epoch 1321, Loss: 0.00000007710243, Improvement: -0.00000000860041, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1322
Epoch 1322, Loss: 0.00000008415581, Improvement: 0.00000000705339, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1323
Epoch 1323, Loss: 0.00000011202005, Improvement: 0.00000002786424, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1324
Epoch 1324, Loss: 0.00000011653613, Improvement: 0.00000000451607, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1325
Epoch 1325, Loss: 0.00000009939896, Improvement: -0.00000001713717, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1326
Epoch 1326, Loss: 0.00000015001035, Improvement: 0.00000005061139, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1327
Epoch 1327, Loss: 0.00000012962232, Improvement: -0.00000002038802, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1328
Epoch 1328, Loss: 0.00000009688382, Improvement: -0.00000003273851, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1329
Epoch 1329, Loss: 0.00000028081060, Improvement: 0.00000018392678, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1330
Epoch 1330, Loss: 0.00000020656873, Improvement: -0.00000007424187, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1331
Epoch 1331, Loss: 0.00000014464064, Improvement: -0.00000006192809, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1332
Epoch 1332, Loss: 0.00000009822400, Improvement: -0.00000004641664, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1333
Epoch 1333, Loss: 0.00000009467194, Improvement: -0.00000000355206, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1334
Epoch 1334, Loss: 0.00000009163612, Improvement: -0.00000000303582, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1335
Epoch 1335, Loss: 0.00000008014114, Improvement: -0.00000001149498, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1336
Epoch 1336, Loss: 0.00000007690973, Improvement: -0.00000000323142, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1337
Epoch 1337, Loss: 0.00000007921746, Improvement: 0.00000000230774, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1338
Epoch 1338, Loss: 0.00000007903372, Improvement: -0.00000000018375, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1339
Epoch 1339, Loss: 0.00000008805273, Improvement: 0.00000000901902, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1340
Epoch 1340, Loss: 0.00000010818604, Improvement: 0.00000002013331, Best Loss: 0.00000005145099 in Epoch 1303
Epoch 1341
A best model at epoch 1341 has been saved with training error 0.00000004698917.
Epoch 1341, Loss: 0.00000008938627, Improvement: -0.00000001879977, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1342
Epoch 1342, Loss: 0.00000007654505, Improvement: -0.00000001284122, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1343
Epoch 1343, Loss: 0.00000007609157, Improvement: -0.00000000045347, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1344
Epoch 1344, Loss: 0.00000009836827, Improvement: 0.00000002227669, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1345
Epoch 1345, Loss: 0.00000007745038, Improvement: -0.00000002091789, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1346
Epoch 1346, Loss: 0.00000012656745, Improvement: 0.00000004911707, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1347
Epoch 1347, Loss: 0.00000011005515, Improvement: -0.00000001651230, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1348
Epoch 1348, Loss: 0.00000019664541, Improvement: 0.00000008659026, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1349
Epoch 1349, Loss: 0.00000012728175, Improvement: -0.00000006936366, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000017795823, Improvement: 0.00000005067648, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1351
Epoch 1351, Loss: 0.00000016693353, Improvement: -0.00000001102470, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1352
Epoch 1352, Loss: 0.00000027294262, Improvement: 0.00000010600909, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1353
Epoch 1353, Loss: 0.00000017047248, Improvement: -0.00000010247014, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1354
Epoch 1354, Loss: 0.00000011704354, Improvement: -0.00000005342894, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1355
Epoch 1355, Loss: 0.00000009941138, Improvement: -0.00000001763217, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1356
Epoch 1356, Loss: 0.00000008447387, Improvement: -0.00000001493751, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1357
Epoch 1357, Loss: 0.00000010027921, Improvement: 0.00000001580535, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1358
Epoch 1358, Loss: 0.00000008619557, Improvement: -0.00000001408365, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1359
Epoch 1359, Loss: 0.00000008969935, Improvement: 0.00000000350378, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1360
Epoch 1360, Loss: 0.00000009042513, Improvement: 0.00000000072578, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1361
Epoch 1361, Loss: 0.00000009060586, Improvement: 0.00000000018073, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1362
Epoch 1362, Loss: 0.00000014198199, Improvement: 0.00000005137613, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1363
Epoch 1363, Loss: 0.00000013549410, Improvement: -0.00000000648789, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1364
Epoch 1364, Loss: 0.00000012478267, Improvement: -0.00000001071143, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1365
Epoch 1365, Loss: 0.00000010097120, Improvement: -0.00000002381148, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1366
Epoch 1366, Loss: 0.00000010782357, Improvement: 0.00000000685238, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1367
Epoch 1367, Loss: 0.00000009678476, Improvement: -0.00000001103881, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1368
Epoch 1368, Loss: 0.00000008806819, Improvement: -0.00000000871658, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1369
Epoch 1369, Loss: 0.00000009417458, Improvement: 0.00000000610639, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1370
Epoch 1370, Loss: 0.00000013301752, Improvement: 0.00000003884294, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1371
Epoch 1371, Loss: 0.00000008956173, Improvement: -0.00000004345579, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1372
Epoch 1372, Loss: 0.00000013480302, Improvement: 0.00000004524129, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1373
Epoch 1373, Loss: 0.00000034859760, Improvement: 0.00000021379458, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1374
Epoch 1374, Loss: 0.00000017493143, Improvement: -0.00000017366618, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1375
Epoch 1375, Loss: 0.00000009657703, Improvement: -0.00000007835440, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1376
Epoch 1376, Loss: 0.00000008086165, Improvement: -0.00000001571538, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1377
Epoch 1377, Loss: 0.00000007448996, Improvement: -0.00000000637169, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1378
Epoch 1378, Loss: 0.00000007208511, Improvement: -0.00000000240485, Best Loss: 0.00000004698917 in Epoch 1341
Epoch 1379
A best model at epoch 1379 has been saved with training error 0.00000004441125.
Epoch 1379, Loss: 0.00000007130808, Improvement: -0.00000000077703, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1380
Epoch 1380, Loss: 0.00000007109270, Improvement: -0.00000000021538, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1381
Epoch 1381, Loss: 0.00000006950026, Improvement: -0.00000000159245, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1382
Epoch 1382, Loss: 0.00000006882353, Improvement: -0.00000000067673, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1383
Epoch 1383, Loss: 0.00000006962290, Improvement: 0.00000000079937, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1384
Epoch 1384, Loss: 0.00000008003889, Improvement: 0.00000001041599, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1385
Epoch 1385, Loss: 0.00000013081609, Improvement: 0.00000005077720, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1386
Epoch 1386, Loss: 0.00000012488488, Improvement: -0.00000000593121, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1387
Epoch 1387, Loss: 0.00000008229895, Improvement: -0.00000004258593, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1388
Epoch 1388, Loss: 0.00000007838975, Improvement: -0.00000000390920, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1389
Epoch 1389, Loss: 0.00000008163680, Improvement: 0.00000000324705, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1390
Epoch 1390, Loss: 0.00000016760121, Improvement: 0.00000008596440, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1391
Epoch 1391, Loss: 0.00000026347389, Improvement: 0.00000009587268, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1392
Epoch 1392, Loss: 0.00000022524766, Improvement: -0.00000003822623, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1393
Epoch 1393, Loss: 0.00000011685450, Improvement: -0.00000010839316, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1394
Epoch 1394, Loss: 0.00000008519412, Improvement: -0.00000003166038, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1395
Epoch 1395, Loss: 0.00000010177991, Improvement: 0.00000001658579, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1396
Epoch 1396, Loss: 0.00000008387858, Improvement: -0.00000001790133, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1397
Epoch 1397, Loss: 0.00000008668494, Improvement: 0.00000000280636, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1398
Epoch 1398, Loss: 0.00000007992036, Improvement: -0.00000000676458, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1399
Epoch 1399, Loss: 0.00000007919468, Improvement: -0.00000000072568, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000006971862, Improvement: -0.00000000947606, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1401
Epoch 1401, Loss: 0.00000006623340, Improvement: -0.00000000348522, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1402
Epoch 1402, Loss: 0.00000006746965, Improvement: 0.00000000123625, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1403
Epoch 1403, Loss: 0.00000007165392, Improvement: 0.00000000418427, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1404
Epoch 1404, Loss: 0.00000006933500, Improvement: -0.00000000231892, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1405
Epoch 1405, Loss: 0.00000008161310, Improvement: 0.00000001227809, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1406
Epoch 1406, Loss: 0.00000010666772, Improvement: 0.00000002505463, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1407
Epoch 1407, Loss: 0.00000007917777, Improvement: -0.00000002748995, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1408
Epoch 1408, Loss: 0.00000006999113, Improvement: -0.00000000918664, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1409
Epoch 1409, Loss: 0.00000006814942, Improvement: -0.00000000184171, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1410
Epoch 1410, Loss: 0.00000014149490, Improvement: 0.00000007334548, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1411
Epoch 1411, Loss: 0.00000019906625, Improvement: 0.00000005757135, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1412
Epoch 1412, Loss: 0.00000013859081, Improvement: -0.00000006047544, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1413
Epoch 1413, Loss: 0.00000008791389, Improvement: -0.00000005067692, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1414
Epoch 1414, Loss: 0.00000009033035, Improvement: 0.00000000241645, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1415
Epoch 1415, Loss: 0.00000011391907, Improvement: 0.00000002358872, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1416
Epoch 1416, Loss: 0.00000009474665, Improvement: -0.00000001917241, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1417
Epoch 1417, Loss: 0.00000008463853, Improvement: -0.00000001010812, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1418
Epoch 1418, Loss: 0.00000009780110, Improvement: 0.00000001316257, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1419
Epoch 1419, Loss: 0.00000008712633, Improvement: -0.00000001067477, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1420
Epoch 1420, Loss: 0.00000009928429, Improvement: 0.00000001215795, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1421
Epoch 1421, Loss: 0.00000010928109, Improvement: 0.00000000999680, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1422
Epoch 1422, Loss: 0.00000015502553, Improvement: 0.00000004574444, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1423
Epoch 1423, Loss: 0.00000025138321, Improvement: 0.00000009635768, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1424
Epoch 1424, Loss: 0.00000027186756, Improvement: 0.00000002048435, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1425
Epoch 1425, Loss: 0.00000024441105, Improvement: -0.00000002745652, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1426
Epoch 1426, Loss: 0.00000013892418, Improvement: -0.00000010548686, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1427
Epoch 1427, Loss: 0.00000008498398, Improvement: -0.00000005394021, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1428
Epoch 1428, Loss: 0.00000007577479, Improvement: -0.00000000920919, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1429
Epoch 1429, Loss: 0.00000006876948, Improvement: -0.00000000700531, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1430
Epoch 1430, Loss: 0.00000006798195, Improvement: -0.00000000078753, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1431
Epoch 1431, Loss: 0.00000007182658, Improvement: 0.00000000384464, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1432
Epoch 1432, Loss: 0.00000007838814, Improvement: 0.00000000656155, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1433
Epoch 1433, Loss: 0.00000006989527, Improvement: -0.00000000849286, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1434
Epoch 1434, Loss: 0.00000008512874, Improvement: 0.00000001523347, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1435
Epoch 1435, Loss: 0.00000016535024, Improvement: 0.00000008022149, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1436
Epoch 1436, Loss: 0.00000015086396, Improvement: -0.00000001448628, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1437
Epoch 1437, Loss: 0.00000010712866, Improvement: -0.00000004373530, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1438
Epoch 1438, Loss: 0.00000009340831, Improvement: -0.00000001372035, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1439
Epoch 1439, Loss: 0.00000009028323, Improvement: -0.00000000312508, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1440
Epoch 1440, Loss: 0.00000009397573, Improvement: 0.00000000369250, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1441
Epoch 1441, Loss: 0.00000007383792, Improvement: -0.00000002013781, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1442
Epoch 1442, Loss: 0.00000006602186, Improvement: -0.00000000781607, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1443
Epoch 1443, Loss: 0.00000006587969, Improvement: -0.00000000014217, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1444
Epoch 1444, Loss: 0.00000006614060, Improvement: 0.00000000026092, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1445
Epoch 1445, Loss: 0.00000007418124, Improvement: 0.00000000804063, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1446
Epoch 1446, Loss: 0.00000011892096, Improvement: 0.00000004473972, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1447
Epoch 1447, Loss: 0.00000012784371, Improvement: 0.00000000892276, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1448
Epoch 1448, Loss: 0.00000009177984, Improvement: -0.00000003606388, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1449
Epoch 1449, Loss: 0.00000009602681, Improvement: 0.00000000424697, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000013351273, Improvement: 0.00000003748593, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1451
Epoch 1451, Loss: 0.00000018752010, Improvement: 0.00000005400737, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1452
Epoch 1452, Loss: 0.00000022374218, Improvement: 0.00000003622208, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1453
Epoch 1453, Loss: 0.00000010664660, Improvement: -0.00000011709559, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1454
Epoch 1454, Loss: 0.00000007505249, Improvement: -0.00000003159411, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1455
Epoch 1455, Loss: 0.00000007723708, Improvement: 0.00000000218460, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1456
Epoch 1456, Loss: 0.00000007100054, Improvement: -0.00000000623654, Best Loss: 0.00000004441125 in Epoch 1379
Epoch 1457
A best model at epoch 1457 has been saved with training error 0.00000004146030.
Epoch 1457, Loss: 0.00000006505888, Improvement: -0.00000000594166, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1458
Epoch 1458, Loss: 0.00000006235326, Improvement: -0.00000000270562, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1459
Epoch 1459, Loss: 0.00000007162565, Improvement: 0.00000000927239, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1460
Epoch 1460, Loss: 0.00000008260425, Improvement: 0.00000001097860, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1461
Epoch 1461, Loss: 0.00000016174078, Improvement: 0.00000007913654, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1462
Epoch 1462, Loss: 0.00000010317823, Improvement: -0.00000005856256, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1463
Epoch 1463, Loss: 0.00000008100070, Improvement: -0.00000002217753, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1464
Epoch 1464, Loss: 0.00000008501248, Improvement: 0.00000000401178, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1465
Epoch 1465, Loss: 0.00000010192886, Improvement: 0.00000001691638, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1466
Epoch 1466, Loss: 0.00000010098704, Improvement: -0.00000000094183, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1467
Epoch 1467, Loss: 0.00000008364817, Improvement: -0.00000001733887, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1468
Epoch 1468, Loss: 0.00000006765824, Improvement: -0.00000001598993, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1469
Epoch 1469, Loss: 0.00000006115420, Improvement: -0.00000000650405, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1470
Epoch 1470, Loss: 0.00000007325948, Improvement: 0.00000001210529, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1471
Epoch 1471, Loss: 0.00000007375413, Improvement: 0.00000000049465, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1472
Epoch 1472, Loss: 0.00000016476060, Improvement: 0.00000009100648, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1473
Epoch 1473, Loss: 0.00000012572686, Improvement: -0.00000003903375, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1474
Epoch 1474, Loss: 0.00000013223682, Improvement: 0.00000000650996, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1475
Epoch 1475, Loss: 0.00000009665229, Improvement: -0.00000003558452, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1476
Epoch 1476, Loss: 0.00000013127131, Improvement: 0.00000003461901, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1477
Epoch 1477, Loss: 0.00000010577027, Improvement: -0.00000002550103, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1478
Epoch 1478, Loss: 0.00000014064299, Improvement: 0.00000003487272, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1479
Epoch 1479, Loss: 0.00000015927514, Improvement: 0.00000001863215, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1480
Epoch 1480, Loss: 0.00000012418930, Improvement: -0.00000003508584, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1481
Epoch 1481, Loss: 0.00000014597075, Improvement: 0.00000002178145, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1482
Epoch 1482, Loss: 0.00000011163861, Improvement: -0.00000003433214, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1483
Epoch 1483, Loss: 0.00000008330421, Improvement: -0.00000002833440, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1484
Epoch 1484, Loss: 0.00000007400669, Improvement: -0.00000000929752, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1485
Epoch 1485, Loss: 0.00000006573236, Improvement: -0.00000000827433, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1486
Epoch 1486, Loss: 0.00000006346339, Improvement: -0.00000000226897, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1487
Epoch 1487, Loss: 0.00000006572289, Improvement: 0.00000000225951, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1488
Epoch 1488, Loss: 0.00000009552569, Improvement: 0.00000002980280, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1489
Epoch 1489, Loss: 0.00000014394971, Improvement: 0.00000004842401, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1490
Epoch 1490, Loss: 0.00000020715490, Improvement: 0.00000006320520, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1491
Epoch 1491, Loss: 0.00000016733978, Improvement: -0.00000003981512, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1492
Epoch 1492, Loss: 0.00000007775555, Improvement: -0.00000008958423, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1493
Epoch 1493, Loss: 0.00000007184103, Improvement: -0.00000000591453, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1494
Epoch 1494, Loss: 0.00000008778894, Improvement: 0.00000001594792, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1495
Epoch 1495, Loss: 0.00000008155005, Improvement: -0.00000000623889, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1496
Epoch 1496, Loss: 0.00000007130138, Improvement: -0.00000001024867, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1497
Epoch 1497, Loss: 0.00000006476080, Improvement: -0.00000000654058, Best Loss: 0.00000004146030 in Epoch 1457
Epoch 1498
A best model at epoch 1498 has been saved with training error 0.00000003444506.
Epoch 1498, Loss: 0.00000005973491, Improvement: -0.00000000502588, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1499
Epoch 1499, Loss: 0.00000006570305, Improvement: 0.00000000596814, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000007317581, Improvement: 0.00000000747275, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1501
Epoch 1501, Loss: 0.00000011611777, Improvement: 0.00000004294197, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1502
Epoch 1502, Loss: 0.00000010119959, Improvement: -0.00000001491818, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1503
Epoch 1503, Loss: 0.00000012102251, Improvement: 0.00000001982292, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1504
Epoch 1504, Loss: 0.00000017841331, Improvement: 0.00000005739081, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1505
Epoch 1505, Loss: 0.00000009017221, Improvement: -0.00000008824110, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1506
Epoch 1506, Loss: 0.00000008544771, Improvement: -0.00000000472450, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1507
Epoch 1507, Loss: 0.00000006605140, Improvement: -0.00000001939631, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1508
Epoch 1508, Loss: 0.00000006889931, Improvement: 0.00000000284791, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1509
Epoch 1509, Loss: 0.00000007021602, Improvement: 0.00000000131671, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1510
Epoch 1510, Loss: 0.00000007657213, Improvement: 0.00000000635610, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1511
Epoch 1511, Loss: 0.00000014338096, Improvement: 0.00000006680883, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1512
Epoch 1512, Loss: 0.00000020361323, Improvement: 0.00000006023227, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1513
Epoch 1513, Loss: 0.00000019040333, Improvement: -0.00000001320990, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1514
Epoch 1514, Loss: 0.00000014429266, Improvement: -0.00000004611067, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1515
Epoch 1515, Loss: 0.00000010597817, Improvement: -0.00000003831449, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1516
Epoch 1516, Loss: 0.00000007660821, Improvement: -0.00000002936996, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1517
Epoch 1517, Loss: 0.00000007805664, Improvement: 0.00000000144843, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1518
Epoch 1518, Loss: 0.00000011779419, Improvement: 0.00000003973755, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1519
Epoch 1519, Loss: 0.00000010394908, Improvement: -0.00000001384512, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1520
Epoch 1520, Loss: 0.00000011903466, Improvement: 0.00000001508558, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1521
Epoch 1521, Loss: 0.00000008773811, Improvement: -0.00000003129655, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1522
Epoch 1522, Loss: 0.00000007007305, Improvement: -0.00000001766506, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1523
Epoch 1523, Loss: 0.00000006837106, Improvement: -0.00000000170200, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1524
Epoch 1524, Loss: 0.00000006095465, Improvement: -0.00000000741641, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1525
Epoch 1525, Loss: 0.00000005595834, Improvement: -0.00000000499631, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1526
Epoch 1526, Loss: 0.00000011414097, Improvement: 0.00000005818263, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1527
Epoch 1527, Loss: 0.00000011661654, Improvement: 0.00000000247558, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1528
Epoch 1528, Loss: 0.00000009550301, Improvement: -0.00000002111353, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1529
Epoch 1529, Loss: 0.00000008452747, Improvement: -0.00000001097554, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1530
Epoch 1530, Loss: 0.00000006953313, Improvement: -0.00000001499434, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1531
Epoch 1531, Loss: 0.00000009905910, Improvement: 0.00000002952597, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1532
Epoch 1532, Loss: 0.00000009796723, Improvement: -0.00000000109188, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1533
Epoch 1533, Loss: 0.00000008423086, Improvement: -0.00000001373637, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1534
Epoch 1534, Loss: 0.00000011663373, Improvement: 0.00000003240287, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1535
Epoch 1535, Loss: 0.00000009167155, Improvement: -0.00000002496217, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1536
Epoch 1536, Loss: 0.00000012587118, Improvement: 0.00000003419962, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1537
Epoch 1537, Loss: 0.00000019428659, Improvement: 0.00000006841542, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1538
Epoch 1538, Loss: 0.00000011580216, Improvement: -0.00000007848444, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1539
Epoch 1539, Loss: 0.00000008362525, Improvement: -0.00000003217691, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1540
Epoch 1540, Loss: 0.00000006788621, Improvement: -0.00000001573904, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1541
Epoch 1541, Loss: 0.00000006862455, Improvement: 0.00000000073834, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1542
Epoch 1542, Loss: 0.00000006028575, Improvement: -0.00000000833880, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1543
Epoch 1543, Loss: 0.00000005662345, Improvement: -0.00000000366230, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1544
Epoch 1544, Loss: 0.00000006364705, Improvement: 0.00000000702360, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1545
Epoch 1545, Loss: 0.00000008496206, Improvement: 0.00000002131501, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1546
Epoch 1546, Loss: 0.00000010833948, Improvement: 0.00000002337742, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1547
Epoch 1547, Loss: 0.00000011380827, Improvement: 0.00000000546879, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1548
Epoch 1548, Loss: 0.00000009913380, Improvement: -0.00000001467447, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1549
Epoch 1549, Loss: 0.00000010336425, Improvement: 0.00000000423045, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000014198402, Improvement: 0.00000003861977, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1551
Epoch 1551, Loss: 0.00000020619246, Improvement: 0.00000006420844, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1552
Epoch 1552, Loss: 0.00000011319224, Improvement: -0.00000009300022, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1553
Epoch 1553, Loss: 0.00000009844028, Improvement: -0.00000001475196, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1554
Epoch 1554, Loss: 0.00000008401685, Improvement: -0.00000001442344, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1555
Epoch 1555, Loss: 0.00000008803957, Improvement: 0.00000000402272, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1556
Epoch 1556, Loss: 0.00000006959293, Improvement: -0.00000001844664, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1557
Epoch 1557, Loss: 0.00000006290212, Improvement: -0.00000000669082, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1558
Epoch 1558, Loss: 0.00000005510685, Improvement: -0.00000000779527, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1559
Epoch 1559, Loss: 0.00000005444670, Improvement: -0.00000000066015, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1560
Epoch 1560, Loss: 0.00000007311756, Improvement: 0.00000001867086, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1561
Epoch 1561, Loss: 0.00000015804926, Improvement: 0.00000008493170, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1562
Epoch 1562, Loss: 0.00000021516528, Improvement: 0.00000005711602, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1563
Epoch 1563, Loss: 0.00000010986764, Improvement: -0.00000010529764, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1564
Epoch 1564, Loss: 0.00000009320109, Improvement: -0.00000001666655, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1565
Epoch 1565, Loss: 0.00000006347343, Improvement: -0.00000002972766, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1566
Epoch 1566, Loss: 0.00000005744425, Improvement: -0.00000000602918, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1567
Epoch 1567, Loss: 0.00000005368786, Improvement: -0.00000000375639, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1568
Epoch 1568, Loss: 0.00000005300640, Improvement: -0.00000000068146, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1569
Epoch 1569, Loss: 0.00000005716204, Improvement: 0.00000000415565, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1570
Epoch 1570, Loss: 0.00000009281322, Improvement: 0.00000003565117, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1571
Epoch 1571, Loss: 0.00000015704020, Improvement: 0.00000006422698, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1572
Epoch 1572, Loss: 0.00000012294352, Improvement: -0.00000003409667, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1573
Epoch 1573, Loss: 0.00000008979410, Improvement: -0.00000003314943, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1574
Epoch 1574, Loss: 0.00000009710998, Improvement: 0.00000000731588, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1575
Epoch 1575, Loss: 0.00000009968118, Improvement: 0.00000000257121, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1576
Epoch 1576, Loss: 0.00000012918855, Improvement: 0.00000002950737, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1577
Epoch 1577, Loss: 0.00000019855606, Improvement: 0.00000006936751, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1578
Epoch 1578, Loss: 0.00000021121313, Improvement: 0.00000001265707, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1579
Epoch 1579, Loss: 0.00000010943395, Improvement: -0.00000010177919, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1580
Epoch 1580, Loss: 0.00000008215815, Improvement: -0.00000002727580, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1581
Epoch 1581, Loss: 0.00000007168860, Improvement: -0.00000001046956, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1582
Epoch 1582, Loss: 0.00000006171570, Improvement: -0.00000000997289, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1583
Epoch 1583, Loss: 0.00000005537845, Improvement: -0.00000000633726, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1584
Epoch 1584, Loss: 0.00000005342646, Improvement: -0.00000000195199, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1585
Epoch 1585, Loss: 0.00000005641365, Improvement: 0.00000000298719, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1586
Epoch 1586, Loss: 0.00000007122927, Improvement: 0.00000001481563, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1587
Epoch 1587, Loss: 0.00000011400077, Improvement: 0.00000004277150, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1588
Epoch 1588, Loss: 0.00000010665537, Improvement: -0.00000000734540, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1589
Epoch 1589, Loss: 0.00000010824524, Improvement: 0.00000000158988, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1590
Epoch 1590, Loss: 0.00000008255854, Improvement: -0.00000002568670, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1591
Epoch 1591, Loss: 0.00000007190327, Improvement: -0.00000001065527, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1592
Epoch 1592, Loss: 0.00000005927386, Improvement: -0.00000001262941, Best Loss: 0.00000003444506 in Epoch 1498
Epoch 1593
A best model at epoch 1593 has been saved with training error 0.00000003324523.
Epoch 1593, Loss: 0.00000005703496, Improvement: -0.00000000223889, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1594
Epoch 1594, Loss: 0.00000008192851, Improvement: 0.00000002489355, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1595
Epoch 1595, Loss: 0.00000008508561, Improvement: 0.00000000315710, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1596
Epoch 1596, Loss: 0.00000007472542, Improvement: -0.00000001036019, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1597
Epoch 1597, Loss: 0.00000006978190, Improvement: -0.00000000494352, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1598
Epoch 1598, Loss: 0.00000010319486, Improvement: 0.00000003341297, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1599
Epoch 1599, Loss: 0.00000024372243, Improvement: 0.00000014052757, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000019751941, Improvement: -0.00000004620303, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1601
Epoch 1601, Loss: 0.00000009049323, Improvement: -0.00000010702618, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1602
Epoch 1602, Loss: 0.00000006146687, Improvement: -0.00000002902636, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1603
Epoch 1603, Loss: 0.00000005659125, Improvement: -0.00000000487563, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1604
Epoch 1604, Loss: 0.00000005573960, Improvement: -0.00000000085165, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1605
Epoch 1605, Loss: 0.00000005289899, Improvement: -0.00000000284061, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1606
Epoch 1606, Loss: 0.00000005309945, Improvement: 0.00000000020046, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1607
Epoch 1607, Loss: 0.00000006593813, Improvement: 0.00000001283868, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1608
Epoch 1608, Loss: 0.00000007480722, Improvement: 0.00000000886909, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1609
Epoch 1609, Loss: 0.00000010645045, Improvement: 0.00000003164324, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1610
Epoch 1610, Loss: 0.00000010391632, Improvement: -0.00000000253414, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1611
Epoch 1611, Loss: 0.00000008267753, Improvement: -0.00000002123879, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1612
Epoch 1612, Loss: 0.00000006387235, Improvement: -0.00000001880518, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1613
Epoch 1613, Loss: 0.00000006301877, Improvement: -0.00000000085357, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1614
Epoch 1614, Loss: 0.00000006322775, Improvement: 0.00000000020898, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1615
Epoch 1615, Loss: 0.00000006190086, Improvement: -0.00000000132689, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1616
Epoch 1616, Loss: 0.00000008549164, Improvement: 0.00000002359077, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1617
Epoch 1617, Loss: 0.00000022496246, Improvement: 0.00000013947082, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1618
Epoch 1618, Loss: 0.00000027154710, Improvement: 0.00000004658464, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1619
Epoch 1619, Loss: 0.00000013793985, Improvement: -0.00000013360725, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1620
Epoch 1620, Loss: 0.00000007177712, Improvement: -0.00000006616273, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1621
Epoch 1621, Loss: 0.00000006369989, Improvement: -0.00000000807724, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1622
Epoch 1622, Loss: 0.00000005445309, Improvement: -0.00000000924680, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1623
Epoch 1623, Loss: 0.00000005019961, Improvement: -0.00000000425348, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1624
Epoch 1624, Loss: 0.00000004870692, Improvement: -0.00000000149269, Best Loss: 0.00000003324523 in Epoch 1593
Epoch 1625
A best model at epoch 1625 has been saved with training error 0.00000003026892.
Epoch 1625, Loss: 0.00000004951193, Improvement: 0.00000000080502, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1626
Epoch 1626, Loss: 0.00000005116914, Improvement: 0.00000000165721, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1627
Epoch 1627, Loss: 0.00000004718968, Improvement: -0.00000000397946, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1628
Epoch 1628, Loss: 0.00000006752126, Improvement: 0.00000002033158, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1629
Epoch 1629, Loss: 0.00000011204625, Improvement: 0.00000004452499, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1630
Epoch 1630, Loss: 0.00000011816960, Improvement: 0.00000000612335, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1631
Epoch 1631, Loss: 0.00000017469793, Improvement: 0.00000005652832, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1632
Epoch 1632, Loss: 0.00000008439956, Improvement: -0.00000009029837, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1633
Epoch 1633, Loss: 0.00000005695909, Improvement: -0.00000002744047, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1634
Epoch 1634, Loss: 0.00000005102691, Improvement: -0.00000000593219, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1635
Epoch 1635, Loss: 0.00000004785220, Improvement: -0.00000000317471, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1636
Epoch 1636, Loss: 0.00000004841539, Improvement: 0.00000000056320, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1637
Epoch 1637, Loss: 0.00000004594255, Improvement: -0.00000000247284, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1638
Epoch 1638, Loss: 0.00000004466693, Improvement: -0.00000000127562, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1639
Epoch 1639, Loss: 0.00000004792777, Improvement: 0.00000000326084, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1640
Epoch 1640, Loss: 0.00000019089180, Improvement: 0.00000014296402, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1641
Epoch 1641, Loss: 0.00000043339065, Improvement: 0.00000024249886, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1642
Epoch 1642, Loss: 0.00000020205825, Improvement: -0.00000023133240, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1643
Epoch 1643, Loss: 0.00000009762553, Improvement: -0.00000010443273, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1644
Epoch 1644, Loss: 0.00000007634605, Improvement: -0.00000002127947, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1645
Epoch 1645, Loss: 0.00000006043927, Improvement: -0.00000001590678, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1646
Epoch 1646, Loss: 0.00000005610044, Improvement: -0.00000000433884, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1647
Epoch 1647, Loss: 0.00000005558478, Improvement: -0.00000000051566, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1648
Epoch 1648, Loss: 0.00000005152689, Improvement: -0.00000000405789, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1649
Epoch 1649, Loss: 0.00000004935792, Improvement: -0.00000000216896, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000004779803, Improvement: -0.00000000155989, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1651
Epoch 1651, Loss: 0.00000004656837, Improvement: -0.00000000122966, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1652
Epoch 1652, Loss: 0.00000004669064, Improvement: 0.00000000012228, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1653
Epoch 1653, Loss: 0.00000004692209, Improvement: 0.00000000023145, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1654
Epoch 1654, Loss: 0.00000004430514, Improvement: -0.00000000261696, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1655
Epoch 1655, Loss: 0.00000004400826, Improvement: -0.00000000029688, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1656
Epoch 1656, Loss: 0.00000004216773, Improvement: -0.00000000184053, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1657
Epoch 1657, Loss: 0.00000004495484, Improvement: 0.00000000278711, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1658
Epoch 1658, Loss: 0.00000005087164, Improvement: 0.00000000591680, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1659
Epoch 1659, Loss: 0.00000009346810, Improvement: 0.00000004259646, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1660
Epoch 1660, Loss: 0.00000008514847, Improvement: -0.00000000831963, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1661
Epoch 1661, Loss: 0.00000009264970, Improvement: 0.00000000750123, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1662
Epoch 1662, Loss: 0.00000006453932, Improvement: -0.00000002811038, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1663
Epoch 1663, Loss: 0.00000005064014, Improvement: -0.00000001389917, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1664
Epoch 1664, Loss: 0.00000004324716, Improvement: -0.00000000739298, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1665
Epoch 1665, Loss: 0.00000004305703, Improvement: -0.00000000019013, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1666
Epoch 1666, Loss: 0.00000004835824, Improvement: 0.00000000530121, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1667
Epoch 1667, Loss: 0.00000004215888, Improvement: -0.00000000619936, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1668
Epoch 1668, Loss: 0.00000004969666, Improvement: 0.00000000753778, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1669
Epoch 1669, Loss: 0.00000008522399, Improvement: 0.00000003552733, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1670
Epoch 1670, Loss: 0.00000009432661, Improvement: 0.00000000910262, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1671
Epoch 1671, Loss: 0.00000010699758, Improvement: 0.00000001267097, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1672
Epoch 1672, Loss: 0.00000010325650, Improvement: -0.00000000374109, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1673
Epoch 1673, Loss: 0.00000011686677, Improvement: 0.00000001361027, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1674
Epoch 1674, Loss: 0.00000028923985, Improvement: 0.00000017237309, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1675
Epoch 1675, Loss: 0.00000021322674, Improvement: -0.00000007601311, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1676
Epoch 1676, Loss: 0.00000013374085, Improvement: -0.00000007948589, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1677
Epoch 1677, Loss: 0.00000008970248, Improvement: -0.00000004403838, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1678
Epoch 1678, Loss: 0.00000006865015, Improvement: -0.00000002105232, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1679
Epoch 1679, Loss: 0.00000008589544, Improvement: 0.00000001724529, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1680
Epoch 1680, Loss: 0.00000006616218, Improvement: -0.00000001973326, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1681
Epoch 1681, Loss: 0.00000004989031, Improvement: -0.00000001627187, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1682
Epoch 1682, Loss: 0.00000004669528, Improvement: -0.00000000319503, Best Loss: 0.00000003026892 in Epoch 1625
Epoch 1683
A best model at epoch 1683 has been saved with training error 0.00000002802830.
Epoch 1683, Loss: 0.00000004480362, Improvement: -0.00000000189166, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1684
Epoch 1684, Loss: 0.00000004426452, Improvement: -0.00000000053910, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1685
Epoch 1685, Loss: 0.00000004064266, Improvement: -0.00000000362186, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1686
Epoch 1686, Loss: 0.00000004973093, Improvement: 0.00000000908826, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1687
Epoch 1687, Loss: 0.00000004977615, Improvement: 0.00000000004522, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1688
Epoch 1688, Loss: 0.00000005711873, Improvement: 0.00000000734258, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1689
Epoch 1689, Loss: 0.00000009449660, Improvement: 0.00000003737787, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1690
Epoch 1690, Loss: 0.00000016805171, Improvement: 0.00000007355512, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1691
Epoch 1691, Loss: 0.00000008579564, Improvement: -0.00000008225608, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1692
Epoch 1692, Loss: 0.00000005285993, Improvement: -0.00000003293571, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1693
Epoch 1693, Loss: 0.00000004598241, Improvement: -0.00000000687752, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1694
Epoch 1694, Loss: 0.00000004410063, Improvement: -0.00000000188178, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1695
Epoch 1695, Loss: 0.00000004075013, Improvement: -0.00000000335050, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1696
Epoch 1696, Loss: 0.00000004009680, Improvement: -0.00000000065332, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1697
Epoch 1697, Loss: 0.00000004650625, Improvement: 0.00000000640945, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1698
Epoch 1698, Loss: 0.00000006893954, Improvement: 0.00000002243329, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1699
Epoch 1699, Loss: 0.00000016808171, Improvement: 0.00000009914217, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000034810535, Improvement: 0.00000018002364, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1701
Epoch 1701, Loss: 0.00000010853523, Improvement: -0.00000023957012, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1702
Epoch 1702, Loss: 0.00000006574743, Improvement: -0.00000004278780, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1703
Epoch 1703, Loss: 0.00000005429673, Improvement: -0.00000001145070, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1704
Epoch 1704, Loss: 0.00000004911748, Improvement: -0.00000000517925, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1705
Epoch 1705, Loss: 0.00000004524735, Improvement: -0.00000000387013, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1706
Epoch 1706, Loss: 0.00000004472222, Improvement: -0.00000000052513, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1707
Epoch 1707, Loss: 0.00000004179183, Improvement: -0.00000000293039, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1708
Epoch 1708, Loss: 0.00000004092371, Improvement: -0.00000000086812, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1709
Epoch 1709, Loss: 0.00000003986197, Improvement: -0.00000000106174, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1710
Epoch 1710, Loss: 0.00000003940975, Improvement: -0.00000000045222, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1711
Epoch 1711, Loss: 0.00000004328670, Improvement: 0.00000000387695, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1712
Epoch 1712, Loss: 0.00000007926098, Improvement: 0.00000003597428, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1713
Epoch 1713, Loss: 0.00000014360183, Improvement: 0.00000006434085, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1714
Epoch 1714, Loss: 0.00000008407955, Improvement: -0.00000005952228, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1715
Epoch 1715, Loss: 0.00000005613553, Improvement: -0.00000002794403, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1716
Epoch 1716, Loss: 0.00000006260314, Improvement: 0.00000000646762, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1717
Epoch 1717, Loss: 0.00000006312004, Improvement: 0.00000000051690, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1718
Epoch 1718, Loss: 0.00000004716958, Improvement: -0.00000001595046, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1719
Epoch 1719, Loss: 0.00000004140143, Improvement: -0.00000000576815, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1720
Epoch 1720, Loss: 0.00000003984083, Improvement: -0.00000000156060, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1721
Epoch 1721, Loss: 0.00000004133225, Improvement: 0.00000000149142, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1722
Epoch 1722, Loss: 0.00000003812486, Improvement: -0.00000000320739, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1723
Epoch 1723, Loss: 0.00000004051760, Improvement: 0.00000000239274, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1724
Epoch 1724, Loss: 0.00000003859753, Improvement: -0.00000000192007, Best Loss: 0.00000002802830 in Epoch 1683
Epoch 1725
A best model at epoch 1725 has been saved with training error 0.00000002791950.
Epoch 1725, Loss: 0.00000003995679, Improvement: 0.00000000135926, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1726
Epoch 1726, Loss: 0.00000004011342, Improvement: 0.00000000015664, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1727
Epoch 1727, Loss: 0.00000005421981, Improvement: 0.00000001410639, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1728
Epoch 1728, Loss: 0.00000009852497, Improvement: 0.00000004430516, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1729
Epoch 1729, Loss: 0.00000008485991, Improvement: -0.00000001366506, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1730
Epoch 1730, Loss: 0.00000009364924, Improvement: 0.00000000878933, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1731
Epoch 1731, Loss: 0.00000005572360, Improvement: -0.00000003792565, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1732
Epoch 1732, Loss: 0.00000004773685, Improvement: -0.00000000798675, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1733
Epoch 1733, Loss: 0.00000004144733, Improvement: -0.00000000628952, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1734
Epoch 1734, Loss: 0.00000007824713, Improvement: 0.00000003679980, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1735
Epoch 1735, Loss: 0.00000019849039, Improvement: 0.00000012024326, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1736
Epoch 1736, Loss: 0.00000017888368, Improvement: -0.00000001960671, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1737
Epoch 1737, Loss: 0.00000010776209, Improvement: -0.00000007112159, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1738
Epoch 1738, Loss: 0.00000007515371, Improvement: -0.00000003260838, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1739
Epoch 1739, Loss: 0.00000005888839, Improvement: -0.00000001626532, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1740
Epoch 1740, Loss: 0.00000004776201, Improvement: -0.00000001112639, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1741
Epoch 1741, Loss: 0.00000004388515, Improvement: -0.00000000387686, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1742
Epoch 1742, Loss: 0.00000004086423, Improvement: -0.00000000302092, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1743
Epoch 1743, Loss: 0.00000003844559, Improvement: -0.00000000241864, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1744
Epoch 1744, Loss: 0.00000003840049, Improvement: -0.00000000004510, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1745
Epoch 1745, Loss: 0.00000004024628, Improvement: 0.00000000184579, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1746
Epoch 1746, Loss: 0.00000005492196, Improvement: 0.00000001467568, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1747
Epoch 1747, Loss: 0.00000008106344, Improvement: 0.00000002614148, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1748
Epoch 1748, Loss: 0.00000009773225, Improvement: 0.00000001666881, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1749
Epoch 1749, Loss: 0.00000009182074, Improvement: -0.00000000591151, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000018554396, Improvement: 0.00000009372322, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1751
Epoch 1751, Loss: 0.00000013125041, Improvement: -0.00000005429354, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1752
Epoch 1752, Loss: 0.00000008757365, Improvement: -0.00000004367676, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1753
Epoch 1753, Loss: 0.00000005460248, Improvement: -0.00000003297117, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1754
Epoch 1754, Loss: 0.00000004818208, Improvement: -0.00000000642041, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1755
Epoch 1755, Loss: 0.00000004990246, Improvement: 0.00000000172039, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1756
Epoch 1756, Loss: 0.00000005647501, Improvement: 0.00000000657255, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1757
Epoch 1757, Loss: 0.00000004758024, Improvement: -0.00000000889477, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1758
Epoch 1758, Loss: 0.00000005123204, Improvement: 0.00000000365180, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1759
Epoch 1759, Loss: 0.00000004492834, Improvement: -0.00000000630370, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1760
Epoch 1760, Loss: 0.00000005568621, Improvement: 0.00000001075787, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1761
Epoch 1761, Loss: 0.00000007892754, Improvement: 0.00000002324133, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1762
Epoch 1762, Loss: 0.00000007734328, Improvement: -0.00000000158426, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1763
Epoch 1763, Loss: 0.00000007871471, Improvement: 0.00000000137143, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1764
Epoch 1764, Loss: 0.00000006156714, Improvement: -0.00000001714758, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1765
Epoch 1765, Loss: 0.00000006752565, Improvement: 0.00000000595851, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1766
Epoch 1766, Loss: 0.00000004686678, Improvement: -0.00000002065887, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1767
Epoch 1767, Loss: 0.00000003918708, Improvement: -0.00000000767970, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1768
Epoch 1768, Loss: 0.00000005198598, Improvement: 0.00000001279890, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1769
Epoch 1769, Loss: 0.00000017712568, Improvement: 0.00000012513970, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1770
Epoch 1770, Loss: 0.00000009238665, Improvement: -0.00000008473904, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1771
Epoch 1771, Loss: 0.00000005002751, Improvement: -0.00000004235913, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1772
Epoch 1772, Loss: 0.00000004533150, Improvement: -0.00000000469601, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1773
Epoch 1773, Loss: 0.00000003950198, Improvement: -0.00000000582952, Best Loss: 0.00000002791950 in Epoch 1725
Epoch 1774
A best model at epoch 1774 has been saved with training error 0.00000002555116.
A best model at epoch 1774 has been saved with training error 0.00000002397081.
Epoch 1774, Loss: 0.00000003582276, Improvement: -0.00000000367921, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1775
Epoch 1775, Loss: 0.00000003905316, Improvement: 0.00000000323040, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1776
Epoch 1776, Loss: 0.00000004835974, Improvement: 0.00000000930658, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1777
Epoch 1777, Loss: 0.00000004490417, Improvement: -0.00000000345557, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1778
Epoch 1778, Loss: 0.00000006988762, Improvement: 0.00000002498345, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1779
Epoch 1779, Loss: 0.00000005604065, Improvement: -0.00000001384697, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1780
Epoch 1780, Loss: 0.00000005583291, Improvement: -0.00000000020774, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1781
Epoch 1781, Loss: 0.00000004801639, Improvement: -0.00000000781652, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1782
Epoch 1782, Loss: 0.00000004997524, Improvement: 0.00000000195885, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1783
Epoch 1783, Loss: 0.00000006618199, Improvement: 0.00000001620675, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1784
Epoch 1784, Loss: 0.00000012763544, Improvement: 0.00000006145345, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1785
Epoch 1785, Loss: 0.00000014654047, Improvement: 0.00000001890504, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1786
Epoch 1786, Loss: 0.00000010862040, Improvement: -0.00000003792008, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1787
Epoch 1787, Loss: 0.00000004838938, Improvement: -0.00000006023102, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1788
Epoch 1788, Loss: 0.00000004911489, Improvement: 0.00000000072551, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1789
Epoch 1789, Loss: 0.00000003984189, Improvement: -0.00000000927299, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1790
Epoch 1790, Loss: 0.00000003786634, Improvement: -0.00000000197556, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1791
Epoch 1791, Loss: 0.00000003654908, Improvement: -0.00000000131725, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1792
Epoch 1792, Loss: 0.00000004389475, Improvement: 0.00000000734567, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1793
Epoch 1793, Loss: 0.00000015962080, Improvement: 0.00000011572604, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1794
Epoch 1794, Loss: 0.00000021211136, Improvement: 0.00000005249057, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1795
Epoch 1795, Loss: 0.00000022284941, Improvement: 0.00000001073805, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1796
Epoch 1796, Loss: 0.00000011651933, Improvement: -0.00000010633008, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1797
Epoch 1797, Loss: 0.00000008661219, Improvement: -0.00000002990713, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1798
Epoch 1798, Loss: 0.00000008086624, Improvement: -0.00000000574595, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1799
Epoch 1799, Loss: 0.00000005205079, Improvement: -0.00000002881546, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000004296828, Improvement: -0.00000000908251, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1801
Epoch 1801, Loss: 0.00000003954534, Improvement: -0.00000000342294, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1802
Epoch 1802, Loss: 0.00000003759669, Improvement: -0.00000000194866, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1803
Epoch 1803, Loss: 0.00000003708836, Improvement: -0.00000000050832, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1804
Epoch 1804, Loss: 0.00000003772901, Improvement: 0.00000000064065, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1805
Epoch 1805, Loss: 0.00000003761406, Improvement: -0.00000000011495, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1806
Epoch 1806, Loss: 0.00000003616442, Improvement: -0.00000000144964, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1807
Epoch 1807, Loss: 0.00000003384656, Improvement: -0.00000000231786, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1808
Epoch 1808, Loss: 0.00000005120388, Improvement: 0.00000001735732, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1809
Epoch 1809, Loss: 0.00000009460079, Improvement: 0.00000004339691, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1810
Epoch 1810, Loss: 0.00000016295120, Improvement: 0.00000006835041, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1811
Epoch 1811, Loss: 0.00000016883938, Improvement: 0.00000000588818, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1812
Epoch 1812, Loss: 0.00000007850451, Improvement: -0.00000009033487, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1813
Epoch 1813, Loss: 0.00000004810964, Improvement: -0.00000003039487, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1814
Epoch 1814, Loss: 0.00000004329718, Improvement: -0.00000000481246, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1815
Epoch 1815, Loss: 0.00000003851120, Improvement: -0.00000000478598, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1816
Epoch 1816, Loss: 0.00000004339140, Improvement: 0.00000000488020, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1817
Epoch 1817, Loss: 0.00000006136632, Improvement: 0.00000001797492, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1818
Epoch 1818, Loss: 0.00000005743365, Improvement: -0.00000000393268, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1819
Epoch 1819, Loss: 0.00000011831863, Improvement: 0.00000006088498, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1820
Epoch 1820, Loss: 0.00000027262049, Improvement: 0.00000015430186, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1821
Epoch 1821, Loss: 0.00000020002610, Improvement: -0.00000007259438, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1822
Epoch 1822, Loss: 0.00000015399687, Improvement: -0.00000004602924, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1823
Epoch 1823, Loss: 0.00000023397827, Improvement: 0.00000007998140, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1824
Epoch 1824, Loss: 0.00000012117920, Improvement: -0.00000011279907, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1825
Epoch 1825, Loss: 0.00000006062004, Improvement: -0.00000006055916, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1826
Epoch 1826, Loss: 0.00000004985509, Improvement: -0.00000001076495, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1827
Epoch 1827, Loss: 0.00000004094694, Improvement: -0.00000000890814, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1828
Epoch 1828, Loss: 0.00000003794777, Improvement: -0.00000000299918, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1829
Epoch 1829, Loss: 0.00000003912809, Improvement: 0.00000000118032, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1830
Epoch 1830, Loss: 0.00000003971776, Improvement: 0.00000000058967, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1831
Epoch 1831, Loss: 0.00000004060865, Improvement: 0.00000000089089, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1832
Epoch 1832, Loss: 0.00000003807419, Improvement: -0.00000000253446, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1833
Epoch 1833, Loss: 0.00000003767340, Improvement: -0.00000000040080, Best Loss: 0.00000002397081 in Epoch 1774
Epoch 1834
A best model at epoch 1834 has been saved with training error 0.00000002370366.
Epoch 1834, Loss: 0.00000003476562, Improvement: -0.00000000290777, Best Loss: 0.00000002370366 in Epoch 1834
Epoch 1835
Epoch 1835, Loss: 0.00000003259031, Improvement: -0.00000000217531, Best Loss: 0.00000002370366 in Epoch 1834
Epoch 1836
Epoch 1836, Loss: 0.00000003210030, Improvement: -0.00000000049001, Best Loss: 0.00000002370366 in Epoch 1834
Epoch 1837
A best model at epoch 1837 has been saved with training error 0.00000002334047.
A best model at epoch 1837 has been saved with training error 0.00000002318074.
Epoch 1837, Loss: 0.00000003191885, Improvement: -0.00000000018145, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1838
Epoch 1838, Loss: 0.00000003366554, Improvement: 0.00000000174669, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1839
Epoch 1839, Loss: 0.00000003576928, Improvement: 0.00000000210374, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1840
Epoch 1840, Loss: 0.00000006348809, Improvement: 0.00000002771881, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1841
Epoch 1841, Loss: 0.00000008719340, Improvement: 0.00000002370531, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1842
Epoch 1842, Loss: 0.00000006015762, Improvement: -0.00000002703578, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1843
Epoch 1843, Loss: 0.00000004471888, Improvement: -0.00000001543874, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1844
Epoch 1844, Loss: 0.00000005570654, Improvement: 0.00000001098766, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1845
Epoch 1845, Loss: 0.00000016147584, Improvement: 0.00000010576930, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1846
Epoch 1846, Loss: 0.00000021580605, Improvement: 0.00000005433020, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1847
Epoch 1847, Loss: 0.00000007272074, Improvement: -0.00000014308530, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1848
Epoch 1848, Loss: 0.00000005139280, Improvement: -0.00000002132794, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1849
Epoch 1849, Loss: 0.00000004455081, Improvement: -0.00000000684199, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000003855922, Improvement: -0.00000000599160, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1851
Epoch 1851, Loss: 0.00000003568713, Improvement: -0.00000000287209, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1852
Epoch 1852, Loss: 0.00000003417300, Improvement: -0.00000000151413, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1853
Epoch 1853, Loss: 0.00000003422416, Improvement: 0.00000000005116, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1854
Epoch 1854, Loss: 0.00000003384080, Improvement: -0.00000000038336, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1855
Epoch 1855, Loss: 0.00000003304589, Improvement: -0.00000000079491, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1856
Epoch 1856, Loss: 0.00000003375817, Improvement: 0.00000000071228, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1857
Epoch 1857, Loss: 0.00000003596760, Improvement: 0.00000000220943, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1858
Epoch 1858, Loss: 0.00000004342047, Improvement: 0.00000000745287, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1859
Epoch 1859, Loss: 0.00000005865878, Improvement: 0.00000001523831, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1860
Epoch 1860, Loss: 0.00000005158787, Improvement: -0.00000000707091, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1861
Epoch 1861, Loss: 0.00000005780218, Improvement: 0.00000000621431, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1862
Epoch 1862, Loss: 0.00000013065782, Improvement: 0.00000007285564, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1863
Epoch 1863, Loss: 0.00000013370667, Improvement: 0.00000000304885, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1864
Epoch 1864, Loss: 0.00000005532596, Improvement: -0.00000007838071, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1865
Epoch 1865, Loss: 0.00000004298254, Improvement: -0.00000001234342, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1866
Epoch 1866, Loss: 0.00000003739679, Improvement: -0.00000000558575, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1867
Epoch 1867, Loss: 0.00000003527608, Improvement: -0.00000000212071, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1868
Epoch 1868, Loss: 0.00000003592658, Improvement: 0.00000000065050, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1869
Epoch 1869, Loss: 0.00000003520418, Improvement: -0.00000000072240, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1870
Epoch 1870, Loss: 0.00000004753059, Improvement: 0.00000001232641, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1871
Epoch 1871, Loss: 0.00000023369445, Improvement: 0.00000018616386, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1872
Epoch 1872, Loss: 0.00000018072789, Improvement: -0.00000005296657, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1873
Epoch 1873, Loss: 0.00000020842035, Improvement: 0.00000002769246, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1874
Epoch 1874, Loss: 0.00000008754015, Improvement: -0.00000012088020, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1875
Epoch 1875, Loss: 0.00000004044807, Improvement: -0.00000004709208, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1876
Epoch 1876, Loss: 0.00000003517537, Improvement: -0.00000000527270, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1877
Epoch 1877, Loss: 0.00000003429674, Improvement: -0.00000000087863, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1878
Epoch 1878, Loss: 0.00000003278401, Improvement: -0.00000000151273, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1879
Epoch 1879, Loss: 0.00000003186912, Improvement: -0.00000000091489, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1880
Epoch 1880, Loss: 0.00000003183496, Improvement: -0.00000000003415, Best Loss: 0.00000002318074 in Epoch 1837
Epoch 1881
A best model at epoch 1881 has been saved with training error 0.00000002310532.
Epoch 1881, Loss: 0.00000003069543, Improvement: -0.00000000113953, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1882
Epoch 1882, Loss: 0.00000003007824, Improvement: -0.00000000061719, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1883
Epoch 1883, Loss: 0.00000003130514, Improvement: 0.00000000122690, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1884
Epoch 1884, Loss: 0.00000003752800, Improvement: 0.00000000622286, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1885
Epoch 1885, Loss: 0.00000004746762, Improvement: 0.00000000993962, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1886
Epoch 1886, Loss: 0.00000005235963, Improvement: 0.00000000489201, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1887
Epoch 1887, Loss: 0.00000004034218, Improvement: -0.00000001201746, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1888
Epoch 1888, Loss: 0.00000003882531, Improvement: -0.00000000151687, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1889
Epoch 1889, Loss: 0.00000006300790, Improvement: 0.00000002418259, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1890
Epoch 1890, Loss: 0.00000006814972, Improvement: 0.00000000514182, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1891
Epoch 1891, Loss: 0.00000005993339, Improvement: -0.00000000821633, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1892
Epoch 1892, Loss: 0.00000007255637, Improvement: 0.00000001262298, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1893
Epoch 1893, Loss: 0.00000006846474, Improvement: -0.00000000409163, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1894
Epoch 1894, Loss: 0.00000006147615, Improvement: -0.00000000698858, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1895
Epoch 1895, Loss: 0.00000003759868, Improvement: -0.00000002387747, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1896
Epoch 1896, Loss: 0.00000005265338, Improvement: 0.00000001505470, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1897
Epoch 1897, Loss: 0.00000009201067, Improvement: 0.00000003935729, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1898
Epoch 1898, Loss: 0.00000025600947, Improvement: 0.00000016399880, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1899
Epoch 1899, Loss: 0.00000017014338, Improvement: -0.00000008586610, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000006916165, Improvement: -0.00000010098173, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1901
Epoch 1901, Loss: 0.00000004933488, Improvement: -0.00000001982677, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1902
Epoch 1902, Loss: 0.00000004149990, Improvement: -0.00000000783498, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1903
Epoch 1903, Loss: 0.00000004009590, Improvement: -0.00000000140399, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1904
Epoch 1904, Loss: 0.00000003781916, Improvement: -0.00000000227674, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1905
Epoch 1905, Loss: 0.00000003670881, Improvement: -0.00000000111035, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1906
Epoch 1906, Loss: 0.00000003235427, Improvement: -0.00000000435454, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1907
Epoch 1907, Loss: 0.00000003355125, Improvement: 0.00000000119699, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1908
Epoch 1908, Loss: 0.00000003173258, Improvement: -0.00000000181867, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1909
Epoch 1909, Loss: 0.00000002968445, Improvement: -0.00000000204813, Best Loss: 0.00000002310532 in Epoch 1881
Epoch 1910
A best model at epoch 1910 has been saved with training error 0.00000002244873.
Epoch 1910, Loss: 0.00000003050722, Improvement: 0.00000000082277, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1911
Epoch 1911, Loss: 0.00000003179893, Improvement: 0.00000000129172, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1912
Epoch 1912, Loss: 0.00000003537514, Improvement: 0.00000000357621, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1913
Epoch 1913, Loss: 0.00000003394809, Improvement: -0.00000000142705, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1914
Epoch 1914, Loss: 0.00000003417750, Improvement: 0.00000000022941, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1915
Epoch 1915, Loss: 0.00000003665696, Improvement: 0.00000000247946, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1916
Epoch 1916, Loss: 0.00000003715097, Improvement: 0.00000000049401, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1917
Epoch 1917, Loss: 0.00000005683845, Improvement: 0.00000001968748, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1918
Epoch 1918, Loss: 0.00000010051371, Improvement: 0.00000004367525, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1919
Epoch 1919, Loss: 0.00000012123108, Improvement: 0.00000002071738, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1920
Epoch 1920, Loss: 0.00000018933609, Improvement: 0.00000006810501, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1921
Epoch 1921, Loss: 0.00000012090875, Improvement: -0.00000006842734, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1922
Epoch 1922, Loss: 0.00000010268493, Improvement: -0.00000001822382, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1923
Epoch 1923, Loss: 0.00000006695586, Improvement: -0.00000003572907, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1924
Epoch 1924, Loss: 0.00000005211519, Improvement: -0.00000001484067, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1925
Epoch 1925, Loss: 0.00000004788614, Improvement: -0.00000000422906, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1926
Epoch 1926, Loss: 0.00000003794752, Improvement: -0.00000000993861, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1927
Epoch 1927, Loss: 0.00000003268533, Improvement: -0.00000000526219, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1928
Epoch 1928, Loss: 0.00000003953887, Improvement: 0.00000000685354, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1929
Epoch 1929, Loss: 0.00000003958967, Improvement: 0.00000000005081, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1930
Epoch 1930, Loss: 0.00000004691115, Improvement: 0.00000000732148, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1931
Epoch 1931, Loss: 0.00000005769636, Improvement: 0.00000001078520, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1932
Epoch 1932, Loss: 0.00000005226698, Improvement: -0.00000000542938, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1933
Epoch 1933, Loss: 0.00000015930276, Improvement: 0.00000010703578, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1934
Epoch 1934, Loss: 0.00000014050055, Improvement: -0.00000001880221, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1935
Epoch 1935, Loss: 0.00000009590563, Improvement: -0.00000004459492, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1936
Epoch 1936, Loss: 0.00000007999912, Improvement: -0.00000001590651, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1937
Epoch 1937, Loss: 0.00000005637330, Improvement: -0.00000002362582, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1938
Epoch 1938, Loss: 0.00000004307447, Improvement: -0.00000001329882, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1939
Epoch 1939, Loss: 0.00000003260942, Improvement: -0.00000001046505, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1940
Epoch 1940, Loss: 0.00000003101741, Improvement: -0.00000000159201, Best Loss: 0.00000002244873 in Epoch 1910
Epoch 1941
A best model at epoch 1941 has been saved with training error 0.00000002196668.
Epoch 1941, Loss: 0.00000002945031, Improvement: -0.00000000156710, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1942
Epoch 1942, Loss: 0.00000003179865, Improvement: 0.00000000234834, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1943
Epoch 1943, Loss: 0.00000003279077, Improvement: 0.00000000099212, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1944
Epoch 1944, Loss: 0.00000009342900, Improvement: 0.00000006063823, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1945
Epoch 1945, Loss: 0.00000011856481, Improvement: 0.00000002513581, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1946
Epoch 1946, Loss: 0.00000012029177, Improvement: 0.00000000172696, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1947
Epoch 1947, Loss: 0.00000012003109, Improvement: -0.00000000026068, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1948
Epoch 1948, Loss: 0.00000008310777, Improvement: -0.00000003692333, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1949
Epoch 1949, Loss: 0.00000005303486, Improvement: -0.00000003007290, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000004096892, Improvement: -0.00000001206594, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1951
Epoch 1951, Loss: 0.00000003256190, Improvement: -0.00000000840702, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1952
Epoch 1952, Loss: 0.00000003191008, Improvement: -0.00000000065182, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1953
Epoch 1953, Loss: 0.00000004238331, Improvement: 0.00000001047323, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1954
Epoch 1954, Loss: 0.00000009515581, Improvement: 0.00000005277250, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1955
Epoch 1955, Loss: 0.00000007583105, Improvement: -0.00000001932476, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1956
Epoch 1956, Loss: 0.00000014022779, Improvement: 0.00000006439673, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1957
Epoch 1957, Loss: 0.00000013593503, Improvement: -0.00000000429276, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1958
Epoch 1958, Loss: 0.00000005424364, Improvement: -0.00000008169139, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1959
Epoch 1959, Loss: 0.00000006328582, Improvement: 0.00000000904218, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1960
Epoch 1960, Loss: 0.00000012991346, Improvement: 0.00000006662764, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1961
Epoch 1961, Loss: 0.00000008555392, Improvement: -0.00000004435954, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1962
Epoch 1962, Loss: 0.00000004933415, Improvement: -0.00000003621977, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1963
Epoch 1963, Loss: 0.00000004311067, Improvement: -0.00000000622349, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1964
Epoch 1964, Loss: 0.00000004294842, Improvement: -0.00000000016225, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1965
Epoch 1965, Loss: 0.00000003637571, Improvement: -0.00000000657270, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1966
Epoch 1966, Loss: 0.00000003377990, Improvement: -0.00000000259581, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1967
Epoch 1967, Loss: 0.00000003094845, Improvement: -0.00000000283145, Best Loss: 0.00000002196668 in Epoch 1941
Epoch 1968
A best model at epoch 1968 has been saved with training error 0.00000001864309.
Epoch 1968, Loss: 0.00000002965848, Improvement: -0.00000000128997, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1969
Epoch 1969, Loss: 0.00000004260415, Improvement: 0.00000001294566, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1970
Epoch 1970, Loss: 0.00000020123238, Improvement: 0.00000015862824, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1971
Epoch 1971, Loss: 0.00000014717286, Improvement: -0.00000005405952, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1972
Epoch 1972, Loss: 0.00000006446273, Improvement: -0.00000008271013, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1973
Epoch 1973, Loss: 0.00000004989549, Improvement: -0.00000001456725, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1974
Epoch 1974, Loss: 0.00000003639610, Improvement: -0.00000001349939, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1975
Epoch 1975, Loss: 0.00000003232125, Improvement: -0.00000000407484, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1976
Epoch 1976, Loss: 0.00000002975022, Improvement: -0.00000000257104, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1977
Epoch 1977, Loss: 0.00000002855055, Improvement: -0.00000000119967, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1978
Epoch 1978, Loss: 0.00000003647863, Improvement: 0.00000000792808, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1979
Epoch 1979, Loss: 0.00000005243806, Improvement: 0.00000001595943, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1980
Epoch 1980, Loss: 0.00000004194274, Improvement: -0.00000001049532, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1981
Epoch 1981, Loss: 0.00000004452914, Improvement: 0.00000000258640, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1982
Epoch 1982, Loss: 0.00000005462391, Improvement: 0.00000001009476, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1983
Epoch 1983, Loss: 0.00000007929035, Improvement: 0.00000002466644, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1984
Epoch 1984, Loss: 0.00000013072500, Improvement: 0.00000005143465, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1985
Epoch 1985, Loss: 0.00000018153765, Improvement: 0.00000005081265, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1986
Epoch 1986, Loss: 0.00000015275116, Improvement: -0.00000002878649, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1987
Epoch 1987, Loss: 0.00000008627808, Improvement: -0.00000006647308, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1988
Epoch 1988, Loss: 0.00000005583858, Improvement: -0.00000003043951, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1989
Epoch 1989, Loss: 0.00000003741834, Improvement: -0.00000001842023, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1990
Epoch 1990, Loss: 0.00000003043936, Improvement: -0.00000000697898, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1991
Epoch 1991, Loss: 0.00000002880894, Improvement: -0.00000000163042, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1992
Epoch 1992, Loss: 0.00000002782518, Improvement: -0.00000000098377, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1993
Epoch 1993, Loss: 0.00000002692017, Improvement: -0.00000000090500, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1994
Epoch 1994, Loss: 0.00000002642920, Improvement: -0.00000000049097, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1995
Epoch 1995, Loss: 0.00000002678967, Improvement: 0.00000000036047, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1996
Epoch 1996, Loss: 0.00000003005078, Improvement: 0.00000000326111, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1997
Epoch 1997, Loss: 0.00000002940293, Improvement: -0.00000000064785, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1998
Epoch 1998, Loss: 0.00000003689944, Improvement: 0.00000000749652, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 1999
Epoch 1999, Loss: 0.00000004509284, Improvement: 0.00000000819340, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000006095436, Improvement: 0.00000001586151, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2001
Epoch 2001, Loss: 0.00000020120821, Improvement: 0.00000014025385, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2002
Epoch 2002, Loss: 0.00000006702044, Improvement: -0.00000013418776, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2003
Epoch 2003, Loss: 0.00000004106365, Improvement: -0.00000002595679, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2004
Epoch 2004, Loss: 0.00000003142441, Improvement: -0.00000000963924, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2005
Epoch 2005, Loss: 0.00000002743816, Improvement: -0.00000000398624, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2006
Epoch 2006, Loss: 0.00000002693576, Improvement: -0.00000000050241, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2007
Epoch 2007, Loss: 0.00000002773340, Improvement: 0.00000000079764, Best Loss: 0.00000001864309 in Epoch 1968
Epoch 2008
A best model at epoch 2008 has been saved with training error 0.00000001846357.
Epoch 2008, Loss: 0.00000002931852, Improvement: 0.00000000158512, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2009
Epoch 2009, Loss: 0.00000003151167, Improvement: 0.00000000219315, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2010
Epoch 2010, Loss: 0.00000005939481, Improvement: 0.00000002788314, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2011
Epoch 2011, Loss: 0.00000008148634, Improvement: 0.00000002209152, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2012
Epoch 2012, Loss: 0.00000006020537, Improvement: -0.00000002128096, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2013
Epoch 2013, Loss: 0.00000004341310, Improvement: -0.00000001679227, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2014
Epoch 2014, Loss: 0.00000003666330, Improvement: -0.00000000674980, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2015
Epoch 2015, Loss: 0.00000004277828, Improvement: 0.00000000611498, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2016
Epoch 2016, Loss: 0.00000003842618, Improvement: -0.00000000435210, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2017
Epoch 2017, Loss: 0.00000003784507, Improvement: -0.00000000058111, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2018
Epoch 2018, Loss: 0.00000004685531, Improvement: 0.00000000901024, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2019
Epoch 2019, Loss: 0.00000003102221, Improvement: -0.00000001583310, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2020
Epoch 2020, Loss: 0.00000002881987, Improvement: -0.00000000220234, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2021
Epoch 2021, Loss: 0.00000005539432, Improvement: 0.00000002657445, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2022
Epoch 2022, Loss: 0.00000009590604, Improvement: 0.00000004051172, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2023
Epoch 2023, Loss: 0.00000012240918, Improvement: 0.00000002650314, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2024
Epoch 2024, Loss: 0.00000005742855, Improvement: -0.00000006498063, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2025
Epoch 2025, Loss: 0.00000005094190, Improvement: -0.00000000648665, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2026
Epoch 2026, Loss: 0.00000007895214, Improvement: 0.00000002801024, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2027
Epoch 2027, Loss: 0.00000014842841, Improvement: 0.00000006947627, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2028
Epoch 2028, Loss: 0.00000004818917, Improvement: -0.00000010023924, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2029
Epoch 2029, Loss: 0.00000003403589, Improvement: -0.00000001415328, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2030
Epoch 2030, Loss: 0.00000003075042, Improvement: -0.00000000328546, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2031
Epoch 2031, Loss: 0.00000002703030, Improvement: -0.00000000372013, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2032
Epoch 2032, Loss: 0.00000002678876, Improvement: -0.00000000024153, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2033
Epoch 2033, Loss: 0.00000002802776, Improvement: 0.00000000123900, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2034
Epoch 2034, Loss: 0.00000003225815, Improvement: 0.00000000423039, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2035
Epoch 2035, Loss: 0.00000003670335, Improvement: 0.00000000444520, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2036
Epoch 2036, Loss: 0.00000003511678, Improvement: -0.00000000158656, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2037
Epoch 2037, Loss: 0.00000003679905, Improvement: 0.00000000168226, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2038
Epoch 2038, Loss: 0.00000003051688, Improvement: -0.00000000628217, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2039
Epoch 2039, Loss: 0.00000003100603, Improvement: 0.00000000048915, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2040
Epoch 2040, Loss: 0.00000003469621, Improvement: 0.00000000369018, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2041
Epoch 2041, Loss: 0.00000005461151, Improvement: 0.00000001991530, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2042
Epoch 2042, Loss: 0.00000005467947, Improvement: 0.00000000006796, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2043
Epoch 2043, Loss: 0.00000004523882, Improvement: -0.00000000944064, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2044
Epoch 2044, Loss: 0.00000004928788, Improvement: 0.00000000404905, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2045
Epoch 2045, Loss: 0.00000006107004, Improvement: 0.00000001178216, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2046
Epoch 2046, Loss: 0.00000004267716, Improvement: -0.00000001839288, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2047
Epoch 2047, Loss: 0.00000003347559, Improvement: -0.00000000920157, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2048
Epoch 2048, Loss: 0.00000002939958, Improvement: -0.00000000407602, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2049
Epoch 2049, Loss: 0.00000003919237, Improvement: 0.00000000979279, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000012016541, Improvement: 0.00000008097305, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2051
Epoch 2051, Loss: 0.00000005108462, Improvement: -0.00000006908079, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2052
Epoch 2052, Loss: 0.00000005779731, Improvement: 0.00000000671269, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2053
Epoch 2053, Loss: 0.00000005773515, Improvement: -0.00000000006217, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2054
Epoch 2054, Loss: 0.00000006047149, Improvement: 0.00000000273635, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2055
Epoch 2055, Loss: 0.00000012653995, Improvement: 0.00000006606845, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2056
Epoch 2056, Loss: 0.00000010795730, Improvement: -0.00000001858264, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2057
Epoch 2057, Loss: 0.00000004720304, Improvement: -0.00000006075426, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2058
Epoch 2058, Loss: 0.00000004116687, Improvement: -0.00000000603617, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2059
Epoch 2059, Loss: 0.00000003346899, Improvement: -0.00000000769788, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2060
Epoch 2060, Loss: 0.00000003276177, Improvement: -0.00000000070722, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2061
Epoch 2061, Loss: 0.00000003100221, Improvement: -0.00000000175956, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2062
Epoch 2062, Loss: 0.00000003706466, Improvement: 0.00000000606245, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2063
Epoch 2063, Loss: 0.00000002714791, Improvement: -0.00000000991675, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2064
Epoch 2064, Loss: 0.00000002775024, Improvement: 0.00000000060234, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2065
Epoch 2065, Loss: 0.00000002802375, Improvement: 0.00000000027350, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2066
Epoch 2066, Loss: 0.00000002552922, Improvement: -0.00000000249453, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2067
Epoch 2067, Loss: 0.00000002683345, Improvement: 0.00000000130423, Best Loss: 0.00000001846357 in Epoch 2008
Epoch 2068
A best model at epoch 2068 has been saved with training error 0.00000001807128.
Epoch 2068, Loss: 0.00000002882069, Improvement: 0.00000000198723, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2069
Epoch 2069, Loss: 0.00000004201021, Improvement: 0.00000001318953, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2070
Epoch 2070, Loss: 0.00000009193610, Improvement: 0.00000004992589, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2071
Epoch 2071, Loss: 0.00000010706175, Improvement: 0.00000001512564, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2072
Epoch 2072, Loss: 0.00000012310149, Improvement: 0.00000001603974, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2073
Epoch 2073, Loss: 0.00000009499599, Improvement: -0.00000002810550, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2074
Epoch 2074, Loss: 0.00000018535049, Improvement: 0.00000009035450, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2075
Epoch 2075, Loss: 0.00000009823457, Improvement: -0.00000008711592, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2076
Epoch 2076, Loss: 0.00000006459151, Improvement: -0.00000003364306, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2077
Epoch 2077, Loss: 0.00000004103013, Improvement: -0.00000002356138, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2078
Epoch 2078, Loss: 0.00000003859009, Improvement: -0.00000000244004, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2079
Epoch 2079, Loss: 0.00000004846443, Improvement: 0.00000000987434, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2080
Epoch 2080, Loss: 0.00000003502619, Improvement: -0.00000001343825, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2081
Epoch 2081, Loss: 0.00000006669692, Improvement: 0.00000003167074, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2082
Epoch 2082, Loss: 0.00000005769959, Improvement: -0.00000000899733, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2083
Epoch 2083, Loss: 0.00000003971787, Improvement: -0.00000001798172, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2084
Epoch 2084, Loss: 0.00000003345553, Improvement: -0.00000000626235, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2085
Epoch 2085, Loss: 0.00000003562175, Improvement: 0.00000000216623, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2086
Epoch 2086, Loss: 0.00000004300187, Improvement: 0.00000000738011, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2087
Epoch 2087, Loss: 0.00000004268286, Improvement: -0.00000000031901, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2088
Epoch 2088, Loss: 0.00000004436098, Improvement: 0.00000000167812, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2089
Epoch 2089, Loss: 0.00000002919565, Improvement: -0.00000001516533, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2090
Epoch 2090, Loss: 0.00000002790093, Improvement: -0.00000000129473, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2091
Epoch 2091, Loss: 0.00000003233744, Improvement: 0.00000000443651, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2092
Epoch 2092, Loss: 0.00000003119401, Improvement: -0.00000000114343, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2093
Epoch 2093, Loss: 0.00000004483612, Improvement: 0.00000001364211, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2094
Epoch 2094, Loss: 0.00000005907033, Improvement: 0.00000001423421, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2095
Epoch 2095, Loss: 0.00000006159383, Improvement: 0.00000000252350, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2096
Epoch 2096, Loss: 0.00000009645453, Improvement: 0.00000003486069, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2097
Epoch 2097, Loss: 0.00000015048708, Improvement: 0.00000005403256, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2098
Epoch 2098, Loss: 0.00000017630905, Improvement: 0.00000002582197, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2099
Epoch 2099, Loss: 0.00000006910221, Improvement: -0.00000010720684, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000004438796, Improvement: -0.00000002471425, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2101
Epoch 2101, Loss: 0.00000003062491, Improvement: -0.00000001376305, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2102
Epoch 2102, Loss: 0.00000002700375, Improvement: -0.00000000362116, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2103
Epoch 2103, Loss: 0.00000002552906, Improvement: -0.00000000147469, Best Loss: 0.00000001807128 in Epoch 2068
Epoch 2104
A best model at epoch 2104 has been saved with training error 0.00000001754662.
Epoch 2104, Loss: 0.00000002933871, Improvement: 0.00000000380965, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2105
Epoch 2105, Loss: 0.00000002770547, Improvement: -0.00000000163324, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2106
Epoch 2106, Loss: 0.00000002544307, Improvement: -0.00000000226240, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2107
Epoch 2107, Loss: 0.00000002485065, Improvement: -0.00000000059242, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2108
Epoch 2108, Loss: 0.00000002527668, Improvement: 0.00000000042603, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2109
Epoch 2109, Loss: 0.00000002637346, Improvement: 0.00000000109679, Best Loss: 0.00000001754662 in Epoch 2104
Epoch 2110
A best model at epoch 2110 has been saved with training error 0.00000001678659.
Epoch 2110, Loss: 0.00000002434314, Improvement: -0.00000000203032, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2111
Epoch 2111, Loss: 0.00000002747217, Improvement: 0.00000000312903, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2112
Epoch 2112, Loss: 0.00000002827452, Improvement: 0.00000000080235, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2113
Epoch 2113, Loss: 0.00000007026794, Improvement: 0.00000004199342, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2114
Epoch 2114, Loss: 0.00000006729698, Improvement: -0.00000000297096, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2115
Epoch 2115, Loss: 0.00000003648257, Improvement: -0.00000003081441, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2116
Epoch 2116, Loss: 0.00000003189087, Improvement: -0.00000000459170, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2117
Epoch 2117, Loss: 0.00000002675254, Improvement: -0.00000000513833, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2118
Epoch 2118, Loss: 0.00000003372042, Improvement: 0.00000000696788, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2119
Epoch 2119, Loss: 0.00000006519980, Improvement: 0.00000003147938, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2120
Epoch 2120, Loss: 0.00000005796128, Improvement: -0.00000000723852, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2121
Epoch 2121, Loss: 0.00000011330288, Improvement: 0.00000005534160, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2122
Epoch 2122, Loss: 0.00000005511898, Improvement: -0.00000005818390, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2123
Epoch 2123, Loss: 0.00000003869536, Improvement: -0.00000001642362, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2124
Epoch 2124, Loss: 0.00000003206795, Improvement: -0.00000000662741, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2125
Epoch 2125, Loss: 0.00000003240279, Improvement: 0.00000000033485, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2126
Epoch 2126, Loss: 0.00000003487181, Improvement: 0.00000000246902, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2127
Epoch 2127, Loss: 0.00000004861749, Improvement: 0.00000001374568, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2128
Epoch 2128, Loss: 0.00000006784810, Improvement: 0.00000001923061, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2129
Epoch 2129, Loss: 0.00000006450080, Improvement: -0.00000000334730, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2130
Epoch 2130, Loss: 0.00000010041383, Improvement: 0.00000003591303, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2131
Epoch 2131, Loss: 0.00000017598472, Improvement: 0.00000007557089, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2132
Epoch 2132, Loss: 0.00000012669480, Improvement: -0.00000004928992, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2133
Epoch 2133, Loss: 0.00000008907243, Improvement: -0.00000003762238, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2134
Epoch 2134, Loss: 0.00000006902876, Improvement: -0.00000002004366, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2135
Epoch 2135, Loss: 0.00000004890524, Improvement: -0.00000002012352, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2136
Epoch 2136, Loss: 0.00000002805859, Improvement: -0.00000002084665, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2137
Epoch 2137, Loss: 0.00000002659358, Improvement: -0.00000000146502, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2138
Epoch 2138, Loss: 0.00000002519658, Improvement: -0.00000000139700, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2139
Epoch 2139, Loss: 0.00000002618244, Improvement: 0.00000000098586, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2140
Epoch 2140, Loss: 0.00000002564629, Improvement: -0.00000000053614, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2141
Epoch 2141, Loss: 0.00000002552808, Improvement: -0.00000000011821, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2142
Epoch 2142, Loss: 0.00000002450578, Improvement: -0.00000000102231, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2143
Epoch 2143, Loss: 0.00000002928655, Improvement: 0.00000000478077, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2144
Epoch 2144, Loss: 0.00000003242391, Improvement: 0.00000000313736, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2145
Epoch 2145, Loss: 0.00000006154038, Improvement: 0.00000002911647, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2146
Epoch 2146, Loss: 0.00000006390741, Improvement: 0.00000000236704, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2147
Epoch 2147, Loss: 0.00000003979825, Improvement: -0.00000002410916, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2148
Epoch 2148, Loss: 0.00000005036590, Improvement: 0.00000001056765, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2149
Epoch 2149, Loss: 0.00000008216454, Improvement: 0.00000003179864, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000016123941, Improvement: 0.00000007907487, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2151
Epoch 2151, Loss: 0.00000017539044, Improvement: 0.00000001415103, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2152
Epoch 2152, Loss: 0.00000008253920, Improvement: -0.00000009285124, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2153
Epoch 2153, Loss: 0.00000004743359, Improvement: -0.00000003510561, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2154
Epoch 2154, Loss: 0.00000003661133, Improvement: -0.00000001082226, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2155
Epoch 2155, Loss: 0.00000003796674, Improvement: 0.00000000135541, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2156
Epoch 2156, Loss: 0.00000002671761, Improvement: -0.00000001124913, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2157
Epoch 2157, Loss: 0.00000002416517, Improvement: -0.00000000255243, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2158
Epoch 2158, Loss: 0.00000002341526, Improvement: -0.00000000074992, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2159
Epoch 2159, Loss: 0.00000002282501, Improvement: -0.00000000059025, Best Loss: 0.00000001678659 in Epoch 2110
Epoch 2160
A best model at epoch 2160 has been saved with training error 0.00000001595921.
Epoch 2160, Loss: 0.00000002399133, Improvement: 0.00000000116632, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2161
Epoch 2161, Loss: 0.00000003421634, Improvement: 0.00000001022501, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2162
Epoch 2162, Loss: 0.00000003768190, Improvement: 0.00000000346556, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2163
Epoch 2163, Loss: 0.00000003338621, Improvement: -0.00000000429569, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2164
Epoch 2164, Loss: 0.00000002998442, Improvement: -0.00000000340179, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2165
Epoch 2165, Loss: 0.00000002925062, Improvement: -0.00000000073380, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2166
Epoch 2166, Loss: 0.00000002608816, Improvement: -0.00000000316246, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2167
Epoch 2167, Loss: 0.00000002316479, Improvement: -0.00000000292338, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2168
Epoch 2168, Loss: 0.00000002173582, Improvement: -0.00000000142896, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2169
Epoch 2169, Loss: 0.00000002511593, Improvement: 0.00000000338011, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2170
Epoch 2170, Loss: 0.00000007136765, Improvement: 0.00000004625172, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2171
Epoch 2171, Loss: 0.00000009449271, Improvement: 0.00000002312506, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2172
Epoch 2172, Loss: 0.00000009770488, Improvement: 0.00000000321218, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2173
Epoch 2173, Loss: 0.00000007540003, Improvement: -0.00000002230486, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2174
Epoch 2174, Loss: 0.00000009637408, Improvement: 0.00000002097405, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2175
Epoch 2175, Loss: 0.00000012033648, Improvement: 0.00000002396240, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2176
Epoch 2176, Loss: 0.00000008115324, Improvement: -0.00000003918324, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2177
Epoch 2177, Loss: 0.00000003860003, Improvement: -0.00000004255321, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2178
Epoch 2178, Loss: 0.00000003281617, Improvement: -0.00000000578386, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2179
Epoch 2179, Loss: 0.00000002393502, Improvement: -0.00000000888115, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2180
Epoch 2180, Loss: 0.00000002438120, Improvement: 0.00000000044617, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2181
Epoch 2181, Loss: 0.00000002817914, Improvement: 0.00000000379795, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2182
Epoch 2182, Loss: 0.00000003768237, Improvement: 0.00000000950322, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2183
Epoch 2183, Loss: 0.00000005802972, Improvement: 0.00000002034735, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2184
Epoch 2184, Loss: 0.00000004524421, Improvement: -0.00000001278550, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2185
Epoch 2185, Loss: 0.00000002631417, Improvement: -0.00000001893005, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2186
Epoch 2186, Loss: 0.00000002423853, Improvement: -0.00000000207564, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2187
Epoch 2187, Loss: 0.00000002296017, Improvement: -0.00000000127836, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2188
Epoch 2188, Loss: 0.00000002377040, Improvement: 0.00000000081023, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2189
Epoch 2189, Loss: 0.00000002432697, Improvement: 0.00000000055657, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2190
Epoch 2190, Loss: 0.00000002237744, Improvement: -0.00000000194953, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2191
Epoch 2191, Loss: 0.00000002251247, Improvement: 0.00000000013503, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2192
Epoch 2192, Loss: 0.00000002322843, Improvement: 0.00000000071596, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2193
Epoch 2193, Loss: 0.00000002433444, Improvement: 0.00000000110602, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2194
Epoch 2194, Loss: 0.00000004142558, Improvement: 0.00000001709114, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2195
Epoch 2195, Loss: 0.00000004799008, Improvement: 0.00000000656450, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2196
Epoch 2196, Loss: 0.00000005443949, Improvement: 0.00000000644942, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2197
Epoch 2197, Loss: 0.00000007568587, Improvement: 0.00000002124638, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2198
Epoch 2198, Loss: 0.00000006095611, Improvement: -0.00000001472976, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2199
Epoch 2199, Loss: 0.00000006282685, Improvement: 0.00000000187074, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000005547297, Improvement: -0.00000000735388, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2201
Epoch 2201, Loss: 0.00000003674483, Improvement: -0.00000001872813, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2202
Epoch 2202, Loss: 0.00000004712646, Improvement: 0.00000001038163, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2203
Epoch 2203, Loss: 0.00000003637077, Improvement: -0.00000001075569, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2204
Epoch 2204, Loss: 0.00000004629900, Improvement: 0.00000000992823, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2205
Epoch 2205, Loss: 0.00000006387040, Improvement: 0.00000001757140, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2206
Epoch 2206, Loss: 0.00000011624851, Improvement: 0.00000005237811, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2207
Epoch 2207, Loss: 0.00000008065362, Improvement: -0.00000003559489, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2208
Epoch 2208, Loss: 0.00000005217266, Improvement: -0.00000002848096, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2209
Epoch 2209, Loss: 0.00000004997434, Improvement: -0.00000000219832, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2210
Epoch 2210, Loss: 0.00000003522473, Improvement: -0.00000001474960, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2211
Epoch 2211, Loss: 0.00000003143574, Improvement: -0.00000000378899, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2212
Epoch 2212, Loss: 0.00000003343249, Improvement: 0.00000000199675, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2213
Epoch 2213, Loss: 0.00000003174993, Improvement: -0.00000000168256, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2214
Epoch 2214, Loss: 0.00000002827193, Improvement: -0.00000000347800, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2215
Epoch 2215, Loss: 0.00000005557255, Improvement: 0.00000002730062, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2216
Epoch 2216, Loss: 0.00000011509052, Improvement: 0.00000005951797, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2217
Epoch 2217, Loss: 0.00000008771958, Improvement: -0.00000002737094, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2218
Epoch 2218, Loss: 0.00000003462637, Improvement: -0.00000005309321, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2219
Epoch 2219, Loss: 0.00000002959111, Improvement: -0.00000000503526, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2220
Epoch 2220, Loss: 0.00000002500569, Improvement: -0.00000000458542, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2221
Epoch 2221, Loss: 0.00000002215240, Improvement: -0.00000000285330, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2222
Epoch 2222, Loss: 0.00000002452775, Improvement: 0.00000000237535, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2223
Epoch 2223, Loss: 0.00000003001475, Improvement: 0.00000000548700, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2224
Epoch 2224, Loss: 0.00000003030831, Improvement: 0.00000000029356, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2225
Epoch 2225, Loss: 0.00000004268314, Improvement: 0.00000001237483, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2226
Epoch 2226, Loss: 0.00000003582784, Improvement: -0.00000000685530, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2227
Epoch 2227, Loss: 0.00000007626609, Improvement: 0.00000004043825, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2228
Epoch 2228, Loss: 0.00000014045173, Improvement: 0.00000006418564, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2229
Epoch 2229, Loss: 0.00000005717984, Improvement: -0.00000008327188, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2230
Epoch 2230, Loss: 0.00000005025095, Improvement: -0.00000000692889, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2231
Epoch 2231, Loss: 0.00000003054951, Improvement: -0.00000001970144, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2232
Epoch 2232, Loss: 0.00000002331694, Improvement: -0.00000000723257, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2233
Epoch 2233, Loss: 0.00000002113309, Improvement: -0.00000000218385, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2234
Epoch 2234, Loss: 0.00000001977723, Improvement: -0.00000000135586, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2235
Epoch 2235, Loss: 0.00000002163929, Improvement: 0.00000000186205, Best Loss: 0.00000001595921 in Epoch 2160
Epoch 2236
A best model at epoch 2236 has been saved with training error 0.00000001548156.
Epoch 2236, Loss: 0.00000002050992, Improvement: -0.00000000112937, Best Loss: 0.00000001548156 in Epoch 2236
Epoch 2237
Epoch 2237, Loss: 0.00000002148882, Improvement: 0.00000000097890, Best Loss: 0.00000001548156 in Epoch 2236
Epoch 2238
A best model at epoch 2238 has been saved with training error 0.00000001352009.
Epoch 2238, Loss: 0.00000001913286, Improvement: -0.00000000235596, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2239
Epoch 2239, Loss: 0.00000001999434, Improvement: 0.00000000086148, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2240
Epoch 2240, Loss: 0.00000002436332, Improvement: 0.00000000436897, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2241
Epoch 2241, Loss: 0.00000010284497, Improvement: 0.00000007848165, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2242
Epoch 2242, Loss: 0.00000011250016, Improvement: 0.00000000965519, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2243
Epoch 2243, Loss: 0.00000008007569, Improvement: -0.00000003242447, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2244
Epoch 2244, Loss: 0.00000005166877, Improvement: -0.00000002840692, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2245
Epoch 2245, Loss: 0.00000003443282, Improvement: -0.00000001723595, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2246
Epoch 2246, Loss: 0.00000002495747, Improvement: -0.00000000947535, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2247
Epoch 2247, Loss: 0.00000002174021, Improvement: -0.00000000321726, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2248
Epoch 2248, Loss: 0.00000002023663, Improvement: -0.00000000150358, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2249
Epoch 2249, Loss: 0.00000001956084, Improvement: -0.00000000067580, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000001977924, Improvement: 0.00000000021841, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2251
Epoch 2251, Loss: 0.00000002345843, Improvement: 0.00000000367918, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2252
Epoch 2252, Loss: 0.00000002408707, Improvement: 0.00000000062865, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2253
Epoch 2253, Loss: 0.00000002243373, Improvement: -0.00000000165334, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2254
Epoch 2254, Loss: 0.00000002674147, Improvement: 0.00000000430774, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2255
Epoch 2255, Loss: 0.00000003924780, Improvement: 0.00000001250633, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2256
Epoch 2256, Loss: 0.00000003468486, Improvement: -0.00000000456294, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2257
Epoch 2257, Loss: 0.00000004625756, Improvement: 0.00000001157270, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2258
Epoch 2258, Loss: 0.00000004965502, Improvement: 0.00000000339746, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2259
Epoch 2259, Loss: 0.00000007701500, Improvement: 0.00000002735998, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2260
Epoch 2260, Loss: 0.00000007241424, Improvement: -0.00000000460076, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2261
Epoch 2261, Loss: 0.00000007438417, Improvement: 0.00000000196993, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2262
Epoch 2262, Loss: 0.00000006352846, Improvement: -0.00000001085571, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2263
Epoch 2263, Loss: 0.00000005999718, Improvement: -0.00000000353128, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2264
Epoch 2264, Loss: 0.00000002760823, Improvement: -0.00000003238894, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2265
Epoch 2265, Loss: 0.00000002464672, Improvement: -0.00000000296151, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2266
Epoch 2266, Loss: 0.00000002363046, Improvement: -0.00000000101626, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2267
Epoch 2267, Loss: 0.00000002426117, Improvement: 0.00000000063071, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2268
Epoch 2268, Loss: 0.00000002932273, Improvement: 0.00000000506156, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2269
Epoch 2269, Loss: 0.00000003190105, Improvement: 0.00000000257832, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2270
Epoch 2270, Loss: 0.00000004225160, Improvement: 0.00000001035055, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2271
Epoch 2271, Loss: 0.00000014781164, Improvement: 0.00000010556004, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2272
Epoch 2272, Loss: 0.00000011918067, Improvement: -0.00000002863097, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2273
Epoch 2273, Loss: 0.00000006043422, Improvement: -0.00000005874645, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2274
Epoch 2274, Loss: 0.00000003739106, Improvement: -0.00000002304316, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2275
Epoch 2275, Loss: 0.00000003144883, Improvement: -0.00000000594223, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2276
Epoch 2276, Loss: 0.00000002532317, Improvement: -0.00000000612566, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2277
Epoch 2277, Loss: 0.00000002134760, Improvement: -0.00000000397557, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2278
Epoch 2278, Loss: 0.00000003037773, Improvement: 0.00000000903012, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2279
Epoch 2279, Loss: 0.00000004969766, Improvement: 0.00000001931993, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2280
Epoch 2280, Loss: 0.00000006117357, Improvement: 0.00000001147592, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2281
Epoch 2281, Loss: 0.00000005140469, Improvement: -0.00000000976888, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2282
Epoch 2282, Loss: 0.00000005624005, Improvement: 0.00000000483536, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2283
Epoch 2283, Loss: 0.00000005253907, Improvement: -0.00000000370099, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2284
Epoch 2284, Loss: 0.00000007005357, Improvement: 0.00000001751450, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2285
Epoch 2285, Loss: 0.00000007660570, Improvement: 0.00000000655213, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2286
Epoch 2286, Loss: 0.00000010021680, Improvement: 0.00000002361110, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2287
Epoch 2287, Loss: 0.00000016771005, Improvement: 0.00000006749325, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2288
Epoch 2288, Loss: 0.00000008871395, Improvement: -0.00000007899610, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2289
Epoch 2289, Loss: 0.00000003699356, Improvement: -0.00000005172039, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2290
Epoch 2290, Loss: 0.00000002970594, Improvement: -0.00000000728763, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2291
Epoch 2291, Loss: 0.00000002386628, Improvement: -0.00000000583966, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2292
Epoch 2292, Loss: 0.00000002183317, Improvement: -0.00000000203311, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2293
Epoch 2293, Loss: 0.00000002300281, Improvement: 0.00000000116964, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2294
Epoch 2294, Loss: 0.00000002012284, Improvement: -0.00000000287998, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2295
Epoch 2295, Loss: 0.00000001933587, Improvement: -0.00000000078697, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2296
Epoch 2296, Loss: 0.00000001943185, Improvement: 0.00000000009598, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2297
Epoch 2297, Loss: 0.00000002147531, Improvement: 0.00000000204347, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2298
Epoch 2298, Loss: 0.00000001942431, Improvement: -0.00000000205100, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2299
Epoch 2299, Loss: 0.00000002378530, Improvement: 0.00000000436099, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000002920517, Improvement: 0.00000000541987, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2301
Epoch 2301, Loss: 0.00000002499114, Improvement: -0.00000000421403, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2302
Epoch 2302, Loss: 0.00000002621526, Improvement: 0.00000000122412, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2303
Epoch 2303, Loss: 0.00000002110636, Improvement: -0.00000000510890, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2304
Epoch 2304, Loss: 0.00000002107337, Improvement: -0.00000000003299, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2305
Epoch 2305, Loss: 0.00000002556447, Improvement: 0.00000000449110, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2306
Epoch 2306, Loss: 0.00000002405270, Improvement: -0.00000000151177, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2307
Epoch 2307, Loss: 0.00000002088086, Improvement: -0.00000000317184, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2308
Epoch 2308, Loss: 0.00000002803904, Improvement: 0.00000000715818, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2309
Epoch 2309, Loss: 0.00000004199663, Improvement: 0.00000001395759, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2310
Epoch 2310, Loss: 0.00000007593428, Improvement: 0.00000003393765, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2311
Epoch 2311, Loss: 0.00000005982004, Improvement: -0.00000001611423, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2312
Epoch 2312, Loss: 0.00000004885174, Improvement: -0.00000001096830, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2313
Epoch 2313, Loss: 0.00000003021314, Improvement: -0.00000001863860, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2314
Epoch 2314, Loss: 0.00000002399745, Improvement: -0.00000000621569, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2315
Epoch 2315, Loss: 0.00000002035263, Improvement: -0.00000000364482, Best Loss: 0.00000001352009 in Epoch 2238
Epoch 2316
A best model at epoch 2316 has been saved with training error 0.00000001311244.
Epoch 2316, Loss: 0.00000001984889, Improvement: -0.00000000050374, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2317
Epoch 2317, Loss: 0.00000001987395, Improvement: 0.00000000002506, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2318
Epoch 2318, Loss: 0.00000002083122, Improvement: 0.00000000095727, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2319
Epoch 2319, Loss: 0.00000003202679, Improvement: 0.00000001119557, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2320
Epoch 2320, Loss: 0.00000003468789, Improvement: 0.00000000266110, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2321
Epoch 2321, Loss: 0.00000003903543, Improvement: 0.00000000434755, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2322
Epoch 2322, Loss: 0.00000005437723, Improvement: 0.00000001534180, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2323
Epoch 2323, Loss: 0.00000008224433, Improvement: 0.00000002786711, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2324
Epoch 2324, Loss: 0.00000003013132, Improvement: -0.00000005211302, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2325
Epoch 2325, Loss: 0.00000002176985, Improvement: -0.00000000836146, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2326
Epoch 2326, Loss: 0.00000002159584, Improvement: -0.00000000017402, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2327
Epoch 2327, Loss: 0.00000002145742, Improvement: -0.00000000013842, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2328
Epoch 2328, Loss: 0.00000004504806, Improvement: 0.00000002359064, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2329
Epoch 2329, Loss: 0.00000009506749, Improvement: 0.00000005001943, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2330
Epoch 2330, Loss: 0.00000011323450, Improvement: 0.00000001816701, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2331
Epoch 2331, Loss: 0.00000008900568, Improvement: -0.00000002422881, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2332
Epoch 2332, Loss: 0.00000013846004, Improvement: 0.00000004945436, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2333
Epoch 2333, Loss: 0.00000007961433, Improvement: -0.00000005884571, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2334
Epoch 2334, Loss: 0.00000003557719, Improvement: -0.00000004403713, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2335
Epoch 2335, Loss: 0.00000002306704, Improvement: -0.00000001251015, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2336
Epoch 2336, Loss: 0.00000001992761, Improvement: -0.00000000313944, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2337
Epoch 2337, Loss: 0.00000001842902, Improvement: -0.00000000149859, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2338
Epoch 2338, Loss: 0.00000001815268, Improvement: -0.00000000027634, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2339
Epoch 2339, Loss: 0.00000001799011, Improvement: -0.00000000016257, Best Loss: 0.00000001311244 in Epoch 2316
Epoch 2340
A best model at epoch 2340 has been saved with training error 0.00000001285907.
Epoch 2340, Loss: 0.00000001809782, Improvement: 0.00000000010771, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2341
Epoch 2341, Loss: 0.00000001875572, Improvement: 0.00000000065789, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2342
Epoch 2342, Loss: 0.00000002433468, Improvement: 0.00000000557896, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2343
Epoch 2343, Loss: 0.00000003050931, Improvement: 0.00000000617463, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2344
Epoch 2344, Loss: 0.00000003169566, Improvement: 0.00000000118636, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2345
Epoch 2345, Loss: 0.00000005965440, Improvement: 0.00000002795873, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2346
Epoch 2346, Loss: 0.00000005582421, Improvement: -0.00000000383018, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2347
Epoch 2347, Loss: 0.00000002568420, Improvement: -0.00000003014001, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2348
Epoch 2348, Loss: 0.00000002251320, Improvement: -0.00000000317100, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2349
Epoch 2349, Loss: 0.00000002169085, Improvement: -0.00000000082235, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000001870816, Improvement: -0.00000000298269, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2351
Epoch 2351, Loss: 0.00000002022761, Improvement: 0.00000000151946, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2352
Epoch 2352, Loss: 0.00000002379550, Improvement: 0.00000000356788, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2353
Epoch 2353, Loss: 0.00000001997455, Improvement: -0.00000000382094, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2354
Epoch 2354, Loss: 0.00000001661775, Improvement: -0.00000000335681, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2355
Epoch 2355, Loss: 0.00000001855719, Improvement: 0.00000000193944, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2356
Epoch 2356, Loss: 0.00000002556887, Improvement: 0.00000000701169, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2357
Epoch 2357, Loss: 0.00000002603717, Improvement: 0.00000000046830, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2358
Epoch 2358, Loss: 0.00000002779099, Improvement: 0.00000000175382, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2359
Epoch 2359, Loss: 0.00000002874205, Improvement: 0.00000000095106, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2360
Epoch 2360, Loss: 0.00000006461489, Improvement: 0.00000003587283, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2361
Epoch 2361, Loss: 0.00000014656808, Improvement: 0.00000008195319, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2362
Epoch 2362, Loss: 0.00000008400401, Improvement: -0.00000006256407, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2363
Epoch 2363, Loss: 0.00000004237968, Improvement: -0.00000004162432, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2364
Epoch 2364, Loss: 0.00000002781695, Improvement: -0.00000001456273, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2365
Epoch 2365, Loss: 0.00000002340639, Improvement: -0.00000000441056, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2366
Epoch 2366, Loss: 0.00000002149658, Improvement: -0.00000000190980, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2367
Epoch 2367, Loss: 0.00000002017050, Improvement: -0.00000000132609, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2368
Epoch 2368, Loss: 0.00000002271895, Improvement: 0.00000000254845, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2369
Epoch 2369, Loss: 0.00000005155449, Improvement: 0.00000002883555, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2370
Epoch 2370, Loss: 0.00000003673332, Improvement: -0.00000001482117, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2371
Epoch 2371, Loss: 0.00000002846490, Improvement: -0.00000000826842, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2372
Epoch 2372, Loss: 0.00000005160477, Improvement: 0.00000002313987, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2373
Epoch 2373, Loss: 0.00000006857725, Improvement: 0.00000001697248, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2374
Epoch 2374, Loss: 0.00000002849232, Improvement: -0.00000004008493, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2375
Epoch 2375, Loss: 0.00000002013621, Improvement: -0.00000000835611, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2376
Epoch 2376, Loss: 0.00000001953361, Improvement: -0.00000000060261, Best Loss: 0.00000001285907 in Epoch 2340
Epoch 2377
A best model at epoch 2377 has been saved with training error 0.00000001231280.
Epoch 2377, Loss: 0.00000001751732, Improvement: -0.00000000201629, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2378
Epoch 2378, Loss: 0.00000001912332, Improvement: 0.00000000160600, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2379
Epoch 2379, Loss: 0.00000002026298, Improvement: 0.00000000113966, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2380
Epoch 2380, Loss: 0.00000002594868, Improvement: 0.00000000568570, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2381
Epoch 2381, Loss: 0.00000002575059, Improvement: -0.00000000019809, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2382
Epoch 2382, Loss: 0.00000003035386, Improvement: 0.00000000460328, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2383
Epoch 2383, Loss: 0.00000008389350, Improvement: 0.00000005353963, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2384
Epoch 2384, Loss: 0.00000017355615, Improvement: 0.00000008966265, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2385
Epoch 2385, Loss: 0.00000011807417, Improvement: -0.00000005548198, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2386
Epoch 2386, Loss: 0.00000004134551, Improvement: -0.00000007672865, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2387
Epoch 2387, Loss: 0.00000002205859, Improvement: -0.00000001928692, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2388
Epoch 2388, Loss: 0.00000001980198, Improvement: -0.00000000225661, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2389
Epoch 2389, Loss: 0.00000002379935, Improvement: 0.00000000399737, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2390
Epoch 2390, Loss: 0.00000001798923, Improvement: -0.00000000581012, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2391
Epoch 2391, Loss: 0.00000001669842, Improvement: -0.00000000129081, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2392
Epoch 2392, Loss: 0.00000001662718, Improvement: -0.00000000007124, Best Loss: 0.00000001231280 in Epoch 2377
Epoch 2393
A best model at epoch 2393 has been saved with training error 0.00000001145671.
Epoch 2393, Loss: 0.00000001671838, Improvement: 0.00000000009120, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2394
Epoch 2394, Loss: 0.00000001642762, Improvement: -0.00000000029075, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2395
Epoch 2395, Loss: 0.00000001680917, Improvement: 0.00000000038155, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2396
Epoch 2396, Loss: 0.00000001675660, Improvement: -0.00000000005258, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2397
Epoch 2397, Loss: 0.00000001656112, Improvement: -0.00000000019548, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2398
Epoch 2398, Loss: 0.00000001828025, Improvement: 0.00000000171914, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2399
Epoch 2399, Loss: 0.00000002174256, Improvement: 0.00000000346231, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000005127144, Improvement: 0.00000002952888, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2401
Epoch 2401, Loss: 0.00000008522950, Improvement: 0.00000003395806, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2402
Epoch 2402, Loss: 0.00000003568211, Improvement: -0.00000004954740, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2403
Epoch 2403, Loss: 0.00000002269729, Improvement: -0.00000001298482, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2404
Epoch 2404, Loss: 0.00000001781404, Improvement: -0.00000000488324, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2405
Epoch 2405, Loss: 0.00000001681755, Improvement: -0.00000000099650, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2406
Epoch 2406, Loss: 0.00000001713393, Improvement: 0.00000000031638, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2407
Epoch 2407, Loss: 0.00000001574398, Improvement: -0.00000000138995, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2408
Epoch 2408, Loss: 0.00000001594721, Improvement: 0.00000000020323, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2409
Epoch 2409, Loss: 0.00000001870242, Improvement: 0.00000000275521, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2410
Epoch 2410, Loss: 0.00000002475277, Improvement: 0.00000000605035, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2411
Epoch 2411, Loss: 0.00000002707836, Improvement: 0.00000000232560, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2412
Epoch 2412, Loss: 0.00000006397046, Improvement: 0.00000003689210, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2413
Epoch 2413, Loss: 0.00000007631328, Improvement: 0.00000001234282, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2414
Epoch 2414, Loss: 0.00000003793601, Improvement: -0.00000003837727, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2415
Epoch 2415, Loss: 0.00000002857234, Improvement: -0.00000000936367, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2416
Epoch 2416, Loss: 0.00000003654237, Improvement: 0.00000000797003, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2417
Epoch 2417, Loss: 0.00000002291022, Improvement: -0.00000001363215, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2418
Epoch 2418, Loss: 0.00000002125224, Improvement: -0.00000000165798, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2419
Epoch 2419, Loss: 0.00000003393281, Improvement: 0.00000001268057, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2420
Epoch 2420, Loss: 0.00000003168789, Improvement: -0.00000000224493, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2421
Epoch 2421, Loss: 0.00000002070689, Improvement: -0.00000001098100, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2422
Epoch 2422, Loss: 0.00000002544962, Improvement: 0.00000000474273, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2423
Epoch 2423, Loss: 0.00000002430036, Improvement: -0.00000000114925, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2424
Epoch 2424, Loss: 0.00000002054370, Improvement: -0.00000000375667, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2425
Epoch 2425, Loss: 0.00000002763534, Improvement: 0.00000000709164, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2426
Epoch 2426, Loss: 0.00000004615917, Improvement: 0.00000001852382, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2427
Epoch 2427, Loss: 0.00000002474661, Improvement: -0.00000002141255, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2428
Epoch 2428, Loss: 0.00000002760281, Improvement: 0.00000000285619, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2429
Epoch 2429, Loss: 0.00000004051809, Improvement: 0.00000001291529, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2430
Epoch 2430, Loss: 0.00000004406672, Improvement: 0.00000000354863, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2431
Epoch 2431, Loss: 0.00000006078659, Improvement: 0.00000001671986, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2432
Epoch 2432, Loss: 0.00000008043312, Improvement: 0.00000001964653, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2433
Epoch 2433, Loss: 0.00000004275102, Improvement: -0.00000003768210, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2434
Epoch 2434, Loss: 0.00000003158425, Improvement: -0.00000001116676, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2435
Epoch 2435, Loss: 0.00000003329742, Improvement: 0.00000000171316, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2436
Epoch 2436, Loss: 0.00000003637176, Improvement: 0.00000000307435, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2437
Epoch 2437, Loss: 0.00000002590391, Improvement: -0.00000001046785, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2438
Epoch 2438, Loss: 0.00000002811105, Improvement: 0.00000000220714, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2439
Epoch 2439, Loss: 0.00000005508138, Improvement: 0.00000002697033, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2440
Epoch 2440, Loss: 0.00000002473809, Improvement: -0.00000003034329, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2441
Epoch 2441, Loss: 0.00000001999344, Improvement: -0.00000000474465, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2442
Epoch 2442, Loss: 0.00000001646087, Improvement: -0.00000000353258, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2443
Epoch 2443, Loss: 0.00000001724439, Improvement: 0.00000000078352, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2444
Epoch 2444, Loss: 0.00000002115726, Improvement: 0.00000000391287, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2445
Epoch 2445, Loss: 0.00000001984720, Improvement: -0.00000000131005, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2446
Epoch 2446, Loss: 0.00000002548904, Improvement: 0.00000000564184, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2447
Epoch 2447, Loss: 0.00000002666929, Improvement: 0.00000000118025, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2448
Epoch 2448, Loss: 0.00000003119774, Improvement: 0.00000000452845, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2449
Epoch 2449, Loss: 0.00000003661932, Improvement: 0.00000000542157, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000003792632, Improvement: 0.00000000130700, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2451
Epoch 2451, Loss: 0.00000007349828, Improvement: 0.00000003557196, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2452
Epoch 2452, Loss: 0.00000006852859, Improvement: -0.00000000496969, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2453
Epoch 2453, Loss: 0.00000003964088, Improvement: -0.00000002888771, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2454
Epoch 2454, Loss: 0.00000002376291, Improvement: -0.00000001587797, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2455
Epoch 2455, Loss: 0.00000002727614, Improvement: 0.00000000351324, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2456
Epoch 2456, Loss: 0.00000002562541, Improvement: -0.00000000165074, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2457
Epoch 2457, Loss: 0.00000004080488, Improvement: 0.00000001517947, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2458
Epoch 2458, Loss: 0.00000004040741, Improvement: -0.00000000039747, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2459
Epoch 2459, Loss: 0.00000003115975, Improvement: -0.00000000924766, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2460
Epoch 2460, Loss: 0.00000004186807, Improvement: 0.00000001070832, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2461
Epoch 2461, Loss: 0.00000003733484, Improvement: -0.00000000453323, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2462
Epoch 2462, Loss: 0.00000004052624, Improvement: 0.00000000319141, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2463
Epoch 2463, Loss: 0.00000004032109, Improvement: -0.00000000020515, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2464
Epoch 2464, Loss: 0.00000003214580, Improvement: -0.00000000817529, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2465
Epoch 2465, Loss: 0.00000002579687, Improvement: -0.00000000634893, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2466
Epoch 2466, Loss: 0.00000003916223, Improvement: 0.00000001336536, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2467
Epoch 2467, Loss: 0.00000006214443, Improvement: 0.00000002298221, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2468
Epoch 2468, Loss: 0.00000014403795, Improvement: 0.00000008189351, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2469
Epoch 2469, Loss: 0.00000014274095, Improvement: -0.00000000129700, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2470
Epoch 2470, Loss: 0.00000007354067, Improvement: -0.00000006920028, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2471
Epoch 2471, Loss: 0.00000002810218, Improvement: -0.00000004543850, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2472
Epoch 2472, Loss: 0.00000003190908, Improvement: 0.00000000380691, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2473
Epoch 2473, Loss: 0.00000002305493, Improvement: -0.00000000885415, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2474
Epoch 2474, Loss: 0.00000001758182, Improvement: -0.00000000547311, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2475
Epoch 2475, Loss: 0.00000001582999, Improvement: -0.00000000175183, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2476
Epoch 2476, Loss: 0.00000001505964, Improvement: -0.00000000077035, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2477
Epoch 2477, Loss: 0.00000001539380, Improvement: 0.00000000033415, Best Loss: 0.00000001145671 in Epoch 2393
Epoch 2478
A best model at epoch 2478 has been saved with training error 0.00000001121318.
Epoch 2478, Loss: 0.00000001479847, Improvement: -0.00000000059533, Best Loss: 0.00000001121318 in Epoch 2478
Epoch 2479
A best model at epoch 2479 has been saved with training error 0.00000001013603.
Epoch 2479, Loss: 0.00000001414415, Improvement: -0.00000000065432, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2480
Epoch 2480, Loss: 0.00000001436210, Improvement: 0.00000000021795, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2481
Epoch 2481, Loss: 0.00000001455430, Improvement: 0.00000000019219, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2482
Epoch 2482, Loss: 0.00000001578957, Improvement: 0.00000000123528, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2483
Epoch 2483, Loss: 0.00000001710575, Improvement: 0.00000000131617, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2484
Epoch 2484, Loss: 0.00000001858315, Improvement: 0.00000000147740, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2485
Epoch 2485, Loss: 0.00000001803313, Improvement: -0.00000000055002, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2486
Epoch 2486, Loss: 0.00000001745985, Improvement: -0.00000000057327, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2487
Epoch 2487, Loss: 0.00000001501014, Improvement: -0.00000000244971, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2488
Epoch 2488, Loss: 0.00000001559437, Improvement: 0.00000000058423, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2489
Epoch 2489, Loss: 0.00000001462442, Improvement: -0.00000000096995, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2490
Epoch 2490, Loss: 0.00000001560258, Improvement: 0.00000000097816, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2491
Epoch 2491, Loss: 0.00000002412761, Improvement: 0.00000000852503, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2492
Epoch 2492, Loss: 0.00000003388339, Improvement: 0.00000000975578, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2493
Epoch 2493, Loss: 0.00000008101173, Improvement: 0.00000004712834, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2494
Epoch 2494, Loss: 0.00000005687974, Improvement: -0.00000002413199, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2495
Epoch 2495, Loss: 0.00000003120360, Improvement: -0.00000002567615, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2496
Epoch 2496, Loss: 0.00000002123460, Improvement: -0.00000000996899, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2497
Epoch 2497, Loss: 0.00000003357004, Improvement: 0.00000001233544, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2498
Epoch 2498, Loss: 0.00000006187628, Improvement: 0.00000002830625, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2499
Epoch 2499, Loss: 0.00000003769010, Improvement: -0.00000002418618, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000004175163, Improvement: 0.00000000406153, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2501
Epoch 2501, Loss: 0.00000008991072, Improvement: 0.00000004815909, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2502
Epoch 2502, Loss: 0.00000006225487, Improvement: -0.00000002765584, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2503
Epoch 2503, Loss: 0.00000003121504, Improvement: -0.00000003103983, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2504
Epoch 2504, Loss: 0.00000003526023, Improvement: 0.00000000404519, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2505
Epoch 2505, Loss: 0.00000003135129, Improvement: -0.00000000390894, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2506
Epoch 2506, Loss: 0.00000002746707, Improvement: -0.00000000388422, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2507
Epoch 2507, Loss: 0.00000002058250, Improvement: -0.00000000688457, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2508
Epoch 2508, Loss: 0.00000001784454, Improvement: -0.00000000273796, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2509
Epoch 2509, Loss: 0.00000001951257, Improvement: 0.00000000166804, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2510
Epoch 2510, Loss: 0.00000001827200, Improvement: -0.00000000124058, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2511
Epoch 2511, Loss: 0.00000002376368, Improvement: 0.00000000549168, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2512
Epoch 2512, Loss: 0.00000003840248, Improvement: 0.00000001463881, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2513
Epoch 2513, Loss: 0.00000003078408, Improvement: -0.00000000761841, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2514
Epoch 2514, Loss: 0.00000011035215, Improvement: 0.00000007956808, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2515
Epoch 2515, Loss: 0.00000022606391, Improvement: 0.00000011571176, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2516
Epoch 2516, Loss: 0.00000013924028, Improvement: -0.00000008682364, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2517
Epoch 2517, Loss: 0.00000004171792, Improvement: -0.00000009752235, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2518
Epoch 2518, Loss: 0.00000002455595, Improvement: -0.00000001716198, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2519
Epoch 2519, Loss: 0.00000001750186, Improvement: -0.00000000705408, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2520
Epoch 2520, Loss: 0.00000001573801, Improvement: -0.00000000176385, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2521
Epoch 2521, Loss: 0.00000001502051, Improvement: -0.00000000071751, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2522
Epoch 2522, Loss: 0.00000001419616, Improvement: -0.00000000082435, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2523
Epoch 2523, Loss: 0.00000001414289, Improvement: -0.00000000005327, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2524
Epoch 2524, Loss: 0.00000001386799, Improvement: -0.00000000027490, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2525
Epoch 2525, Loss: 0.00000001367917, Improvement: -0.00000000018882, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2526
Epoch 2526, Loss: 0.00000001374511, Improvement: 0.00000000006594, Best Loss: 0.00000001013603 in Epoch 2479
Epoch 2527
A best model at epoch 2527 has been saved with training error 0.00000000970217.
Epoch 2527, Loss: 0.00000001342256, Improvement: -0.00000000032255, Best Loss: 0.00000000970217 in Epoch 2527
Epoch 2528
Epoch 2528, Loss: 0.00000001346743, Improvement: 0.00000000004487, Best Loss: 0.00000000970217 in Epoch 2527
Epoch 2529
Epoch 2529, Loss: 0.00000001604834, Improvement: 0.00000000258091, Best Loss: 0.00000000970217 in Epoch 2527
Epoch 2530
A best model at epoch 2530 has been saved with training error 0.00000000960978.
Epoch 2530, Loss: 0.00000001386613, Improvement: -0.00000000218221, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2531
Epoch 2531, Loss: 0.00000001339417, Improvement: -0.00000000047196, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2532
Epoch 2532, Loss: 0.00000001449806, Improvement: 0.00000000110389, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2533
Epoch 2533, Loss: 0.00000001464988, Improvement: 0.00000000015182, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2534
Epoch 2534, Loss: 0.00000001510395, Improvement: 0.00000000045408, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2535
Epoch 2535, Loss: 0.00000001586774, Improvement: 0.00000000076379, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2536
Epoch 2536, Loss: 0.00000001341397, Improvement: -0.00000000245377, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2537
Epoch 2537, Loss: 0.00000001429136, Improvement: 0.00000000087739, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2538
Epoch 2538, Loss: 0.00000001420713, Improvement: -0.00000000008424, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2539
Epoch 2539, Loss: 0.00000001578816, Improvement: 0.00000000158103, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2540
Epoch 2540, Loss: 0.00000001496987, Improvement: -0.00000000081829, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2541
Epoch 2541, Loss: 0.00000001464840, Improvement: -0.00000000032147, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2542
Epoch 2542, Loss: 0.00000001777500, Improvement: 0.00000000312661, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2543
Epoch 2543, Loss: 0.00000001553777, Improvement: -0.00000000223723, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2544
Epoch 2544, Loss: 0.00000001652722, Improvement: 0.00000000098945, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2545
Epoch 2545, Loss: 0.00000002241059, Improvement: 0.00000000588337, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2546
Epoch 2546, Loss: 0.00000002597802, Improvement: 0.00000000356743, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2547
Epoch 2547, Loss: 0.00000002466634, Improvement: -0.00000000131168, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2548
Epoch 2548, Loss: 0.00000002479582, Improvement: 0.00000000012948, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2549
Epoch 2549, Loss: 0.00000003917508, Improvement: 0.00000001437926, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000007041466, Improvement: 0.00000003123958, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2551
Epoch 2551, Loss: 0.00000003878595, Improvement: -0.00000003162871, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2552
Epoch 2552, Loss: 0.00000002547276, Improvement: -0.00000001331319, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2553
Epoch 2553, Loss: 0.00000003475237, Improvement: 0.00000000927961, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2554
Epoch 2554, Loss: 0.00000005397324, Improvement: 0.00000001922087, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2555
Epoch 2555, Loss: 0.00000005723769, Improvement: 0.00000000326445, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2556
Epoch 2556, Loss: 0.00000004275801, Improvement: -0.00000001447968, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2557
Epoch 2557, Loss: 0.00000003716745, Improvement: -0.00000000559056, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2558
Epoch 2558, Loss: 0.00000004071689, Improvement: 0.00000000354943, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2559
Epoch 2559, Loss: 0.00000005130090, Improvement: 0.00000001058401, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2560
Epoch 2560, Loss: 0.00000002383897, Improvement: -0.00000002746193, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2561
Epoch 2561, Loss: 0.00000002538643, Improvement: 0.00000000154746, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2562
Epoch 2562, Loss: 0.00000004043383, Improvement: 0.00000001504740, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2563
Epoch 2563, Loss: 0.00000006535778, Improvement: 0.00000002492395, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2564
Epoch 2564, Loss: 0.00000005668381, Improvement: -0.00000000867397, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2565
Epoch 2565, Loss: 0.00000005982393, Improvement: 0.00000000314012, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2566
Epoch 2566, Loss: 0.00000005093278, Improvement: -0.00000000889116, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2567
Epoch 2567, Loss: 0.00000002400569, Improvement: -0.00000002692709, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2568
Epoch 2568, Loss: 0.00000001760461, Improvement: -0.00000000640108, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2569
Epoch 2569, Loss: 0.00000001669628, Improvement: -0.00000000090833, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2570
Epoch 2570, Loss: 0.00000001737086, Improvement: 0.00000000067459, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2571
Epoch 2571, Loss: 0.00000002238006, Improvement: 0.00000000500919, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2572
Epoch 2572, Loss: 0.00000003392738, Improvement: 0.00000001154733, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2573
Epoch 2573, Loss: 0.00000001799848, Improvement: -0.00000001592891, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2574
Epoch 2574, Loss: 0.00000002977525, Improvement: 0.00000001177678, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2575
Epoch 2575, Loss: 0.00000004209278, Improvement: 0.00000001231753, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2576
Epoch 2576, Loss: 0.00000005609102, Improvement: 0.00000001399823, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2577
Epoch 2577, Loss: 0.00000005652773, Improvement: 0.00000000043672, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2578
Epoch 2578, Loss: 0.00000002627395, Improvement: -0.00000003025378, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2579
Epoch 2579, Loss: 0.00000005211220, Improvement: 0.00000002583824, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2580
Epoch 2580, Loss: 0.00000005931484, Improvement: 0.00000000720265, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2581
Epoch 2581, Loss: 0.00000007559154, Improvement: 0.00000001627669, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2582
Epoch 2582, Loss: 0.00000005749671, Improvement: -0.00000001809483, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2583
Epoch 2583, Loss: 0.00000003449439, Improvement: -0.00000002300231, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2584
Epoch 2584, Loss: 0.00000001995843, Improvement: -0.00000001453596, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2585
Epoch 2585, Loss: 0.00000001505878, Improvement: -0.00000000489965, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2586
Epoch 2586, Loss: 0.00000001505564, Improvement: -0.00000000000314, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2587
Epoch 2587, Loss: 0.00000001939969, Improvement: 0.00000000434405, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2588
Epoch 2588, Loss: 0.00000002492410, Improvement: 0.00000000552441, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2589
Epoch 2589, Loss: 0.00000001742228, Improvement: -0.00000000750182, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2590
Epoch 2590, Loss: 0.00000001635776, Improvement: -0.00000000106452, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2591
Epoch 2591, Loss: 0.00000001385328, Improvement: -0.00000000250449, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2592
Epoch 2592, Loss: 0.00000001413777, Improvement: 0.00000000028449, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2593
Epoch 2593, Loss: 0.00000001537426, Improvement: 0.00000000123649, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2594
Epoch 2594, Loss: 0.00000001523034, Improvement: -0.00000000014392, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2595
Epoch 2595, Loss: 0.00000012302058, Improvement: 0.00000010779024, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2596
Epoch 2596, Loss: 0.00000004786627, Improvement: -0.00000007515431, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2597
Epoch 2597, Loss: 0.00000002741299, Improvement: -0.00000002045329, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2598
Epoch 2598, Loss: 0.00000002706329, Improvement: -0.00000000034970, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2599
Epoch 2599, Loss: 0.00000001961078, Improvement: -0.00000000745250, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000001623627, Improvement: -0.00000000337451, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2601
Epoch 2601, Loss: 0.00000001562346, Improvement: -0.00000000061281, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2602
Epoch 2602, Loss: 0.00000003431447, Improvement: 0.00000001869101, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2603
Epoch 2603, Loss: 0.00000002188795, Improvement: -0.00000001242652, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2604
Epoch 2604, Loss: 0.00000002347889, Improvement: 0.00000000159094, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2605
Epoch 2605, Loss: 0.00000001942108, Improvement: -0.00000000405781, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2606
Epoch 2606, Loss: 0.00000001907283, Improvement: -0.00000000034825, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2607
Epoch 2607, Loss: 0.00000002263823, Improvement: 0.00000000356540, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2608
Epoch 2608, Loss: 0.00000002390154, Improvement: 0.00000000126332, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2609
Epoch 2609, Loss: 0.00000001950679, Improvement: -0.00000000439475, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2610
Epoch 2610, Loss: 0.00000001399334, Improvement: -0.00000000551345, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2611
Epoch 2611, Loss: 0.00000002196742, Improvement: 0.00000000797408, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2612
Epoch 2612, Loss: 0.00000003147851, Improvement: 0.00000000951109, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2613
Epoch 2613, Loss: 0.00000004232615, Improvement: 0.00000001084763, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2614
Epoch 2614, Loss: 0.00000004718037, Improvement: 0.00000000485422, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2615
Epoch 2615, Loss: 0.00000004797886, Improvement: 0.00000000079850, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2616
Epoch 2616, Loss: 0.00000003337795, Improvement: -0.00000001460092, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2617
Epoch 2617, Loss: 0.00000002603421, Improvement: -0.00000000734373, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2618
Epoch 2618, Loss: 0.00000002425527, Improvement: -0.00000000177895, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2619
Epoch 2619, Loss: 0.00000002832531, Improvement: 0.00000000407004, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2620
Epoch 2620, Loss: 0.00000002618172, Improvement: -0.00000000214359, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2621
Epoch 2621, Loss: 0.00000003762177, Improvement: 0.00000001144005, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2622
Epoch 2622, Loss: 0.00000004952592, Improvement: 0.00000001190415, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2623
Epoch 2623, Loss: 0.00000010538607, Improvement: 0.00000005586015, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2624
Epoch 2624, Loss: 0.00000008749641, Improvement: -0.00000001788966, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2625
Epoch 2625, Loss: 0.00000003980211, Improvement: -0.00000004769430, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2626
Epoch 2626, Loss: 0.00000003057457, Improvement: -0.00000000922754, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2627
Epoch 2627, Loss: 0.00000002354956, Improvement: -0.00000000702502, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2628
Epoch 2628, Loss: 0.00000002289666, Improvement: -0.00000000065289, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2629
Epoch 2629, Loss: 0.00000001619003, Improvement: -0.00000000670663, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2630
Epoch 2630, Loss: 0.00000001543964, Improvement: -0.00000000075039, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2631
Epoch 2631, Loss: 0.00000001516728, Improvement: -0.00000000027236, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2632
Epoch 2632, Loss: 0.00000001719191, Improvement: 0.00000000202463, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2633
Epoch 2633, Loss: 0.00000001608434, Improvement: -0.00000000110757, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2634
Epoch 2634, Loss: 0.00000001941228, Improvement: 0.00000000332794, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2635
Epoch 2635, Loss: 0.00000001736431, Improvement: -0.00000000204797, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2636
Epoch 2636, Loss: 0.00000005622728, Improvement: 0.00000003886297, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2637
Epoch 2637, Loss: 0.00000005234462, Improvement: -0.00000000388266, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2638
Epoch 2638, Loss: 0.00000002909170, Improvement: -0.00000002325293, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2639
Epoch 2639, Loss: 0.00000002171555, Improvement: -0.00000000737614, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2640
Epoch 2640, Loss: 0.00000001874749, Improvement: -0.00000000296806, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2641
Epoch 2641, Loss: 0.00000003677984, Improvement: 0.00000001803234, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2642
Epoch 2642, Loss: 0.00000007512462, Improvement: 0.00000003834479, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2643
Epoch 2643, Loss: 0.00000003978525, Improvement: -0.00000003533937, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2644
Epoch 2644, Loss: 0.00000001728036, Improvement: -0.00000002250490, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2645
Epoch 2645, Loss: 0.00000001581417, Improvement: -0.00000000146618, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2646
Epoch 2646, Loss: 0.00000001433056, Improvement: -0.00000000148361, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2647
Epoch 2647, Loss: 0.00000001358938, Improvement: -0.00000000074119, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2648
Epoch 2648, Loss: 0.00000001327434, Improvement: -0.00000000031504, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2649
Epoch 2649, Loss: 0.00000001348874, Improvement: 0.00000000021440, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000001497300, Improvement: 0.00000000148426, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2651
Epoch 2651, Loss: 0.00000001658821, Improvement: 0.00000000161521, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2652
Epoch 2652, Loss: 0.00000001382276, Improvement: -0.00000000276545, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2653
Epoch 2653, Loss: 0.00000001432988, Improvement: 0.00000000050712, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2654
Epoch 2654, Loss: 0.00000002916807, Improvement: 0.00000001483819, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2655
Epoch 2655, Loss: 0.00000003288868, Improvement: 0.00000000372061, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2656
Epoch 2656, Loss: 0.00000005166603, Improvement: 0.00000001877735, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2657
Epoch 2657, Loss: 0.00000006232691, Improvement: 0.00000001066088, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2658
Epoch 2658, Loss: 0.00000009130452, Improvement: 0.00000002897761, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2659
Epoch 2659, Loss: 0.00000004284811, Improvement: -0.00000004845641, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2660
Epoch 2660, Loss: 0.00000003179871, Improvement: -0.00000001104940, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2661
Epoch 2661, Loss: 0.00000005097929, Improvement: 0.00000001918058, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2662
Epoch 2662, Loss: 0.00000003021522, Improvement: -0.00000002076407, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2663
Epoch 2663, Loss: 0.00000001958493, Improvement: -0.00000001063029, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2664
Epoch 2664, Loss: 0.00000002044361, Improvement: 0.00000000085869, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2665
Epoch 2665, Loss: 0.00000002217919, Improvement: 0.00000000173557, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2666
Epoch 2666, Loss: 0.00000001484294, Improvement: -0.00000000733625, Best Loss: 0.00000000960978 in Epoch 2530
Epoch 2667
A best model at epoch 2667 has been saved with training error 0.00000000846846.
Epoch 2667, Loss: 0.00000001281758, Improvement: -0.00000000202536, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2668
Epoch 2668, Loss: 0.00000001303521, Improvement: 0.00000000021763, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2669
Epoch 2669, Loss: 0.00000001526601, Improvement: 0.00000000223080, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2670
Epoch 2670, Loss: 0.00000002048376, Improvement: 0.00000000521775, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2671
Epoch 2671, Loss: 0.00000002890063, Improvement: 0.00000000841686, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2672
Epoch 2672, Loss: 0.00000006518869, Improvement: 0.00000003628806, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2673
Epoch 2673, Loss: 0.00000005092599, Improvement: -0.00000001426269, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2674
Epoch 2674, Loss: 0.00000004881821, Improvement: -0.00000000210778, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2675
Epoch 2675, Loss: 0.00000003499483, Improvement: -0.00000001382338, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2676
Epoch 2676, Loss: 0.00000002019517, Improvement: -0.00000001479967, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2677
Epoch 2677, Loss: 0.00000001544430, Improvement: -0.00000000475086, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2678
Epoch 2678, Loss: 0.00000003707984, Improvement: 0.00000002163553, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2679
Epoch 2679, Loss: 0.00000004325668, Improvement: 0.00000000617684, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2680
Epoch 2680, Loss: 0.00000003600173, Improvement: -0.00000000725495, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2681
Epoch 2681, Loss: 0.00000002555934, Improvement: -0.00000001044239, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2682
Epoch 2682, Loss: 0.00000003468371, Improvement: 0.00000000912437, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2683
Epoch 2683, Loss: 0.00000003922329, Improvement: 0.00000000453958, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2684
Epoch 2684, Loss: 0.00000004776773, Improvement: 0.00000000854444, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2685
Epoch 2685, Loss: 0.00000004699291, Improvement: -0.00000000077482, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2686
Epoch 2686, Loss: 0.00000002520933, Improvement: -0.00000002178358, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2687
Epoch 2687, Loss: 0.00000003444742, Improvement: 0.00000000923809, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2688
Epoch 2688, Loss: 0.00000002191951, Improvement: -0.00000001252792, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2689
Epoch 2689, Loss: 0.00000002021973, Improvement: -0.00000000169977, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2690
Epoch 2690, Loss: 0.00000001478855, Improvement: -0.00000000543118, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2691
Epoch 2691, Loss: 0.00000001575022, Improvement: 0.00000000096167, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2692
Epoch 2692, Loss: 0.00000001638370, Improvement: 0.00000000063347, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2693
Epoch 2693, Loss: 0.00000001806048, Improvement: 0.00000000167678, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2694
Epoch 2694, Loss: 0.00000002437489, Improvement: 0.00000000631441, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2695
Epoch 2695, Loss: 0.00000003368310, Improvement: 0.00000000930821, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2696
Epoch 2696, Loss: 0.00000003183308, Improvement: -0.00000000185003, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2697
Epoch 2697, Loss: 0.00000003761542, Improvement: 0.00000000578235, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2698
Epoch 2698, Loss: 0.00000008870805, Improvement: 0.00000005109262, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2699
Epoch 2699, Loss: 0.00000006668838, Improvement: -0.00000002201967, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000005135383, Improvement: -0.00000001533455, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2701
Epoch 2701, Loss: 0.00000006199600, Improvement: 0.00000001064217, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2702
Epoch 2702, Loss: 0.00000009983061, Improvement: 0.00000003783461, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2703
Epoch 2703, Loss: 0.00000002961115, Improvement: -0.00000007021946, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2704
Epoch 2704, Loss: 0.00000001800167, Improvement: -0.00000001160948, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2705
Epoch 2705, Loss: 0.00000001386897, Improvement: -0.00000000413269, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2706
Epoch 2706, Loss: 0.00000001370681, Improvement: -0.00000000016217, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2707
Epoch 2707, Loss: 0.00000001473324, Improvement: 0.00000000102643, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2708
Epoch 2708, Loss: 0.00000001567400, Improvement: 0.00000000094076, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2709
Epoch 2709, Loss: 0.00000001468103, Improvement: -0.00000000099297, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2710
Epoch 2710, Loss: 0.00000001944647, Improvement: 0.00000000476545, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2711
Epoch 2711, Loss: 0.00000002301039, Improvement: 0.00000000356392, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2712
Epoch 2712, Loss: 0.00000002151974, Improvement: -0.00000000149065, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2713
Epoch 2713, Loss: 0.00000001632498, Improvement: -0.00000000519476, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2714
Epoch 2714, Loss: 0.00000001508555, Improvement: -0.00000000123943, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2715
Epoch 2715, Loss: 0.00000001488003, Improvement: -0.00000000020553, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2716
Epoch 2716, Loss: 0.00000001369036, Improvement: -0.00000000118967, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2717
Epoch 2717, Loss: 0.00000001293419, Improvement: -0.00000000075616, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2718
Epoch 2718, Loss: 0.00000001293876, Improvement: 0.00000000000456, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2719
Epoch 2719, Loss: 0.00000001342216, Improvement: 0.00000000048340, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2720
Epoch 2720, Loss: 0.00000001586042, Improvement: 0.00000000243825, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2721
Epoch 2721, Loss: 0.00000002627379, Improvement: 0.00000001041337, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2722
Epoch 2722, Loss: 0.00000002014559, Improvement: -0.00000000612820, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2723
Epoch 2723, Loss: 0.00000002336003, Improvement: 0.00000000321443, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2724
Epoch 2724, Loss: 0.00000004324242, Improvement: 0.00000001988240, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2725
Epoch 2725, Loss: 0.00000003172078, Improvement: -0.00000001152164, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2726
Epoch 2726, Loss: 0.00000001830551, Improvement: -0.00000001341527, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2727
Epoch 2727, Loss: 0.00000002177364, Improvement: 0.00000000346813, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2728
Epoch 2728, Loss: 0.00000002757452, Improvement: 0.00000000580088, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2729
Epoch 2729, Loss: 0.00000003101919, Improvement: 0.00000000344467, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2730
Epoch 2730, Loss: 0.00000003132572, Improvement: 0.00000000030653, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2731
Epoch 2731, Loss: 0.00000003129719, Improvement: -0.00000000002853, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2732
Epoch 2732, Loss: 0.00000004165770, Improvement: 0.00000001036050, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2733
Epoch 2733, Loss: 0.00000006312860, Improvement: 0.00000002147090, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2734
Epoch 2734, Loss: 0.00000006077238, Improvement: -0.00000000235622, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2735
Epoch 2735, Loss: 0.00000003904251, Improvement: -0.00000002172987, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2736
Epoch 2736, Loss: 0.00000002581110, Improvement: -0.00000001323141, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2737
Epoch 2737, Loss: 0.00000003092364, Improvement: 0.00000000511255, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2738
Epoch 2738, Loss: 0.00000005113444, Improvement: 0.00000002021080, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2739
Epoch 2739, Loss: 0.00000004923446, Improvement: -0.00000000189998, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2740
Epoch 2740, Loss: 0.00000005459219, Improvement: 0.00000000535773, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2741
Epoch 2741, Loss: 0.00000003603961, Improvement: -0.00000001855258, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2742
Epoch 2742, Loss: 0.00000002445907, Improvement: -0.00000001158054, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2743
Epoch 2743, Loss: 0.00000001517403, Improvement: -0.00000000928504, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2744
Epoch 2744, Loss: 0.00000001388098, Improvement: -0.00000000129305, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2745
Epoch 2745, Loss: 0.00000001455644, Improvement: 0.00000000067546, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2746
Epoch 2746, Loss: 0.00000002318472, Improvement: 0.00000000862828, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2747
Epoch 2747, Loss: 0.00000003361407, Improvement: 0.00000001042935, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2748
Epoch 2748, Loss: 0.00000005877043, Improvement: 0.00000002515635, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2749
Epoch 2749, Loss: 0.00000004816599, Improvement: -0.00000001060444, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000004975547, Improvement: 0.00000000158948, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2751
Epoch 2751, Loss: 0.00000002808004, Improvement: -0.00000002167544, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2752
Epoch 2752, Loss: 0.00000004551837, Improvement: 0.00000001743833, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2753
Epoch 2753, Loss: 0.00000002430872, Improvement: -0.00000002120965, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2754
Epoch 2754, Loss: 0.00000001612465, Improvement: -0.00000000818407, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2755
Epoch 2755, Loss: 0.00000001267183, Improvement: -0.00000000345282, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2756
Epoch 2756, Loss: 0.00000001395714, Improvement: 0.00000000128531, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2757
Epoch 2757, Loss: 0.00000001336984, Improvement: -0.00000000058731, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2758
Epoch 2758, Loss: 0.00000001794882, Improvement: 0.00000000457899, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2759
Epoch 2759, Loss: 0.00000001686775, Improvement: -0.00000000108107, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2760
Epoch 2760, Loss: 0.00000004950771, Improvement: 0.00000003263996, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2761
Epoch 2761, Loss: 0.00000006367352, Improvement: 0.00000001416581, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2762
Epoch 2762, Loss: 0.00000006008244, Improvement: -0.00000000359108, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2763
Epoch 2763, Loss: 0.00000002178851, Improvement: -0.00000003829393, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2764
Epoch 2764, Loss: 0.00000001787867, Improvement: -0.00000000390984, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2765
Epoch 2765, Loss: 0.00000001728587, Improvement: -0.00000000059280, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2766
Epoch 2766, Loss: 0.00000004151188, Improvement: 0.00000002422600, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2767
Epoch 2767, Loss: 0.00000010912812, Improvement: 0.00000006761625, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2768
Epoch 2768, Loss: 0.00000006186589, Improvement: -0.00000004726223, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2769
Epoch 2769, Loss: 0.00000003119047, Improvement: -0.00000003067542, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2770
Epoch 2770, Loss: 0.00000002184898, Improvement: -0.00000000934149, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2771
Epoch 2771, Loss: 0.00000003101458, Improvement: 0.00000000916560, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2772
Epoch 2772, Loss: 0.00000002317325, Improvement: -0.00000000784133, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2773
Epoch 2773, Loss: 0.00000001706107, Improvement: -0.00000000611218, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2774
Epoch 2774, Loss: 0.00000001505834, Improvement: -0.00000000200274, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2775
Epoch 2775, Loss: 0.00000001505381, Improvement: -0.00000000000453, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2776
Epoch 2776, Loss: 0.00000001400744, Improvement: -0.00000000104637, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2777
Epoch 2777, Loss: 0.00000001505534, Improvement: 0.00000000104790, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2778
Epoch 2778, Loss: 0.00000001535939, Improvement: 0.00000000030405, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2779
Epoch 2779, Loss: 0.00000001428350, Improvement: -0.00000000107589, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2780
Epoch 2780, Loss: 0.00000002316637, Improvement: 0.00000000888286, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2781
Epoch 2781, Loss: 0.00000002707334, Improvement: 0.00000000390697, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2782
Epoch 2782, Loss: 0.00000003198410, Improvement: 0.00000000491076, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2783
Epoch 2783, Loss: 0.00000007194883, Improvement: 0.00000003996474, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2784
Epoch 2784, Loss: 0.00000005331319, Improvement: -0.00000001863564, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2785
Epoch 2785, Loss: 0.00000003599017, Improvement: -0.00000001732303, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2786
Epoch 2786, Loss: 0.00000002805574, Improvement: -0.00000000793442, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2787
Epoch 2787, Loss: 0.00000002211243, Improvement: -0.00000000594332, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2788
Epoch 2788, Loss: 0.00000001662413, Improvement: -0.00000000548830, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2789
Epoch 2789, Loss: 0.00000002434059, Improvement: 0.00000000771646, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2790
Epoch 2790, Loss: 0.00000001775085, Improvement: -0.00000000658973, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2791
Epoch 2791, Loss: 0.00000001509635, Improvement: -0.00000000265451, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2792
Epoch 2792, Loss: 0.00000001484798, Improvement: -0.00000000024837, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2793
Epoch 2793, Loss: 0.00000002198757, Improvement: 0.00000000713960, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2794
Epoch 2794, Loss: 0.00000003227510, Improvement: 0.00000001028753, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2795
Epoch 2795, Loss: 0.00000003927501, Improvement: 0.00000000699991, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2796
Epoch 2796, Loss: 0.00000007075547, Improvement: 0.00000003148046, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2797
Epoch 2797, Loss: 0.00000002772383, Improvement: -0.00000004303164, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2798
Epoch 2798, Loss: 0.00000001694707, Improvement: -0.00000001077676, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2799
Epoch 2799, Loss: 0.00000001256301, Improvement: -0.00000000438406, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000001201016, Improvement: -0.00000000055285, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2801
Epoch 2801, Loss: 0.00000001238138, Improvement: 0.00000000037122, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2802
Epoch 2802, Loss: 0.00000001284580, Improvement: 0.00000000046442, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2803
Epoch 2803, Loss: 0.00000001238227, Improvement: -0.00000000046353, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2804
Epoch 2804, Loss: 0.00000001589800, Improvement: 0.00000000351573, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2805
Epoch 2805, Loss: 0.00000002031987, Improvement: 0.00000000442186, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2806
Epoch 2806, Loss: 0.00000001604539, Improvement: -0.00000000427448, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2807
Epoch 2807, Loss: 0.00000001583718, Improvement: -0.00000000020821, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2808
Epoch 2808, Loss: 0.00000003759937, Improvement: 0.00000002176219, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2809
Epoch 2809, Loss: 0.00000010176409, Improvement: 0.00000006416472, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2810
Epoch 2810, Loss: 0.00000016283339, Improvement: 0.00000006106930, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2811
Epoch 2811, Loss: 0.00000006888083, Improvement: -0.00000009395255, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2812
Epoch 2812, Loss: 0.00000003512209, Improvement: -0.00000003375874, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2813
Epoch 2813, Loss: 0.00000001954187, Improvement: -0.00000001558023, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2814
Epoch 2814, Loss: 0.00000001490673, Improvement: -0.00000000463514, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2815
Epoch 2815, Loss: 0.00000001292694, Improvement: -0.00000000197978, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2816
Epoch 2816, Loss: 0.00000001137389, Improvement: -0.00000000155305, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2817
Epoch 2817, Loss: 0.00000001157193, Improvement: 0.00000000019803, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2818
Epoch 2818, Loss: 0.00000001545969, Improvement: 0.00000000388777, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2819
Epoch 2819, Loss: 0.00000001295696, Improvement: -0.00000000250274, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2820
Epoch 2820, Loss: 0.00000001167640, Improvement: -0.00000000128055, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2821
Epoch 2821, Loss: 0.00000001194846, Improvement: 0.00000000027205, Best Loss: 0.00000000846846 in Epoch 2667
Epoch 2822
A best model at epoch 2822 has been saved with training error 0.00000000729204.
Epoch 2822, Loss: 0.00000001155826, Improvement: -0.00000000039020, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2823
Epoch 2823, Loss: 0.00000001174644, Improvement: 0.00000000018818, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2824
Epoch 2824, Loss: 0.00000001777990, Improvement: 0.00000000603346, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2825
Epoch 2825, Loss: 0.00000001690464, Improvement: -0.00000000087525, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2826
Epoch 2826, Loss: 0.00000004097892, Improvement: 0.00000002407428, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2827
Epoch 2827, Loss: 0.00000002731391, Improvement: -0.00000001366501, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2828
Epoch 2828, Loss: 0.00000001417146, Improvement: -0.00000001314246, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2829
Epoch 2829, Loss: 0.00000001245721, Improvement: -0.00000000171425, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2830
Epoch 2830, Loss: 0.00000001196890, Improvement: -0.00000000048830, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2831
Epoch 2831, Loss: 0.00000001123094, Improvement: -0.00000000073796, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2832
Epoch 2832, Loss: 0.00000001144683, Improvement: 0.00000000021589, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2833
Epoch 2833, Loss: 0.00000001830549, Improvement: 0.00000000685866, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2834
Epoch 2834, Loss: 0.00000004041799, Improvement: 0.00000002211250, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2835
Epoch 2835, Loss: 0.00000001907608, Improvement: -0.00000002134191, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2836
Epoch 2836, Loss: 0.00000001161844, Improvement: -0.00000000745764, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2837
Epoch 2837, Loss: 0.00000001302191, Improvement: 0.00000000140347, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2838
Epoch 2838, Loss: 0.00000001656717, Improvement: 0.00000000354526, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2839
Epoch 2839, Loss: 0.00000002896968, Improvement: 0.00000001240251, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2840
Epoch 2840, Loss: 0.00000002974392, Improvement: 0.00000000077424, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2841
Epoch 2841, Loss: 0.00000002247139, Improvement: -0.00000000727253, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2842
Epoch 2842, Loss: 0.00000003128123, Improvement: 0.00000000880984, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2843
Epoch 2843, Loss: 0.00000007037590, Improvement: 0.00000003909467, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2844
Epoch 2844, Loss: 0.00000006904519, Improvement: -0.00000000133071, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2845
Epoch 2845, Loss: 0.00000004582840, Improvement: -0.00000002321679, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2846
Epoch 2846, Loss: 0.00000002991060, Improvement: -0.00000001591780, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2847
Epoch 2847, Loss: 0.00000002041340, Improvement: -0.00000000949720, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2848
Epoch 2848, Loss: 0.00000001858302, Improvement: -0.00000000183038, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2849
Epoch 2849, Loss: 0.00000001330519, Improvement: -0.00000000527783, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000001463179, Improvement: 0.00000000132660, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2851
Epoch 2851, Loss: 0.00000001504889, Improvement: 0.00000000041710, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2852
Epoch 2852, Loss: 0.00000001251500, Improvement: -0.00000000253390, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2853
Epoch 2853, Loss: 0.00000002118242, Improvement: 0.00000000866742, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2854
Epoch 2854, Loss: 0.00000002321718, Improvement: 0.00000000203476, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2855
Epoch 2855, Loss: 0.00000005855731, Improvement: 0.00000003534013, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2856
Epoch 2856, Loss: 0.00000004425077, Improvement: -0.00000001430654, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2857
Epoch 2857, Loss: 0.00000003325962, Improvement: -0.00000001099115, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2858
Epoch 2858, Loss: 0.00000003059479, Improvement: -0.00000000266483, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2859
Epoch 2859, Loss: 0.00000002450686, Improvement: -0.00000000608793, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2860
Epoch 2860, Loss: 0.00000002153927, Improvement: -0.00000000296759, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2861
Epoch 2861, Loss: 0.00000001849011, Improvement: -0.00000000304916, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2862
Epoch 2862, Loss: 0.00000001911238, Improvement: 0.00000000062227, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2863
Epoch 2863, Loss: 0.00000001527317, Improvement: -0.00000000383921, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2864
Epoch 2864, Loss: 0.00000001630570, Improvement: 0.00000000103253, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2865
Epoch 2865, Loss: 0.00000001318888, Improvement: -0.00000000311682, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2866
Epoch 2866, Loss: 0.00000001515401, Improvement: 0.00000000196513, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2867
Epoch 2867, Loss: 0.00000002217728, Improvement: 0.00000000702327, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2868
Epoch 2868, Loss: 0.00000002124149, Improvement: -0.00000000093578, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2869
Epoch 2869, Loss: 0.00000002302880, Improvement: 0.00000000178731, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2870
Epoch 2870, Loss: 0.00000003249814, Improvement: 0.00000000946934, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2871
Epoch 2871, Loss: 0.00000002311334, Improvement: -0.00000000938480, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2872
Epoch 2872, Loss: 0.00000002552139, Improvement: 0.00000000240805, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2873
Epoch 2873, Loss: 0.00000005049876, Improvement: 0.00000002497737, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2874
Epoch 2874, Loss: 0.00000007082904, Improvement: 0.00000002033028, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2875
Epoch 2875, Loss: 0.00000003581347, Improvement: -0.00000003501557, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2876
Epoch 2876, Loss: 0.00000002507681, Improvement: -0.00000001073666, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2877
Epoch 2877, Loss: 0.00000002946469, Improvement: 0.00000000438787, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2878
Epoch 2878, Loss: 0.00000003695446, Improvement: 0.00000000748977, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2879
Epoch 2879, Loss: 0.00000004255391, Improvement: 0.00000000559945, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2880
Epoch 2880, Loss: 0.00000002230967, Improvement: -0.00000002024424, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2881
Epoch 2881, Loss: 0.00000003060014, Improvement: 0.00000000829047, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2882
Epoch 2882, Loss: 0.00000004640616, Improvement: 0.00000001580602, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2883
Epoch 2883, Loss: 0.00000003608929, Improvement: -0.00000001031687, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2884
Epoch 2884, Loss: 0.00000002221311, Improvement: -0.00000001387618, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2885
Epoch 2885, Loss: 0.00000001498637, Improvement: -0.00000000722674, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2886
Epoch 2886, Loss: 0.00000001268906, Improvement: -0.00000000229731, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2887
Epoch 2887, Loss: 0.00000001397296, Improvement: 0.00000000128390, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2888
Epoch 2888, Loss: 0.00000002690511, Improvement: 0.00000001293215, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2889
Epoch 2889, Loss: 0.00000006932287, Improvement: 0.00000004241776, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2890
Epoch 2890, Loss: 0.00000006664323, Improvement: -0.00000000267964, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2891
Epoch 2891, Loss: 0.00000002872783, Improvement: -0.00000003791539, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2892
Epoch 2892, Loss: 0.00000001849450, Improvement: -0.00000001023334, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2893
Epoch 2893, Loss: 0.00000001886299, Improvement: 0.00000000036849, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2894
Epoch 2894, Loss: 0.00000001324164, Improvement: -0.00000000562135, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2895
Epoch 2895, Loss: 0.00000001139266, Improvement: -0.00000000184898, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2896
Epoch 2896, Loss: 0.00000001109817, Improvement: -0.00000000029449, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2897
Epoch 2897, Loss: 0.00000001091313, Improvement: -0.00000000018503, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2898
Epoch 2898, Loss: 0.00000001125827, Improvement: 0.00000000034513, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2899
Epoch 2899, Loss: 0.00000001152588, Improvement: 0.00000000026761, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000001491809, Improvement: 0.00000000339221, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2901
Epoch 2901, Loss: 0.00000002133165, Improvement: 0.00000000641356, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2902
Epoch 2902, Loss: 0.00000001877720, Improvement: -0.00000000255444, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2903
Epoch 2903, Loss: 0.00000002347845, Improvement: 0.00000000470125, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2904
Epoch 2904, Loss: 0.00000002901182, Improvement: 0.00000000553336, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2905
Epoch 2905, Loss: 0.00000001954775, Improvement: -0.00000000946407, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2906
Epoch 2906, Loss: 0.00000001892464, Improvement: -0.00000000062311, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2907
Epoch 2907, Loss: 0.00000002142937, Improvement: 0.00000000250473, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2908
Epoch 2908, Loss: 0.00000003482699, Improvement: 0.00000001339762, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2909
Epoch 2909, Loss: 0.00000003648923, Improvement: 0.00000000166224, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2910
Epoch 2910, Loss: 0.00000005144759, Improvement: 0.00000001495836, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2911
Epoch 2911, Loss: 0.00000003445569, Improvement: -0.00000001699190, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2912
Epoch 2912, Loss: 0.00000001716149, Improvement: -0.00000001729420, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2913
Epoch 2913, Loss: 0.00000001588335, Improvement: -0.00000000127814, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2914
Epoch 2914, Loss: 0.00000001621463, Improvement: 0.00000000033128, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2915
Epoch 2915, Loss: 0.00000002191176, Improvement: 0.00000000569713, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2916
Epoch 2916, Loss: 0.00000002775809, Improvement: 0.00000000584633, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2917
Epoch 2917, Loss: 0.00000003688252, Improvement: 0.00000000912443, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2918
Epoch 2918, Loss: 0.00000005358323, Improvement: 0.00000001670071, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2919
Epoch 2919, Loss: 0.00000002921813, Improvement: -0.00000002436510, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2920
Epoch 2920, Loss: 0.00000004266550, Improvement: 0.00000001344737, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2921
Epoch 2921, Loss: 0.00000002763080, Improvement: -0.00000001503470, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2922
Epoch 2922, Loss: 0.00000001851659, Improvement: -0.00000000911421, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2923
Epoch 2923, Loss: 0.00000002008454, Improvement: 0.00000000156795, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2924
Epoch 2924, Loss: 0.00000001776281, Improvement: -0.00000000232173, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2925
Epoch 2925, Loss: 0.00000001982559, Improvement: 0.00000000206278, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2926
Epoch 2926, Loss: 0.00000001641157, Improvement: -0.00000000341402, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2927
Epoch 2927, Loss: 0.00000002307100, Improvement: 0.00000000665943, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2928
Epoch 2928, Loss: 0.00000003382840, Improvement: 0.00000001075740, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2929
Epoch 2929, Loss: 0.00000004175994, Improvement: 0.00000000793154, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2930
Epoch 2930, Loss: 0.00000003245121, Improvement: -0.00000000930873, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2931
Epoch 2931, Loss: 0.00000002844719, Improvement: -0.00000000400402, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2932
Epoch 2932, Loss: 0.00000003459971, Improvement: 0.00000000615252, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2933
Epoch 2933, Loss: 0.00000002005035, Improvement: -0.00000001454936, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2934
Epoch 2934, Loss: 0.00000001936156, Improvement: -0.00000000068879, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2935
Epoch 2935, Loss: 0.00000001605839, Improvement: -0.00000000330317, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2936
Epoch 2936, Loss: 0.00000002716629, Improvement: 0.00000001110790, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2937
Epoch 2937, Loss: 0.00000002533053, Improvement: -0.00000000183576, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2938
Epoch 2938, Loss: 0.00000001793092, Improvement: -0.00000000739961, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2939
Epoch 2939, Loss: 0.00000002824599, Improvement: 0.00000001031506, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2940
Epoch 2940, Loss: 0.00000003871046, Improvement: 0.00000001046448, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2941
Epoch 2941, Loss: 0.00000003431975, Improvement: -0.00000000439072, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2942
Epoch 2942, Loss: 0.00000004504968, Improvement: 0.00000001072993, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2943
Epoch 2943, Loss: 0.00000002298401, Improvement: -0.00000002206567, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2944
Epoch 2944, Loss: 0.00000002331334, Improvement: 0.00000000032933, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2945
Epoch 2945, Loss: 0.00000002510706, Improvement: 0.00000000179372, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2946
Epoch 2946, Loss: 0.00000003531432, Improvement: 0.00000001020727, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2947
Epoch 2947, Loss: 0.00000004796191, Improvement: 0.00000001264759, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2948
Epoch 2948, Loss: 0.00000006444889, Improvement: 0.00000001648698, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2949
Epoch 2949, Loss: 0.00000002312958, Improvement: -0.00000004131932, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000002175190, Improvement: -0.00000000137768, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2951
Epoch 2951, Loss: 0.00000001737646, Improvement: -0.00000000437544, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2952
Epoch 2952, Loss: 0.00000001397960, Improvement: -0.00000000339685, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2953
Epoch 2953, Loss: 0.00000001281355, Improvement: -0.00000000116606, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2954
Epoch 2954, Loss: 0.00000001201396, Improvement: -0.00000000079959, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2955
Epoch 2955, Loss: 0.00000001037133, Improvement: -0.00000000164263, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2956
Epoch 2956, Loss: 0.00000001155144, Improvement: 0.00000000118011, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2957
Epoch 2957, Loss: 0.00000001176517, Improvement: 0.00000000021373, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2958
Epoch 2958, Loss: 0.00000001178632, Improvement: 0.00000000002116, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2959
Epoch 2959, Loss: 0.00000001334597, Improvement: 0.00000000155965, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2960
Epoch 2960, Loss: 0.00000001624911, Improvement: 0.00000000290314, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2961
Epoch 2961, Loss: 0.00000001809926, Improvement: 0.00000000185014, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2962
Epoch 2962, Loss: 0.00000004002017, Improvement: 0.00000002192092, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2963
Epoch 2963, Loss: 0.00000003625106, Improvement: -0.00000000376911, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2964
Epoch 2964, Loss: 0.00000003888736, Improvement: 0.00000000263630, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2965
Epoch 2965, Loss: 0.00000003168552, Improvement: -0.00000000720183, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2966
Epoch 2966, Loss: 0.00000004386723, Improvement: 0.00000001218171, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2967
Epoch 2967, Loss: 0.00000003496515, Improvement: -0.00000000890209, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2968
Epoch 2968, Loss: 0.00000003354601, Improvement: -0.00000000141914, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2969
Epoch 2969, Loss: 0.00000004482869, Improvement: 0.00000001128268, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2970
Epoch 2970, Loss: 0.00000001931503, Improvement: -0.00000002551366, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2971
Epoch 2971, Loss: 0.00000001592821, Improvement: -0.00000000338682, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2972
Epoch 2972, Loss: 0.00000001528742, Improvement: -0.00000000064079, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2973
Epoch 2973, Loss: 0.00000002227848, Improvement: 0.00000000699106, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2974
Epoch 2974, Loss: 0.00000003026569, Improvement: 0.00000000798721, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2975
Epoch 2975, Loss: 0.00000003284859, Improvement: 0.00000000258291, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2976
Epoch 2976, Loss: 0.00000007851904, Improvement: 0.00000004567044, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2977
Epoch 2977, Loss: 0.00000004181238, Improvement: -0.00000003670665, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2978
Epoch 2978, Loss: 0.00000003127802, Improvement: -0.00000001053436, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2979
Epoch 2979, Loss: 0.00000003447830, Improvement: 0.00000000320028, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2980
Epoch 2980, Loss: 0.00000002481475, Improvement: -0.00000000966355, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2981
Epoch 2981, Loss: 0.00000002097794, Improvement: -0.00000000383681, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2982
Epoch 2982, Loss: 0.00000002847623, Improvement: 0.00000000749828, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2983
Epoch 2983, Loss: 0.00000001795454, Improvement: -0.00000001052168, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2984
Epoch 2984, Loss: 0.00000001525262, Improvement: -0.00000000270192, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2985
Epoch 2985, Loss: 0.00000001773438, Improvement: 0.00000000248176, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2986
Epoch 2986, Loss: 0.00000001264899, Improvement: -0.00000000508539, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2987
Epoch 2987, Loss: 0.00000001125790, Improvement: -0.00000000139109, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2988
Epoch 2988, Loss: 0.00000001198990, Improvement: 0.00000000073199, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2989
Epoch 2989, Loss: 0.00000001673631, Improvement: 0.00000000474641, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2990
Epoch 2990, Loss: 0.00000004244061, Improvement: 0.00000002570430, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2991
Epoch 2991, Loss: 0.00000002714026, Improvement: -0.00000001530035, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2992
Epoch 2992, Loss: 0.00000001907966, Improvement: -0.00000000806060, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2993
Epoch 2993, Loss: 0.00000001300403, Improvement: -0.00000000607564, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2994
Epoch 2994, Loss: 0.00000001086256, Improvement: -0.00000000214147, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2995
Epoch 2995, Loss: 0.00000001094189, Improvement: 0.00000000007933, Best Loss: 0.00000000729204 in Epoch 2822
Epoch 2996
A best model at epoch 2996 has been saved with training error 0.00000000613401.
Epoch 2996, Loss: 0.00000001108280, Improvement: 0.00000000014092, Best Loss: 0.00000000613401 in Epoch 2996
Epoch 2997
Epoch 2997, Loss: 0.00000001457464, Improvement: 0.00000000349184, Best Loss: 0.00000000613401 in Epoch 2996
Epoch 2998
Epoch 2998, Loss: 0.00000002059136, Improvement: 0.00000000601672, Best Loss: 0.00000000613401 in Epoch 2996
Epoch 2999
Epoch 2999, Loss: 0.00000002461585, Improvement: 0.00000000402449, Best Loss: 0.00000000613401 in Epoch 2996
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000002845274, Improvement: 0.00000000383689, Best Loss: 0.00000000613401 in Epoch 2996
