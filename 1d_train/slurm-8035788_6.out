The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00009924521146.
A best model at epoch 1 has been saved with training error 0.00006267466233.
A best model at epoch 1 has been saved with training error 0.00005675037028.
A best model at epoch 1 has been saved with training error 0.00004912608711.
A best model at epoch 1 has been saved with training error 0.00004060442006.
Epoch 1, Loss: 0.00012015373250, Improvement: 0.00012015373250, Best Loss: 0.00004060442006 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.00003126462616.
A best model at epoch 2 has been saved with training error 0.00002995123577.
Epoch 2, Loss: 0.00005374329376, Improvement: -0.00006641043874, Best Loss: 0.00002995123577 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.00002846956704.
A best model at epoch 3 has been saved with training error 0.00002775341272.
Epoch 3, Loss: 0.00004665680808, Improvement: -0.00000708648568, Best Loss: 0.00002775341272 in Epoch 3
Epoch 4
Epoch 4, Loss: 0.00004366813628, Improvement: -0.00000298867180, Best Loss: 0.00002775341272 in Epoch 3
Epoch 5
A best model at epoch 5 has been saved with training error 0.00002390081136.
Epoch 5, Loss: 0.00004133263392, Improvement: -0.00000233550236, Best Loss: 0.00002390081136 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.00003938106956, Improvement: -0.00000195156435, Best Loss: 0.00002390081136 in Epoch 5
Epoch 7
A best model at epoch 7 has been saved with training error 0.00001851960224.
Epoch 7, Loss: 0.00003803262343, Improvement: -0.00000134844613, Best Loss: 0.00001851960224 in Epoch 7
Epoch 8
Epoch 8, Loss: 0.00003718271337, Improvement: -0.00000084991007, Best Loss: 0.00001851960224 in Epoch 7
Epoch 9
Epoch 9, Loss: 0.00003668172185, Improvement: -0.00000050099152, Best Loss: 0.00001851960224 in Epoch 7
Epoch 10
Epoch 10, Loss: 0.00003628769628, Improvement: -0.00000039402557, Best Loss: 0.00001851960224 in Epoch 7
Epoch 11
Epoch 11, Loss: 0.00003593665151, Improvement: -0.00000035104476, Best Loss: 0.00001851960224 in Epoch 7
Epoch 12
Epoch 12, Loss: 0.00003557636137, Improvement: -0.00000036029014, Best Loss: 0.00001851960224 in Epoch 7
Epoch 13
Epoch 13, Loss: 0.00003515500712, Improvement: -0.00000042135425, Best Loss: 0.00001851960224 in Epoch 7
Epoch 14
Epoch 14, Loss: 0.00003467051110, Improvement: -0.00000048449601, Best Loss: 0.00001851960224 in Epoch 7
Epoch 15
Epoch 15, Loss: 0.00003403145229, Improvement: -0.00000063905882, Best Loss: 0.00001851960224 in Epoch 7
Epoch 16
Epoch 16, Loss: 0.00003326307178, Improvement: -0.00000076838051, Best Loss: 0.00001851960224 in Epoch 7
Epoch 17
Epoch 17, Loss: 0.00003216742862, Improvement: -0.00000109564317, Best Loss: 0.00001851960224 in Epoch 7
Epoch 18
A best model at epoch 18 has been saved with training error 0.00001744145993.
Epoch 18, Loss: 0.00003078190621, Improvement: -0.00000138552241, Best Loss: 0.00001744145993 in Epoch 18
Epoch 19
A best model at epoch 19 has been saved with training error 0.00001697100197.
Epoch 19, Loss: 0.00002893774281, Improvement: -0.00000184416340, Best Loss: 0.00001697100197 in Epoch 19
Epoch 20
A best model at epoch 20 has been saved with training error 0.00001650525701.
Epoch 20, Loss: 0.00002662373372, Improvement: -0.00000231400909, Best Loss: 0.00001650525701 in Epoch 20
Epoch 21
A best model at epoch 21 has been saved with training error 0.00001485021494.
A best model at epoch 21 has been saved with training error 0.00001461292959.
Epoch 21, Loss: 0.00002380404612, Improvement: -0.00000281968760, Best Loss: 0.00001461292959 in Epoch 21
Epoch 22
A best model at epoch 22 has been saved with training error 0.00001092615730.
Epoch 22, Loss: 0.00002050898870, Improvement: -0.00000329505742, Best Loss: 0.00001092615730 in Epoch 22
Epoch 23
Epoch 23, Loss: 0.00001723999453, Improvement: -0.00000326899417, Best Loss: 0.00001092615730 in Epoch 22
Epoch 24
A best model at epoch 24 has been saved with training error 0.00000798696146.
Epoch 24, Loss: 0.00001478039171, Improvement: -0.00000245960282, Best Loss: 0.00000798696146 in Epoch 24
Epoch 25
A best model at epoch 25 has been saved with training error 0.00000660070509.
Epoch 25, Loss: 0.00001332171378, Improvement: -0.00000145867793, Best Loss: 0.00000660070509 in Epoch 25
Epoch 26
Epoch 26, Loss: 0.00001278094796, Improvement: -0.00000054076581, Best Loss: 0.00000660070509 in Epoch 25
Epoch 27
Epoch 27, Loss: 0.00001248582332, Improvement: -0.00000029512464, Best Loss: 0.00000660070509 in Epoch 25
Epoch 28
A best model at epoch 28 has been saved with training error 0.00000632024239.
Epoch 28, Loss: 0.00001221254288, Improvement: -0.00000027328044, Best Loss: 0.00000632024239 in Epoch 28
Epoch 29
Epoch 29, Loss: 0.00001192349653, Improvement: -0.00000028904635, Best Loss: 0.00000632024239 in Epoch 28
Epoch 30
A best model at epoch 30 has been saved with training error 0.00000530876150.
Epoch 30, Loss: 0.00001162386873, Improvement: -0.00000029962780, Best Loss: 0.00000530876150 in Epoch 30
Epoch 31
Epoch 31, Loss: 0.00001125202839, Improvement: -0.00000037184034, Best Loss: 0.00000530876150 in Epoch 30
Epoch 32
A best model at epoch 32 has been saved with training error 0.00000517634180.
Epoch 32, Loss: 0.00001080305426, Improvement: -0.00000044897413, Best Loss: 0.00000517634180 in Epoch 32
Epoch 33
Epoch 33, Loss: 0.00001022892827, Improvement: -0.00000057412599, Best Loss: 0.00000517634180 in Epoch 32
Epoch 34
A best model at epoch 34 has been saved with training error 0.00000445959495.
Epoch 34, Loss: 0.00000954317909, Improvement: -0.00000068574918, Best Loss: 0.00000445959495 in Epoch 34
Epoch 35
Epoch 35, Loss: 0.00000867937065, Improvement: -0.00000086380844, Best Loss: 0.00000445959495 in Epoch 34
Epoch 36
A best model at epoch 36 has been saved with training error 0.00000435249649.
Epoch 36, Loss: 0.00000766212970, Improvement: -0.00000101724095, Best Loss: 0.00000435249649 in Epoch 36
Epoch 37
A best model at epoch 37 has been saved with training error 0.00000394160679.
A best model at epoch 37 has been saved with training error 0.00000372203863.
Epoch 37, Loss: 0.00000653822405, Improvement: -0.00000112390566, Best Loss: 0.00000372203863 in Epoch 37
Epoch 38
A best model at epoch 38 has been saved with training error 0.00000343997613.
A best model at epoch 38 has been saved with training error 0.00000285523038.
Epoch 38, Loss: 0.00000548175580, Improvement: -0.00000105646825, Best Loss: 0.00000285523038 in Epoch 38
Epoch 39
A best model at epoch 39 has been saved with training error 0.00000274733407.
Epoch 39, Loss: 0.00000455204298, Improvement: -0.00000092971281, Best Loss: 0.00000274733407 in Epoch 39
Epoch 40
A best model at epoch 40 has been saved with training error 0.00000260740580.
A best model at epoch 40 has been saved with training error 0.00000198838006.
Epoch 40, Loss: 0.00000385641036, Improvement: -0.00000069563262, Best Loss: 0.00000198838006 in Epoch 40
Epoch 41
Epoch 41, Loss: 0.00000338985971, Improvement: -0.00000046655066, Best Loss: 0.00000198838006 in Epoch 40
Epoch 42
Epoch 42, Loss: 0.00000307372082, Improvement: -0.00000031613888, Best Loss: 0.00000198838006 in Epoch 40
Epoch 43
A best model at epoch 43 has been saved with training error 0.00000175738478.
A best model at epoch 43 has been saved with training error 0.00000165981453.
Epoch 43, Loss: 0.00000286603694, Improvement: -0.00000020768388, Best Loss: 0.00000165981453 in Epoch 43
Epoch 44
A best model at epoch 44 has been saved with training error 0.00000157351178.
Epoch 44, Loss: 0.00000271318256, Improvement: -0.00000015285438, Best Loss: 0.00000157351178 in Epoch 44
Epoch 45
A best model at epoch 45 has been saved with training error 0.00000148346942.
Epoch 45, Loss: 0.00000258562966, Improvement: -0.00000012755290, Best Loss: 0.00000148346942 in Epoch 45
Epoch 46
A best model at epoch 46 has been saved with training error 0.00000140673433.
Epoch 46, Loss: 0.00000247156879, Improvement: -0.00000011406087, Best Loss: 0.00000140673433 in Epoch 46
Epoch 47
Epoch 47, Loss: 0.00000237720399, Improvement: -0.00000009436479, Best Loss: 0.00000140673433 in Epoch 46
Epoch 48
A best model at epoch 48 has been saved with training error 0.00000139558381.
Epoch 48, Loss: 0.00000227498017, Improvement: -0.00000010222382, Best Loss: 0.00000139558381 in Epoch 48
Epoch 49
A best model at epoch 49 has been saved with training error 0.00000134261154.
Epoch 49, Loss: 0.00000217763989, Improvement: -0.00000009734028, Best Loss: 0.00000134261154 in Epoch 49
Epoch 50
A best model at epoch 50 has been saved with training error 0.00000132035814.
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00000208393025, Improvement: -0.00000009370964, Best Loss: 0.00000132035814 in Epoch 50
Epoch 51
Epoch 51, Loss: 0.00000199814449, Improvement: -0.00000008578576, Best Loss: 0.00000132035814 in Epoch 50
Epoch 52
Epoch 52, Loss: 0.00000190384367, Improvement: -0.00000009430082, Best Loss: 0.00000132035814 in Epoch 50
Epoch 53
A best model at epoch 53 has been saved with training error 0.00000123050665.
A best model at epoch 53 has been saved with training error 0.00000117152206.
Epoch 53, Loss: 0.00000182221794, Improvement: -0.00000008162572, Best Loss: 0.00000117152206 in Epoch 53
Epoch 54
Epoch 54, Loss: 0.00000175593382, Improvement: -0.00000006628413, Best Loss: 0.00000117152206 in Epoch 53
Epoch 55
A best model at epoch 55 has been saved with training error 0.00000095932614.
Epoch 55, Loss: 0.00000166166502, Improvement: -0.00000009426880, Best Loss: 0.00000095932614 in Epoch 55
Epoch 56
Epoch 56, Loss: 0.00000157200394, Improvement: -0.00000008966109, Best Loss: 0.00000095932614 in Epoch 55
Epoch 57
Epoch 57, Loss: 0.00000149011594, Improvement: -0.00000008188799, Best Loss: 0.00000095932614 in Epoch 55
Epoch 58
A best model at epoch 58 has been saved with training error 0.00000073059510.
Epoch 58, Loss: 0.00000140785860, Improvement: -0.00000008225735, Best Loss: 0.00000073059510 in Epoch 58
Epoch 59
Epoch 59, Loss: 0.00000134447466, Improvement: -0.00000006338393, Best Loss: 0.00000073059510 in Epoch 58
Epoch 60
Epoch 60, Loss: 0.00000126680418, Improvement: -0.00000007767048, Best Loss: 0.00000073059510 in Epoch 58
Epoch 61
Epoch 61, Loss: 0.00000121189844, Improvement: -0.00000005490573, Best Loss: 0.00000073059510 in Epoch 58
Epoch 62
Epoch 62, Loss: 0.00000115403257, Improvement: -0.00000005786587, Best Loss: 0.00000073059510 in Epoch 58
Epoch 63
Epoch 63, Loss: 0.00000107023601, Improvement: -0.00000008379657, Best Loss: 0.00000073059510 in Epoch 58
Epoch 64
A best model at epoch 64 has been saved with training error 0.00000070099031.
A best model at epoch 64 has been saved with training error 0.00000062050611.
Epoch 64, Loss: 0.00000101091383, Improvement: -0.00000005932218, Best Loss: 0.00000062050611 in Epoch 64
Epoch 65
Epoch 65, Loss: 0.00000094922599, Improvement: -0.00000006168785, Best Loss: 0.00000062050611 in Epoch 64
Epoch 66
Epoch 66, Loss: 0.00000089498307, Improvement: -0.00000005424291, Best Loss: 0.00000062050611 in Epoch 64
Epoch 67
A best model at epoch 67 has been saved with training error 0.00000051532697.
A best model at epoch 67 has been saved with training error 0.00000049906527.
Epoch 67, Loss: 0.00000084099931, Improvement: -0.00000005398376, Best Loss: 0.00000049906527 in Epoch 67
Epoch 68
Epoch 68, Loss: 0.00000080382893, Improvement: -0.00000003717038, Best Loss: 0.00000049906527 in Epoch 67
Epoch 69
A best model at epoch 69 has been saved with training error 0.00000045111196.
Epoch 69, Loss: 0.00000074803488, Improvement: -0.00000005579405, Best Loss: 0.00000045111196 in Epoch 69
Epoch 70
Epoch 70, Loss: 0.00000072443976, Improvement: -0.00000002359512, Best Loss: 0.00000045111196 in Epoch 69
Epoch 71
Epoch 71, Loss: 0.00000068418382, Improvement: -0.00000004025594, Best Loss: 0.00000045111196 in Epoch 69
Epoch 72
A best model at epoch 72 has been saved with training error 0.00000042686480.
Epoch 72, Loss: 0.00000064617623, Improvement: -0.00000003800759, Best Loss: 0.00000042686480 in Epoch 72
Epoch 73
Epoch 73, Loss: 0.00000062537725, Improvement: -0.00000002079897, Best Loss: 0.00000042686480 in Epoch 72
Epoch 74
A best model at epoch 74 has been saved with training error 0.00000040288731.
A best model at epoch 74 has been saved with training error 0.00000037058359.
Epoch 74, Loss: 0.00000058169413, Improvement: -0.00000004368312, Best Loss: 0.00000037058359 in Epoch 74
Epoch 75
Epoch 75, Loss: 0.00000055989804, Improvement: -0.00000002179610, Best Loss: 0.00000037058359 in Epoch 74
Epoch 76
A best model at epoch 76 has been saved with training error 0.00000032683735.
Epoch 76, Loss: 0.00000054758357, Improvement: -0.00000001231446, Best Loss: 0.00000032683735 in Epoch 76
Epoch 77
Epoch 77, Loss: 0.00000052703347, Improvement: -0.00000002055010, Best Loss: 0.00000032683735 in Epoch 76
Epoch 78
Epoch 78, Loss: 0.00000054325533, Improvement: 0.00000001622186, Best Loss: 0.00000032683735 in Epoch 76
Epoch 79
Epoch 79, Loss: 0.00000063485480, Improvement: 0.00000009159947, Best Loss: 0.00000032683735 in Epoch 76
Epoch 80
Epoch 80, Loss: 0.00000056095575, Improvement: -0.00000007389905, Best Loss: 0.00000032683735 in Epoch 76
Epoch 81
Epoch 81, Loss: 0.00000050578077, Improvement: -0.00000005517498, Best Loss: 0.00000032683735 in Epoch 76
Epoch 82
Epoch 82, Loss: 0.00000048413871, Improvement: -0.00000002164205, Best Loss: 0.00000032683735 in Epoch 76
Epoch 83
A best model at epoch 83 has been saved with training error 0.00000030772344.
A best model at epoch 83 has been saved with training error 0.00000029110097.
Epoch 83, Loss: 0.00000047985951, Improvement: -0.00000000427921, Best Loss: 0.00000029110097 in Epoch 83
Epoch 84
Epoch 84, Loss: 0.00000047027441, Improvement: -0.00000000958510, Best Loss: 0.00000029110097 in Epoch 83
Epoch 85
Epoch 85, Loss: 0.00000046431515, Improvement: -0.00000000595926, Best Loss: 0.00000029110097 in Epoch 83
Epoch 86
Epoch 86, Loss: 0.00000046331581, Improvement: -0.00000000099933, Best Loss: 0.00000029110097 in Epoch 83
Epoch 87
Epoch 87, Loss: 0.00000046364049, Improvement: 0.00000000032468, Best Loss: 0.00000029110097 in Epoch 83
Epoch 88
Epoch 88, Loss: 0.00000045151460, Improvement: -0.00000001212589, Best Loss: 0.00000029110097 in Epoch 83
Epoch 89
Epoch 89, Loss: 0.00000045369445, Improvement: 0.00000000217985, Best Loss: 0.00000029110097 in Epoch 83
Epoch 90
Epoch 90, Loss: 0.00000045279717, Improvement: -0.00000000089727, Best Loss: 0.00000029110097 in Epoch 83
Epoch 91
Epoch 91, Loss: 0.00000044898258, Improvement: -0.00000000381460, Best Loss: 0.00000029110097 in Epoch 83
Epoch 92
A best model at epoch 92 has been saved with training error 0.00000028388055.
Epoch 92, Loss: 0.00000044748567, Improvement: -0.00000000149690, Best Loss: 0.00000028388055 in Epoch 92
Epoch 93
A best model at epoch 93 has been saved with training error 0.00000026966339.
Epoch 93, Loss: 0.00000045530386, Improvement: 0.00000000781819, Best Loss: 0.00000026966339 in Epoch 93
Epoch 94
Epoch 94, Loss: 0.00000044975705, Improvement: -0.00000000554681, Best Loss: 0.00000026966339 in Epoch 93
Epoch 95
Epoch 95, Loss: 0.00000044329679, Improvement: -0.00000000646026, Best Loss: 0.00000026966339 in Epoch 93
Epoch 96
A best model at epoch 96 has been saved with training error 0.00000026542176.
Epoch 96, Loss: 0.00000044076584, Improvement: -0.00000000253095, Best Loss: 0.00000026542176 in Epoch 96
Epoch 97
Epoch 97, Loss: 0.00000042791452, Improvement: -0.00000001285132, Best Loss: 0.00000026542176 in Epoch 96
Epoch 98
Epoch 98, Loss: 0.00000042464536, Improvement: -0.00000000326916, Best Loss: 0.00000026542176 in Epoch 96
Epoch 99
Epoch 99, Loss: 0.00000042442253, Improvement: -0.00000000022283, Best Loss: 0.00000026542176 in Epoch 96
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00000041895894, Improvement: -0.00000000546358, Best Loss: 0.00000026542176 in Epoch 96
Epoch 101
Epoch 101, Loss: 0.00000041535332, Improvement: -0.00000000360562, Best Loss: 0.00000026542176 in Epoch 96
Epoch 102
Epoch 102, Loss: 0.00000042341134, Improvement: 0.00000000805802, Best Loss: 0.00000026542176 in Epoch 96
Epoch 103
A best model at epoch 103 has been saved with training error 0.00000026186180.
Epoch 103, Loss: 0.00000041919234, Improvement: -0.00000000421901, Best Loss: 0.00000026186180 in Epoch 103
Epoch 104
Epoch 104, Loss: 0.00000042060272, Improvement: 0.00000000141038, Best Loss: 0.00000026186180 in Epoch 103
Epoch 105
Epoch 105, Loss: 0.00000041141880, Improvement: -0.00000000918392, Best Loss: 0.00000026186180 in Epoch 103
Epoch 106
Epoch 106, Loss: 0.00000041775060, Improvement: 0.00000000633180, Best Loss: 0.00000026186180 in Epoch 103
Epoch 107
Epoch 107, Loss: 0.00000047245939, Improvement: 0.00000005470879, Best Loss: 0.00000026186180 in Epoch 103
Epoch 108
Epoch 108, Loss: 0.00000042911637, Improvement: -0.00000004334302, Best Loss: 0.00000026186180 in Epoch 103
Epoch 109
Epoch 109, Loss: 0.00000058625890, Improvement: 0.00000015714253, Best Loss: 0.00000026186180 in Epoch 103
Epoch 110
Epoch 110, Loss: 0.00000075967054, Improvement: 0.00000017341164, Best Loss: 0.00000026186180 in Epoch 103
Epoch 111
Epoch 111, Loss: 0.00000061815127, Improvement: -0.00000014151927, Best Loss: 0.00000026186180 in Epoch 103
Epoch 112
Epoch 112, Loss: 0.00000055447875, Improvement: -0.00000006367252, Best Loss: 0.00000026186180 in Epoch 103
Epoch 113
Epoch 113, Loss: 0.00000048339652, Improvement: -0.00000007108223, Best Loss: 0.00000026186180 in Epoch 103
Epoch 114
Epoch 114, Loss: 0.00000043065539, Improvement: -0.00000005274113, Best Loss: 0.00000026186180 in Epoch 103
Epoch 115
Epoch 115, Loss: 0.00000038917088, Improvement: -0.00000004148451, Best Loss: 0.00000026186180 in Epoch 103
Epoch 116
A best model at epoch 116 has been saved with training error 0.00000025924766.
A best model at epoch 116 has been saved with training error 0.00000025647529.
Epoch 116, Loss: 0.00000038705498, Improvement: -0.00000000211590, Best Loss: 0.00000025647529 in Epoch 116
Epoch 117
Epoch 117, Loss: 0.00000038548985, Improvement: -0.00000000156512, Best Loss: 0.00000025647529 in Epoch 116
Epoch 118
Epoch 118, Loss: 0.00000038581061, Improvement: 0.00000000032076, Best Loss: 0.00000025647529 in Epoch 116
Epoch 119
A best model at epoch 119 has been saved with training error 0.00000023600469.
Epoch 119, Loss: 0.00000038349651, Improvement: -0.00000000231410, Best Loss: 0.00000023600469 in Epoch 119
Epoch 120
A best model at epoch 120 has been saved with training error 0.00000022067624.
Epoch 120, Loss: 0.00000039394231, Improvement: 0.00000001044580, Best Loss: 0.00000022067624 in Epoch 120
Epoch 121
Epoch 121, Loss: 0.00000039608890, Improvement: 0.00000000214659, Best Loss: 0.00000022067624 in Epoch 120
Epoch 122
Epoch 122, Loss: 0.00000045005994, Improvement: 0.00000005397104, Best Loss: 0.00000022067624 in Epoch 120
Epoch 123
Epoch 123, Loss: 0.00000044070390, Improvement: -0.00000000935604, Best Loss: 0.00000022067624 in Epoch 120
Epoch 124
Epoch 124, Loss: 0.00000044164737, Improvement: 0.00000000094347, Best Loss: 0.00000022067624 in Epoch 120
Epoch 125
Epoch 125, Loss: 0.00000040535340, Improvement: -0.00000003629397, Best Loss: 0.00000022067624 in Epoch 120
Epoch 126
Epoch 126, Loss: 0.00000044940377, Improvement: 0.00000004405036, Best Loss: 0.00000022067624 in Epoch 120
Epoch 127
Epoch 127, Loss: 0.00000044232992, Improvement: -0.00000000707384, Best Loss: 0.00000022067624 in Epoch 120
Epoch 128
Epoch 128, Loss: 0.00000057509816, Improvement: 0.00000013276823, Best Loss: 0.00000022067624 in Epoch 120
Epoch 129
Epoch 129, Loss: 0.00000057943026, Improvement: 0.00000000433210, Best Loss: 0.00000022067624 in Epoch 120
Epoch 130
Epoch 130, Loss: 0.00000087470337, Improvement: 0.00000029527311, Best Loss: 0.00000022067624 in Epoch 120
Epoch 131
Epoch 131, Loss: 0.00000142170511, Improvement: 0.00000054700174, Best Loss: 0.00000022067624 in Epoch 120
Epoch 132
Epoch 132, Loss: 0.00000111321251, Improvement: -0.00000030849260, Best Loss: 0.00000022067624 in Epoch 120
Epoch 133
Epoch 133, Loss: 0.00000067968987, Improvement: -0.00000043352264, Best Loss: 0.00000022067624 in Epoch 120
Epoch 134
Epoch 134, Loss: 0.00000043898649, Improvement: -0.00000024070338, Best Loss: 0.00000022067624 in Epoch 120
Epoch 135
Epoch 135, Loss: 0.00000037863486, Improvement: -0.00000006035163, Best Loss: 0.00000022067624 in Epoch 120
Epoch 136
Epoch 136, Loss: 0.00000037104739, Improvement: -0.00000000758748, Best Loss: 0.00000022067624 in Epoch 120
Epoch 137
Epoch 137, Loss: 0.00000036379899, Improvement: -0.00000000724840, Best Loss: 0.00000022067624 in Epoch 120
Epoch 138
Epoch 138, Loss: 0.00000036507573, Improvement: 0.00000000127674, Best Loss: 0.00000022067624 in Epoch 120
Epoch 139
Epoch 139, Loss: 0.00000036204452, Improvement: -0.00000000303121, Best Loss: 0.00000022067624 in Epoch 120
Epoch 140
Epoch 140, Loss: 0.00000036158323, Improvement: -0.00000000046130, Best Loss: 0.00000022067624 in Epoch 120
Epoch 141
Epoch 141, Loss: 0.00000037977324, Improvement: 0.00000001819001, Best Loss: 0.00000022067624 in Epoch 120
Epoch 142
Epoch 142, Loss: 0.00000041206403, Improvement: 0.00000003229079, Best Loss: 0.00000022067624 in Epoch 120
Epoch 143
Epoch 143, Loss: 0.00000046051831, Improvement: 0.00000004845428, Best Loss: 0.00000022067624 in Epoch 120
Epoch 144
Epoch 144, Loss: 0.00000040654740, Improvement: -0.00000005397091, Best Loss: 0.00000022067624 in Epoch 120
Epoch 145
Epoch 145, Loss: 0.00000039894345, Improvement: -0.00000000760395, Best Loss: 0.00000022067624 in Epoch 120
Epoch 146
Epoch 146, Loss: 0.00000040712189, Improvement: 0.00000000817845, Best Loss: 0.00000022067624 in Epoch 120
Epoch 147
A best model at epoch 147 has been saved with training error 0.00000020062403.
Epoch 147, Loss: 0.00000036270929, Improvement: -0.00000004441260, Best Loss: 0.00000020062403 in Epoch 147
Epoch 148
Epoch 148, Loss: 0.00000036976192, Improvement: 0.00000000705263, Best Loss: 0.00000020062403 in Epoch 147
Epoch 149
Epoch 149, Loss: 0.00000038505817, Improvement: 0.00000001529625, Best Loss: 0.00000020062403 in Epoch 147
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00000065516117, Improvement: 0.00000027010299, Best Loss: 0.00000020062403 in Epoch 147
Epoch 151
Epoch 151, Loss: 0.00000106329453, Improvement: 0.00000040813337, Best Loss: 0.00000020062403 in Epoch 147
Epoch 152
Epoch 152, Loss: 0.00000061787260, Improvement: -0.00000044542194, Best Loss: 0.00000020062403 in Epoch 147
Epoch 153
Epoch 153, Loss: 0.00000057343329, Improvement: -0.00000004443931, Best Loss: 0.00000020062403 in Epoch 147
Epoch 154
Epoch 154, Loss: 0.00000040741560, Improvement: -0.00000016601769, Best Loss: 0.00000020062403 in Epoch 147
Epoch 155
Epoch 155, Loss: 0.00000040132621, Improvement: -0.00000000608938, Best Loss: 0.00000020062403 in Epoch 147
Epoch 156
Epoch 156, Loss: 0.00000040960309, Improvement: 0.00000000827688, Best Loss: 0.00000020062403 in Epoch 147
Epoch 157
Epoch 157, Loss: 0.00000037125832, Improvement: -0.00000003834477, Best Loss: 0.00000020062403 in Epoch 147
Epoch 158
Epoch 158, Loss: 0.00000037194211, Improvement: 0.00000000068379, Best Loss: 0.00000020062403 in Epoch 147
Epoch 159
Epoch 159, Loss: 0.00000036538524, Improvement: -0.00000000655687, Best Loss: 0.00000020062403 in Epoch 147
Epoch 160
Epoch 160, Loss: 0.00000035353249, Improvement: -0.00000001185275, Best Loss: 0.00000020062403 in Epoch 147
Epoch 161
Epoch 161, Loss: 0.00000038849073, Improvement: 0.00000003495824, Best Loss: 0.00000020062403 in Epoch 147
Epoch 162
Epoch 162, Loss: 0.00000054248273, Improvement: 0.00000015399200, Best Loss: 0.00000020062403 in Epoch 147
Epoch 163
Epoch 163, Loss: 0.00000081057746, Improvement: 0.00000026809473, Best Loss: 0.00000020062403 in Epoch 147
Epoch 164
Epoch 164, Loss: 0.00000059296516, Improvement: -0.00000021761230, Best Loss: 0.00000020062403 in Epoch 147
Epoch 165
Epoch 165, Loss: 0.00000056931881, Improvement: -0.00000002364635, Best Loss: 0.00000020062403 in Epoch 147
Epoch 166
Epoch 166, Loss: 0.00000037862525, Improvement: -0.00000019069356, Best Loss: 0.00000020062403 in Epoch 147
Epoch 167
Epoch 167, Loss: 0.00000035553342, Improvement: -0.00000002309183, Best Loss: 0.00000020062403 in Epoch 147
Epoch 168
Epoch 168, Loss: 0.00000034612083, Improvement: -0.00000000941259, Best Loss: 0.00000020062403 in Epoch 147
Epoch 169
Epoch 169, Loss: 0.00000034654847, Improvement: 0.00000000042765, Best Loss: 0.00000020062403 in Epoch 147
Epoch 170
Epoch 170, Loss: 0.00000034353361, Improvement: -0.00000000301486, Best Loss: 0.00000020062403 in Epoch 147
Epoch 171
Epoch 171, Loss: 0.00000039035784, Improvement: 0.00000004682423, Best Loss: 0.00000020062403 in Epoch 147
Epoch 172
Epoch 172, Loss: 0.00000078768711, Improvement: 0.00000039732927, Best Loss: 0.00000020062403 in Epoch 147
Epoch 173
Epoch 173, Loss: 0.00000196488187, Improvement: 0.00000117719476, Best Loss: 0.00000020062403 in Epoch 147
Epoch 174
Epoch 174, Loss: 0.00000121895389, Improvement: -0.00000074592798, Best Loss: 0.00000020062403 in Epoch 147
Epoch 175
Epoch 175, Loss: 0.00000060578156, Improvement: -0.00000061317233, Best Loss: 0.00000020062403 in Epoch 147
Epoch 176
Epoch 176, Loss: 0.00000055675566, Improvement: -0.00000004902590, Best Loss: 0.00000020062403 in Epoch 147
Epoch 177
Epoch 177, Loss: 0.00000038747879, Improvement: -0.00000016927687, Best Loss: 0.00000020062403 in Epoch 147
Epoch 178
Epoch 178, Loss: 0.00000035439514, Improvement: -0.00000003308365, Best Loss: 0.00000020062403 in Epoch 147
Epoch 179
Epoch 179, Loss: 0.00000034108795, Improvement: -0.00000001330719, Best Loss: 0.00000020062403 in Epoch 147
Epoch 180
A best model at epoch 180 has been saved with training error 0.00000016066946.
Epoch 180, Loss: 0.00000034404046, Improvement: 0.00000000295251, Best Loss: 0.00000016066946 in Epoch 180
Epoch 181
Epoch 181, Loss: 0.00000033644305, Improvement: -0.00000000759740, Best Loss: 0.00000016066946 in Epoch 180
Epoch 182
Epoch 182, Loss: 0.00000033426693, Improvement: -0.00000000217612, Best Loss: 0.00000016066946 in Epoch 180
Epoch 183
Epoch 183, Loss: 0.00000033430204, Improvement: 0.00000000003512, Best Loss: 0.00000016066946 in Epoch 180
Epoch 184
Epoch 184, Loss: 0.00000033379260, Improvement: -0.00000000050944, Best Loss: 0.00000016066946 in Epoch 180
Epoch 185
Epoch 185, Loss: 0.00000033279822, Improvement: -0.00000000099438, Best Loss: 0.00000016066946 in Epoch 180
Epoch 186
Epoch 186, Loss: 0.00000033230579, Improvement: -0.00000000049243, Best Loss: 0.00000016066946 in Epoch 180
Epoch 187
Epoch 187, Loss: 0.00000033260646, Improvement: 0.00000000030067, Best Loss: 0.00000016066946 in Epoch 180
Epoch 188
Epoch 188, Loss: 0.00000033653040, Improvement: 0.00000000392394, Best Loss: 0.00000016066946 in Epoch 180
Epoch 189
Epoch 189, Loss: 0.00000033530644, Improvement: -0.00000000122396, Best Loss: 0.00000016066946 in Epoch 180
Epoch 190
Epoch 190, Loss: 0.00000033660655, Improvement: 0.00000000130011, Best Loss: 0.00000016066946 in Epoch 180
Epoch 191
Epoch 191, Loss: 0.00000033787066, Improvement: 0.00000000126412, Best Loss: 0.00000016066946 in Epoch 180
Epoch 192
Epoch 192, Loss: 0.00000034470006, Improvement: 0.00000000682940, Best Loss: 0.00000016066946 in Epoch 180
Epoch 193
Epoch 193, Loss: 0.00000033645760, Improvement: -0.00000000824246, Best Loss: 0.00000016066946 in Epoch 180
Epoch 194
Epoch 194, Loss: 0.00000033355167, Improvement: -0.00000000290593, Best Loss: 0.00000016066946 in Epoch 180
Epoch 195
Epoch 195, Loss: 0.00000033744923, Improvement: 0.00000000389756, Best Loss: 0.00000016066946 in Epoch 180
Epoch 196
Epoch 196, Loss: 0.00000033564998, Improvement: -0.00000000179925, Best Loss: 0.00000016066946 in Epoch 180
Epoch 197
Epoch 197, Loss: 0.00000036230126, Improvement: 0.00000002665129, Best Loss: 0.00000016066946 in Epoch 180
Epoch 198
Epoch 198, Loss: 0.00000039821277, Improvement: 0.00000003591150, Best Loss: 0.00000016066946 in Epoch 180
Epoch 199
Epoch 199, Loss: 0.00000035480537, Improvement: -0.00000004340740, Best Loss: 0.00000016066946 in Epoch 180
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00000041275989, Improvement: 0.00000005795453, Best Loss: 0.00000016066946 in Epoch 180
Epoch 201
Epoch 201, Loss: 0.00000057432208, Improvement: 0.00000016156219, Best Loss: 0.00000016066946 in Epoch 180
Epoch 202
Epoch 202, Loss: 0.00000045910093, Improvement: -0.00000011522115, Best Loss: 0.00000016066946 in Epoch 180
Epoch 203
Epoch 203, Loss: 0.00000089321847, Improvement: 0.00000043411754, Best Loss: 0.00000016066946 in Epoch 180
Epoch 204
Epoch 204, Loss: 0.00000100081644, Improvement: 0.00000010759797, Best Loss: 0.00000016066946 in Epoch 180
Epoch 205
Epoch 205, Loss: 0.00000044208462, Improvement: -0.00000055873182, Best Loss: 0.00000016066946 in Epoch 180
Epoch 206
Epoch 206, Loss: 0.00000033011496, Improvement: -0.00000011196967, Best Loss: 0.00000016066946 in Epoch 180
Epoch 207
Epoch 207, Loss: 0.00000032770251, Improvement: -0.00000000241244, Best Loss: 0.00000016066946 in Epoch 180
Epoch 208
Epoch 208, Loss: 0.00000032932601, Improvement: 0.00000000162350, Best Loss: 0.00000016066946 in Epoch 180
Epoch 209
Epoch 209, Loss: 0.00000034442356, Improvement: 0.00000001509755, Best Loss: 0.00000016066946 in Epoch 180
Epoch 210
Epoch 210, Loss: 0.00000034677275, Improvement: 0.00000000234919, Best Loss: 0.00000016066946 in Epoch 180
Epoch 211
Epoch 211, Loss: 0.00000038916534, Improvement: 0.00000004239259, Best Loss: 0.00000016066946 in Epoch 180
Epoch 212
Epoch 212, Loss: 0.00000039474468, Improvement: 0.00000000557934, Best Loss: 0.00000016066946 in Epoch 180
Epoch 213
Epoch 213, Loss: 0.00000040023324, Improvement: 0.00000000548856, Best Loss: 0.00000016066946 in Epoch 180
Epoch 214
Epoch 214, Loss: 0.00000055088756, Improvement: 0.00000015065432, Best Loss: 0.00000016066946 in Epoch 180
Epoch 215
Epoch 215, Loss: 0.00000079325090, Improvement: 0.00000024236334, Best Loss: 0.00000016066946 in Epoch 180
Epoch 216
Epoch 216, Loss: 0.00000093233094, Improvement: 0.00000013908004, Best Loss: 0.00000016066946 in Epoch 180
Epoch 217
Epoch 217, Loss: 0.00000251150994, Improvement: 0.00000157917901, Best Loss: 0.00000016066946 in Epoch 180
Epoch 218
Epoch 218, Loss: 0.00000071586937, Improvement: -0.00000179564058, Best Loss: 0.00000016066946 in Epoch 180
Epoch 219
Epoch 219, Loss: 0.00000036085161, Improvement: -0.00000035501776, Best Loss: 0.00000016066946 in Epoch 180
Epoch 220
Epoch 220, Loss: 0.00000033017227, Improvement: -0.00000003067934, Best Loss: 0.00000016066946 in Epoch 180
Epoch 221
Epoch 221, Loss: 0.00000032126110, Improvement: -0.00000000891117, Best Loss: 0.00000016066946 in Epoch 180
Epoch 222
Epoch 222, Loss: 0.00000031893175, Improvement: -0.00000000232935, Best Loss: 0.00000016066946 in Epoch 180
Epoch 223
Epoch 223, Loss: 0.00000031947026, Improvement: 0.00000000053851, Best Loss: 0.00000016066946 in Epoch 180
Epoch 224
Epoch 224, Loss: 0.00000031975520, Improvement: 0.00000000028494, Best Loss: 0.00000016066946 in Epoch 180
Epoch 225
Epoch 225, Loss: 0.00000031751291, Improvement: -0.00000000224229, Best Loss: 0.00000016066946 in Epoch 180
Epoch 226
A best model at epoch 226 has been saved with training error 0.00000015167652.
Epoch 226, Loss: 0.00000031664909, Improvement: -0.00000000086382, Best Loss: 0.00000015167652 in Epoch 226
Epoch 227
Epoch 227, Loss: 0.00000031663803, Improvement: -0.00000000001106, Best Loss: 0.00000015167652 in Epoch 226
Epoch 228
Epoch 228, Loss: 0.00000031649658, Improvement: -0.00000000014145, Best Loss: 0.00000015167652 in Epoch 226
Epoch 229
Epoch 229, Loss: 0.00000031567515, Improvement: -0.00000000082143, Best Loss: 0.00000015167652 in Epoch 226
Epoch 230
Epoch 230, Loss: 0.00000031557443, Improvement: -0.00000000010072, Best Loss: 0.00000015167652 in Epoch 226
Epoch 231
Epoch 231, Loss: 0.00000031540932, Improvement: -0.00000000016512, Best Loss: 0.00000015167652 in Epoch 226
Epoch 232
Epoch 232, Loss: 0.00000031495728, Improvement: -0.00000000045204, Best Loss: 0.00000015167652 in Epoch 226
Epoch 233
Epoch 233, Loss: 0.00000031461599, Improvement: -0.00000000034129, Best Loss: 0.00000015167652 in Epoch 226
Epoch 234
Epoch 234, Loss: 0.00000031391266, Improvement: -0.00000000070333, Best Loss: 0.00000015167652 in Epoch 226
Epoch 235
Epoch 235, Loss: 0.00000031345668, Improvement: -0.00000000045598, Best Loss: 0.00000015167652 in Epoch 226
Epoch 236
Epoch 236, Loss: 0.00000031368265, Improvement: 0.00000000022597, Best Loss: 0.00000015167652 in Epoch 226
Epoch 237
Epoch 237, Loss: 0.00000031327256, Improvement: -0.00000000041010, Best Loss: 0.00000015167652 in Epoch 226
Epoch 238
Epoch 238, Loss: 0.00000031291124, Improvement: -0.00000000036132, Best Loss: 0.00000015167652 in Epoch 226
Epoch 239
Epoch 239, Loss: 0.00000031355661, Improvement: 0.00000000064537, Best Loss: 0.00000015167652 in Epoch 226
Epoch 240
Epoch 240, Loss: 0.00000031216928, Improvement: -0.00000000138733, Best Loss: 0.00000015167652 in Epoch 226
Epoch 241
Epoch 241, Loss: 0.00000031201142, Improvement: -0.00000000015787, Best Loss: 0.00000015167652 in Epoch 226
Epoch 242
Epoch 242, Loss: 0.00000031529785, Improvement: 0.00000000328643, Best Loss: 0.00000015167652 in Epoch 226
Epoch 243
Epoch 243, Loss: 0.00000031251897, Improvement: -0.00000000277888, Best Loss: 0.00000015167652 in Epoch 226
Epoch 244
Epoch 244, Loss: 0.00000031225161, Improvement: -0.00000000026736, Best Loss: 0.00000015167652 in Epoch 226
Epoch 245
Epoch 245, Loss: 0.00000031180213, Improvement: -0.00000000044948, Best Loss: 0.00000015167652 in Epoch 226
Epoch 246
Epoch 246, Loss: 0.00000031259566, Improvement: 0.00000000079353, Best Loss: 0.00000015167652 in Epoch 226
Epoch 247
Epoch 247, Loss: 0.00000031296697, Improvement: 0.00000000037131, Best Loss: 0.00000015167652 in Epoch 226
Epoch 248
Epoch 248, Loss: 0.00000031038616, Improvement: -0.00000000258081, Best Loss: 0.00000015167652 in Epoch 226
Epoch 249
Epoch 249, Loss: 0.00000031013255, Improvement: -0.00000000025361, Best Loss: 0.00000015167652 in Epoch 226
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000030876602, Improvement: -0.00000000136652, Best Loss: 0.00000015167652 in Epoch 226
Epoch 251
Epoch 251, Loss: 0.00000030941400, Improvement: 0.00000000064797, Best Loss: 0.00000015167652 in Epoch 226
Epoch 252
Epoch 252, Loss: 0.00000030999835, Improvement: 0.00000000058435, Best Loss: 0.00000015167652 in Epoch 226
Epoch 253
Epoch 253, Loss: 0.00000030960700, Improvement: -0.00000000039135, Best Loss: 0.00000015167652 in Epoch 226
Epoch 254
Epoch 254, Loss: 0.00000030810046, Improvement: -0.00000000150654, Best Loss: 0.00000015167652 in Epoch 226
Epoch 255
Epoch 255, Loss: 0.00000031302109, Improvement: 0.00000000492064, Best Loss: 0.00000015167652 in Epoch 226
Epoch 256
Epoch 256, Loss: 0.00000034375576, Improvement: 0.00000003073466, Best Loss: 0.00000015167652 in Epoch 226
Epoch 257
Epoch 257, Loss: 0.00000039616992, Improvement: 0.00000005241417, Best Loss: 0.00000015167652 in Epoch 226
Epoch 258
Epoch 258, Loss: 0.00000102622123, Improvement: 0.00000063005131, Best Loss: 0.00000015167652 in Epoch 226
Epoch 259
Epoch 259, Loss: 0.00000059996970, Improvement: -0.00000042625153, Best Loss: 0.00000015167652 in Epoch 226
Epoch 260
Epoch 260, Loss: 0.00000067250545, Improvement: 0.00000007253575, Best Loss: 0.00000015167652 in Epoch 226
Epoch 261
Epoch 261, Loss: 0.00000049408360, Improvement: -0.00000017842185, Best Loss: 0.00000015167652 in Epoch 226
Epoch 262
Epoch 262, Loss: 0.00000041251427, Improvement: -0.00000008156933, Best Loss: 0.00000015167652 in Epoch 226
Epoch 263
Epoch 263, Loss: 0.00000044793751, Improvement: 0.00000003542324, Best Loss: 0.00000015167652 in Epoch 226
Epoch 264
Epoch 264, Loss: 0.00000039000991, Improvement: -0.00000005792760, Best Loss: 0.00000015167652 in Epoch 226
Epoch 265
Epoch 265, Loss: 0.00000033300201, Improvement: -0.00000005700789, Best Loss: 0.00000015167652 in Epoch 226
Epoch 266
Epoch 266, Loss: 0.00000031398147, Improvement: -0.00000001902054, Best Loss: 0.00000015167652 in Epoch 226
Epoch 267
Epoch 267, Loss: 0.00000031654385, Improvement: 0.00000000256238, Best Loss: 0.00000015167652 in Epoch 226
Epoch 268
Epoch 268, Loss: 0.00000031029198, Improvement: -0.00000000625186, Best Loss: 0.00000015167652 in Epoch 226
Epoch 269
Epoch 269, Loss: 0.00000030713696, Improvement: -0.00000000315503, Best Loss: 0.00000015167652 in Epoch 226
Epoch 270
Epoch 270, Loss: 0.00000033683335, Improvement: 0.00000002969639, Best Loss: 0.00000015167652 in Epoch 226
Epoch 271
Epoch 271, Loss: 0.00000035135995, Improvement: 0.00000001452660, Best Loss: 0.00000015167652 in Epoch 226
Epoch 272
Epoch 272, Loss: 0.00000031681842, Improvement: -0.00000003454154, Best Loss: 0.00000015167652 in Epoch 226
Epoch 273
Epoch 273, Loss: 0.00000036664727, Improvement: 0.00000004982885, Best Loss: 0.00000015167652 in Epoch 226
Epoch 274
Epoch 274, Loss: 0.00000073360789, Improvement: 0.00000036696062, Best Loss: 0.00000015167652 in Epoch 226
Epoch 275
Epoch 275, Loss: 0.00000065832700, Improvement: -0.00000007528088, Best Loss: 0.00000015167652 in Epoch 226
Epoch 276
Epoch 276, Loss: 0.00000074087784, Improvement: 0.00000008255084, Best Loss: 0.00000015167652 in Epoch 226
Epoch 277
Epoch 277, Loss: 0.00000093347500, Improvement: 0.00000019259716, Best Loss: 0.00000015167652 in Epoch 226
Epoch 278
Epoch 278, Loss: 0.00000056019013, Improvement: -0.00000037328487, Best Loss: 0.00000015167652 in Epoch 226
Epoch 279
Epoch 279, Loss: 0.00000043904752, Improvement: -0.00000012114261, Best Loss: 0.00000015167652 in Epoch 226
Epoch 280
Epoch 280, Loss: 0.00000032448395, Improvement: -0.00000011456357, Best Loss: 0.00000015167652 in Epoch 226
Epoch 281
Epoch 281, Loss: 0.00000033144545, Improvement: 0.00000000696150, Best Loss: 0.00000015167652 in Epoch 226
Epoch 282
Epoch 282, Loss: 0.00000032368728, Improvement: -0.00000000775817, Best Loss: 0.00000015167652 in Epoch 226
Epoch 283
Epoch 283, Loss: 0.00000031814230, Improvement: -0.00000000554498, Best Loss: 0.00000015167652 in Epoch 226
Epoch 284
Epoch 284, Loss: 0.00000030504302, Improvement: -0.00000001309927, Best Loss: 0.00000015167652 in Epoch 226
Epoch 285
Epoch 285, Loss: 0.00000029951610, Improvement: -0.00000000552692, Best Loss: 0.00000015167652 in Epoch 226
Epoch 286
Epoch 286, Loss: 0.00000029904646, Improvement: -0.00000000046965, Best Loss: 0.00000015167652 in Epoch 226
Epoch 287
Epoch 287, Loss: 0.00000029901656, Improvement: -0.00000000002989, Best Loss: 0.00000015167652 in Epoch 226
Epoch 288
Epoch 288, Loss: 0.00000030619397, Improvement: 0.00000000717741, Best Loss: 0.00000015167652 in Epoch 226
Epoch 289
Epoch 289, Loss: 0.00000030390821, Improvement: -0.00000000228576, Best Loss: 0.00000015167652 in Epoch 226
Epoch 290
Epoch 290, Loss: 0.00000030886348, Improvement: 0.00000000495526, Best Loss: 0.00000015167652 in Epoch 226
Epoch 291
Epoch 291, Loss: 0.00000030498748, Improvement: -0.00000000387599, Best Loss: 0.00000015167652 in Epoch 226
Epoch 292
Epoch 292, Loss: 0.00000029777088, Improvement: -0.00000000721660, Best Loss: 0.00000015167652 in Epoch 226
Epoch 293
Epoch 293, Loss: 0.00000029914676, Improvement: 0.00000000137587, Best Loss: 0.00000015167652 in Epoch 226
Epoch 294
Epoch 294, Loss: 0.00000029630390, Improvement: -0.00000000284286, Best Loss: 0.00000015167652 in Epoch 226
Epoch 295
Epoch 295, Loss: 0.00000029651061, Improvement: 0.00000000020671, Best Loss: 0.00000015167652 in Epoch 226
Epoch 296
Epoch 296, Loss: 0.00000030047334, Improvement: 0.00000000396272, Best Loss: 0.00000015167652 in Epoch 226
Epoch 297
Epoch 297, Loss: 0.00000030867125, Improvement: 0.00000000819791, Best Loss: 0.00000015167652 in Epoch 226
Epoch 298
Epoch 298, Loss: 0.00000034470178, Improvement: 0.00000003603053, Best Loss: 0.00000015167652 in Epoch 226
Epoch 299
Epoch 299, Loss: 0.00000070037026, Improvement: 0.00000035566848, Best Loss: 0.00000015167652 in Epoch 226
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000193532758, Improvement: 0.00000123495732, Best Loss: 0.00000015167652 in Epoch 226
Epoch 301
Epoch 301, Loss: 0.00000139213914, Improvement: -0.00000054318844, Best Loss: 0.00000015167652 in Epoch 226
Epoch 302
Epoch 302, Loss: 0.00000052188627, Improvement: -0.00000087025286, Best Loss: 0.00000015167652 in Epoch 226
Epoch 303
Epoch 303, Loss: 0.00000035426586, Improvement: -0.00000016762041, Best Loss: 0.00000015167652 in Epoch 226
Epoch 304
Epoch 304, Loss: 0.00000032096637, Improvement: -0.00000003329950, Best Loss: 0.00000015167652 in Epoch 226
Epoch 305
Epoch 305, Loss: 0.00000029825092, Improvement: -0.00000002271545, Best Loss: 0.00000015167652 in Epoch 226
Epoch 306
Epoch 306, Loss: 0.00000029259841, Improvement: -0.00000000565250, Best Loss: 0.00000015167652 in Epoch 226
Epoch 307
Epoch 307, Loss: 0.00000029388255, Improvement: 0.00000000128414, Best Loss: 0.00000015167652 in Epoch 226
Epoch 308
Epoch 308, Loss: 0.00000029486672, Improvement: 0.00000000098417, Best Loss: 0.00000015167652 in Epoch 226
Epoch 309
Epoch 309, Loss: 0.00000029202396, Improvement: -0.00000000284276, Best Loss: 0.00000015167652 in Epoch 226
Epoch 310
Epoch 310, Loss: 0.00000029081089, Improvement: -0.00000000121308, Best Loss: 0.00000015167652 in Epoch 226
Epoch 311
Epoch 311, Loss: 0.00000029079849, Improvement: -0.00000000001240, Best Loss: 0.00000015167652 in Epoch 226
Epoch 312
Epoch 312, Loss: 0.00000029052129, Improvement: -0.00000000027720, Best Loss: 0.00000015167652 in Epoch 226
Epoch 313
Epoch 313, Loss: 0.00000029070432, Improvement: 0.00000000018304, Best Loss: 0.00000015167652 in Epoch 226
Epoch 314
Epoch 314, Loss: 0.00000029292001, Improvement: 0.00000000221569, Best Loss: 0.00000015167652 in Epoch 226
Epoch 315
Epoch 315, Loss: 0.00000029054299, Improvement: -0.00000000237703, Best Loss: 0.00000015167652 in Epoch 226
Epoch 316
Epoch 316, Loss: 0.00000028953575, Improvement: -0.00000000100723, Best Loss: 0.00000015167652 in Epoch 226
Epoch 317
Epoch 317, Loss: 0.00000028958501, Improvement: 0.00000000004926, Best Loss: 0.00000015167652 in Epoch 226
Epoch 318
Epoch 318, Loss: 0.00000029012140, Improvement: 0.00000000053639, Best Loss: 0.00000015167652 in Epoch 226
Epoch 319
Epoch 319, Loss: 0.00000028934792, Improvement: -0.00000000077348, Best Loss: 0.00000015167652 in Epoch 226
Epoch 320
Epoch 320, Loss: 0.00000028862926, Improvement: -0.00000000071867, Best Loss: 0.00000015167652 in Epoch 226
Epoch 321
Epoch 321, Loss: 0.00000028862886, Improvement: -0.00000000000039, Best Loss: 0.00000015167652 in Epoch 226
Epoch 322
Epoch 322, Loss: 0.00000028906442, Improvement: 0.00000000043556, Best Loss: 0.00000015167652 in Epoch 226
Epoch 323
Epoch 323, Loss: 0.00000028991483, Improvement: 0.00000000085041, Best Loss: 0.00000015167652 in Epoch 226
Epoch 324
Epoch 324, Loss: 0.00000028797912, Improvement: -0.00000000193571, Best Loss: 0.00000015167652 in Epoch 226
Epoch 325
Epoch 325, Loss: 0.00000028851441, Improvement: 0.00000000053529, Best Loss: 0.00000015167652 in Epoch 226
Epoch 326
Epoch 326, Loss: 0.00000029169295, Improvement: 0.00000000317854, Best Loss: 0.00000015167652 in Epoch 226
Epoch 327
Epoch 327, Loss: 0.00000029305599, Improvement: 0.00000000136304, Best Loss: 0.00000015167652 in Epoch 226
Epoch 328
Epoch 328, Loss: 0.00000029657021, Improvement: 0.00000000351422, Best Loss: 0.00000015167652 in Epoch 226
Epoch 329
Epoch 329, Loss: 0.00000031884421, Improvement: 0.00000002227400, Best Loss: 0.00000015167652 in Epoch 226
Epoch 330
Epoch 330, Loss: 0.00000030722480, Improvement: -0.00000001161941, Best Loss: 0.00000015167652 in Epoch 226
Epoch 331
Epoch 331, Loss: 0.00000030433684, Improvement: -0.00000000288796, Best Loss: 0.00000015167652 in Epoch 226
Epoch 332
Epoch 332, Loss: 0.00000032208886, Improvement: 0.00000001775202, Best Loss: 0.00000015167652 in Epoch 226
Epoch 333
Epoch 333, Loss: 0.00000034535313, Improvement: 0.00000002326427, Best Loss: 0.00000015167652 in Epoch 226
Epoch 334
Epoch 334, Loss: 0.00000061314017, Improvement: 0.00000026778704, Best Loss: 0.00000015167652 in Epoch 226
Epoch 335
Epoch 335, Loss: 0.00000083285281, Improvement: 0.00000021971264, Best Loss: 0.00000015167652 in Epoch 226
Epoch 336
Epoch 336, Loss: 0.00000058066127, Improvement: -0.00000025219154, Best Loss: 0.00000015167652 in Epoch 226
Epoch 337
Epoch 337, Loss: 0.00000032926395, Improvement: -0.00000025139732, Best Loss: 0.00000015167652 in Epoch 226
Epoch 338
Epoch 338, Loss: 0.00000030294605, Improvement: -0.00000002631790, Best Loss: 0.00000015167652 in Epoch 226
Epoch 339
Epoch 339, Loss: 0.00000032037986, Improvement: 0.00000001743381, Best Loss: 0.00000015167652 in Epoch 226
Epoch 340
Epoch 340, Loss: 0.00000029935216, Improvement: -0.00000002102770, Best Loss: 0.00000015167652 in Epoch 226
Epoch 341
Epoch 341, Loss: 0.00000030624356, Improvement: 0.00000000689140, Best Loss: 0.00000015167652 in Epoch 226
Epoch 342
Epoch 342, Loss: 0.00000032189872, Improvement: 0.00000001565516, Best Loss: 0.00000015167652 in Epoch 226
Epoch 343
Epoch 343, Loss: 0.00000053558560, Improvement: 0.00000021368688, Best Loss: 0.00000015167652 in Epoch 226
Epoch 344
Epoch 344, Loss: 0.00000072479433, Improvement: 0.00000018920873, Best Loss: 0.00000015167652 in Epoch 226
Epoch 345
Epoch 345, Loss: 0.00000050971951, Improvement: -0.00000021507482, Best Loss: 0.00000015167652 in Epoch 226
Epoch 346
Epoch 346, Loss: 0.00000055214846, Improvement: 0.00000004242894, Best Loss: 0.00000015167652 in Epoch 226
Epoch 347
Epoch 347, Loss: 0.00000049588542, Improvement: -0.00000005626304, Best Loss: 0.00000015167652 in Epoch 226
Epoch 348
Epoch 348, Loss: 0.00000046688926, Improvement: -0.00000002899615, Best Loss: 0.00000015167652 in Epoch 226
Epoch 349
Epoch 349, Loss: 0.00000035206120, Improvement: -0.00000011482806, Best Loss: 0.00000015167652 in Epoch 226
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000039594738, Improvement: 0.00000004388618, Best Loss: 0.00000015167652 in Epoch 226
Epoch 351
Epoch 351, Loss: 0.00000032661266, Improvement: -0.00000006933472, Best Loss: 0.00000015167652 in Epoch 226
Epoch 352
Epoch 352, Loss: 0.00000031819972, Improvement: -0.00000000841294, Best Loss: 0.00000015167652 in Epoch 226
Epoch 353
Epoch 353, Loss: 0.00000032374478, Improvement: 0.00000000554507, Best Loss: 0.00000015167652 in Epoch 226
Epoch 354
Epoch 354, Loss: 0.00000030323729, Improvement: -0.00000002050749, Best Loss: 0.00000015167652 in Epoch 226
Epoch 355
Epoch 355, Loss: 0.00000031208574, Improvement: 0.00000000884845, Best Loss: 0.00000015167652 in Epoch 226
Epoch 356
Epoch 356, Loss: 0.00000028898045, Improvement: -0.00000002310529, Best Loss: 0.00000015167652 in Epoch 226
Epoch 357
Epoch 357, Loss: 0.00000028467318, Improvement: -0.00000000430727, Best Loss: 0.00000015167652 in Epoch 226
Epoch 358
Epoch 358, Loss: 0.00000028096673, Improvement: -0.00000000370644, Best Loss: 0.00000015167652 in Epoch 226
Epoch 359
Epoch 359, Loss: 0.00000028017809, Improvement: -0.00000000078864, Best Loss: 0.00000015167652 in Epoch 226
Epoch 360
Epoch 360, Loss: 0.00000028320224, Improvement: 0.00000000302415, Best Loss: 0.00000015167652 in Epoch 226
Epoch 361
Epoch 361, Loss: 0.00000028306524, Improvement: -0.00000000013700, Best Loss: 0.00000015167652 in Epoch 226
Epoch 362
Epoch 362, Loss: 0.00000028036471, Improvement: -0.00000000270053, Best Loss: 0.00000015167652 in Epoch 226
Epoch 363
Epoch 363, Loss: 0.00000027858903, Improvement: -0.00000000177568, Best Loss: 0.00000015167652 in Epoch 226
Epoch 364
Epoch 364, Loss: 0.00000027582030, Improvement: -0.00000000276873, Best Loss: 0.00000015167652 in Epoch 226
Epoch 365
Epoch 365, Loss: 0.00000028128412, Improvement: 0.00000000546382, Best Loss: 0.00000015167652 in Epoch 226
Epoch 366
Epoch 366, Loss: 0.00000033254563, Improvement: 0.00000005126151, Best Loss: 0.00000015167652 in Epoch 226
Epoch 367
Epoch 367, Loss: 0.00000043857595, Improvement: 0.00000010603032, Best Loss: 0.00000015167652 in Epoch 226
Epoch 368
Epoch 368, Loss: 0.00000033002190, Improvement: -0.00000010855405, Best Loss: 0.00000015167652 in Epoch 226
Epoch 369
Epoch 369, Loss: 0.00000068719791, Improvement: 0.00000035717601, Best Loss: 0.00000015167652 in Epoch 226
Epoch 370
Epoch 370, Loss: 0.00000062310784, Improvement: -0.00000006409008, Best Loss: 0.00000015167652 in Epoch 226
Epoch 371
Epoch 371, Loss: 0.00000042838775, Improvement: -0.00000019472009, Best Loss: 0.00000015167652 in Epoch 226
Epoch 372
Epoch 372, Loss: 0.00000038414761, Improvement: -0.00000004424014, Best Loss: 0.00000015167652 in Epoch 226
Epoch 373
Epoch 373, Loss: 0.00000034386961, Improvement: -0.00000004027800, Best Loss: 0.00000015167652 in Epoch 226
Epoch 374
Epoch 374, Loss: 0.00000036295066, Improvement: 0.00000001908105, Best Loss: 0.00000015167652 in Epoch 226
Epoch 375
Epoch 375, Loss: 0.00000040116221, Improvement: 0.00000003821155, Best Loss: 0.00000015167652 in Epoch 226
Epoch 376
Epoch 376, Loss: 0.00000048533573, Improvement: 0.00000008417352, Best Loss: 0.00000015167652 in Epoch 226
Epoch 377
Epoch 377, Loss: 0.00000062515927, Improvement: 0.00000013982353, Best Loss: 0.00000015167652 in Epoch 226
Epoch 378
Epoch 378, Loss: 0.00000037179519, Improvement: -0.00000025336408, Best Loss: 0.00000015167652 in Epoch 226
Epoch 379
Epoch 379, Loss: 0.00000037157735, Improvement: -0.00000000021784, Best Loss: 0.00000015167652 in Epoch 226
Epoch 380
Epoch 380, Loss: 0.00000036622973, Improvement: -0.00000000534762, Best Loss: 0.00000015167652 in Epoch 226
Epoch 381
Epoch 381, Loss: 0.00000044758929, Improvement: 0.00000008135956, Best Loss: 0.00000015167652 in Epoch 226
Epoch 382
Epoch 382, Loss: 0.00000045285122, Improvement: 0.00000000526194, Best Loss: 0.00000015167652 in Epoch 226
Epoch 383
Epoch 383, Loss: 0.00000040685132, Improvement: -0.00000004599990, Best Loss: 0.00000015167652 in Epoch 226
Epoch 384
Epoch 384, Loss: 0.00000042065996, Improvement: 0.00000001380864, Best Loss: 0.00000015167652 in Epoch 226
Epoch 385
Epoch 385, Loss: 0.00000068090848, Improvement: 0.00000026024851, Best Loss: 0.00000015167652 in Epoch 226
Epoch 386
Epoch 386, Loss: 0.00000073205484, Improvement: 0.00000005114636, Best Loss: 0.00000015167652 in Epoch 226
Epoch 387
Epoch 387, Loss: 0.00000043303069, Improvement: -0.00000029902415, Best Loss: 0.00000015167652 in Epoch 226
Epoch 388
Epoch 388, Loss: 0.00000029247787, Improvement: -0.00000014055282, Best Loss: 0.00000015167652 in Epoch 226
Epoch 389
Epoch 389, Loss: 0.00000028938707, Improvement: -0.00000000309080, Best Loss: 0.00000015167652 in Epoch 226
Epoch 390
Epoch 390, Loss: 0.00000029041466, Improvement: 0.00000000102759, Best Loss: 0.00000015167652 in Epoch 226
Epoch 391
Epoch 391, Loss: 0.00000028133958, Improvement: -0.00000000907508, Best Loss: 0.00000015167652 in Epoch 226
Epoch 392
Epoch 392, Loss: 0.00000027778056, Improvement: -0.00000000355903, Best Loss: 0.00000015167652 in Epoch 226
Epoch 393
Epoch 393, Loss: 0.00000029424870, Improvement: 0.00000001646814, Best Loss: 0.00000015167652 in Epoch 226
Epoch 394
Epoch 394, Loss: 0.00000028939399, Improvement: -0.00000000485471, Best Loss: 0.00000015167652 in Epoch 226
Epoch 395
Epoch 395, Loss: 0.00000031240665, Improvement: 0.00000002301266, Best Loss: 0.00000015167652 in Epoch 226
Epoch 396
Epoch 396, Loss: 0.00000029683080, Improvement: -0.00000001557585, Best Loss: 0.00000015167652 in Epoch 226
Epoch 397
Epoch 397, Loss: 0.00000030850432, Improvement: 0.00000001167352, Best Loss: 0.00000015167652 in Epoch 226
Epoch 398
Epoch 398, Loss: 0.00000029214238, Improvement: -0.00000001636194, Best Loss: 0.00000015167652 in Epoch 226
Epoch 399
Epoch 399, Loss: 0.00000029762941, Improvement: 0.00000000548703, Best Loss: 0.00000015167652 in Epoch 226
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000031139720, Improvement: 0.00000001376778, Best Loss: 0.00000015167652 in Epoch 226
Epoch 401
Epoch 401, Loss: 0.00000032695359, Improvement: 0.00000001555639, Best Loss: 0.00000015167652 in Epoch 226
Epoch 402
Epoch 402, Loss: 0.00000045075541, Improvement: 0.00000012380181, Best Loss: 0.00000015167652 in Epoch 226
Epoch 403
Epoch 403, Loss: 0.00000071344283, Improvement: 0.00000026268742, Best Loss: 0.00000015167652 in Epoch 226
Epoch 404
Epoch 404, Loss: 0.00000039035892, Improvement: -0.00000032308390, Best Loss: 0.00000015167652 in Epoch 226
Epoch 405
Epoch 405, Loss: 0.00000055041438, Improvement: 0.00000016005546, Best Loss: 0.00000015167652 in Epoch 226
Epoch 406
Epoch 406, Loss: 0.00000078844988, Improvement: 0.00000023803550, Best Loss: 0.00000015167652 in Epoch 226
Epoch 407
Epoch 407, Loss: 0.00000044913462, Improvement: -0.00000033931526, Best Loss: 0.00000015167652 in Epoch 226
Epoch 408
Epoch 408, Loss: 0.00000031702990, Improvement: -0.00000013210472, Best Loss: 0.00000015167652 in Epoch 226
Epoch 409
Epoch 409, Loss: 0.00000027823145, Improvement: -0.00000003879844, Best Loss: 0.00000015167652 in Epoch 226
Epoch 410
Epoch 410, Loss: 0.00000027055934, Improvement: -0.00000000767211, Best Loss: 0.00000015167652 in Epoch 226
Epoch 411
Epoch 411, Loss: 0.00000026540438, Improvement: -0.00000000515497, Best Loss: 0.00000015167652 in Epoch 226
Epoch 412
Epoch 412, Loss: 0.00000026000260, Improvement: -0.00000000540178, Best Loss: 0.00000015167652 in Epoch 226
Epoch 413
Epoch 413, Loss: 0.00000025927335, Improvement: -0.00000000072924, Best Loss: 0.00000015167652 in Epoch 226
Epoch 414
Epoch 414, Loss: 0.00000026030908, Improvement: 0.00000000103573, Best Loss: 0.00000015167652 in Epoch 226
Epoch 415
Epoch 415, Loss: 0.00000025940633, Improvement: -0.00000000090275, Best Loss: 0.00000015167652 in Epoch 226
Epoch 416
Epoch 416, Loss: 0.00000025850995, Improvement: -0.00000000089638, Best Loss: 0.00000015167652 in Epoch 226
Epoch 417
Epoch 417, Loss: 0.00000025866958, Improvement: 0.00000000015963, Best Loss: 0.00000015167652 in Epoch 226
Epoch 418
Epoch 418, Loss: 0.00000025839549, Improvement: -0.00000000027409, Best Loss: 0.00000015167652 in Epoch 226
Epoch 419
Epoch 419, Loss: 0.00000025773303, Improvement: -0.00000000066246, Best Loss: 0.00000015167652 in Epoch 226
Epoch 420
A best model at epoch 420 has been saved with training error 0.00000014779869.
Epoch 420, Loss: 0.00000025686255, Improvement: -0.00000000087049, Best Loss: 0.00000014779869 in Epoch 420
Epoch 421
Epoch 421, Loss: 0.00000025681691, Improvement: -0.00000000004564, Best Loss: 0.00000014779869 in Epoch 420
Epoch 422
Epoch 422, Loss: 0.00000025708469, Improvement: 0.00000000026779, Best Loss: 0.00000014779869 in Epoch 420
Epoch 423
Epoch 423, Loss: 0.00000025591274, Improvement: -0.00000000117195, Best Loss: 0.00000014779869 in Epoch 420
Epoch 424
Epoch 424, Loss: 0.00000025577794, Improvement: -0.00000000013481, Best Loss: 0.00000014779869 in Epoch 420
Epoch 425
A best model at epoch 425 has been saved with training error 0.00000014596827.
Epoch 425, Loss: 0.00000025672296, Improvement: 0.00000000094503, Best Loss: 0.00000014596827 in Epoch 425
Epoch 426
Epoch 426, Loss: 0.00000025639679, Improvement: -0.00000000032617, Best Loss: 0.00000014596827 in Epoch 425
Epoch 427
Epoch 427, Loss: 0.00000025525957, Improvement: -0.00000000113722, Best Loss: 0.00000014596827 in Epoch 425
Epoch 428
Epoch 428, Loss: 0.00000025572732, Improvement: 0.00000000046774, Best Loss: 0.00000014596827 in Epoch 425
Epoch 429
Epoch 429, Loss: 0.00000025686032, Improvement: 0.00000000113300, Best Loss: 0.00000014596827 in Epoch 425
Epoch 430
Epoch 430, Loss: 0.00000026162768, Improvement: 0.00000000476736, Best Loss: 0.00000014596827 in Epoch 425
Epoch 431
Epoch 431, Loss: 0.00000026016676, Improvement: -0.00000000146092, Best Loss: 0.00000014596827 in Epoch 425
Epoch 432
Epoch 432, Loss: 0.00000026388130, Improvement: 0.00000000371454, Best Loss: 0.00000014596827 in Epoch 425
Epoch 433
Epoch 433, Loss: 0.00000027973770, Improvement: 0.00000001585641, Best Loss: 0.00000014596827 in Epoch 425
Epoch 434
Epoch 434, Loss: 0.00000031010642, Improvement: 0.00000003036871, Best Loss: 0.00000014596827 in Epoch 425
Epoch 435
Epoch 435, Loss: 0.00000029722813, Improvement: -0.00000001287828, Best Loss: 0.00000014596827 in Epoch 425
Epoch 436
Epoch 436, Loss: 0.00000030471242, Improvement: 0.00000000748429, Best Loss: 0.00000014596827 in Epoch 425
Epoch 437
Epoch 437, Loss: 0.00000028638124, Improvement: -0.00000001833118, Best Loss: 0.00000014596827 in Epoch 425
Epoch 438
Epoch 438, Loss: 0.00000025807839, Improvement: -0.00000002830285, Best Loss: 0.00000014596827 in Epoch 425
Epoch 439
Epoch 439, Loss: 0.00000026563942, Improvement: 0.00000000756103, Best Loss: 0.00000014596827 in Epoch 425
Epoch 440
Epoch 440, Loss: 0.00000027067502, Improvement: 0.00000000503560, Best Loss: 0.00000014596827 in Epoch 425
Epoch 441
Epoch 441, Loss: 0.00000027392229, Improvement: 0.00000000324727, Best Loss: 0.00000014596827 in Epoch 425
Epoch 442
Epoch 442, Loss: 0.00000085533482, Improvement: 0.00000058141253, Best Loss: 0.00000014596827 in Epoch 425
Epoch 443
Epoch 443, Loss: 0.00000067965501, Improvement: -0.00000017567981, Best Loss: 0.00000014596827 in Epoch 425
Epoch 444
Epoch 444, Loss: 0.00000036471156, Improvement: -0.00000031494345, Best Loss: 0.00000014596827 in Epoch 425
Epoch 445
Epoch 445, Loss: 0.00000027003965, Improvement: -0.00000009467191, Best Loss: 0.00000014596827 in Epoch 425
Epoch 446
Epoch 446, Loss: 0.00000026168377, Improvement: -0.00000000835588, Best Loss: 0.00000014596827 in Epoch 425
Epoch 447
Epoch 447, Loss: 0.00000025311777, Improvement: -0.00000000856599, Best Loss: 0.00000014596827 in Epoch 425
Epoch 448
Epoch 448, Loss: 0.00000024839495, Improvement: -0.00000000472282, Best Loss: 0.00000014596827 in Epoch 425
Epoch 449
Epoch 449, Loss: 0.00000024536515, Improvement: -0.00000000302980, Best Loss: 0.00000014596827 in Epoch 425
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000024562718, Improvement: 0.00000000026202, Best Loss: 0.00000014596827 in Epoch 425
Epoch 451
Epoch 451, Loss: 0.00000024466133, Improvement: -0.00000000096584, Best Loss: 0.00000014596827 in Epoch 425
Epoch 452
Epoch 452, Loss: 0.00000024391801, Improvement: -0.00000000074332, Best Loss: 0.00000014596827 in Epoch 425
Epoch 453
Epoch 453, Loss: 0.00000024324759, Improvement: -0.00000000067042, Best Loss: 0.00000014596827 in Epoch 425
Epoch 454
Epoch 454, Loss: 0.00000024406021, Improvement: 0.00000000081262, Best Loss: 0.00000014596827 in Epoch 425
Epoch 455
Epoch 455, Loss: 0.00000024345207, Improvement: -0.00000000060814, Best Loss: 0.00000014596827 in Epoch 425
Epoch 456
Epoch 456, Loss: 0.00000024416085, Improvement: 0.00000000070877, Best Loss: 0.00000014596827 in Epoch 425
Epoch 457
Epoch 457, Loss: 0.00000024507503, Improvement: 0.00000000091418, Best Loss: 0.00000014596827 in Epoch 425
Epoch 458
Epoch 458, Loss: 0.00000024218090, Improvement: -0.00000000289413, Best Loss: 0.00000014596827 in Epoch 425
Epoch 459
A best model at epoch 459 has been saved with training error 0.00000013286032.
Epoch 459, Loss: 0.00000024153610, Improvement: -0.00000000064480, Best Loss: 0.00000013286032 in Epoch 459
Epoch 460
Epoch 460, Loss: 0.00000024097812, Improvement: -0.00000000055798, Best Loss: 0.00000013286032 in Epoch 459
Epoch 461
Epoch 461, Loss: 0.00000024009506, Improvement: -0.00000000088306, Best Loss: 0.00000013286032 in Epoch 459
Epoch 462
Epoch 462, Loss: 0.00000024020913, Improvement: 0.00000000011407, Best Loss: 0.00000013286032 in Epoch 459
Epoch 463
Epoch 463, Loss: 0.00000024048665, Improvement: 0.00000000027752, Best Loss: 0.00000013286032 in Epoch 459
Epoch 464
Epoch 464, Loss: 0.00000023925333, Improvement: -0.00000000123333, Best Loss: 0.00000013286032 in Epoch 459
Epoch 465
Epoch 465, Loss: 0.00000023945057, Improvement: 0.00000000019724, Best Loss: 0.00000013286032 in Epoch 459
Epoch 466
Epoch 466, Loss: 0.00000024106860, Improvement: 0.00000000161803, Best Loss: 0.00000013286032 in Epoch 459
Epoch 467
Epoch 467, Loss: 0.00000023977356, Improvement: -0.00000000129504, Best Loss: 0.00000013286032 in Epoch 459
Epoch 468
Epoch 468, Loss: 0.00000024159155, Improvement: 0.00000000181798, Best Loss: 0.00000013286032 in Epoch 459
Epoch 469
Epoch 469, Loss: 0.00000023915651, Improvement: -0.00000000243503, Best Loss: 0.00000013286032 in Epoch 459
Epoch 470
Epoch 470, Loss: 0.00000023997020, Improvement: 0.00000000081369, Best Loss: 0.00000013286032 in Epoch 459
Epoch 471
Epoch 471, Loss: 0.00000024145155, Improvement: 0.00000000148135, Best Loss: 0.00000013286032 in Epoch 459
Epoch 472
Epoch 472, Loss: 0.00000023671621, Improvement: -0.00000000473534, Best Loss: 0.00000013286032 in Epoch 459
Epoch 473
Epoch 473, Loss: 0.00000023922689, Improvement: 0.00000000251068, Best Loss: 0.00000013286032 in Epoch 459
Epoch 474
Epoch 474, Loss: 0.00000025723964, Improvement: 0.00000001801276, Best Loss: 0.00000013286032 in Epoch 459
Epoch 475
Epoch 475, Loss: 0.00000027691188, Improvement: 0.00000001967224, Best Loss: 0.00000013286032 in Epoch 459
Epoch 476
Epoch 476, Loss: 0.00000045324167, Improvement: 0.00000017632979, Best Loss: 0.00000013286032 in Epoch 459
Epoch 477
Epoch 477, Loss: 0.00000081613081, Improvement: 0.00000036288914, Best Loss: 0.00000013286032 in Epoch 459
Epoch 478
Epoch 478, Loss: 0.00000112181717, Improvement: 0.00000030568636, Best Loss: 0.00000013286032 in Epoch 459
Epoch 479
Epoch 479, Loss: 0.00000067754996, Improvement: -0.00000044426720, Best Loss: 0.00000013286032 in Epoch 459
Epoch 480
Epoch 480, Loss: 0.00000031707703, Improvement: -0.00000036047293, Best Loss: 0.00000013286032 in Epoch 459
Epoch 481
Epoch 481, Loss: 0.00000025518201, Improvement: -0.00000006189502, Best Loss: 0.00000013286032 in Epoch 459
Epoch 482
Epoch 482, Loss: 0.00000023576985, Improvement: -0.00000001941217, Best Loss: 0.00000013286032 in Epoch 459
Epoch 483
Epoch 483, Loss: 0.00000023569562, Improvement: -0.00000000007422, Best Loss: 0.00000013286032 in Epoch 459
Epoch 484
Epoch 484, Loss: 0.00000023200067, Improvement: -0.00000000369495, Best Loss: 0.00000013286032 in Epoch 459
Epoch 485
Epoch 485, Loss: 0.00000023113184, Improvement: -0.00000000086884, Best Loss: 0.00000013286032 in Epoch 459
Epoch 486
Epoch 486, Loss: 0.00000023067978, Improvement: -0.00000000045206, Best Loss: 0.00000013286032 in Epoch 459
Epoch 487
Epoch 487, Loss: 0.00000022965302, Improvement: -0.00000000102676, Best Loss: 0.00000013286032 in Epoch 459
Epoch 488
Epoch 488, Loss: 0.00000022935690, Improvement: -0.00000000029612, Best Loss: 0.00000013286032 in Epoch 459
Epoch 489
Epoch 489, Loss: 0.00000022896124, Improvement: -0.00000000039566, Best Loss: 0.00000013286032 in Epoch 459
Epoch 490
Epoch 490, Loss: 0.00000022885657, Improvement: -0.00000000010467, Best Loss: 0.00000013286032 in Epoch 459
Epoch 491
Epoch 491, Loss: 0.00000022904077, Improvement: 0.00000000018420, Best Loss: 0.00000013286032 in Epoch 459
Epoch 492
A best model at epoch 492 has been saved with training error 0.00000012778661.
Epoch 492, Loss: 0.00000022841493, Improvement: -0.00000000062584, Best Loss: 0.00000012778661 in Epoch 492
Epoch 493
Epoch 493, Loss: 0.00000022755173, Improvement: -0.00000000086320, Best Loss: 0.00000012778661 in Epoch 492
Epoch 494
Epoch 494, Loss: 0.00000022737922, Improvement: -0.00000000017252, Best Loss: 0.00000012778661 in Epoch 492
Epoch 495
Epoch 495, Loss: 0.00000022681711, Improvement: -0.00000000056211, Best Loss: 0.00000012778661 in Epoch 492
Epoch 496
Epoch 496, Loss: 0.00000022643843, Improvement: -0.00000000037868, Best Loss: 0.00000012778661 in Epoch 492
Epoch 497
Epoch 497, Loss: 0.00000022697582, Improvement: 0.00000000053739, Best Loss: 0.00000012778661 in Epoch 492
Epoch 498
Epoch 498, Loss: 0.00000022658487, Improvement: -0.00000000039095, Best Loss: 0.00000012778661 in Epoch 492
Epoch 499
Epoch 499, Loss: 0.00000022539543, Improvement: -0.00000000118944, Best Loss: 0.00000012778661 in Epoch 492
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000022560824, Improvement: 0.00000000021282, Best Loss: 0.00000012778661 in Epoch 492
Epoch 501
Epoch 501, Loss: 0.00000022580232, Improvement: 0.00000000019407, Best Loss: 0.00000012778661 in Epoch 492
Epoch 502
Epoch 502, Loss: 0.00000022485264, Improvement: -0.00000000094968, Best Loss: 0.00000012778661 in Epoch 492
Epoch 503
Epoch 503, Loss: 0.00000022483089, Improvement: -0.00000000002175, Best Loss: 0.00000012778661 in Epoch 492
Epoch 504
Epoch 504, Loss: 0.00000022642744, Improvement: 0.00000000159655, Best Loss: 0.00000012778661 in Epoch 492
Epoch 505
Epoch 505, Loss: 0.00000022468178, Improvement: -0.00000000174566, Best Loss: 0.00000012778661 in Epoch 492
Epoch 506
Epoch 506, Loss: 0.00000022313302, Improvement: -0.00000000154875, Best Loss: 0.00000012778661 in Epoch 492
Epoch 507
Epoch 507, Loss: 0.00000022605974, Improvement: 0.00000000292672, Best Loss: 0.00000012778661 in Epoch 492
Epoch 508
Epoch 508, Loss: 0.00000022784546, Improvement: 0.00000000178572, Best Loss: 0.00000012778661 in Epoch 492
Epoch 509
Epoch 509, Loss: 0.00000024505007, Improvement: 0.00000001720461, Best Loss: 0.00000012778661 in Epoch 492
Epoch 510
Epoch 510, Loss: 0.00000027076643, Improvement: 0.00000002571636, Best Loss: 0.00000012778661 in Epoch 492
Epoch 511
Epoch 511, Loss: 0.00000033510467, Improvement: 0.00000006433823, Best Loss: 0.00000012778661 in Epoch 492
Epoch 512
Epoch 512, Loss: 0.00000033562315, Improvement: 0.00000000051848, Best Loss: 0.00000012778661 in Epoch 492
Epoch 513
Epoch 513, Loss: 0.00000032977500, Improvement: -0.00000000584815, Best Loss: 0.00000012778661 in Epoch 492
Epoch 514
Epoch 514, Loss: 0.00000041888024, Improvement: 0.00000008910523, Best Loss: 0.00000012778661 in Epoch 492
Epoch 515
Epoch 515, Loss: 0.00000033837589, Improvement: -0.00000008050435, Best Loss: 0.00000012778661 in Epoch 492
Epoch 516
Epoch 516, Loss: 0.00000060682741, Improvement: 0.00000026845152, Best Loss: 0.00000012778661 in Epoch 492
Epoch 517
Epoch 517, Loss: 0.00000031905671, Improvement: -0.00000028777070, Best Loss: 0.00000012778661 in Epoch 492
Epoch 518
Epoch 518, Loss: 0.00000025520726, Improvement: -0.00000006384945, Best Loss: 0.00000012778661 in Epoch 492
Epoch 519
Epoch 519, Loss: 0.00000023299760, Improvement: -0.00000002220966, Best Loss: 0.00000012778661 in Epoch 492
Epoch 520
Epoch 520, Loss: 0.00000022255095, Improvement: -0.00000001044665, Best Loss: 0.00000012778661 in Epoch 492
Epoch 521
Epoch 521, Loss: 0.00000021725980, Improvement: -0.00000000529114, Best Loss: 0.00000012778661 in Epoch 492
Epoch 522
Epoch 522, Loss: 0.00000021653273, Improvement: -0.00000000072708, Best Loss: 0.00000012778661 in Epoch 492
Epoch 523
Epoch 523, Loss: 0.00000021602001, Improvement: -0.00000000051272, Best Loss: 0.00000012778661 in Epoch 492
Epoch 524
Epoch 524, Loss: 0.00000021537942, Improvement: -0.00000000064059, Best Loss: 0.00000012778661 in Epoch 492
Epoch 525
Epoch 525, Loss: 0.00000021609276, Improvement: 0.00000000071334, Best Loss: 0.00000012778661 in Epoch 492
Epoch 526
Epoch 526, Loss: 0.00000021610266, Improvement: 0.00000000000990, Best Loss: 0.00000012778661 in Epoch 492
Epoch 527
Epoch 527, Loss: 0.00000021503924, Improvement: -0.00000000106342, Best Loss: 0.00000012778661 in Epoch 492
Epoch 528
Epoch 528, Loss: 0.00000021633736, Improvement: 0.00000000129812, Best Loss: 0.00000012778661 in Epoch 492
Epoch 529
A best model at epoch 529 has been saved with training error 0.00000011234543.
Epoch 529, Loss: 0.00000021579353, Improvement: -0.00000000054383, Best Loss: 0.00000011234543 in Epoch 529
Epoch 530
Epoch 530, Loss: 0.00000021766370, Improvement: 0.00000000187017, Best Loss: 0.00000011234543 in Epoch 529
Epoch 531
Epoch 531, Loss: 0.00000022482616, Improvement: 0.00000000716245, Best Loss: 0.00000011234543 in Epoch 529
Epoch 532
Epoch 532, Loss: 0.00000023292380, Improvement: 0.00000000809764, Best Loss: 0.00000011234543 in Epoch 529
Epoch 533
Epoch 533, Loss: 0.00000022995608, Improvement: -0.00000000296772, Best Loss: 0.00000011234543 in Epoch 529
Epoch 534
Epoch 534, Loss: 0.00000027566123, Improvement: 0.00000004570516, Best Loss: 0.00000011234543 in Epoch 529
Epoch 535
Epoch 535, Loss: 0.00000037112417, Improvement: 0.00000009546294, Best Loss: 0.00000011234543 in Epoch 529
Epoch 536
Epoch 536, Loss: 0.00000028068540, Improvement: -0.00000009043877, Best Loss: 0.00000011234543 in Epoch 529
Epoch 537
Epoch 537, Loss: 0.00000028539040, Improvement: 0.00000000470501, Best Loss: 0.00000011234543 in Epoch 529
Epoch 538
Epoch 538, Loss: 0.00000022990468, Improvement: -0.00000005548572, Best Loss: 0.00000011234543 in Epoch 529
Epoch 539
Epoch 539, Loss: 0.00000023625903, Improvement: 0.00000000635435, Best Loss: 0.00000011234543 in Epoch 529
Epoch 540
Epoch 540, Loss: 0.00000029561315, Improvement: 0.00000005935412, Best Loss: 0.00000011234543 in Epoch 529
Epoch 541
Epoch 541, Loss: 0.00000049397178, Improvement: 0.00000019835864, Best Loss: 0.00000011234543 in Epoch 529
Epoch 542
Epoch 542, Loss: 0.00000031806094, Improvement: -0.00000017591084, Best Loss: 0.00000011234543 in Epoch 529
Epoch 543
Epoch 543, Loss: 0.00000024830210, Improvement: -0.00000006975884, Best Loss: 0.00000011234543 in Epoch 529
Epoch 544
Epoch 544, Loss: 0.00000021869795, Improvement: -0.00000002960415, Best Loss: 0.00000011234543 in Epoch 529
Epoch 545
Epoch 545, Loss: 0.00000020896813, Improvement: -0.00000000972982, Best Loss: 0.00000011234543 in Epoch 529
Epoch 546
Epoch 546, Loss: 0.00000020823886, Improvement: -0.00000000072927, Best Loss: 0.00000011234543 in Epoch 529
Epoch 547
Epoch 547, Loss: 0.00000020731009, Improvement: -0.00000000092877, Best Loss: 0.00000011234543 in Epoch 529
Epoch 548
Epoch 548, Loss: 0.00000020697313, Improvement: -0.00000000033696, Best Loss: 0.00000011234543 in Epoch 529
Epoch 549
Epoch 549, Loss: 0.00000020742387, Improvement: 0.00000000045074, Best Loss: 0.00000011234543 in Epoch 529
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000020881489, Improvement: 0.00000000139102, Best Loss: 0.00000011234543 in Epoch 529
Epoch 551
Epoch 551, Loss: 0.00000021375419, Improvement: 0.00000000493930, Best Loss: 0.00000011234543 in Epoch 529
Epoch 552
Epoch 552, Loss: 0.00000021192347, Improvement: -0.00000000183072, Best Loss: 0.00000011234543 in Epoch 529
Epoch 553
Epoch 553, Loss: 0.00000020803583, Improvement: -0.00000000388764, Best Loss: 0.00000011234543 in Epoch 529
Epoch 554
Epoch 554, Loss: 0.00000020911075, Improvement: 0.00000000107492, Best Loss: 0.00000011234543 in Epoch 529
Epoch 555
Epoch 555, Loss: 0.00000023657839, Improvement: 0.00000002746764, Best Loss: 0.00000011234543 in Epoch 529
Epoch 556
Epoch 556, Loss: 0.00000034560666, Improvement: 0.00000010902827, Best Loss: 0.00000011234543 in Epoch 529
Epoch 557
Epoch 557, Loss: 0.00000057099143, Improvement: 0.00000022538477, Best Loss: 0.00000011234543 in Epoch 529
Epoch 558
Epoch 558, Loss: 0.00000040959081, Improvement: -0.00000016140062, Best Loss: 0.00000011234543 in Epoch 529
Epoch 559
Epoch 559, Loss: 0.00000066553704, Improvement: 0.00000025594623, Best Loss: 0.00000011234543 in Epoch 529
Epoch 560
Epoch 560, Loss: 0.00000043653053, Improvement: -0.00000022900651, Best Loss: 0.00000011234543 in Epoch 529
Epoch 561
Epoch 561, Loss: 0.00000030451860, Improvement: -0.00000013201193, Best Loss: 0.00000011234543 in Epoch 529
Epoch 562
Epoch 562, Loss: 0.00000027764842, Improvement: -0.00000002687018, Best Loss: 0.00000011234543 in Epoch 529
Epoch 563
Epoch 563, Loss: 0.00000021808586, Improvement: -0.00000005956256, Best Loss: 0.00000011234543 in Epoch 529
Epoch 564
Epoch 564, Loss: 0.00000020299038, Improvement: -0.00000001509548, Best Loss: 0.00000011234543 in Epoch 529
Epoch 565
Epoch 565, Loss: 0.00000020021581, Improvement: -0.00000000277457, Best Loss: 0.00000011234543 in Epoch 529
Epoch 566
Epoch 566, Loss: 0.00000019939285, Improvement: -0.00000000082295, Best Loss: 0.00000011234543 in Epoch 529
Epoch 567
Epoch 567, Loss: 0.00000019853222, Improvement: -0.00000000086064, Best Loss: 0.00000011234543 in Epoch 529
Epoch 568
Epoch 568, Loss: 0.00000019927406, Improvement: 0.00000000074185, Best Loss: 0.00000011234543 in Epoch 529
Epoch 569
Epoch 569, Loss: 0.00000019919459, Improvement: -0.00000000007947, Best Loss: 0.00000011234543 in Epoch 529
Epoch 570
Epoch 570, Loss: 0.00000019950989, Improvement: 0.00000000031530, Best Loss: 0.00000011234543 in Epoch 529
Epoch 571
Epoch 571, Loss: 0.00000019839386, Improvement: -0.00000000111603, Best Loss: 0.00000011234543 in Epoch 529
Epoch 572
Epoch 572, Loss: 0.00000019754030, Improvement: -0.00000000085356, Best Loss: 0.00000011234543 in Epoch 529
Epoch 573
Epoch 573, Loss: 0.00000019669168, Improvement: -0.00000000084861, Best Loss: 0.00000011234543 in Epoch 529
Epoch 574
Epoch 574, Loss: 0.00000019721132, Improvement: 0.00000000051964, Best Loss: 0.00000011234543 in Epoch 529
Epoch 575
Epoch 575, Loss: 0.00000019558139, Improvement: -0.00000000162993, Best Loss: 0.00000011234543 in Epoch 529
Epoch 576
Epoch 576, Loss: 0.00000019634206, Improvement: 0.00000000076067, Best Loss: 0.00000011234543 in Epoch 529
Epoch 577
Epoch 577, Loss: 0.00000019591044, Improvement: -0.00000000043162, Best Loss: 0.00000011234543 in Epoch 529
Epoch 578
Epoch 578, Loss: 0.00000019515139, Improvement: -0.00000000075905, Best Loss: 0.00000011234543 in Epoch 529
Epoch 579
Epoch 579, Loss: 0.00000019727894, Improvement: 0.00000000212755, Best Loss: 0.00000011234543 in Epoch 529
Epoch 580
Epoch 580, Loss: 0.00000019609692, Improvement: -0.00000000118201, Best Loss: 0.00000011234543 in Epoch 529
Epoch 581
Epoch 581, Loss: 0.00000019556788, Improvement: -0.00000000052904, Best Loss: 0.00000011234543 in Epoch 529
Epoch 582
Epoch 582, Loss: 0.00000019635451, Improvement: 0.00000000078663, Best Loss: 0.00000011234543 in Epoch 529
Epoch 583
Epoch 583, Loss: 0.00000019494411, Improvement: -0.00000000141040, Best Loss: 0.00000011234543 in Epoch 529
Epoch 584
Epoch 584, Loss: 0.00000020480512, Improvement: 0.00000000986101, Best Loss: 0.00000011234543 in Epoch 529
Epoch 585
Epoch 585, Loss: 0.00000020228883, Improvement: -0.00000000251629, Best Loss: 0.00000011234543 in Epoch 529
Epoch 586
Epoch 586, Loss: 0.00000020409831, Improvement: 0.00000000180949, Best Loss: 0.00000011234543 in Epoch 529
Epoch 587
Epoch 587, Loss: 0.00000021107360, Improvement: 0.00000000697529, Best Loss: 0.00000011234543 in Epoch 529
Epoch 588
Epoch 588, Loss: 0.00000021693748, Improvement: 0.00000000586388, Best Loss: 0.00000011234543 in Epoch 529
Epoch 589
Epoch 589, Loss: 0.00000023154883, Improvement: 0.00000001461134, Best Loss: 0.00000011234543 in Epoch 529
Epoch 590
Epoch 590, Loss: 0.00000033156514, Improvement: 0.00000010001631, Best Loss: 0.00000011234543 in Epoch 529
Epoch 591
Epoch 591, Loss: 0.00000038591916, Improvement: 0.00000005435402, Best Loss: 0.00000011234543 in Epoch 529
Epoch 592
Epoch 592, Loss: 0.00000028078084, Improvement: -0.00000010513833, Best Loss: 0.00000011234543 in Epoch 529
Epoch 593
Epoch 593, Loss: 0.00000027606716, Improvement: -0.00000000471367, Best Loss: 0.00000011234543 in Epoch 529
Epoch 594
Epoch 594, Loss: 0.00000023253992, Improvement: -0.00000004352725, Best Loss: 0.00000011234543 in Epoch 529
Epoch 595
Epoch 595, Loss: 0.00000021149950, Improvement: -0.00000002104041, Best Loss: 0.00000011234543 in Epoch 529
Epoch 596
Epoch 596, Loss: 0.00000024994319, Improvement: 0.00000003844368, Best Loss: 0.00000011234543 in Epoch 529
Epoch 597
Epoch 597, Loss: 0.00000024714995, Improvement: -0.00000000279323, Best Loss: 0.00000011234543 in Epoch 529
Epoch 598
Epoch 598, Loss: 0.00000022581474, Improvement: -0.00000002133521, Best Loss: 0.00000011234543 in Epoch 529
Epoch 599
Epoch 599, Loss: 0.00000023622719, Improvement: 0.00000001041245, Best Loss: 0.00000011234543 in Epoch 529
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000021372762, Improvement: -0.00000002249958, Best Loss: 0.00000011234543 in Epoch 529
Epoch 601
Epoch 601, Loss: 0.00000022495443, Improvement: 0.00000001122682, Best Loss: 0.00000011234543 in Epoch 529
Epoch 602
Epoch 602, Loss: 0.00000026396625, Improvement: 0.00000003901182, Best Loss: 0.00000011234543 in Epoch 529
Epoch 603
Epoch 603, Loss: 0.00000021321163, Improvement: -0.00000005075462, Best Loss: 0.00000011234543 in Epoch 529
Epoch 604
Epoch 604, Loss: 0.00000022016980, Improvement: 0.00000000695817, Best Loss: 0.00000011234543 in Epoch 529
Epoch 605
Epoch 605, Loss: 0.00000025024079, Improvement: 0.00000003007099, Best Loss: 0.00000011234543 in Epoch 529
Epoch 606
Epoch 606, Loss: 0.00000025958597, Improvement: 0.00000000934518, Best Loss: 0.00000011234543 in Epoch 529
Epoch 607
Epoch 607, Loss: 0.00000050926991, Improvement: 0.00000024968394, Best Loss: 0.00000011234543 in Epoch 529
Epoch 608
Epoch 608, Loss: 0.00000051050619, Improvement: 0.00000000123629, Best Loss: 0.00000011234543 in Epoch 529
Epoch 609
Epoch 609, Loss: 0.00000025291294, Improvement: -0.00000025759325, Best Loss: 0.00000011234543 in Epoch 529
Epoch 610
Epoch 610, Loss: 0.00000021018120, Improvement: -0.00000004273174, Best Loss: 0.00000011234543 in Epoch 529
Epoch 611
Epoch 611, Loss: 0.00000019049952, Improvement: -0.00000001968168, Best Loss: 0.00000011234543 in Epoch 529
Epoch 612
A best model at epoch 612 has been saved with training error 0.00000010988569.
Epoch 612, Loss: 0.00000018785971, Improvement: -0.00000000263981, Best Loss: 0.00000010988569 in Epoch 612
Epoch 613
Epoch 613, Loss: 0.00000018635888, Improvement: -0.00000000150083, Best Loss: 0.00000010988569 in Epoch 612
Epoch 614
Epoch 614, Loss: 0.00000018617955, Improvement: -0.00000000017932, Best Loss: 0.00000010988569 in Epoch 612
Epoch 615
Epoch 615, Loss: 0.00000018738285, Improvement: 0.00000000120330, Best Loss: 0.00000010988569 in Epoch 612
Epoch 616
Epoch 616, Loss: 0.00000019063030, Improvement: 0.00000000324745, Best Loss: 0.00000010988569 in Epoch 612
Epoch 617
Epoch 617, Loss: 0.00000018789811, Improvement: -0.00000000273219, Best Loss: 0.00000010988569 in Epoch 612
Epoch 618
Epoch 618, Loss: 0.00000018666626, Improvement: -0.00000000123185, Best Loss: 0.00000010988569 in Epoch 612
Epoch 619
Epoch 619, Loss: 0.00000018657995, Improvement: -0.00000000008631, Best Loss: 0.00000010988569 in Epoch 612
Epoch 620
Epoch 620, Loss: 0.00000018606900, Improvement: -0.00000000051094, Best Loss: 0.00000010988569 in Epoch 612
Epoch 621
Epoch 621, Loss: 0.00000018581734, Improvement: -0.00000000025167, Best Loss: 0.00000010988569 in Epoch 612
Epoch 622
Epoch 622, Loss: 0.00000018642216, Improvement: 0.00000000060482, Best Loss: 0.00000010988569 in Epoch 612
Epoch 623
Epoch 623, Loss: 0.00000018543055, Improvement: -0.00000000099161, Best Loss: 0.00000010988569 in Epoch 612
Epoch 624
Epoch 624, Loss: 0.00000018444825, Improvement: -0.00000000098230, Best Loss: 0.00000010988569 in Epoch 612
Epoch 625
Epoch 625, Loss: 0.00000018456764, Improvement: 0.00000000011939, Best Loss: 0.00000010988569 in Epoch 612
Epoch 626
Epoch 626, Loss: 0.00000018447623, Improvement: -0.00000000009141, Best Loss: 0.00000010988569 in Epoch 612
Epoch 627
A best model at epoch 627 has been saved with training error 0.00000009881455.
Epoch 627, Loss: 0.00000018482488, Improvement: 0.00000000034866, Best Loss: 0.00000009881455 in Epoch 627
Epoch 628
Epoch 628, Loss: 0.00000018492839, Improvement: 0.00000000010351, Best Loss: 0.00000009881455 in Epoch 627
Epoch 629
Epoch 629, Loss: 0.00000018434417, Improvement: -0.00000000058422, Best Loss: 0.00000009881455 in Epoch 627
Epoch 630
Epoch 630, Loss: 0.00000018539506, Improvement: 0.00000000105089, Best Loss: 0.00000009881455 in Epoch 627
Epoch 631
Epoch 631, Loss: 0.00000018407482, Improvement: -0.00000000132024, Best Loss: 0.00000009881455 in Epoch 627
Epoch 632
Epoch 632, Loss: 0.00000018352666, Improvement: -0.00000000054816, Best Loss: 0.00000009881455 in Epoch 627
Epoch 633
Epoch 633, Loss: 0.00000018312805, Improvement: -0.00000000039861, Best Loss: 0.00000009881455 in Epoch 627
Epoch 634
Epoch 634, Loss: 0.00000018277271, Improvement: -0.00000000035534, Best Loss: 0.00000009881455 in Epoch 627
Epoch 635
Epoch 635, Loss: 0.00000018308828, Improvement: 0.00000000031556, Best Loss: 0.00000009881455 in Epoch 627
Epoch 636
Epoch 636, Loss: 0.00000018806111, Improvement: 0.00000000497283, Best Loss: 0.00000009881455 in Epoch 627
Epoch 637
Epoch 637, Loss: 0.00000020049828, Improvement: 0.00000001243718, Best Loss: 0.00000009881455 in Epoch 627
Epoch 638
Epoch 638, Loss: 0.00000019298672, Improvement: -0.00000000751156, Best Loss: 0.00000009881455 in Epoch 627
Epoch 639
Epoch 639, Loss: 0.00000018799084, Improvement: -0.00000000499587, Best Loss: 0.00000009881455 in Epoch 627
Epoch 640
Epoch 640, Loss: 0.00000019280073, Improvement: 0.00000000480989, Best Loss: 0.00000009881455 in Epoch 627
Epoch 641
Epoch 641, Loss: 0.00000018793722, Improvement: -0.00000000486351, Best Loss: 0.00000009881455 in Epoch 627
Epoch 642
Epoch 642, Loss: 0.00000018955794, Improvement: 0.00000000162073, Best Loss: 0.00000009881455 in Epoch 627
Epoch 643
Epoch 643, Loss: 0.00000022244827, Improvement: 0.00000003289033, Best Loss: 0.00000009881455 in Epoch 627
Epoch 644
Epoch 644, Loss: 0.00000028629948, Improvement: 0.00000006385121, Best Loss: 0.00000009881455 in Epoch 627
Epoch 645
Epoch 645, Loss: 0.00000033302519, Improvement: 0.00000004672571, Best Loss: 0.00000009881455 in Epoch 627
Epoch 646
Epoch 646, Loss: 0.00000021346112, Improvement: -0.00000011956407, Best Loss: 0.00000009881455 in Epoch 627
Epoch 647
Epoch 647, Loss: 0.00000020329023, Improvement: -0.00000001017089, Best Loss: 0.00000009881455 in Epoch 627
Epoch 648
Epoch 648, Loss: 0.00000020401380, Improvement: 0.00000000072357, Best Loss: 0.00000009881455 in Epoch 627
Epoch 649
Epoch 649, Loss: 0.00000020288470, Improvement: -0.00000000112911, Best Loss: 0.00000009881455 in Epoch 627
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000021511125, Improvement: 0.00000001222656, Best Loss: 0.00000009881455 in Epoch 627
Epoch 651
Epoch 651, Loss: 0.00000025692953, Improvement: 0.00000004181828, Best Loss: 0.00000009881455 in Epoch 627
Epoch 652
Epoch 652, Loss: 0.00000025922497, Improvement: 0.00000000229544, Best Loss: 0.00000009881455 in Epoch 627
Epoch 653
Epoch 653, Loss: 0.00000024544372, Improvement: -0.00000001378124, Best Loss: 0.00000009881455 in Epoch 627
Epoch 654
Epoch 654, Loss: 0.00000022101664, Improvement: -0.00000002442708, Best Loss: 0.00000009881455 in Epoch 627
Epoch 655
Epoch 655, Loss: 0.00000021068375, Improvement: -0.00000001033289, Best Loss: 0.00000009881455 in Epoch 627
Epoch 656
Epoch 656, Loss: 0.00000021645798, Improvement: 0.00000000577423, Best Loss: 0.00000009881455 in Epoch 627
Epoch 657
Epoch 657, Loss: 0.00000031481094, Improvement: 0.00000009835295, Best Loss: 0.00000009881455 in Epoch 627
Epoch 658
Epoch 658, Loss: 0.00000044665487, Improvement: 0.00000013184393, Best Loss: 0.00000009881455 in Epoch 627
Epoch 659
Epoch 659, Loss: 0.00000033588694, Improvement: -0.00000011076793, Best Loss: 0.00000009881455 in Epoch 627
Epoch 660
Epoch 660, Loss: 0.00000031618371, Improvement: -0.00000001970323, Best Loss: 0.00000009881455 in Epoch 627
Epoch 661
Epoch 661, Loss: 0.00000034034860, Improvement: 0.00000002416489, Best Loss: 0.00000009881455 in Epoch 627
Epoch 662
Epoch 662, Loss: 0.00000021560605, Improvement: -0.00000012474255, Best Loss: 0.00000009881455 in Epoch 627
Epoch 663
Epoch 663, Loss: 0.00000020056209, Improvement: -0.00000001504396, Best Loss: 0.00000009881455 in Epoch 627
Epoch 664
Epoch 664, Loss: 0.00000019925752, Improvement: -0.00000000130457, Best Loss: 0.00000009881455 in Epoch 627
Epoch 665
Epoch 665, Loss: 0.00000018874304, Improvement: -0.00000001051447, Best Loss: 0.00000009881455 in Epoch 627
Epoch 666
Epoch 666, Loss: 0.00000018050401, Improvement: -0.00000000823903, Best Loss: 0.00000009881455 in Epoch 627
Epoch 667
Epoch 667, Loss: 0.00000017839979, Improvement: -0.00000000210422, Best Loss: 0.00000009881455 in Epoch 627
Epoch 668
Epoch 668, Loss: 0.00000017764502, Improvement: -0.00000000075477, Best Loss: 0.00000009881455 in Epoch 627
Epoch 669
Epoch 669, Loss: 0.00000017736629, Improvement: -0.00000000027873, Best Loss: 0.00000009881455 in Epoch 627
Epoch 670
Epoch 670, Loss: 0.00000017817870, Improvement: 0.00000000081241, Best Loss: 0.00000009881455 in Epoch 627
Epoch 671
Epoch 671, Loss: 0.00000017746565, Improvement: -0.00000000071305, Best Loss: 0.00000009881455 in Epoch 627
Epoch 672
Epoch 672, Loss: 0.00000017718548, Improvement: -0.00000000028017, Best Loss: 0.00000009881455 in Epoch 627
Epoch 673
Epoch 673, Loss: 0.00000017671499, Improvement: -0.00000000047049, Best Loss: 0.00000009881455 in Epoch 627
Epoch 674
Epoch 674, Loss: 0.00000017909489, Improvement: 0.00000000237990, Best Loss: 0.00000009881455 in Epoch 627
Epoch 675
Epoch 675, Loss: 0.00000017987387, Improvement: 0.00000000077898, Best Loss: 0.00000009881455 in Epoch 627
Epoch 676
Epoch 676, Loss: 0.00000020469535, Improvement: 0.00000002482148, Best Loss: 0.00000009881455 in Epoch 627
Epoch 677
Epoch 677, Loss: 0.00000020784050, Improvement: 0.00000000314515, Best Loss: 0.00000009881455 in Epoch 627
Epoch 678
Epoch 678, Loss: 0.00000022138301, Improvement: 0.00000001354250, Best Loss: 0.00000009881455 in Epoch 627
Epoch 679
Epoch 679, Loss: 0.00000028916540, Improvement: 0.00000006778239, Best Loss: 0.00000009881455 in Epoch 627
Epoch 680
Epoch 680, Loss: 0.00000020468269, Improvement: -0.00000008448271, Best Loss: 0.00000009881455 in Epoch 627
Epoch 681
Epoch 681, Loss: 0.00000019524881, Improvement: -0.00000000943388, Best Loss: 0.00000009881455 in Epoch 627
Epoch 682
Epoch 682, Loss: 0.00000018231067, Improvement: -0.00000001293814, Best Loss: 0.00000009881455 in Epoch 627
Epoch 683
Epoch 683, Loss: 0.00000018124243, Improvement: -0.00000000106824, Best Loss: 0.00000009881455 in Epoch 627
Epoch 684
Epoch 684, Loss: 0.00000018613145, Improvement: 0.00000000488903, Best Loss: 0.00000009881455 in Epoch 627
Epoch 685
Epoch 685, Loss: 0.00000018901036, Improvement: 0.00000000287891, Best Loss: 0.00000009881455 in Epoch 627
Epoch 686
Epoch 686, Loss: 0.00000022238146, Improvement: 0.00000003337110, Best Loss: 0.00000009881455 in Epoch 627
Epoch 687
Epoch 687, Loss: 0.00000032658903, Improvement: 0.00000010420757, Best Loss: 0.00000009881455 in Epoch 627
Epoch 688
Epoch 688, Loss: 0.00000031635205, Improvement: -0.00000001023699, Best Loss: 0.00000009881455 in Epoch 627
Epoch 689
Epoch 689, Loss: 0.00000021886226, Improvement: -0.00000009748979, Best Loss: 0.00000009881455 in Epoch 627
Epoch 690
Epoch 690, Loss: 0.00000018567227, Improvement: -0.00000003318999, Best Loss: 0.00000009881455 in Epoch 627
Epoch 691
Epoch 691, Loss: 0.00000018723233, Improvement: 0.00000000156006, Best Loss: 0.00000009881455 in Epoch 627
Epoch 692
Epoch 692, Loss: 0.00000018072781, Improvement: -0.00000000650452, Best Loss: 0.00000009881455 in Epoch 627
Epoch 693
Epoch 693, Loss: 0.00000017565879, Improvement: -0.00000000506902, Best Loss: 0.00000009881455 in Epoch 627
Epoch 694
A best model at epoch 694 has been saved with training error 0.00000009832334.
Epoch 694, Loss: 0.00000017387452, Improvement: -0.00000000178427, Best Loss: 0.00000009832334 in Epoch 694
Epoch 695
Epoch 695, Loss: 0.00000017396992, Improvement: 0.00000000009541, Best Loss: 0.00000009832334 in Epoch 694
Epoch 696
Epoch 696, Loss: 0.00000017454786, Improvement: 0.00000000057794, Best Loss: 0.00000009832334 in Epoch 694
Epoch 697
Epoch 697, Loss: 0.00000017419732, Improvement: -0.00000000035054, Best Loss: 0.00000009832334 in Epoch 694
Epoch 698
Epoch 698, Loss: 0.00000017416974, Improvement: -0.00000000002758, Best Loss: 0.00000009832334 in Epoch 694
Epoch 699
Epoch 699, Loss: 0.00000017508783, Improvement: 0.00000000091809, Best Loss: 0.00000009832334 in Epoch 694
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000017859537, Improvement: 0.00000000350754, Best Loss: 0.00000009832334 in Epoch 694
Epoch 701
Epoch 701, Loss: 0.00000017877631, Improvement: 0.00000000018094, Best Loss: 0.00000009832334 in Epoch 694
Epoch 702
Epoch 702, Loss: 0.00000017744549, Improvement: -0.00000000133082, Best Loss: 0.00000009832334 in Epoch 694
Epoch 703
Epoch 703, Loss: 0.00000017870454, Improvement: 0.00000000125905, Best Loss: 0.00000009832334 in Epoch 694
Epoch 704
Epoch 704, Loss: 0.00000017644731, Improvement: -0.00000000225723, Best Loss: 0.00000009832334 in Epoch 694
Epoch 705
Epoch 705, Loss: 0.00000017596870, Improvement: -0.00000000047861, Best Loss: 0.00000009832334 in Epoch 694
Epoch 706
Epoch 706, Loss: 0.00000019045451, Improvement: 0.00000001448580, Best Loss: 0.00000009832334 in Epoch 694
Epoch 707
Epoch 707, Loss: 0.00000035782093, Improvement: 0.00000016736642, Best Loss: 0.00000009832334 in Epoch 694
Epoch 708
Epoch 708, Loss: 0.00000030518012, Improvement: -0.00000005264081, Best Loss: 0.00000009832334 in Epoch 694
Epoch 709
Epoch 709, Loss: 0.00000023930797, Improvement: -0.00000006587214, Best Loss: 0.00000009832334 in Epoch 694
Epoch 710
Epoch 710, Loss: 0.00000019002410, Improvement: -0.00000004928387, Best Loss: 0.00000009832334 in Epoch 694
Epoch 711
Epoch 711, Loss: 0.00000017836307, Improvement: -0.00000001166103, Best Loss: 0.00000009832334 in Epoch 694
Epoch 712
Epoch 712, Loss: 0.00000017303076, Improvement: -0.00000000533231, Best Loss: 0.00000009832334 in Epoch 694
Epoch 713
Epoch 713, Loss: 0.00000017686208, Improvement: 0.00000000383132, Best Loss: 0.00000009832334 in Epoch 694
Epoch 714
Epoch 714, Loss: 0.00000017218780, Improvement: -0.00000000467428, Best Loss: 0.00000009832334 in Epoch 694
Epoch 715
Epoch 715, Loss: 0.00000017167355, Improvement: -0.00000000051425, Best Loss: 0.00000009832334 in Epoch 694
Epoch 716
Epoch 716, Loss: 0.00000017080532, Improvement: -0.00000000086823, Best Loss: 0.00000009832334 in Epoch 694
Epoch 717
Epoch 717, Loss: 0.00000017216577, Improvement: 0.00000000136044, Best Loss: 0.00000009832334 in Epoch 694
Epoch 718
Epoch 718, Loss: 0.00000017221350, Improvement: 0.00000000004773, Best Loss: 0.00000009832334 in Epoch 694
Epoch 719
Epoch 719, Loss: 0.00000017021219, Improvement: -0.00000000200131, Best Loss: 0.00000009832334 in Epoch 694
Epoch 720
Epoch 720, Loss: 0.00000017185423, Improvement: 0.00000000164204, Best Loss: 0.00000009832334 in Epoch 694
Epoch 721
Epoch 721, Loss: 0.00000017097944, Improvement: -0.00000000087479, Best Loss: 0.00000009832334 in Epoch 694
Epoch 722
Epoch 722, Loss: 0.00000017381798, Improvement: 0.00000000283854, Best Loss: 0.00000009832334 in Epoch 694
Epoch 723
Epoch 723, Loss: 0.00000018117045, Improvement: 0.00000000735247, Best Loss: 0.00000009832334 in Epoch 694
Epoch 724
Epoch 724, Loss: 0.00000018488559, Improvement: 0.00000000371514, Best Loss: 0.00000009832334 in Epoch 694
Epoch 725
Epoch 725, Loss: 0.00000022700513, Improvement: 0.00000004211954, Best Loss: 0.00000009832334 in Epoch 694
Epoch 726
Epoch 726, Loss: 0.00000026285507, Improvement: 0.00000003584994, Best Loss: 0.00000009832334 in Epoch 694
Epoch 727
Epoch 727, Loss: 0.00000021830659, Improvement: -0.00000004454848, Best Loss: 0.00000009832334 in Epoch 694
Epoch 728
Epoch 728, Loss: 0.00000019866353, Improvement: -0.00000001964306, Best Loss: 0.00000009832334 in Epoch 694
Epoch 729
Epoch 729, Loss: 0.00000029325865, Improvement: 0.00000009459512, Best Loss: 0.00000009832334 in Epoch 694
Epoch 730
Epoch 730, Loss: 0.00000021846317, Improvement: -0.00000007479548, Best Loss: 0.00000009832334 in Epoch 694
Epoch 731
Epoch 731, Loss: 0.00000018744803, Improvement: -0.00000003101513, Best Loss: 0.00000009832334 in Epoch 694
Epoch 732
Epoch 732, Loss: 0.00000017731427, Improvement: -0.00000001013377, Best Loss: 0.00000009832334 in Epoch 694
Epoch 733
Epoch 733, Loss: 0.00000017362282, Improvement: -0.00000000369144, Best Loss: 0.00000009832334 in Epoch 694
Epoch 734
Epoch 734, Loss: 0.00000023987197, Improvement: 0.00000006624915, Best Loss: 0.00000009832334 in Epoch 694
Epoch 735
Epoch 735, Loss: 0.00000022369566, Improvement: -0.00000001617631, Best Loss: 0.00000009832334 in Epoch 694
Epoch 736
Epoch 736, Loss: 0.00000024454462, Improvement: 0.00000002084896, Best Loss: 0.00000009832334 in Epoch 694
Epoch 737
Epoch 737, Loss: 0.00000027312420, Improvement: 0.00000002857958, Best Loss: 0.00000009832334 in Epoch 694
Epoch 738
Epoch 738, Loss: 0.00000027713852, Improvement: 0.00000000401432, Best Loss: 0.00000009832334 in Epoch 694
Epoch 739
Epoch 739, Loss: 0.00000028266420, Improvement: 0.00000000552567, Best Loss: 0.00000009832334 in Epoch 694
Epoch 740
Epoch 740, Loss: 0.00000022464697, Improvement: -0.00000005801722, Best Loss: 0.00000009832334 in Epoch 694
Epoch 741
Epoch 741, Loss: 0.00000023830332, Improvement: 0.00000001365635, Best Loss: 0.00000009832334 in Epoch 694
Epoch 742
Epoch 742, Loss: 0.00000020527557, Improvement: -0.00000003302775, Best Loss: 0.00000009832334 in Epoch 694
Epoch 743
Epoch 743, Loss: 0.00000022586377, Improvement: 0.00000002058820, Best Loss: 0.00000009832334 in Epoch 694
Epoch 744
Epoch 744, Loss: 0.00000019443965, Improvement: -0.00000003142413, Best Loss: 0.00000009832334 in Epoch 694
Epoch 745
Epoch 745, Loss: 0.00000018946463, Improvement: -0.00000000497502, Best Loss: 0.00000009832334 in Epoch 694
Epoch 746
Epoch 746, Loss: 0.00000021890798, Improvement: 0.00000002944335, Best Loss: 0.00000009832334 in Epoch 694
Epoch 747
Epoch 747, Loss: 0.00000017396239, Improvement: -0.00000004494559, Best Loss: 0.00000009832334 in Epoch 694
Epoch 748
Epoch 748, Loss: 0.00000016927438, Improvement: -0.00000000468801, Best Loss: 0.00000009832334 in Epoch 694
Epoch 749
Epoch 749, Loss: 0.00000016638199, Improvement: -0.00000000289240, Best Loss: 0.00000009832334 in Epoch 694
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000016586129, Improvement: -0.00000000052069, Best Loss: 0.00000009832334 in Epoch 694
Epoch 751
Epoch 751, Loss: 0.00000016639387, Improvement: 0.00000000053258, Best Loss: 0.00000009832334 in Epoch 694
Epoch 752
Epoch 752, Loss: 0.00000017844851, Improvement: 0.00000001205464, Best Loss: 0.00000009832334 in Epoch 694
Epoch 753
Epoch 753, Loss: 0.00000018362920, Improvement: 0.00000000518070, Best Loss: 0.00000009832334 in Epoch 694
Epoch 754
Epoch 754, Loss: 0.00000021425017, Improvement: 0.00000003062096, Best Loss: 0.00000009832334 in Epoch 694
Epoch 755
Epoch 755, Loss: 0.00000037336388, Improvement: 0.00000015911372, Best Loss: 0.00000009832334 in Epoch 694
Epoch 756
Epoch 756, Loss: 0.00000025538541, Improvement: -0.00000011797848, Best Loss: 0.00000009832334 in Epoch 694
Epoch 757
Epoch 757, Loss: 0.00000019650041, Improvement: -0.00000005888500, Best Loss: 0.00000009832334 in Epoch 694
Epoch 758
Epoch 758, Loss: 0.00000017403895, Improvement: -0.00000002246146, Best Loss: 0.00000009832334 in Epoch 694
Epoch 759
Epoch 759, Loss: 0.00000016892884, Improvement: -0.00000000511011, Best Loss: 0.00000009832334 in Epoch 694
Epoch 760
Epoch 760, Loss: 0.00000017544396, Improvement: 0.00000000651513, Best Loss: 0.00000009832334 in Epoch 694
Epoch 761
Epoch 761, Loss: 0.00000016695281, Improvement: -0.00000000849115, Best Loss: 0.00000009832334 in Epoch 694
Epoch 762
Epoch 762, Loss: 0.00000016372771, Improvement: -0.00000000322510, Best Loss: 0.00000009832334 in Epoch 694
Epoch 763
Epoch 763, Loss: 0.00000016287943, Improvement: -0.00000000084828, Best Loss: 0.00000009832334 in Epoch 694
Epoch 764
Epoch 764, Loss: 0.00000016551733, Improvement: 0.00000000263790, Best Loss: 0.00000009832334 in Epoch 694
Epoch 765
Epoch 765, Loss: 0.00000016431117, Improvement: -0.00000000120616, Best Loss: 0.00000009832334 in Epoch 694
Epoch 766
A best model at epoch 766 has been saved with training error 0.00000009625986.
Epoch 766, Loss: 0.00000015932608, Improvement: -0.00000000498509, Best Loss: 0.00000009625986 in Epoch 766
Epoch 767
Epoch 767, Loss: 0.00000015990790, Improvement: 0.00000000058182, Best Loss: 0.00000009625986 in Epoch 766
Epoch 768
Epoch 768, Loss: 0.00000016147371, Improvement: 0.00000000156581, Best Loss: 0.00000009625986 in Epoch 766
Epoch 769
Epoch 769, Loss: 0.00000016223882, Improvement: 0.00000000076512, Best Loss: 0.00000009625986 in Epoch 766
Epoch 770
Epoch 770, Loss: 0.00000016161235, Improvement: -0.00000000062647, Best Loss: 0.00000009625986 in Epoch 766
Epoch 771
Epoch 771, Loss: 0.00000015993924, Improvement: -0.00000000167311, Best Loss: 0.00000009625986 in Epoch 766
Epoch 772
Epoch 772, Loss: 0.00000015933490, Improvement: -0.00000000060434, Best Loss: 0.00000009625986 in Epoch 766
Epoch 773
Epoch 773, Loss: 0.00000016244088, Improvement: 0.00000000310598, Best Loss: 0.00000009625986 in Epoch 766
Epoch 774
Epoch 774, Loss: 0.00000017113560, Improvement: 0.00000000869472, Best Loss: 0.00000009625986 in Epoch 766
Epoch 775
Epoch 775, Loss: 0.00000018245191, Improvement: 0.00000001131631, Best Loss: 0.00000009625986 in Epoch 766
Epoch 776
Epoch 776, Loss: 0.00000018262919, Improvement: 0.00000000017728, Best Loss: 0.00000009625986 in Epoch 766
Epoch 777
Epoch 777, Loss: 0.00000017390652, Improvement: -0.00000000872267, Best Loss: 0.00000009625986 in Epoch 766
Epoch 778
Epoch 778, Loss: 0.00000016667066, Improvement: -0.00000000723586, Best Loss: 0.00000009625986 in Epoch 766
Epoch 779
Epoch 779, Loss: 0.00000016226570, Improvement: -0.00000000440495, Best Loss: 0.00000009625986 in Epoch 766
Epoch 780
Epoch 780, Loss: 0.00000016379062, Improvement: 0.00000000152492, Best Loss: 0.00000009625986 in Epoch 766
Epoch 781
Epoch 781, Loss: 0.00000017238052, Improvement: 0.00000000858990, Best Loss: 0.00000009625986 in Epoch 766
Epoch 782
Epoch 782, Loss: 0.00000021790623, Improvement: 0.00000004552571, Best Loss: 0.00000009625986 in Epoch 766
Epoch 783
Epoch 783, Loss: 0.00000029033256, Improvement: 0.00000007242633, Best Loss: 0.00000009625986 in Epoch 766
Epoch 784
Epoch 784, Loss: 0.00000023975072, Improvement: -0.00000005058184, Best Loss: 0.00000009625986 in Epoch 766
Epoch 785
Epoch 785, Loss: 0.00000019398607, Improvement: -0.00000004576465, Best Loss: 0.00000009625986 in Epoch 766
Epoch 786
Epoch 786, Loss: 0.00000017689851, Improvement: -0.00000001708756, Best Loss: 0.00000009625986 in Epoch 766
Epoch 787
Epoch 787, Loss: 0.00000016518658, Improvement: -0.00000001171193, Best Loss: 0.00000009625986 in Epoch 766
Epoch 788
Epoch 788, Loss: 0.00000016017345, Improvement: -0.00000000501313, Best Loss: 0.00000009625986 in Epoch 766
Epoch 789
Epoch 789, Loss: 0.00000015784802, Improvement: -0.00000000232544, Best Loss: 0.00000009625986 in Epoch 766
Epoch 790
Epoch 790, Loss: 0.00000015875172, Improvement: 0.00000000090370, Best Loss: 0.00000009625986 in Epoch 766
Epoch 791
Epoch 791, Loss: 0.00000016452731, Improvement: 0.00000000577559, Best Loss: 0.00000009625986 in Epoch 766
Epoch 792
Epoch 792, Loss: 0.00000016270313, Improvement: -0.00000000182418, Best Loss: 0.00000009625986 in Epoch 766
Epoch 793
Epoch 793, Loss: 0.00000016329014, Improvement: 0.00000000058701, Best Loss: 0.00000009625986 in Epoch 766
Epoch 794
Epoch 794, Loss: 0.00000015605524, Improvement: -0.00000000723490, Best Loss: 0.00000009625986 in Epoch 766
Epoch 795
Epoch 795, Loss: 0.00000015771865, Improvement: 0.00000000166341, Best Loss: 0.00000009625986 in Epoch 766
Epoch 796
Epoch 796, Loss: 0.00000016287180, Improvement: 0.00000000515315, Best Loss: 0.00000009625986 in Epoch 766
Epoch 797
Epoch 797, Loss: 0.00000019292614, Improvement: 0.00000003005434, Best Loss: 0.00000009625986 in Epoch 766
Epoch 798
Epoch 798, Loss: 0.00000030535472, Improvement: 0.00000011242859, Best Loss: 0.00000009625986 in Epoch 766
Epoch 799
Epoch 799, Loss: 0.00000040610330, Improvement: 0.00000010074857, Best Loss: 0.00000009625986 in Epoch 766
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000033252605, Improvement: -0.00000007357725, Best Loss: 0.00000009625986 in Epoch 766
Epoch 801
Epoch 801, Loss: 0.00000018631284, Improvement: -0.00000014621321, Best Loss: 0.00000009625986 in Epoch 766
Epoch 802
Epoch 802, Loss: 0.00000015248036, Improvement: -0.00000003383248, Best Loss: 0.00000009625986 in Epoch 766
Epoch 803
Epoch 803, Loss: 0.00000014990489, Improvement: -0.00000000257546, Best Loss: 0.00000009625986 in Epoch 766
Epoch 804
Epoch 804, Loss: 0.00000014830341, Improvement: -0.00000000160149, Best Loss: 0.00000009625986 in Epoch 766
Epoch 805
Epoch 805, Loss: 0.00000014821667, Improvement: -0.00000000008673, Best Loss: 0.00000009625986 in Epoch 766
Epoch 806
Epoch 806, Loss: 0.00000014818173, Improvement: -0.00000000003494, Best Loss: 0.00000009625986 in Epoch 766
Epoch 807
Epoch 807, Loss: 0.00000015023174, Improvement: 0.00000000205001, Best Loss: 0.00000009625986 in Epoch 766
Epoch 808
Epoch 808, Loss: 0.00000014969128, Improvement: -0.00000000054046, Best Loss: 0.00000009625986 in Epoch 766
Epoch 809
Epoch 809, Loss: 0.00000014828155, Improvement: -0.00000000140973, Best Loss: 0.00000009625986 in Epoch 766
Epoch 810
Epoch 810, Loss: 0.00000014795212, Improvement: -0.00000000032943, Best Loss: 0.00000009625986 in Epoch 766
Epoch 811
Epoch 811, Loss: 0.00000014690336, Improvement: -0.00000000104876, Best Loss: 0.00000009625986 in Epoch 766
Epoch 812
Epoch 812, Loss: 0.00000014607423, Improvement: -0.00000000082913, Best Loss: 0.00000009625986 in Epoch 766
Epoch 813
Epoch 813, Loss: 0.00000014610855, Improvement: 0.00000000003432, Best Loss: 0.00000009625986 in Epoch 766
Epoch 814
Epoch 814, Loss: 0.00000014693179, Improvement: 0.00000000082323, Best Loss: 0.00000009625986 in Epoch 766
Epoch 815
Epoch 815, Loss: 0.00000014570414, Improvement: -0.00000000122765, Best Loss: 0.00000009625986 in Epoch 766
Epoch 816
Epoch 816, Loss: 0.00000014556902, Improvement: -0.00000000013512, Best Loss: 0.00000009625986 in Epoch 766
Epoch 817
Epoch 817, Loss: 0.00000014538377, Improvement: -0.00000000018525, Best Loss: 0.00000009625986 in Epoch 766
Epoch 818
Epoch 818, Loss: 0.00000014518160, Improvement: -0.00000000020218, Best Loss: 0.00000009625986 in Epoch 766
Epoch 819
Epoch 819, Loss: 0.00000014538438, Improvement: 0.00000000020278, Best Loss: 0.00000009625986 in Epoch 766
Epoch 820
Epoch 820, Loss: 0.00000014838580, Improvement: 0.00000000300142, Best Loss: 0.00000009625986 in Epoch 766
Epoch 821
Epoch 821, Loss: 0.00000015261034, Improvement: 0.00000000422454, Best Loss: 0.00000009625986 in Epoch 766
Epoch 822
Epoch 822, Loss: 0.00000014998930, Improvement: -0.00000000262104, Best Loss: 0.00000009625986 in Epoch 766
Epoch 823
Epoch 823, Loss: 0.00000014508007, Improvement: -0.00000000490923, Best Loss: 0.00000009625986 in Epoch 766
Epoch 824
Epoch 824, Loss: 0.00000015089894, Improvement: 0.00000000581886, Best Loss: 0.00000009625986 in Epoch 766
Epoch 825
A best model at epoch 825 has been saved with training error 0.00000008162823.
Epoch 825, Loss: 0.00000014267105, Improvement: -0.00000000822789, Best Loss: 0.00000008162823 in Epoch 825
Epoch 826
Epoch 826, Loss: 0.00000014188901, Improvement: -0.00000000078204, Best Loss: 0.00000008162823 in Epoch 825
Epoch 827
Epoch 827, Loss: 0.00000014347690, Improvement: 0.00000000158789, Best Loss: 0.00000008162823 in Epoch 825
Epoch 828
Epoch 828, Loss: 0.00000014346276, Improvement: -0.00000000001414, Best Loss: 0.00000008162823 in Epoch 825
Epoch 829
Epoch 829, Loss: 0.00000017975608, Improvement: 0.00000003629332, Best Loss: 0.00000008162823 in Epoch 825
Epoch 830
Epoch 830, Loss: 0.00000019671645, Improvement: 0.00000001696037, Best Loss: 0.00000008162823 in Epoch 825
Epoch 831
Epoch 831, Loss: 0.00000024696325, Improvement: 0.00000005024680, Best Loss: 0.00000008162823 in Epoch 825
Epoch 832
Epoch 832, Loss: 0.00000026511275, Improvement: 0.00000001814950, Best Loss: 0.00000008162823 in Epoch 825
Epoch 833
Epoch 833, Loss: 0.00000022307514, Improvement: -0.00000004203760, Best Loss: 0.00000008162823 in Epoch 825
Epoch 834
Epoch 834, Loss: 0.00000018896887, Improvement: -0.00000003410627, Best Loss: 0.00000008162823 in Epoch 825
Epoch 835
Epoch 835, Loss: 0.00000016973208, Improvement: -0.00000001923679, Best Loss: 0.00000008162823 in Epoch 825
Epoch 836
Epoch 836, Loss: 0.00000018218916, Improvement: 0.00000001245708, Best Loss: 0.00000008162823 in Epoch 825
Epoch 837
Epoch 837, Loss: 0.00000015989617, Improvement: -0.00000002229300, Best Loss: 0.00000008162823 in Epoch 825
Epoch 838
Epoch 838, Loss: 0.00000014639534, Improvement: -0.00000001350083, Best Loss: 0.00000008162823 in Epoch 825
Epoch 839
Epoch 839, Loss: 0.00000014483557, Improvement: -0.00000000155977, Best Loss: 0.00000008162823 in Epoch 825
Epoch 840
Epoch 840, Loss: 0.00000015256489, Improvement: 0.00000000772932, Best Loss: 0.00000008162823 in Epoch 825
Epoch 841
Epoch 841, Loss: 0.00000014453514, Improvement: -0.00000000802974, Best Loss: 0.00000008162823 in Epoch 825
Epoch 842
Epoch 842, Loss: 0.00000013914104, Improvement: -0.00000000539410, Best Loss: 0.00000008162823 in Epoch 825
Epoch 843
Epoch 843, Loss: 0.00000013723712, Improvement: -0.00000000190392, Best Loss: 0.00000008162823 in Epoch 825
Epoch 844
A best model at epoch 844 has been saved with training error 0.00000008114030.
Epoch 844, Loss: 0.00000013503127, Improvement: -0.00000000220585, Best Loss: 0.00000008114030 in Epoch 844
Epoch 845
Epoch 845, Loss: 0.00000013398049, Improvement: -0.00000000105078, Best Loss: 0.00000008114030 in Epoch 844
Epoch 846
A best model at epoch 846 has been saved with training error 0.00000007997656.
Epoch 846, Loss: 0.00000013339767, Improvement: -0.00000000058282, Best Loss: 0.00000007997656 in Epoch 846
Epoch 847
Epoch 847, Loss: 0.00000013205560, Improvement: -0.00000000134207, Best Loss: 0.00000007997656 in Epoch 846
Epoch 848
Epoch 848, Loss: 0.00000013165848, Improvement: -0.00000000039712, Best Loss: 0.00000007997656 in Epoch 846
Epoch 849
Epoch 849, Loss: 0.00000013483221, Improvement: 0.00000000317374, Best Loss: 0.00000007997656 in Epoch 846
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000013229986, Improvement: -0.00000000253235, Best Loss: 0.00000007997656 in Epoch 846
Epoch 851
Epoch 851, Loss: 0.00000013521403, Improvement: 0.00000000291417, Best Loss: 0.00000007997656 in Epoch 846
Epoch 852
Epoch 852, Loss: 0.00000013130488, Improvement: -0.00000000390915, Best Loss: 0.00000007997656 in Epoch 846
Epoch 853
Epoch 853, Loss: 0.00000014072889, Improvement: 0.00000000942401, Best Loss: 0.00000007997656 in Epoch 846
Epoch 854
Epoch 854, Loss: 0.00000014969319, Improvement: 0.00000000896430, Best Loss: 0.00000007997656 in Epoch 846
Epoch 855
Epoch 855, Loss: 0.00000014137062, Improvement: -0.00000000832257, Best Loss: 0.00000007997656 in Epoch 846
Epoch 856
Epoch 856, Loss: 0.00000013755751, Improvement: -0.00000000381311, Best Loss: 0.00000007997656 in Epoch 846
Epoch 857
Epoch 857, Loss: 0.00000014050482, Improvement: 0.00000000294731, Best Loss: 0.00000007997656 in Epoch 846
Epoch 858
Epoch 858, Loss: 0.00000022605617, Improvement: 0.00000008555135, Best Loss: 0.00000007997656 in Epoch 846
Epoch 859
Epoch 859, Loss: 0.00000022621129, Improvement: 0.00000000015512, Best Loss: 0.00000007997656 in Epoch 846
Epoch 860
Epoch 860, Loss: 0.00000015319943, Improvement: -0.00000007301186, Best Loss: 0.00000007997656 in Epoch 846
Epoch 861
A best model at epoch 861 has been saved with training error 0.00000007754226.
Epoch 861, Loss: 0.00000013697651, Improvement: -0.00000001622292, Best Loss: 0.00000007754226 in Epoch 861
Epoch 862
Epoch 862, Loss: 0.00000013239346, Improvement: -0.00000000458305, Best Loss: 0.00000007754226 in Epoch 861
Epoch 863
Epoch 863, Loss: 0.00000013164259, Improvement: -0.00000000075087, Best Loss: 0.00000007754226 in Epoch 861
Epoch 864
Epoch 864, Loss: 0.00000012606717, Improvement: -0.00000000557543, Best Loss: 0.00000007754226 in Epoch 861
Epoch 865
Epoch 865, Loss: 0.00000012794721, Improvement: 0.00000000188005, Best Loss: 0.00000007754226 in Epoch 861
Epoch 866
Epoch 866, Loss: 0.00000012433661, Improvement: -0.00000000361060, Best Loss: 0.00000007754226 in Epoch 861
Epoch 867
Epoch 867, Loss: 0.00000012293775, Improvement: -0.00000000139886, Best Loss: 0.00000007754226 in Epoch 861
Epoch 868
Epoch 868, Loss: 0.00000013528532, Improvement: 0.00000001234757, Best Loss: 0.00000007754226 in Epoch 861
Epoch 869
Epoch 869, Loss: 0.00000014488128, Improvement: 0.00000000959596, Best Loss: 0.00000007754226 in Epoch 861
Epoch 870
Epoch 870, Loss: 0.00000015584455, Improvement: 0.00000001096327, Best Loss: 0.00000007754226 in Epoch 861
Epoch 871
Epoch 871, Loss: 0.00000016414633, Improvement: 0.00000000830178, Best Loss: 0.00000007754226 in Epoch 861
Epoch 872
Epoch 872, Loss: 0.00000014432776, Improvement: -0.00000001981856, Best Loss: 0.00000007754226 in Epoch 861
Epoch 873
Epoch 873, Loss: 0.00000014079936, Improvement: -0.00000000352840, Best Loss: 0.00000007754226 in Epoch 861
Epoch 874
Epoch 874, Loss: 0.00000012122518, Improvement: -0.00000001957418, Best Loss: 0.00000007754226 in Epoch 861
Epoch 875
Epoch 875, Loss: 0.00000012560313, Improvement: 0.00000000437795, Best Loss: 0.00000007754226 in Epoch 861
Epoch 876
Epoch 876, Loss: 0.00000012774676, Improvement: 0.00000000214363, Best Loss: 0.00000007754226 in Epoch 861
Epoch 877
Epoch 877, Loss: 0.00000013263319, Improvement: 0.00000000488643, Best Loss: 0.00000007754226 in Epoch 861
Epoch 878
Epoch 878, Loss: 0.00000012579597, Improvement: -0.00000000683721, Best Loss: 0.00000007754226 in Epoch 861
Epoch 879
A best model at epoch 879 has been saved with training error 0.00000007080897.
Epoch 879, Loss: 0.00000011752967, Improvement: -0.00000000826630, Best Loss: 0.00000007080897 in Epoch 879
Epoch 880
Epoch 880, Loss: 0.00000012508785, Improvement: 0.00000000755818, Best Loss: 0.00000007080897 in Epoch 879
Epoch 881
Epoch 881, Loss: 0.00000011888811, Improvement: -0.00000000619974, Best Loss: 0.00000007080897 in Epoch 879
Epoch 882
Epoch 882, Loss: 0.00000012547686, Improvement: 0.00000000658876, Best Loss: 0.00000007080897 in Epoch 879
Epoch 883
Epoch 883, Loss: 0.00000020031420, Improvement: 0.00000007483734, Best Loss: 0.00000007080897 in Epoch 879
Epoch 884
Epoch 884, Loss: 0.00000023794185, Improvement: 0.00000003762765, Best Loss: 0.00000007080897 in Epoch 879
Epoch 885
Epoch 885, Loss: 0.00000017841755, Improvement: -0.00000005952429, Best Loss: 0.00000007080897 in Epoch 879
Epoch 886
Epoch 886, Loss: 0.00000012800729, Improvement: -0.00000005041026, Best Loss: 0.00000007080897 in Epoch 879
Epoch 887
Epoch 887, Loss: 0.00000012132000, Improvement: -0.00000000668729, Best Loss: 0.00000007080897 in Epoch 879
Epoch 888
Epoch 888, Loss: 0.00000010811228, Improvement: -0.00000001320772, Best Loss: 0.00000007080897 in Epoch 879
Epoch 889
A best model at epoch 889 has been saved with training error 0.00000006933280.
Epoch 889, Loss: 0.00000010544258, Improvement: -0.00000000266970, Best Loss: 0.00000006933280 in Epoch 889
Epoch 890
Epoch 890, Loss: 0.00000010865655, Improvement: 0.00000000321397, Best Loss: 0.00000006933280 in Epoch 889
Epoch 891
Epoch 891, Loss: 0.00000010826036, Improvement: -0.00000000039618, Best Loss: 0.00000006933280 in Epoch 889
Epoch 892
Epoch 892, Loss: 0.00000010152339, Improvement: -0.00000000673697, Best Loss: 0.00000006933280 in Epoch 889
Epoch 893
A best model at epoch 893 has been saved with training error 0.00000006539928.
Epoch 893, Loss: 0.00000009822239, Improvement: -0.00000000330101, Best Loss: 0.00000006539928 in Epoch 893
Epoch 894
Epoch 894, Loss: 0.00000009930533, Improvement: 0.00000000108294, Best Loss: 0.00000006539928 in Epoch 893
Epoch 895
Epoch 895, Loss: 0.00000010059752, Improvement: 0.00000000129219, Best Loss: 0.00000006539928 in Epoch 893
Epoch 896
Epoch 896, Loss: 0.00000009853659, Improvement: -0.00000000206093, Best Loss: 0.00000006539928 in Epoch 893
Epoch 897
A best model at epoch 897 has been saved with training error 0.00000006399608.
Epoch 897, Loss: 0.00000010484322, Improvement: 0.00000000630662, Best Loss: 0.00000006399608 in Epoch 897
Epoch 898
Epoch 898, Loss: 0.00000010452203, Improvement: -0.00000000032119, Best Loss: 0.00000006399608 in Epoch 897
Epoch 899
Epoch 899, Loss: 0.00000010265161, Improvement: -0.00000000187042, Best Loss: 0.00000006399608 in Epoch 897
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000015831717, Improvement: 0.00000005566556, Best Loss: 0.00000006399608 in Epoch 897
Epoch 901
Epoch 901, Loss: 0.00000020176723, Improvement: 0.00000004345006, Best Loss: 0.00000006399608 in Epoch 897
Epoch 902
Epoch 902, Loss: 0.00000033845955, Improvement: 0.00000013669232, Best Loss: 0.00000006399608 in Epoch 897
Epoch 903
Epoch 903, Loss: 0.00000019142547, Improvement: -0.00000014703408, Best Loss: 0.00000006399608 in Epoch 897
Epoch 904
Epoch 904, Loss: 0.00000014237725, Improvement: -0.00000004904822, Best Loss: 0.00000006399608 in Epoch 897
Epoch 905
Epoch 905, Loss: 0.00000010641301, Improvement: -0.00000003596424, Best Loss: 0.00000006399608 in Epoch 897
Epoch 906
Epoch 906, Loss: 0.00000009914892, Improvement: -0.00000000726409, Best Loss: 0.00000006399608 in Epoch 897
Epoch 907
Epoch 907, Loss: 0.00000009018699, Improvement: -0.00000000896192, Best Loss: 0.00000006399608 in Epoch 897
Epoch 908
Epoch 908, Loss: 0.00000008941269, Improvement: -0.00000000077430, Best Loss: 0.00000006399608 in Epoch 897
Epoch 909
Epoch 909, Loss: 0.00000009051760, Improvement: 0.00000000110491, Best Loss: 0.00000006399608 in Epoch 897
Epoch 910
A best model at epoch 910 has been saved with training error 0.00000005446534.
Epoch 910, Loss: 0.00000008885158, Improvement: -0.00000000166601, Best Loss: 0.00000005446534 in Epoch 910
Epoch 911
Epoch 911, Loss: 0.00000008730213, Improvement: -0.00000000154945, Best Loss: 0.00000005446534 in Epoch 910
Epoch 912
Epoch 912, Loss: 0.00000008342983, Improvement: -0.00000000387230, Best Loss: 0.00000005446534 in Epoch 910
Epoch 913
Epoch 913, Loss: 0.00000008280959, Improvement: -0.00000000062024, Best Loss: 0.00000005446534 in Epoch 910
Epoch 914
Epoch 914, Loss: 0.00000008287675, Improvement: 0.00000000006715, Best Loss: 0.00000005446534 in Epoch 910
Epoch 915
Epoch 915, Loss: 0.00000007984046, Improvement: -0.00000000303629, Best Loss: 0.00000005446534 in Epoch 910
Epoch 916
Epoch 916, Loss: 0.00000007948350, Improvement: -0.00000000035696, Best Loss: 0.00000005446534 in Epoch 910
Epoch 917
Epoch 917, Loss: 0.00000007780800, Improvement: -0.00000000167550, Best Loss: 0.00000005446534 in Epoch 910
Epoch 918
Epoch 918, Loss: 0.00000007565710, Improvement: -0.00000000215090, Best Loss: 0.00000005446534 in Epoch 910
Epoch 919
Epoch 919, Loss: 0.00000007763940, Improvement: 0.00000000198230, Best Loss: 0.00000005446534 in Epoch 910
Epoch 920
Epoch 920, Loss: 0.00000007667063, Improvement: -0.00000000096877, Best Loss: 0.00000005446534 in Epoch 910
Epoch 921
A best model at epoch 921 has been saved with training error 0.00000005228411.
Epoch 921, Loss: 0.00000007655598, Improvement: -0.00000000011466, Best Loss: 0.00000005228411 in Epoch 921
Epoch 922
Epoch 922, Loss: 0.00000007799509, Improvement: 0.00000000143911, Best Loss: 0.00000005228411 in Epoch 921
Epoch 923
Epoch 923, Loss: 0.00000007837246, Improvement: 0.00000000037738, Best Loss: 0.00000005228411 in Epoch 921
Epoch 924
A best model at epoch 924 has been saved with training error 0.00000005218746.
A best model at epoch 924 has been saved with training error 0.00000004843888.
Epoch 924, Loss: 0.00000007159418, Improvement: -0.00000000677829, Best Loss: 0.00000004843888 in Epoch 924
Epoch 925
A best model at epoch 925 has been saved with training error 0.00000004326114.
Epoch 925, Loss: 0.00000006877051, Improvement: -0.00000000282366, Best Loss: 0.00000004326114 in Epoch 925
Epoch 926
A best model at epoch 926 has been saved with training error 0.00000004022585.
Epoch 926, Loss: 0.00000006943913, Improvement: 0.00000000066862, Best Loss: 0.00000004022585 in Epoch 926
Epoch 927
Epoch 927, Loss: 0.00000007182778, Improvement: 0.00000000238864, Best Loss: 0.00000004022585 in Epoch 926
Epoch 928
Epoch 928, Loss: 0.00000007062206, Improvement: -0.00000000120571, Best Loss: 0.00000004022585 in Epoch 926
Epoch 929
Epoch 929, Loss: 0.00000006944161, Improvement: -0.00000000118045, Best Loss: 0.00000004022585 in Epoch 926
Epoch 930
Epoch 930, Loss: 0.00000007598100, Improvement: 0.00000000653939, Best Loss: 0.00000004022585 in Epoch 926
Epoch 931
Epoch 931, Loss: 0.00000009375417, Improvement: 0.00000001777317, Best Loss: 0.00000004022585 in Epoch 926
Epoch 932
Epoch 932, Loss: 0.00000013829652, Improvement: 0.00000004454235, Best Loss: 0.00000004022585 in Epoch 926
Epoch 933
Epoch 933, Loss: 0.00000015534987, Improvement: 0.00000001705334, Best Loss: 0.00000004022585 in Epoch 926
Epoch 934
Epoch 934, Loss: 0.00000009360387, Improvement: -0.00000006174600, Best Loss: 0.00000004022585 in Epoch 926
Epoch 935
Epoch 935, Loss: 0.00000006908913, Improvement: -0.00000002451474, Best Loss: 0.00000004022585 in Epoch 926
Epoch 936
Epoch 936, Loss: 0.00000006202581, Improvement: -0.00000000706332, Best Loss: 0.00000004022585 in Epoch 926
Epoch 937
Epoch 937, Loss: 0.00000006622652, Improvement: 0.00000000420071, Best Loss: 0.00000004022585 in Epoch 926
Epoch 938
Epoch 938, Loss: 0.00000006498490, Improvement: -0.00000000124162, Best Loss: 0.00000004022585 in Epoch 926
Epoch 939
Epoch 939, Loss: 0.00000006349473, Improvement: -0.00000000149017, Best Loss: 0.00000004022585 in Epoch 926
Epoch 940
Epoch 940, Loss: 0.00000005767914, Improvement: -0.00000000581558, Best Loss: 0.00000004022585 in Epoch 926
Epoch 941
Epoch 941, Loss: 0.00000005826619, Improvement: 0.00000000058705, Best Loss: 0.00000004022585 in Epoch 926
Epoch 942
Epoch 942, Loss: 0.00000005840965, Improvement: 0.00000000014346, Best Loss: 0.00000004022585 in Epoch 926
Epoch 943
Epoch 943, Loss: 0.00000005812928, Improvement: -0.00000000028037, Best Loss: 0.00000004022585 in Epoch 926
Epoch 944
Epoch 944, Loss: 0.00000005844295, Improvement: 0.00000000031367, Best Loss: 0.00000004022585 in Epoch 926
Epoch 945
Epoch 945, Loss: 0.00000010386885, Improvement: 0.00000004542590, Best Loss: 0.00000004022585 in Epoch 926
Epoch 946
Epoch 946, Loss: 0.00000016215716, Improvement: 0.00000005828831, Best Loss: 0.00000004022585 in Epoch 926
Epoch 947
Epoch 947, Loss: 0.00000010084420, Improvement: -0.00000006131296, Best Loss: 0.00000004022585 in Epoch 926
Epoch 948
Epoch 948, Loss: 0.00000007314573, Improvement: -0.00000002769847, Best Loss: 0.00000004022585 in Epoch 926
Epoch 949
Epoch 949, Loss: 0.00000006310327, Improvement: -0.00000001004246, Best Loss: 0.00000004022585 in Epoch 926
Epoch 950
A best model at epoch 950 has been saved with training error 0.00000003839511.
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000005381085, Improvement: -0.00000000929242, Best Loss: 0.00000003839511 in Epoch 950
Epoch 951
Epoch 951, Loss: 0.00000005321913, Improvement: -0.00000000059171, Best Loss: 0.00000003839511 in Epoch 950
Epoch 952
A best model at epoch 952 has been saved with training error 0.00000003792505.
Epoch 952, Loss: 0.00000005300474, Improvement: -0.00000000021439, Best Loss: 0.00000003792505 in Epoch 952
Epoch 953
Epoch 953, Loss: 0.00000005326821, Improvement: 0.00000000026347, Best Loss: 0.00000003792505 in Epoch 952
Epoch 954
Epoch 954, Loss: 0.00000005860149, Improvement: 0.00000000533328, Best Loss: 0.00000003792505 in Epoch 952
Epoch 955
A best model at epoch 955 has been saved with training error 0.00000003542724.
Epoch 955, Loss: 0.00000005436713, Improvement: -0.00000000423436, Best Loss: 0.00000003542724 in Epoch 955
Epoch 956
Epoch 956, Loss: 0.00000005892664, Improvement: 0.00000000455951, Best Loss: 0.00000003542724 in Epoch 955
Epoch 957
Epoch 957, Loss: 0.00000006766907, Improvement: 0.00000000874243, Best Loss: 0.00000003542724 in Epoch 955
Epoch 958
Epoch 958, Loss: 0.00000007786025, Improvement: 0.00000001019117, Best Loss: 0.00000003542724 in Epoch 955
Epoch 959
Epoch 959, Loss: 0.00000006307097, Improvement: -0.00000001478928, Best Loss: 0.00000003542724 in Epoch 955
Epoch 960
Epoch 960, Loss: 0.00000006885066, Improvement: 0.00000000577969, Best Loss: 0.00000003542724 in Epoch 955
Epoch 961
Epoch 961, Loss: 0.00000008206663, Improvement: 0.00000001321597, Best Loss: 0.00000003542724 in Epoch 955
Epoch 962
A best model at epoch 962 has been saved with training error 0.00000003334125.
Epoch 962, Loss: 0.00000005610863, Improvement: -0.00000002595800, Best Loss: 0.00000003334125 in Epoch 962
Epoch 963
Epoch 963, Loss: 0.00000005771487, Improvement: 0.00000000160624, Best Loss: 0.00000003334125 in Epoch 962
Epoch 964
Epoch 964, Loss: 0.00000005318808, Improvement: -0.00000000452679, Best Loss: 0.00000003334125 in Epoch 962
Epoch 965
Epoch 965, Loss: 0.00000005304101, Improvement: -0.00000000014707, Best Loss: 0.00000003334125 in Epoch 962
Epoch 966
Epoch 966, Loss: 0.00000004925291, Improvement: -0.00000000378810, Best Loss: 0.00000003334125 in Epoch 962
Epoch 967
Epoch 967, Loss: 0.00000005139285, Improvement: 0.00000000213994, Best Loss: 0.00000003334125 in Epoch 962
Epoch 968
Epoch 968, Loss: 0.00000006546043, Improvement: 0.00000001406758, Best Loss: 0.00000003334125 in Epoch 962
Epoch 969
Epoch 969, Loss: 0.00000010522795, Improvement: 0.00000003976752, Best Loss: 0.00000003334125 in Epoch 962
Epoch 970
Epoch 970, Loss: 0.00000011903456, Improvement: 0.00000001380662, Best Loss: 0.00000003334125 in Epoch 962
Epoch 971
Epoch 971, Loss: 0.00000016332330, Improvement: 0.00000004428874, Best Loss: 0.00000003334125 in Epoch 962
Epoch 972
Epoch 972, Loss: 0.00000013796045, Improvement: -0.00000002536285, Best Loss: 0.00000003334125 in Epoch 962
Epoch 973
Epoch 973, Loss: 0.00000007321769, Improvement: -0.00000006474276, Best Loss: 0.00000003334125 in Epoch 962
Epoch 974
Epoch 974, Loss: 0.00000005180364, Improvement: -0.00000002141405, Best Loss: 0.00000003334125 in Epoch 962
Epoch 975
Epoch 975, Loss: 0.00000004700508, Improvement: -0.00000000479856, Best Loss: 0.00000003334125 in Epoch 962
Epoch 976
A best model at epoch 976 has been saved with training error 0.00000003178698.
Epoch 976, Loss: 0.00000004342762, Improvement: -0.00000000357746, Best Loss: 0.00000003178698 in Epoch 976
Epoch 977
A best model at epoch 977 has been saved with training error 0.00000002972323.
Epoch 977, Loss: 0.00000004023104, Improvement: -0.00000000319658, Best Loss: 0.00000002972323 in Epoch 977
Epoch 978
A best model at epoch 978 has been saved with training error 0.00000002866231.
Epoch 978, Loss: 0.00000003927623, Improvement: -0.00000000095481, Best Loss: 0.00000002866231 in Epoch 978
Epoch 979
Epoch 979, Loss: 0.00000004382329, Improvement: 0.00000000454706, Best Loss: 0.00000002866231 in Epoch 978
Epoch 980
Epoch 980, Loss: 0.00000004310410, Improvement: -0.00000000071918, Best Loss: 0.00000002866231 in Epoch 978
Epoch 981
Epoch 981, Loss: 0.00000004658298, Improvement: 0.00000000347887, Best Loss: 0.00000002866231 in Epoch 978
Epoch 982
Epoch 982, Loss: 0.00000004129652, Improvement: -0.00000000528646, Best Loss: 0.00000002866231 in Epoch 978
Epoch 983
Epoch 983, Loss: 0.00000004053288, Improvement: -0.00000000076364, Best Loss: 0.00000002866231 in Epoch 978
Epoch 984
A best model at epoch 984 has been saved with training error 0.00000002552861.
Epoch 984, Loss: 0.00000004272451, Improvement: 0.00000000219163, Best Loss: 0.00000002552861 in Epoch 984
Epoch 985
Epoch 985, Loss: 0.00000005365104, Improvement: 0.00000001092653, Best Loss: 0.00000002552861 in Epoch 984
Epoch 986
Epoch 986, Loss: 0.00000004929975, Improvement: -0.00000000435129, Best Loss: 0.00000002552861 in Epoch 984
Epoch 987
Epoch 987, Loss: 0.00000004500804, Improvement: -0.00000000429171, Best Loss: 0.00000002552861 in Epoch 984
Epoch 988
A best model at epoch 988 has been saved with training error 0.00000002467863.
Epoch 988, Loss: 0.00000004284824, Improvement: -0.00000000215980, Best Loss: 0.00000002467863 in Epoch 988
Epoch 989
Epoch 989, Loss: 0.00000004013182, Improvement: -0.00000000271642, Best Loss: 0.00000002467863 in Epoch 988
Epoch 990
A best model at epoch 990 has been saved with training error 0.00000002405389.
Epoch 990, Loss: 0.00000003689288, Improvement: -0.00000000323894, Best Loss: 0.00000002405389 in Epoch 990
Epoch 991
Epoch 991, Loss: 0.00000003869959, Improvement: 0.00000000180671, Best Loss: 0.00000002405389 in Epoch 990
Epoch 992
Epoch 992, Loss: 0.00000005498655, Improvement: 0.00000001628696, Best Loss: 0.00000002405389 in Epoch 990
Epoch 993
Epoch 993, Loss: 0.00000004680207, Improvement: -0.00000000818448, Best Loss: 0.00000002405389 in Epoch 990
Epoch 994
Epoch 994, Loss: 0.00000004866172, Improvement: 0.00000000185965, Best Loss: 0.00000002405389 in Epoch 990
Epoch 995
Epoch 995, Loss: 0.00000005253677, Improvement: 0.00000000387505, Best Loss: 0.00000002405389 in Epoch 990
Epoch 996
Epoch 996, Loss: 0.00000010545491, Improvement: 0.00000005291814, Best Loss: 0.00000002405389 in Epoch 990
Epoch 997
Epoch 997, Loss: 0.00000009133521, Improvement: -0.00000001411971, Best Loss: 0.00000002405389 in Epoch 990
Epoch 998
Epoch 998, Loss: 0.00000004830703, Improvement: -0.00000004302818, Best Loss: 0.00000002405389 in Epoch 990
Epoch 999
Epoch 999, Loss: 0.00000003901858, Improvement: -0.00000000928845, Best Loss: 0.00000002405389 in Epoch 990
Epoch 1000
A best model at epoch 1000 has been saved with training error 0.00000002358668.
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000003802039, Improvement: -0.00000000099819, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1001
Epoch 1001, Loss: 0.00000003799785, Improvement: -0.00000000002254, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1002
Epoch 1002, Loss: 0.00000003645366, Improvement: -0.00000000154419, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1003
Epoch 1003, Loss: 0.00000003576540, Improvement: -0.00000000068826, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1004
Epoch 1004, Loss: 0.00000003645422, Improvement: 0.00000000068882, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1005
Epoch 1005, Loss: 0.00000003632868, Improvement: -0.00000000012554, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1006
Epoch 1006, Loss: 0.00000003788599, Improvement: 0.00000000155731, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1007
Epoch 1007, Loss: 0.00000004108505, Improvement: 0.00000000319907, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1008
Epoch 1008, Loss: 0.00000003659837, Improvement: -0.00000000448668, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1009
Epoch 1009, Loss: 0.00000003696742, Improvement: 0.00000000036905, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1010
Epoch 1010, Loss: 0.00000004382267, Improvement: 0.00000000685525, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1011
Epoch 1011, Loss: 0.00000005798883, Improvement: 0.00000001416616, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1012
Epoch 1012, Loss: 0.00000005243676, Improvement: -0.00000000555207, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1013
Epoch 1013, Loss: 0.00000007905936, Improvement: 0.00000002662259, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1014
Epoch 1014, Loss: 0.00000009825697, Improvement: 0.00000001919762, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1015
Epoch 1015, Loss: 0.00000012151557, Improvement: 0.00000002325859, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1016
Epoch 1016, Loss: 0.00000007592945, Improvement: -0.00000004558612, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1017
Epoch 1017, Loss: 0.00000004627696, Improvement: -0.00000002965249, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1018
Epoch 1018, Loss: 0.00000004653910, Improvement: 0.00000000026214, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1019
Epoch 1019, Loss: 0.00000005249870, Improvement: 0.00000000595960, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1020
Epoch 1020, Loss: 0.00000004167081, Improvement: -0.00000001082789, Best Loss: 0.00000002358668 in Epoch 1000
Epoch 1021
A best model at epoch 1021 has been saved with training error 0.00000002334215.
Epoch 1021, Loss: 0.00000003550040, Improvement: -0.00000000617041, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1022
Epoch 1022, Loss: 0.00000003959198, Improvement: 0.00000000409157, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1023
Epoch 1023, Loss: 0.00000003663911, Improvement: -0.00000000295286, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1024
Epoch 1024, Loss: 0.00000003435072, Improvement: -0.00000000228840, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1025
Epoch 1025, Loss: 0.00000005025989, Improvement: 0.00000001590917, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1026
Epoch 1026, Loss: 0.00000014056083, Improvement: 0.00000009030094, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1027
Epoch 1027, Loss: 0.00000012993416, Improvement: -0.00000001062666, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1028
Epoch 1028, Loss: 0.00000006027437, Improvement: -0.00000006965980, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1029
Epoch 1029, Loss: 0.00000004222862, Improvement: -0.00000001804574, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1030
Epoch 1030, Loss: 0.00000003599206, Improvement: -0.00000000623657, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1031
Epoch 1031, Loss: 0.00000003263489, Improvement: -0.00000000335717, Best Loss: 0.00000002334215 in Epoch 1021
Epoch 1032
A best model at epoch 1032 has been saved with training error 0.00000002247019.
Epoch 1032, Loss: 0.00000003235617, Improvement: -0.00000000027872, Best Loss: 0.00000002247019 in Epoch 1032
Epoch 1033
A best model at epoch 1033 has been saved with training error 0.00000002110016.
Epoch 1033, Loss: 0.00000003102791, Improvement: -0.00000000132827, Best Loss: 0.00000002110016 in Epoch 1033
Epoch 1034
Epoch 1034, Loss: 0.00000003148226, Improvement: 0.00000000045435, Best Loss: 0.00000002110016 in Epoch 1033
Epoch 1035
Epoch 1035, Loss: 0.00000003407905, Improvement: 0.00000000259679, Best Loss: 0.00000002110016 in Epoch 1033
Epoch 1036
Epoch 1036, Loss: 0.00000003451441, Improvement: 0.00000000043536, Best Loss: 0.00000002110016 in Epoch 1033
Epoch 1037
Epoch 1037, Loss: 0.00000003502510, Improvement: 0.00000000051070, Best Loss: 0.00000002110016 in Epoch 1033
Epoch 1038
A best model at epoch 1038 has been saved with training error 0.00000001712072.
Epoch 1038, Loss: 0.00000003408699, Improvement: -0.00000000093811, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1039
Epoch 1039, Loss: 0.00000003180250, Improvement: -0.00000000228450, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1040
Epoch 1040, Loss: 0.00000003191783, Improvement: 0.00000000011533, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1041
Epoch 1041, Loss: 0.00000003190777, Improvement: -0.00000000001006, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1042
Epoch 1042, Loss: 0.00000003222387, Improvement: 0.00000000031610, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1043
Epoch 1043, Loss: 0.00000003356352, Improvement: 0.00000000133965, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1044
Epoch 1044, Loss: 0.00000003317699, Improvement: -0.00000000038653, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1045
Epoch 1045, Loss: 0.00000003441040, Improvement: 0.00000000123340, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1046
Epoch 1046, Loss: 0.00000004566638, Improvement: 0.00000001125599, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1047
Epoch 1047, Loss: 0.00000004186829, Improvement: -0.00000000379809, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1048
Epoch 1048, Loss: 0.00000004529036, Improvement: 0.00000000342207, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1049
Epoch 1049, Loss: 0.00000005130778, Improvement: 0.00000000601742, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000006267838, Improvement: 0.00000001137060, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1051
Epoch 1051, Loss: 0.00000008928680, Improvement: 0.00000002660842, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1052
Epoch 1052, Loss: 0.00000006976982, Improvement: -0.00000001951698, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1053
Epoch 1053, Loss: 0.00000005215718, Improvement: -0.00000001761265, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1054
Epoch 1054, Loss: 0.00000004242198, Improvement: -0.00000000973520, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1055
Epoch 1055, Loss: 0.00000005655148, Improvement: 0.00000001412950, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1056
Epoch 1056, Loss: 0.00000006643555, Improvement: 0.00000000988407, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1057
Epoch 1057, Loss: 0.00000004748816, Improvement: -0.00000001894738, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1058
Epoch 1058, Loss: 0.00000004662342, Improvement: -0.00000000086475, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1059
Epoch 1059, Loss: 0.00000005378982, Improvement: 0.00000000716640, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1060
Epoch 1060, Loss: 0.00000004466972, Improvement: -0.00000000912010, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1061
Epoch 1061, Loss: 0.00000005140796, Improvement: 0.00000000673824, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1062
Epoch 1062, Loss: 0.00000004950464, Improvement: -0.00000000190332, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1063
Epoch 1063, Loss: 0.00000003833988, Improvement: -0.00000001116476, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1064
Epoch 1064, Loss: 0.00000003432171, Improvement: -0.00000000401818, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1065
Epoch 1065, Loss: 0.00000003012581, Improvement: -0.00000000419590, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1066
Epoch 1066, Loss: 0.00000003012374, Improvement: -0.00000000000207, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1067
Epoch 1067, Loss: 0.00000003210779, Improvement: 0.00000000198405, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1068
Epoch 1068, Loss: 0.00000004931913, Improvement: 0.00000001721134, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1069
Epoch 1069, Loss: 0.00000005027462, Improvement: 0.00000000095550, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1070
Epoch 1070, Loss: 0.00000005787743, Improvement: 0.00000000760281, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1071
Epoch 1071, Loss: 0.00000005155263, Improvement: -0.00000000632480, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1072
Epoch 1072, Loss: 0.00000004880732, Improvement: -0.00000000274531, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1073
Epoch 1073, Loss: 0.00000007049045, Improvement: 0.00000002168313, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1074
Epoch 1074, Loss: 0.00000007202271, Improvement: 0.00000000153226, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1075
Epoch 1075, Loss: 0.00000011478649, Improvement: 0.00000004276378, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1076
Epoch 1076, Loss: 0.00000013041648, Improvement: 0.00000001562998, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1077
Epoch 1077, Loss: 0.00000007456330, Improvement: -0.00000005585318, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1078
Epoch 1078, Loss: 0.00000004437874, Improvement: -0.00000003018456, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1079
Epoch 1079, Loss: 0.00000003413282, Improvement: -0.00000001024591, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1080
Epoch 1080, Loss: 0.00000003179781, Improvement: -0.00000000233501, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1081
Epoch 1081, Loss: 0.00000003061674, Improvement: -0.00000000118108, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1082
Epoch 1082, Loss: 0.00000002792305, Improvement: -0.00000000269368, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1083
Epoch 1083, Loss: 0.00000002760136, Improvement: -0.00000000032169, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1084
Epoch 1084, Loss: 0.00000003022443, Improvement: 0.00000000262307, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1085
Epoch 1085, Loss: 0.00000003198291, Improvement: 0.00000000175848, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1086
Epoch 1086, Loss: 0.00000003076207, Improvement: -0.00000000122084, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1087
Epoch 1087, Loss: 0.00000003160852, Improvement: 0.00000000084645, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1088
Epoch 1088, Loss: 0.00000003312553, Improvement: 0.00000000151701, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1089
Epoch 1089, Loss: 0.00000003542034, Improvement: 0.00000000229480, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1090
Epoch 1090, Loss: 0.00000003087715, Improvement: -0.00000000454319, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1091
Epoch 1091, Loss: 0.00000002817931, Improvement: -0.00000000269784, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1092
Epoch 1092, Loss: 0.00000002923218, Improvement: 0.00000000105288, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1093
Epoch 1093, Loss: 0.00000003126110, Improvement: 0.00000000202892, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1094
Epoch 1094, Loss: 0.00000003598464, Improvement: 0.00000000472353, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1095
Epoch 1095, Loss: 0.00000003686092, Improvement: 0.00000000087629, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1096
Epoch 1096, Loss: 0.00000003677933, Improvement: -0.00000000008159, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1097
Epoch 1097, Loss: 0.00000006849231, Improvement: 0.00000003171298, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1098
Epoch 1098, Loss: 0.00000011498070, Improvement: 0.00000004648839, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1099
Epoch 1099, Loss: 0.00000011304373, Improvement: -0.00000000193698, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000009663666, Improvement: -0.00000001640706, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1101
Epoch 1101, Loss: 0.00000007435029, Improvement: -0.00000002228637, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1102
Epoch 1102, Loss: 0.00000004798808, Improvement: -0.00000002636221, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1103
Epoch 1103, Loss: 0.00000005601022, Improvement: 0.00000000802214, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1104
Epoch 1104, Loss: 0.00000005694867, Improvement: 0.00000000093846, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1105
Epoch 1105, Loss: 0.00000005116125, Improvement: -0.00000000578742, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1106
Epoch 1106, Loss: 0.00000003535252, Improvement: -0.00000001580873, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1107
Epoch 1107, Loss: 0.00000002953847, Improvement: -0.00000000581406, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1108
Epoch 1108, Loss: 0.00000002784295, Improvement: -0.00000000169552, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1109
Epoch 1109, Loss: 0.00000002739008, Improvement: -0.00000000045288, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1110
Epoch 1110, Loss: 0.00000002680135, Improvement: -0.00000000058873, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1111
Epoch 1111, Loss: 0.00000002641752, Improvement: -0.00000000038383, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1112
Epoch 1112, Loss: 0.00000002759756, Improvement: 0.00000000118004, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1113
Epoch 1113, Loss: 0.00000003360454, Improvement: 0.00000000600698, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1114
Epoch 1114, Loss: 0.00000005170511, Improvement: 0.00000001810057, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1115
Epoch 1115, Loss: 0.00000003664257, Improvement: -0.00000001506254, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1116
Epoch 1116, Loss: 0.00000003204914, Improvement: -0.00000000459342, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1117
Epoch 1117, Loss: 0.00000002961886, Improvement: -0.00000000243028, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1118
Epoch 1118, Loss: 0.00000003533409, Improvement: 0.00000000571523, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1119
Epoch 1119, Loss: 0.00000003272076, Improvement: -0.00000000261333, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1120
Epoch 1120, Loss: 0.00000002910552, Improvement: -0.00000000361525, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1121
Epoch 1121, Loss: 0.00000003842630, Improvement: 0.00000000932078, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1122
Epoch 1122, Loss: 0.00000006219589, Improvement: 0.00000002376959, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1123
Epoch 1123, Loss: 0.00000008896684, Improvement: 0.00000002677095, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1124
Epoch 1124, Loss: 0.00000006839128, Improvement: -0.00000002057556, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1125
Epoch 1125, Loss: 0.00000005361074, Improvement: -0.00000001478054, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1126
Epoch 1126, Loss: 0.00000003910565, Improvement: -0.00000001450509, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1127
Epoch 1127, Loss: 0.00000003722832, Improvement: -0.00000000187733, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1128
Epoch 1128, Loss: 0.00000003012869, Improvement: -0.00000000709963, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1129
Epoch 1129, Loss: 0.00000002982687, Improvement: -0.00000000030182, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1130
Epoch 1130, Loss: 0.00000002788981, Improvement: -0.00000000193706, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1131
Epoch 1131, Loss: 0.00000003305328, Improvement: 0.00000000516347, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1132
Epoch 1132, Loss: 0.00000003074334, Improvement: -0.00000000230994, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1133
Epoch 1133, Loss: 0.00000005522345, Improvement: 0.00000002448011, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1134
Epoch 1134, Loss: 0.00000005218130, Improvement: -0.00000000304215, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1135
Epoch 1135, Loss: 0.00000006726446, Improvement: 0.00000001508316, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1136
Epoch 1136, Loss: 0.00000003748464, Improvement: -0.00000002977982, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1137
Epoch 1137, Loss: 0.00000003146678, Improvement: -0.00000000601786, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1138
Epoch 1138, Loss: 0.00000002909044, Improvement: -0.00000000237634, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1139
Epoch 1139, Loss: 0.00000002801041, Improvement: -0.00000000108003, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1140
Epoch 1140, Loss: 0.00000003374712, Improvement: 0.00000000573670, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1141
Epoch 1141, Loss: 0.00000003218162, Improvement: -0.00000000156550, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1142
Epoch 1142, Loss: 0.00000003438436, Improvement: 0.00000000220274, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1143
Epoch 1143, Loss: 0.00000003246819, Improvement: -0.00000000191617, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1144
Epoch 1144, Loss: 0.00000002597101, Improvement: -0.00000000649718, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1145
Epoch 1145, Loss: 0.00000002841091, Improvement: 0.00000000243990, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1146
Epoch 1146, Loss: 0.00000003837559, Improvement: 0.00000000996468, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1147
Epoch 1147, Loss: 0.00000006816403, Improvement: 0.00000002978844, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1148
Epoch 1148, Loss: 0.00000010540918, Improvement: 0.00000003724515, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1149
Epoch 1149, Loss: 0.00000005356799, Improvement: -0.00000005184119, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000003331842, Improvement: -0.00000002024957, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1151
Epoch 1151, Loss: 0.00000003743535, Improvement: 0.00000000411693, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1152
Epoch 1152, Loss: 0.00000004592086, Improvement: 0.00000000848550, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1153
Epoch 1153, Loss: 0.00000005685217, Improvement: 0.00000001093131, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1154
Epoch 1154, Loss: 0.00000003274246, Improvement: -0.00000002410971, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1155
Epoch 1155, Loss: 0.00000003427368, Improvement: 0.00000000153122, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1156
Epoch 1156, Loss: 0.00000003232565, Improvement: -0.00000000194803, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1157
Epoch 1157, Loss: 0.00000003384188, Improvement: 0.00000000151623, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1158
Epoch 1158, Loss: 0.00000004990894, Improvement: 0.00000001606706, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1159
Epoch 1159, Loss: 0.00000004830372, Improvement: -0.00000000160522, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1160
Epoch 1160, Loss: 0.00000003538478, Improvement: -0.00000001291894, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1161
Epoch 1161, Loss: 0.00000002941639, Improvement: -0.00000000596839, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1162
Epoch 1162, Loss: 0.00000003676417, Improvement: 0.00000000734779, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1163
Epoch 1163, Loss: 0.00000005041660, Improvement: 0.00000001365243, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1164
Epoch 1164, Loss: 0.00000004505286, Improvement: -0.00000000536374, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1165
Epoch 1165, Loss: 0.00000006820213, Improvement: 0.00000002314927, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1166
Epoch 1166, Loss: 0.00000009031658, Improvement: 0.00000002211444, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1167
Epoch 1167, Loss: 0.00000009694238, Improvement: 0.00000000662581, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1168
Epoch 1168, Loss: 0.00000007922662, Improvement: -0.00000001771577, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1169
Epoch 1169, Loss: 0.00000006282268, Improvement: -0.00000001640393, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1170
Epoch 1170, Loss: 0.00000005576921, Improvement: -0.00000000705347, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1171
Epoch 1171, Loss: 0.00000003339424, Improvement: -0.00000002237497, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1172
Epoch 1172, Loss: 0.00000003670738, Improvement: 0.00000000331314, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1173
Epoch 1173, Loss: 0.00000003196633, Improvement: -0.00000000474105, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1174
Epoch 1174, Loss: 0.00000002728716, Improvement: -0.00000000467916, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1175
Epoch 1175, Loss: 0.00000002977894, Improvement: 0.00000000249177, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1176
Epoch 1176, Loss: 0.00000002961072, Improvement: -0.00000000016821, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1177
Epoch 1177, Loss: 0.00000002828584, Improvement: -0.00000000132489, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1178
Epoch 1178, Loss: 0.00000002580907, Improvement: -0.00000000247677, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1179
Epoch 1179, Loss: 0.00000002453641, Improvement: -0.00000000127265, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1180
Epoch 1180, Loss: 0.00000002456858, Improvement: 0.00000000003217, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1181
Epoch 1181, Loss: 0.00000002706452, Improvement: 0.00000000249594, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1182
Epoch 1182, Loss: 0.00000003289156, Improvement: 0.00000000582704, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1183
Epoch 1183, Loss: 0.00000003300597, Improvement: 0.00000000011441, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1184
Epoch 1184, Loss: 0.00000002643106, Improvement: -0.00000000657492, Best Loss: 0.00000001712072 in Epoch 1038
Epoch 1185
A best model at epoch 1185 has been saved with training error 0.00000001594936.
Epoch 1185, Loss: 0.00000002406577, Improvement: -0.00000000236528, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1186
Epoch 1186, Loss: 0.00000002729680, Improvement: 0.00000000323103, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1187
Epoch 1187, Loss: 0.00000003512230, Improvement: 0.00000000782550, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1188
Epoch 1188, Loss: 0.00000003043993, Improvement: -0.00000000468237, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1189
Epoch 1189, Loss: 0.00000002839772, Improvement: -0.00000000204221, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1190
Epoch 1190, Loss: 0.00000003660860, Improvement: 0.00000000821088, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1191
Epoch 1191, Loss: 0.00000003783574, Improvement: 0.00000000122715, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1192
Epoch 1192, Loss: 0.00000003364519, Improvement: -0.00000000419055, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1193
Epoch 1193, Loss: 0.00000003020116, Improvement: -0.00000000344403, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1194
Epoch 1194, Loss: 0.00000002701195, Improvement: -0.00000000318922, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1195
Epoch 1195, Loss: 0.00000005214543, Improvement: 0.00000002513348, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1196
Epoch 1196, Loss: 0.00000005170941, Improvement: -0.00000000043602, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1197
Epoch 1197, Loss: 0.00000005877185, Improvement: 0.00000000706244, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1198
Epoch 1198, Loss: 0.00000005176398, Improvement: -0.00000000700787, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1199
Epoch 1199, Loss: 0.00000003549851, Improvement: -0.00000001626546, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000003485590, Improvement: -0.00000000064262, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1201
Epoch 1201, Loss: 0.00000005510743, Improvement: 0.00000002025154, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1202
Epoch 1202, Loss: 0.00000006671257, Improvement: 0.00000001160514, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1203
Epoch 1203, Loss: 0.00000005839096, Improvement: -0.00000000832161, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1204
Epoch 1204, Loss: 0.00000004797027, Improvement: -0.00000001042068, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1205
Epoch 1205, Loss: 0.00000004892913, Improvement: 0.00000000095886, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1206
Epoch 1206, Loss: 0.00000004647093, Improvement: -0.00000000245820, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1207
Epoch 1207, Loss: 0.00000005554367, Improvement: 0.00000000907274, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1208
Epoch 1208, Loss: 0.00000005642976, Improvement: 0.00000000088609, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1209
Epoch 1209, Loss: 0.00000007897306, Improvement: 0.00000002254330, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1210
Epoch 1210, Loss: 0.00000005113172, Improvement: -0.00000002784134, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1211
Epoch 1211, Loss: 0.00000004672747, Improvement: -0.00000000440424, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1212
Epoch 1212, Loss: 0.00000004657880, Improvement: -0.00000000014868, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1213
Epoch 1213, Loss: 0.00000005246437, Improvement: 0.00000000588557, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1214
Epoch 1214, Loss: 0.00000003365755, Improvement: -0.00000001880682, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1215
Epoch 1215, Loss: 0.00000008754479, Improvement: 0.00000005388723, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1216
Epoch 1216, Loss: 0.00000006302302, Improvement: -0.00000002452177, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1217
Epoch 1217, Loss: 0.00000002800900, Improvement: -0.00000003501402, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1218
Epoch 1218, Loss: 0.00000002365658, Improvement: -0.00000000435242, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1219
Epoch 1219, Loss: 0.00000002365126, Improvement: -0.00000000000533, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1220
Epoch 1220, Loss: 0.00000002294101, Improvement: -0.00000000071025, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1221
Epoch 1221, Loss: 0.00000002344491, Improvement: 0.00000000050390, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1222
Epoch 1222, Loss: 0.00000002341217, Improvement: -0.00000000003273, Best Loss: 0.00000001594936 in Epoch 1185
Epoch 1223
A best model at epoch 1223 has been saved with training error 0.00000001512055.
Epoch 1223, Loss: 0.00000002465707, Improvement: 0.00000000124490, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1224
Epoch 1224, Loss: 0.00000002422464, Improvement: -0.00000000043243, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1225
Epoch 1225, Loss: 0.00000002536172, Improvement: 0.00000000113708, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1226
Epoch 1226, Loss: 0.00000002504872, Improvement: -0.00000000031299, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1227
Epoch 1227, Loss: 0.00000002538232, Improvement: 0.00000000033360, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1228
Epoch 1228, Loss: 0.00000002347345, Improvement: -0.00000000190888, Best Loss: 0.00000001512055 in Epoch 1223
Epoch 1229
A best model at epoch 1229 has been saved with training error 0.00000001444807.
Epoch 1229, Loss: 0.00000002263267, Improvement: -0.00000000084078, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1230
Epoch 1230, Loss: 0.00000002206945, Improvement: -0.00000000056322, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1231
Epoch 1231, Loss: 0.00000002842373, Improvement: 0.00000000635428, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1232
Epoch 1232, Loss: 0.00000002818430, Improvement: -0.00000000023943, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1233
Epoch 1233, Loss: 0.00000002411663, Improvement: -0.00000000406767, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1234
Epoch 1234, Loss: 0.00000002274706, Improvement: -0.00000000136957, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1235
Epoch 1235, Loss: 0.00000002965828, Improvement: 0.00000000691122, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1236
Epoch 1236, Loss: 0.00000003326087, Improvement: 0.00000000360259, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1237
Epoch 1237, Loss: 0.00000004232233, Improvement: 0.00000000906146, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1238
Epoch 1238, Loss: 0.00000003863252, Improvement: -0.00000000368981, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1239
Epoch 1239, Loss: 0.00000003292386, Improvement: -0.00000000570866, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1240
Epoch 1240, Loss: 0.00000003010430, Improvement: -0.00000000281956, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1241
Epoch 1241, Loss: 0.00000002464015, Improvement: -0.00000000546415, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1242
Epoch 1242, Loss: 0.00000004215569, Improvement: 0.00000001751553, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1243
Epoch 1243, Loss: 0.00000006615113, Improvement: 0.00000002399545, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1244
Epoch 1244, Loss: 0.00000007396854, Improvement: 0.00000000781741, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1245
Epoch 1245, Loss: 0.00000008582952, Improvement: 0.00000001186098, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1246
Epoch 1246, Loss: 0.00000006257682, Improvement: -0.00000002325270, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1247
Epoch 1247, Loss: 0.00000003265151, Improvement: -0.00000002992531, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1248
Epoch 1248, Loss: 0.00000002683538, Improvement: -0.00000000581613, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1249
Epoch 1249, Loss: 0.00000002640692, Improvement: -0.00000000042846, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000002528500, Improvement: -0.00000000112193, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1251
Epoch 1251, Loss: 0.00000002241096, Improvement: -0.00000000287404, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1252
Epoch 1252, Loss: 0.00000002180623, Improvement: -0.00000000060472, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1253
Epoch 1253, Loss: 0.00000002164315, Improvement: -0.00000000016308, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1254
Epoch 1254, Loss: 0.00000002332397, Improvement: 0.00000000168081, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1255
Epoch 1255, Loss: 0.00000002231682, Improvement: -0.00000000100715, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1256
Epoch 1256, Loss: 0.00000002325837, Improvement: 0.00000000094154, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1257
Epoch 1257, Loss: 0.00000002326813, Improvement: 0.00000000000976, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1258
Epoch 1258, Loss: 0.00000003060152, Improvement: 0.00000000733339, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1259
Epoch 1259, Loss: 0.00000004038683, Improvement: 0.00000000978532, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1260
Epoch 1260, Loss: 0.00000005295917, Improvement: 0.00000001257234, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1261
Epoch 1261, Loss: 0.00000006470850, Improvement: 0.00000001174933, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1262
Epoch 1262, Loss: 0.00000004313840, Improvement: -0.00000002157010, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1263
Epoch 1263, Loss: 0.00000003942570, Improvement: -0.00000000371270, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1264
Epoch 1264, Loss: 0.00000003828605, Improvement: -0.00000000113965, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1265
Epoch 1265, Loss: 0.00000003417712, Improvement: -0.00000000410893, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1266
Epoch 1266, Loss: 0.00000003170171, Improvement: -0.00000000247541, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1267
Epoch 1267, Loss: 0.00000003137643, Improvement: -0.00000000032529, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1268
Epoch 1268, Loss: 0.00000002714100, Improvement: -0.00000000423543, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1269
Epoch 1269, Loss: 0.00000002716400, Improvement: 0.00000000002300, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1270
Epoch 1270, Loss: 0.00000002968386, Improvement: 0.00000000251986, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1271
Epoch 1271, Loss: 0.00000002916907, Improvement: -0.00000000051479, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1272
Epoch 1272, Loss: 0.00000002735121, Improvement: -0.00000000181786, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1273
Epoch 1273, Loss: 0.00000002614325, Improvement: -0.00000000120796, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1274
Epoch 1274, Loss: 0.00000003482232, Improvement: 0.00000000867907, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1275
Epoch 1275, Loss: 0.00000004350771, Improvement: 0.00000000868539, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1276
Epoch 1276, Loss: 0.00000003796281, Improvement: -0.00000000554490, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1277
Epoch 1277, Loss: 0.00000003694815, Improvement: -0.00000000101466, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1278
Epoch 1278, Loss: 0.00000003935579, Improvement: 0.00000000240765, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1279
Epoch 1279, Loss: 0.00000003929586, Improvement: -0.00000000005993, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1280
Epoch 1280, Loss: 0.00000002689910, Improvement: -0.00000001239676, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1281
Epoch 1281, Loss: 0.00000003719397, Improvement: 0.00000001029487, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1282
Epoch 1282, Loss: 0.00000006439368, Improvement: 0.00000002719971, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1283
Epoch 1283, Loss: 0.00000006338289, Improvement: -0.00000000101079, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1284
Epoch 1284, Loss: 0.00000003462674, Improvement: -0.00000002875615, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1285
Epoch 1285, Loss: 0.00000004496102, Improvement: 0.00000001033428, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1286
Epoch 1286, Loss: 0.00000006393350, Improvement: 0.00000001897249, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1287
Epoch 1287, Loss: 0.00000009916685, Improvement: 0.00000003523334, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1288
Epoch 1288, Loss: 0.00000005587883, Improvement: -0.00000004328802, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1289
Epoch 1289, Loss: 0.00000004083636, Improvement: -0.00000001504246, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1290
Epoch 1290, Loss: 0.00000003370572, Improvement: -0.00000000713064, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1291
Epoch 1291, Loss: 0.00000002574692, Improvement: -0.00000000795880, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1292
Epoch 1292, Loss: 0.00000002412154, Improvement: -0.00000000162538, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1293
Epoch 1293, Loss: 0.00000002214452, Improvement: -0.00000000197702, Best Loss: 0.00000001444807 in Epoch 1229
Epoch 1294
A best model at epoch 1294 has been saved with training error 0.00000001324457.
Epoch 1294, Loss: 0.00000002272710, Improvement: 0.00000000058258, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1295
Epoch 1295, Loss: 0.00000002104404, Improvement: -0.00000000168307, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1296
Epoch 1296, Loss: 0.00000002171334, Improvement: 0.00000000066931, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1297
Epoch 1297, Loss: 0.00000002391814, Improvement: 0.00000000220479, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1298
Epoch 1298, Loss: 0.00000002451155, Improvement: 0.00000000059341, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1299
Epoch 1299, Loss: 0.00000003646964, Improvement: 0.00000001195809, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000003142558, Improvement: -0.00000000504405, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1301
Epoch 1301, Loss: 0.00000003269053, Improvement: 0.00000000126495, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1302
Epoch 1302, Loss: 0.00000002804792, Improvement: -0.00000000464261, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1303
Epoch 1303, Loss: 0.00000002431367, Improvement: -0.00000000373425, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1304
Epoch 1304, Loss: 0.00000002638109, Improvement: 0.00000000206742, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1305
Epoch 1305, Loss: 0.00000003339625, Improvement: 0.00000000701516, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1306
Epoch 1306, Loss: 0.00000002897865, Improvement: -0.00000000441760, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1307
Epoch 1307, Loss: 0.00000002557511, Improvement: -0.00000000340354, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1308
Epoch 1308, Loss: 0.00000003333218, Improvement: 0.00000000775707, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1309
Epoch 1309, Loss: 0.00000003584194, Improvement: 0.00000000250976, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1310
Epoch 1310, Loss: 0.00000005228752, Improvement: 0.00000001644558, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1311
Epoch 1311, Loss: 0.00000004527988, Improvement: -0.00000000700764, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1312
Epoch 1312, Loss: 0.00000005567876, Improvement: 0.00000001039888, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1313
Epoch 1313, Loss: 0.00000007164330, Improvement: 0.00000001596453, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1314
Epoch 1314, Loss: 0.00000006894797, Improvement: -0.00000000269533, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1315
Epoch 1315, Loss: 0.00000005096870, Improvement: -0.00000001797927, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1316
Epoch 1316, Loss: 0.00000004134012, Improvement: -0.00000000962857, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1317
Epoch 1317, Loss: 0.00000003451157, Improvement: -0.00000000682855, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1318
Epoch 1318, Loss: 0.00000002973046, Improvement: -0.00000000478111, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1319
Epoch 1319, Loss: 0.00000002223083, Improvement: -0.00000000749963, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1320
Epoch 1320, Loss: 0.00000002234884, Improvement: 0.00000000011801, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1321
Epoch 1321, Loss: 0.00000002062845, Improvement: -0.00000000172039, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1322
Epoch 1322, Loss: 0.00000002045369, Improvement: -0.00000000017476, Best Loss: 0.00000001324457 in Epoch 1294
Epoch 1323
A best model at epoch 1323 has been saved with training error 0.00000001296479.
Epoch 1323, Loss: 0.00000002016240, Improvement: -0.00000000029129, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1324
Epoch 1324, Loss: 0.00000002091700, Improvement: 0.00000000075460, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1325
Epoch 1325, Loss: 0.00000002275245, Improvement: 0.00000000183546, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1326
Epoch 1326, Loss: 0.00000002366529, Improvement: 0.00000000091283, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1327
Epoch 1327, Loss: 0.00000002187476, Improvement: -0.00000000179053, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1328
Epoch 1328, Loss: 0.00000002156428, Improvement: -0.00000000031048, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1329
Epoch 1329, Loss: 0.00000002280428, Improvement: 0.00000000124000, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1330
Epoch 1330, Loss: 0.00000002227935, Improvement: -0.00000000052492, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1331
Epoch 1331, Loss: 0.00000002180480, Improvement: -0.00000000047455, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1332
Epoch 1332, Loss: 0.00000002234670, Improvement: 0.00000000054190, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1333
Epoch 1333, Loss: 0.00000002355519, Improvement: 0.00000000120849, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1334
Epoch 1334, Loss: 0.00000002194152, Improvement: -0.00000000161367, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1335
Epoch 1335, Loss: 0.00000002146099, Improvement: -0.00000000048054, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1336
Epoch 1336, Loss: 0.00000002237830, Improvement: 0.00000000091731, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1337
Epoch 1337, Loss: 0.00000002660644, Improvement: 0.00000000422813, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1338
Epoch 1338, Loss: 0.00000003095883, Improvement: 0.00000000435239, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1339
Epoch 1339, Loss: 0.00000004244575, Improvement: 0.00000001148692, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1340
Epoch 1340, Loss: 0.00000009248501, Improvement: 0.00000005003927, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1341
Epoch 1341, Loss: 0.00000012509563, Improvement: 0.00000003261062, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1342
Epoch 1342, Loss: 0.00000010068744, Improvement: -0.00000002440819, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1343
Epoch 1343, Loss: 0.00000004346984, Improvement: -0.00000005721760, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1344
Epoch 1344, Loss: 0.00000003320364, Improvement: -0.00000001026621, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1345
Epoch 1345, Loss: 0.00000002316541, Improvement: -0.00000001003823, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1346
Epoch 1346, Loss: 0.00000002064055, Improvement: -0.00000000252486, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1347
Epoch 1347, Loss: 0.00000002045838, Improvement: -0.00000000018217, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1348
Epoch 1348, Loss: 0.00000001944292, Improvement: -0.00000000101545, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1349
Epoch 1349, Loss: 0.00000001927727, Improvement: -0.00000000016565, Best Loss: 0.00000001296479 in Epoch 1323
Epoch 1350
A best model at epoch 1350 has been saved with training error 0.00000001211777.
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000001944909, Improvement: 0.00000000017182, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1351
Epoch 1351, Loss: 0.00000001928135, Improvement: -0.00000000016774, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1352
Epoch 1352, Loss: 0.00000002051722, Improvement: 0.00000000123588, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1353
Epoch 1353, Loss: 0.00000002151588, Improvement: 0.00000000099866, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1354
Epoch 1354, Loss: 0.00000002085699, Improvement: -0.00000000065889, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1355
Epoch 1355, Loss: 0.00000001951708, Improvement: -0.00000000133992, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1356
Epoch 1356, Loss: 0.00000002113019, Improvement: 0.00000000161312, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1357
Epoch 1357, Loss: 0.00000002001560, Improvement: -0.00000000111460, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1358
Epoch 1358, Loss: 0.00000002086510, Improvement: 0.00000000084950, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1359
Epoch 1359, Loss: 0.00000002182412, Improvement: 0.00000000095902, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1360
Epoch 1360, Loss: 0.00000001991558, Improvement: -0.00000000190854, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1361
Epoch 1361, Loss: 0.00000001908549, Improvement: -0.00000000083009, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1362
Epoch 1362, Loss: 0.00000001935154, Improvement: 0.00000000026605, Best Loss: 0.00000001211777 in Epoch 1350
Epoch 1363
A best model at epoch 1363 has been saved with training error 0.00000001184564.
Epoch 1363, Loss: 0.00000001985125, Improvement: 0.00000000049971, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1364
Epoch 1364, Loss: 0.00000001998261, Improvement: 0.00000000013135, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1365
Epoch 1365, Loss: 0.00000002605711, Improvement: 0.00000000607450, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1366
Epoch 1366, Loss: 0.00000002780975, Improvement: 0.00000000175264, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1367
Epoch 1367, Loss: 0.00000002811997, Improvement: 0.00000000031022, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1368
Epoch 1368, Loss: 0.00000003615211, Improvement: 0.00000000803214, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1369
Epoch 1369, Loss: 0.00000004705354, Improvement: 0.00000001090144, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1370
Epoch 1370, Loss: 0.00000003811596, Improvement: -0.00000000893758, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1371
Epoch 1371, Loss: 0.00000005738550, Improvement: 0.00000001926954, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1372
Epoch 1372, Loss: 0.00000006955079, Improvement: 0.00000001216529, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1373
Epoch 1373, Loss: 0.00000004532300, Improvement: -0.00000002422779, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1374
Epoch 1374, Loss: 0.00000003474895, Improvement: -0.00000001057405, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1375
Epoch 1375, Loss: 0.00000002486254, Improvement: -0.00000000988641, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1376
Epoch 1376, Loss: 0.00000002126779, Improvement: -0.00000000359475, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1377
Epoch 1377, Loss: 0.00000002024781, Improvement: -0.00000000101997, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1378
Epoch 1378, Loss: 0.00000001961823, Improvement: -0.00000000062958, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1379
Epoch 1379, Loss: 0.00000001971953, Improvement: 0.00000000010130, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1380
Epoch 1380, Loss: 0.00000002121490, Improvement: 0.00000000149537, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1381
Epoch 1381, Loss: 0.00000002451504, Improvement: 0.00000000330014, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1382
Epoch 1382, Loss: 0.00000002376895, Improvement: -0.00000000074609, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1383
Epoch 1383, Loss: 0.00000002433280, Improvement: 0.00000000056385, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1384
Epoch 1384, Loss: 0.00000002804549, Improvement: 0.00000000371270, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1385
Epoch 1385, Loss: 0.00000002209990, Improvement: -0.00000000594559, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1386
Epoch 1386, Loss: 0.00000002258147, Improvement: 0.00000000048157, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1387
Epoch 1387, Loss: 0.00000003516897, Improvement: 0.00000001258750, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1388
Epoch 1388, Loss: 0.00000002664690, Improvement: -0.00000000852207, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1389
Epoch 1389, Loss: 0.00000003150931, Improvement: 0.00000000486242, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1390
Epoch 1390, Loss: 0.00000005324875, Improvement: 0.00000002173944, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1391
Epoch 1391, Loss: 0.00000004561272, Improvement: -0.00000000763603, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1392
Epoch 1392, Loss: 0.00000003977376, Improvement: -0.00000000583896, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1393
Epoch 1393, Loss: 0.00000004409485, Improvement: 0.00000000432109, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1394
Epoch 1394, Loss: 0.00000003539868, Improvement: -0.00000000869617, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1395
Epoch 1395, Loss: 0.00000003026441, Improvement: -0.00000000513427, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1396
Epoch 1396, Loss: 0.00000002217818, Improvement: -0.00000000808623, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1397
Epoch 1397, Loss: 0.00000002287933, Improvement: 0.00000000070115, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1398
Epoch 1398, Loss: 0.00000002880002, Improvement: 0.00000000592069, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1399
Epoch 1399, Loss: 0.00000003235093, Improvement: 0.00000000355092, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000003787431, Improvement: 0.00000000552337, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1401
Epoch 1401, Loss: 0.00000004443825, Improvement: 0.00000000656394, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1402
Epoch 1402, Loss: 0.00000004613153, Improvement: 0.00000000169328, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1403
Epoch 1403, Loss: 0.00000009266778, Improvement: 0.00000004653625, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1404
Epoch 1404, Loss: 0.00000007166138, Improvement: -0.00000002100640, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1405
Epoch 1405, Loss: 0.00000006530873, Improvement: -0.00000000635265, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1406
Epoch 1406, Loss: 0.00000004019850, Improvement: -0.00000002511023, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1407
Epoch 1407, Loss: 0.00000003192079, Improvement: -0.00000000827771, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1408
Epoch 1408, Loss: 0.00000002474303, Improvement: -0.00000000717776, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1409
Epoch 1409, Loss: 0.00000002295947, Improvement: -0.00000000178356, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1410
Epoch 1410, Loss: 0.00000001932112, Improvement: -0.00000000363835, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1411
Epoch 1411, Loss: 0.00000001896758, Improvement: -0.00000000035354, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1412
Epoch 1412, Loss: 0.00000001994547, Improvement: 0.00000000097789, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1413
Epoch 1413, Loss: 0.00000002257463, Improvement: 0.00000000262916, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1414
Epoch 1414, Loss: 0.00000002020899, Improvement: -0.00000000236564, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1415
Epoch 1415, Loss: 0.00000001937767, Improvement: -0.00000000083132, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1416
Epoch 1416, Loss: 0.00000001836944, Improvement: -0.00000000100823, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1417
Epoch 1417, Loss: 0.00000001999811, Improvement: 0.00000000162867, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1418
Epoch 1418, Loss: 0.00000002157424, Improvement: 0.00000000157613, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1419
Epoch 1419, Loss: 0.00000002503729, Improvement: 0.00000000346306, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1420
Epoch 1420, Loss: 0.00000002325661, Improvement: -0.00000000178068, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1421
Epoch 1421, Loss: 0.00000002018073, Improvement: -0.00000000307588, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1422
Epoch 1422, Loss: 0.00000002338244, Improvement: 0.00000000320171, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1423
Epoch 1423, Loss: 0.00000002226551, Improvement: -0.00000000111693, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1424
Epoch 1424, Loss: 0.00000002205653, Improvement: -0.00000000020899, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1425
Epoch 1425, Loss: 0.00000003377502, Improvement: 0.00000001171850, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1426
Epoch 1426, Loss: 0.00000002838627, Improvement: -0.00000000538875, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1427
Epoch 1427, Loss: 0.00000002690981, Improvement: -0.00000000147646, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1428
Epoch 1428, Loss: 0.00000002399446, Improvement: -0.00000000291535, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1429
Epoch 1429, Loss: 0.00000002381777, Improvement: -0.00000000017669, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1430
Epoch 1430, Loss: 0.00000002904575, Improvement: 0.00000000522799, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1431
Epoch 1431, Loss: 0.00000004211788, Improvement: 0.00000001307213, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1432
Epoch 1432, Loss: 0.00000004491348, Improvement: 0.00000000279560, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1433
Epoch 1433, Loss: 0.00000005689555, Improvement: 0.00000001198207, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1434
Epoch 1434, Loss: 0.00000004201828, Improvement: -0.00000001487727, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1435
Epoch 1435, Loss: 0.00000003487242, Improvement: -0.00000000714586, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1436
Epoch 1436, Loss: 0.00000002484022, Improvement: -0.00000001003220, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1437
Epoch 1437, Loss: 0.00000002430324, Improvement: -0.00000000053698, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1438
Epoch 1438, Loss: 0.00000002131499, Improvement: -0.00000000298825, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1439
Epoch 1439, Loss: 0.00000002159533, Improvement: 0.00000000028035, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1440
Epoch 1440, Loss: 0.00000002249278, Improvement: 0.00000000089745, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1441
Epoch 1441, Loss: 0.00000002472651, Improvement: 0.00000000223373, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1442
Epoch 1442, Loss: 0.00000002341180, Improvement: -0.00000000131472, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1443
Epoch 1443, Loss: 0.00000002319985, Improvement: -0.00000000021194, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1444
Epoch 1444, Loss: 0.00000002630975, Improvement: 0.00000000310990, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1445
Epoch 1445, Loss: 0.00000002796407, Improvement: 0.00000000165432, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1446
Epoch 1446, Loss: 0.00000003256372, Improvement: 0.00000000459965, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1447
Epoch 1447, Loss: 0.00000005189885, Improvement: 0.00000001933513, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1448
Epoch 1448, Loss: 0.00000006831993, Improvement: 0.00000001642108, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1449
Epoch 1449, Loss: 0.00000003598034, Improvement: -0.00000003233959, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000002741895, Improvement: -0.00000000856139, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1451
Epoch 1451, Loss: 0.00000002403444, Improvement: -0.00000000338451, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1452
Epoch 1452, Loss: 0.00000002260204, Improvement: -0.00000000143240, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1453
Epoch 1453, Loss: 0.00000002464512, Improvement: 0.00000000204308, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1454
Epoch 1454, Loss: 0.00000003071661, Improvement: 0.00000000607149, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1455
Epoch 1455, Loss: 0.00000006517953, Improvement: 0.00000003446292, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1456
Epoch 1456, Loss: 0.00000005698061, Improvement: -0.00000000819892, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1457
Epoch 1457, Loss: 0.00000003634916, Improvement: -0.00000002063145, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1458
Epoch 1458, Loss: 0.00000002664451, Improvement: -0.00000000970466, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1459
Epoch 1459, Loss: 0.00000003652391, Improvement: 0.00000000987940, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1460
Epoch 1460, Loss: 0.00000002354023, Improvement: -0.00000001298367, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1461
Epoch 1461, Loss: 0.00000001879187, Improvement: -0.00000000474836, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1462
Epoch 1462, Loss: 0.00000001831574, Improvement: -0.00000000047613, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1463
Epoch 1463, Loss: 0.00000001818222, Improvement: -0.00000000013352, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1464
Epoch 1464, Loss: 0.00000002016366, Improvement: 0.00000000198144, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1465
Epoch 1465, Loss: 0.00000001883619, Improvement: -0.00000000132747, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1466
Epoch 1466, Loss: 0.00000001950111, Improvement: 0.00000000066492, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1467
Epoch 1467, Loss: 0.00000001944943, Improvement: -0.00000000005168, Best Loss: 0.00000001184564 in Epoch 1363
Epoch 1468
A best model at epoch 1468 has been saved with training error 0.00000001113529.
Epoch 1468, Loss: 0.00000001780808, Improvement: -0.00000000164135, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1469
Epoch 1469, Loss: 0.00000001737915, Improvement: -0.00000000042893, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1470
Epoch 1470, Loss: 0.00000001953809, Improvement: 0.00000000215894, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1471
Epoch 1471, Loss: 0.00000002016352, Improvement: 0.00000000062543, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1472
Epoch 1472, Loss: 0.00000002053600, Improvement: 0.00000000037248, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1473
Epoch 1473, Loss: 0.00000001930093, Improvement: -0.00000000123507, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1474
Epoch 1474, Loss: 0.00000001927199, Improvement: -0.00000000002894, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1475
Epoch 1475, Loss: 0.00000002083103, Improvement: 0.00000000155904, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1476
Epoch 1476, Loss: 0.00000002309775, Improvement: 0.00000000226672, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1477
Epoch 1477, Loss: 0.00000002780883, Improvement: 0.00000000471108, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1478
Epoch 1478, Loss: 0.00000002557980, Improvement: -0.00000000222903, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1479
Epoch 1479, Loss: 0.00000002444954, Improvement: -0.00000000113025, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1480
Epoch 1480, Loss: 0.00000002714824, Improvement: 0.00000000269869, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1481
Epoch 1481, Loss: 0.00000002897335, Improvement: 0.00000000182511, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1482
Epoch 1482, Loss: 0.00000003700834, Improvement: 0.00000000803499, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1483
Epoch 1483, Loss: 0.00000004228570, Improvement: 0.00000000527737, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1484
Epoch 1484, Loss: 0.00000004408184, Improvement: 0.00000000179613, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1485
Epoch 1485, Loss: 0.00000003247816, Improvement: -0.00000001160368, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1486
Epoch 1486, Loss: 0.00000001996855, Improvement: -0.00000001250961, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1487
Epoch 1487, Loss: 0.00000002203797, Improvement: 0.00000000206942, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1488
Epoch 1488, Loss: 0.00000001919619, Improvement: -0.00000000284178, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1489
Epoch 1489, Loss: 0.00000001746066, Improvement: -0.00000000173553, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1490
Epoch 1490, Loss: 0.00000001730611, Improvement: -0.00000000015455, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1491
Epoch 1491, Loss: 0.00000001871448, Improvement: 0.00000000140837, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1492
Epoch 1492, Loss: 0.00000002075319, Improvement: 0.00000000203872, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1493
Epoch 1493, Loss: 0.00000002004995, Improvement: -0.00000000070324, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1494
Epoch 1494, Loss: 0.00000001970703, Improvement: -0.00000000034292, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1495
Epoch 1495, Loss: 0.00000001951768, Improvement: -0.00000000018935, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1496
Epoch 1496, Loss: 0.00000002694896, Improvement: 0.00000000743128, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1497
Epoch 1497, Loss: 0.00000003669963, Improvement: 0.00000000975067, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1498
Epoch 1498, Loss: 0.00000006989641, Improvement: 0.00000003319678, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1499
Epoch 1499, Loss: 0.00000003943937, Improvement: -0.00000003045704, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000002961916, Improvement: -0.00000000982020, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1501
Epoch 1501, Loss: 0.00000003291255, Improvement: 0.00000000329338, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1502
Epoch 1502, Loss: 0.00000003033279, Improvement: -0.00000000257976, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1503
Epoch 1503, Loss: 0.00000002514400, Improvement: -0.00000000518879, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1504
Epoch 1504, Loss: 0.00000003518555, Improvement: 0.00000001004155, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1505
Epoch 1505, Loss: 0.00000003439528, Improvement: -0.00000000079027, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1506
Epoch 1506, Loss: 0.00000003146451, Improvement: -0.00000000293077, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1507
Epoch 1507, Loss: 0.00000003338609, Improvement: 0.00000000192158, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1508
Epoch 1508, Loss: 0.00000005050083, Improvement: 0.00000001711474, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1509
Epoch 1509, Loss: 0.00000004726234, Improvement: -0.00000000323849, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1510
Epoch 1510, Loss: 0.00000006359432, Improvement: 0.00000001633198, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1511
Epoch 1511, Loss: 0.00000005433769, Improvement: -0.00000000925663, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1512
Epoch 1512, Loss: 0.00000004988778, Improvement: -0.00000000444991, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1513
Epoch 1513, Loss: 0.00000003646880, Improvement: -0.00000001341898, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1514
Epoch 1514, Loss: 0.00000002742592, Improvement: -0.00000000904288, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1515
Epoch 1515, Loss: 0.00000002526549, Improvement: -0.00000000216042, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1516
Epoch 1516, Loss: 0.00000002398131, Improvement: -0.00000000128418, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1517
Epoch 1517, Loss: 0.00000002065728, Improvement: -0.00000000332403, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1518
Epoch 1518, Loss: 0.00000001829409, Improvement: -0.00000000236319, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1519
Epoch 1519, Loss: 0.00000001669164, Improvement: -0.00000000160245, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1520
Epoch 1520, Loss: 0.00000001662943, Improvement: -0.00000000006221, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1521
Epoch 1521, Loss: 0.00000001637193, Improvement: -0.00000000025750, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1522
Epoch 1522, Loss: 0.00000001708849, Improvement: 0.00000000071656, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1523
Epoch 1523, Loss: 0.00000001759721, Improvement: 0.00000000050872, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1524
Epoch 1524, Loss: 0.00000002220177, Improvement: 0.00000000460456, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1525
Epoch 1525, Loss: 0.00000002202706, Improvement: -0.00000000017471, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1526
Epoch 1526, Loss: 0.00000002017852, Improvement: -0.00000000184853, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1527
Epoch 1527, Loss: 0.00000001983319, Improvement: -0.00000000034533, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1528
Epoch 1528, Loss: 0.00000003506495, Improvement: 0.00000001523176, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1529
Epoch 1529, Loss: 0.00000003415644, Improvement: -0.00000000090851, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1530
Epoch 1530, Loss: 0.00000002903180, Improvement: -0.00000000512464, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1531
Epoch 1531, Loss: 0.00000003481346, Improvement: 0.00000000578166, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1532
Epoch 1532, Loss: 0.00000003352720, Improvement: -0.00000000128626, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1533
Epoch 1533, Loss: 0.00000002222808, Improvement: -0.00000001129913, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1534
Epoch 1534, Loss: 0.00000001973471, Improvement: -0.00000000249337, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1535
Epoch 1535, Loss: 0.00000001711188, Improvement: -0.00000000262283, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1536
Epoch 1536, Loss: 0.00000002324457, Improvement: 0.00000000613270, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1537
Epoch 1537, Loss: 0.00000002421098, Improvement: 0.00000000096640, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1538
Epoch 1538, Loss: 0.00000002529411, Improvement: 0.00000000108314, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1539
Epoch 1539, Loss: 0.00000002420301, Improvement: -0.00000000109110, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1540
Epoch 1540, Loss: 0.00000002199525, Improvement: -0.00000000220776, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1541
Epoch 1541, Loss: 0.00000001961312, Improvement: -0.00000000238213, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1542
Epoch 1542, Loss: 0.00000001987720, Improvement: 0.00000000026408, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1543
Epoch 1543, Loss: 0.00000002718814, Improvement: 0.00000000731094, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1544
Epoch 1544, Loss: 0.00000002534202, Improvement: -0.00000000184612, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1545
Epoch 1545, Loss: 0.00000004076700, Improvement: 0.00000001542498, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1546
Epoch 1546, Loss: 0.00000004945874, Improvement: 0.00000000869174, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1547
Epoch 1547, Loss: 0.00000005113450, Improvement: 0.00000000167576, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1548
Epoch 1548, Loss: 0.00000003535807, Improvement: -0.00000001577644, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1549
Epoch 1549, Loss: 0.00000002383282, Improvement: -0.00000001152525, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000002098000, Improvement: -0.00000000285282, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1551
Epoch 1551, Loss: 0.00000001807924, Improvement: -0.00000000290076, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1552
Epoch 1552, Loss: 0.00000001734572, Improvement: -0.00000000073352, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1553
Epoch 1553, Loss: 0.00000001783679, Improvement: 0.00000000049107, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1554
Epoch 1554, Loss: 0.00000001866343, Improvement: 0.00000000082664, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1555
Epoch 1555, Loss: 0.00000002173082, Improvement: 0.00000000306739, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1556
Epoch 1556, Loss: 0.00000002313444, Improvement: 0.00000000140362, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1557
Epoch 1557, Loss: 0.00000002684634, Improvement: 0.00000000371190, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1558
Epoch 1558, Loss: 0.00000003789321, Improvement: 0.00000001104688, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1559
Epoch 1559, Loss: 0.00000002714884, Improvement: -0.00000001074438, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1560
Epoch 1560, Loss: 0.00000002166734, Improvement: -0.00000000548150, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1561
Epoch 1561, Loss: 0.00000001813103, Improvement: -0.00000000353631, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1562
Epoch 1562, Loss: 0.00000002023624, Improvement: 0.00000000210521, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1563
Epoch 1563, Loss: 0.00000002544503, Improvement: 0.00000000520879, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1564
Epoch 1564, Loss: 0.00000002374597, Improvement: -0.00000000169906, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1565
Epoch 1565, Loss: 0.00000002387745, Improvement: 0.00000000013148, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1566
Epoch 1566, Loss: 0.00000002815283, Improvement: 0.00000000427538, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1567
Epoch 1567, Loss: 0.00000002483734, Improvement: -0.00000000331548, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1568
Epoch 1568, Loss: 0.00000003264124, Improvement: 0.00000000780389, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1569
Epoch 1569, Loss: 0.00000002776003, Improvement: -0.00000000488120, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1570
Epoch 1570, Loss: 0.00000002316454, Improvement: -0.00000000459549, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1571
Epoch 1571, Loss: 0.00000002758106, Improvement: 0.00000000441652, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1572
Epoch 1572, Loss: 0.00000002688953, Improvement: -0.00000000069153, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1573
Epoch 1573, Loss: 0.00000003060315, Improvement: 0.00000000371362, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1574
Epoch 1574, Loss: 0.00000003812898, Improvement: 0.00000000752582, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1575
Epoch 1575, Loss: 0.00000002807843, Improvement: -0.00000001005055, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1576
Epoch 1576, Loss: 0.00000002990711, Improvement: 0.00000000182868, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1577
Epoch 1577, Loss: 0.00000004098426, Improvement: 0.00000001107715, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1578
Epoch 1578, Loss: 0.00000003304979, Improvement: -0.00000000793447, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1579
Epoch 1579, Loss: 0.00000005007763, Improvement: 0.00000001702783, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1580
Epoch 1580, Loss: 0.00000004585690, Improvement: -0.00000000422072, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1581
Epoch 1581, Loss: 0.00000002198686, Improvement: -0.00000002387004, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1582
Epoch 1582, Loss: 0.00000001811227, Improvement: -0.00000000387459, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1583
Epoch 1583, Loss: 0.00000002369630, Improvement: 0.00000000558403, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1584
Epoch 1584, Loss: 0.00000004802045, Improvement: 0.00000002432415, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1585
Epoch 1585, Loss: 0.00000004292549, Improvement: -0.00000000509496, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1586
Epoch 1586, Loss: 0.00000004693650, Improvement: 0.00000000401101, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1587
Epoch 1587, Loss: 0.00000006716790, Improvement: 0.00000002023140, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1588
Epoch 1588, Loss: 0.00000003918141, Improvement: -0.00000002798649, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1589
Epoch 1589, Loss: 0.00000002636725, Improvement: -0.00000001281416, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1590
Epoch 1590, Loss: 0.00000002403525, Improvement: -0.00000000233200, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1591
Epoch 1591, Loss: 0.00000002184238, Improvement: -0.00000000219288, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1592
Epoch 1592, Loss: 0.00000001888235, Improvement: -0.00000000296003, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1593
Epoch 1593, Loss: 0.00000001721677, Improvement: -0.00000000166558, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1594
Epoch 1594, Loss: 0.00000001639725, Improvement: -0.00000000081953, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1595
Epoch 1595, Loss: 0.00000001568684, Improvement: -0.00000000071041, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1596
Epoch 1596, Loss: 0.00000001559347, Improvement: -0.00000000009337, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1597
Epoch 1597, Loss: 0.00000001549810, Improvement: -0.00000000009537, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1598
Epoch 1598, Loss: 0.00000002009903, Improvement: 0.00000000460092, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1599
Epoch 1599, Loss: 0.00000001890157, Improvement: -0.00000000119746, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000001646012, Improvement: -0.00000000244144, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1601
Epoch 1601, Loss: 0.00000001576513, Improvement: -0.00000000069500, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1602
Epoch 1602, Loss: 0.00000001794138, Improvement: 0.00000000217625, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1603
Epoch 1603, Loss: 0.00000002290248, Improvement: 0.00000000496110, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1604
Epoch 1604, Loss: 0.00000003772234, Improvement: 0.00000001481986, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1605
Epoch 1605, Loss: 0.00000004934609, Improvement: 0.00000001162375, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1606
Epoch 1606, Loss: 0.00000003846453, Improvement: -0.00000001088156, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1607
Epoch 1607, Loss: 0.00000004810883, Improvement: 0.00000000964430, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1608
Epoch 1608, Loss: 0.00000004165963, Improvement: -0.00000000644920, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1609
Epoch 1609, Loss: 0.00000002606575, Improvement: -0.00000001559388, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1610
Epoch 1610, Loss: 0.00000002193124, Improvement: -0.00000000413450, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1611
Epoch 1611, Loss: 0.00000001790032, Improvement: -0.00000000403092, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1612
Epoch 1612, Loss: 0.00000001885125, Improvement: 0.00000000095092, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1613
Epoch 1613, Loss: 0.00000001687550, Improvement: -0.00000000197574, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1614
Epoch 1614, Loss: 0.00000001544282, Improvement: -0.00000000143268, Best Loss: 0.00000001113529 in Epoch 1468
Epoch 1615
A best model at epoch 1615 has been saved with training error 0.00000001055981.
Epoch 1615, Loss: 0.00000001514901, Improvement: -0.00000000029381, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1616
Epoch 1616, Loss: 0.00000001512258, Improvement: -0.00000000002643, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1617
Epoch 1617, Loss: 0.00000001568986, Improvement: 0.00000000056727, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1618
Epoch 1618, Loss: 0.00000001602891, Improvement: 0.00000000033905, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1619
Epoch 1619, Loss: 0.00000001751276, Improvement: 0.00000000148385, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1620
Epoch 1620, Loss: 0.00000001782045, Improvement: 0.00000000030769, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1621
Epoch 1621, Loss: 0.00000002700833, Improvement: 0.00000000918788, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1622
Epoch 1622, Loss: 0.00000004005306, Improvement: 0.00000001304473, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1623
Epoch 1623, Loss: 0.00000003245775, Improvement: -0.00000000759531, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1624
Epoch 1624, Loss: 0.00000002452788, Improvement: -0.00000000792987, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1625
Epoch 1625, Loss: 0.00000002387991, Improvement: -0.00000000064797, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1626
Epoch 1626, Loss: 0.00000001922009, Improvement: -0.00000000465982, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1627
Epoch 1627, Loss: 0.00000001703460, Improvement: -0.00000000218549, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1628
Epoch 1628, Loss: 0.00000001683554, Improvement: -0.00000000019906, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1629
Epoch 1629, Loss: 0.00000001708193, Improvement: 0.00000000024640, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1630
Epoch 1630, Loss: 0.00000001880675, Improvement: 0.00000000172481, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1631
Epoch 1631, Loss: 0.00000002116874, Improvement: 0.00000000236199, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1632
Epoch 1632, Loss: 0.00000004466911, Improvement: 0.00000002350038, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1633
Epoch 1633, Loss: 0.00000005572144, Improvement: 0.00000001105233, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1634
Epoch 1634, Loss: 0.00000006469041, Improvement: 0.00000000896896, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1635
Epoch 1635, Loss: 0.00000004450412, Improvement: -0.00000002018629, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1636
Epoch 1636, Loss: 0.00000003203340, Improvement: -0.00000001247072, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1637
Epoch 1637, Loss: 0.00000002408240, Improvement: -0.00000000795099, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1638
Epoch 1638, Loss: 0.00000002591292, Improvement: 0.00000000183052, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1639
Epoch 1639, Loss: 0.00000002230593, Improvement: -0.00000000360700, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1640
Epoch 1640, Loss: 0.00000001808813, Improvement: -0.00000000421780, Best Loss: 0.00000001055981 in Epoch 1615
Epoch 1641
A best model at epoch 1641 has been saved with training error 0.00000000983310.
Epoch 1641, Loss: 0.00000001587813, Improvement: -0.00000000221000, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1642
Epoch 1642, Loss: 0.00000001578220, Improvement: -0.00000000009593, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1643
Epoch 1643, Loss: 0.00000001598235, Improvement: 0.00000000020015, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1644
Epoch 1644, Loss: 0.00000001557786, Improvement: -0.00000000040449, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1645
Epoch 1645, Loss: 0.00000001518280, Improvement: -0.00000000039506, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1646
Epoch 1646, Loss: 0.00000001498104, Improvement: -0.00000000020176, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1647
Epoch 1647, Loss: 0.00000001545133, Improvement: 0.00000000047029, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1648
Epoch 1648, Loss: 0.00000001522474, Improvement: -0.00000000022660, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1649
Epoch 1649, Loss: 0.00000001456644, Improvement: -0.00000000065830, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000001487129, Improvement: 0.00000000030485, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1651
Epoch 1651, Loss: 0.00000001562571, Improvement: 0.00000000075442, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1652
Epoch 1652, Loss: 0.00000001660588, Improvement: 0.00000000098017, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1653
Epoch 1653, Loss: 0.00000001563398, Improvement: -0.00000000097190, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1654
Epoch 1654, Loss: 0.00000001691968, Improvement: 0.00000000128570, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1655
Epoch 1655, Loss: 0.00000001619634, Improvement: -0.00000000072335, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1656
Epoch 1656, Loss: 0.00000001718051, Improvement: 0.00000000098418, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1657
Epoch 1657, Loss: 0.00000001763087, Improvement: 0.00000000045035, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1658
Epoch 1658, Loss: 0.00000002441089, Improvement: 0.00000000678002, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1659
Epoch 1659, Loss: 0.00000003343849, Improvement: 0.00000000902760, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1660
Epoch 1660, Loss: 0.00000003151383, Improvement: -0.00000000192466, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1661
Epoch 1661, Loss: 0.00000002408429, Improvement: -0.00000000742955, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1662
Epoch 1662, Loss: 0.00000002057848, Improvement: -0.00000000350580, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1663
Epoch 1663, Loss: 0.00000002129371, Improvement: 0.00000000071523, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1664
Epoch 1664, Loss: 0.00000002007252, Improvement: -0.00000000122120, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1665
Epoch 1665, Loss: 0.00000002232714, Improvement: 0.00000000225462, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1666
Epoch 1666, Loss: 0.00000002498659, Improvement: 0.00000000265946, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1667
Epoch 1667, Loss: 0.00000002304466, Improvement: -0.00000000194193, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1668
Epoch 1668, Loss: 0.00000002100976, Improvement: -0.00000000203490, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1669
Epoch 1669, Loss: 0.00000002716491, Improvement: 0.00000000615515, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1670
Epoch 1670, Loss: 0.00000004061490, Improvement: 0.00000001344999, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1671
Epoch 1671, Loss: 0.00000007329607, Improvement: 0.00000003268117, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1672
Epoch 1672, Loss: 0.00000004044035, Improvement: -0.00000003285572, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1673
Epoch 1673, Loss: 0.00000002233808, Improvement: -0.00000001810227, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1674
Epoch 1674, Loss: 0.00000001714379, Improvement: -0.00000000519429, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1675
Epoch 1675, Loss: 0.00000001597254, Improvement: -0.00000000117125, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1676
Epoch 1676, Loss: 0.00000001708468, Improvement: 0.00000000111214, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1677
Epoch 1677, Loss: 0.00000001763837, Improvement: 0.00000000055369, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1678
Epoch 1678, Loss: 0.00000002023516, Improvement: 0.00000000259679, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1679
Epoch 1679, Loss: 0.00000002142207, Improvement: 0.00000000118691, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1680
Epoch 1680, Loss: 0.00000002274091, Improvement: 0.00000000131884, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1681
Epoch 1681, Loss: 0.00000001939519, Improvement: -0.00000000334572, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1682
Epoch 1682, Loss: 0.00000001675258, Improvement: -0.00000000264261, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1683
Epoch 1683, Loss: 0.00000001554315, Improvement: -0.00000000120943, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1684
Epoch 1684, Loss: 0.00000001491286, Improvement: -0.00000000063030, Best Loss: 0.00000000983310 in Epoch 1641
Epoch 1685
A best model at epoch 1685 has been saved with training error 0.00000000977130.
A best model at epoch 1685 has been saved with training error 0.00000000877292.
Epoch 1685, Loss: 0.00000001409304, Improvement: -0.00000000081982, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1686
Epoch 1686, Loss: 0.00000001469118, Improvement: 0.00000000059814, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1687
Epoch 1687, Loss: 0.00000001507646, Improvement: 0.00000000038529, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1688
Epoch 1688, Loss: 0.00000001614947, Improvement: 0.00000000107301, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1689
Epoch 1689, Loss: 0.00000002641507, Improvement: 0.00000001026560, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1690
Epoch 1690, Loss: 0.00000003909768, Improvement: 0.00000001268261, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1691
Epoch 1691, Loss: 0.00000003163886, Improvement: -0.00000000745882, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1692
Epoch 1692, Loss: 0.00000003809413, Improvement: 0.00000000645527, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1693
Epoch 1693, Loss: 0.00000003342220, Improvement: -0.00000000467193, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1694
Epoch 1694, Loss: 0.00000003176505, Improvement: -0.00000000165715, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1695
Epoch 1695, Loss: 0.00000002619186, Improvement: -0.00000000557319, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1696
Epoch 1696, Loss: 0.00000002546430, Improvement: -0.00000000072756, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1697
Epoch 1697, Loss: 0.00000001796392, Improvement: -0.00000000750037, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1698
Epoch 1698, Loss: 0.00000001655715, Improvement: -0.00000000140678, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1699
Epoch 1699, Loss: 0.00000001693104, Improvement: 0.00000000037389, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000001656340, Improvement: -0.00000000036764, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1701
Epoch 1701, Loss: 0.00000001674683, Improvement: 0.00000000018344, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1702
Epoch 1702, Loss: 0.00000001728515, Improvement: 0.00000000053831, Best Loss: 0.00000000877292 in Epoch 1685
Epoch 1703
A best model at epoch 1703 has been saved with training error 0.00000000870684.
Epoch 1703, Loss: 0.00000001459350, Improvement: -0.00000000269165, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1704
Epoch 1704, Loss: 0.00000001676205, Improvement: 0.00000000216855, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1705
Epoch 1705, Loss: 0.00000002807351, Improvement: 0.00000001131146, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1706
Epoch 1706, Loss: 0.00000003406027, Improvement: 0.00000000598676, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1707
Epoch 1707, Loss: 0.00000004730025, Improvement: 0.00000001323998, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1708
Epoch 1708, Loss: 0.00000006943100, Improvement: 0.00000002213075, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1709
Epoch 1709, Loss: 0.00000004571684, Improvement: -0.00000002371416, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1710
Epoch 1710, Loss: 0.00000002719009, Improvement: -0.00000001852675, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1711
Epoch 1711, Loss: 0.00000001876359, Improvement: -0.00000000842650, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1712
Epoch 1712, Loss: 0.00000001530958, Improvement: -0.00000000345402, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1713
Epoch 1713, Loss: 0.00000001420122, Improvement: -0.00000000110836, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1714
Epoch 1714, Loss: 0.00000001361327, Improvement: -0.00000000058795, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1715
Epoch 1715, Loss: 0.00000001340188, Improvement: -0.00000000021139, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1716
Epoch 1716, Loss: 0.00000001320739, Improvement: -0.00000000019449, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1717
Epoch 1717, Loss: 0.00000001325508, Improvement: 0.00000000004768, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1718
Epoch 1718, Loss: 0.00000001345284, Improvement: 0.00000000019776, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1719
Epoch 1719, Loss: 0.00000001421474, Improvement: 0.00000000076190, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1720
Epoch 1720, Loss: 0.00000001359139, Improvement: -0.00000000062335, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1721
Epoch 1721, Loss: 0.00000001362827, Improvement: 0.00000000003688, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1722
Epoch 1722, Loss: 0.00000001383378, Improvement: 0.00000000020551, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1723
Epoch 1723, Loss: 0.00000001402784, Improvement: 0.00000000019406, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1724
Epoch 1724, Loss: 0.00000001420559, Improvement: 0.00000000017776, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1725
Epoch 1725, Loss: 0.00000001381899, Improvement: -0.00000000038660, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1726
Epoch 1726, Loss: 0.00000001419630, Improvement: 0.00000000037731, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1727
Epoch 1727, Loss: 0.00000001387471, Improvement: -0.00000000032159, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1728
Epoch 1728, Loss: 0.00000001469649, Improvement: 0.00000000082178, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1729
Epoch 1729, Loss: 0.00000001479673, Improvement: 0.00000000010024, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1730
Epoch 1730, Loss: 0.00000001461625, Improvement: -0.00000000018049, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1731
Epoch 1731, Loss: 0.00000001741429, Improvement: 0.00000000279805, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1732
Epoch 1732, Loss: 0.00000002750791, Improvement: 0.00000001009362, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1733
Epoch 1733, Loss: 0.00000003206610, Improvement: 0.00000000455819, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1734
Epoch 1734, Loss: 0.00000004822601, Improvement: 0.00000001615991, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1735
Epoch 1735, Loss: 0.00000003535793, Improvement: -0.00000001286808, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1736
Epoch 1736, Loss: 0.00000003607136, Improvement: 0.00000000071343, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1737
Epoch 1737, Loss: 0.00000002244044, Improvement: -0.00000001363091, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1738
Epoch 1738, Loss: 0.00000002032690, Improvement: -0.00000000211354, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1739
Epoch 1739, Loss: 0.00000001604922, Improvement: -0.00000000427768, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1740
Epoch 1740, Loss: 0.00000001424091, Improvement: -0.00000000180831, Best Loss: 0.00000000870684 in Epoch 1703
Epoch 1741
A best model at epoch 1741 has been saved with training error 0.00000000783866.
Epoch 1741, Loss: 0.00000001373897, Improvement: -0.00000000050194, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1742
Epoch 1742, Loss: 0.00000001344918, Improvement: -0.00000000028980, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1743
Epoch 1743, Loss: 0.00000001364366, Improvement: 0.00000000019448, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1744
Epoch 1744, Loss: 0.00000001440059, Improvement: 0.00000000075694, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1745
Epoch 1745, Loss: 0.00000001655909, Improvement: 0.00000000215849, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1746
Epoch 1746, Loss: 0.00000001748920, Improvement: 0.00000000093011, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1747
Epoch 1747, Loss: 0.00000001732036, Improvement: -0.00000000016884, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1748
Epoch 1748, Loss: 0.00000001865519, Improvement: 0.00000000133484, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1749
Epoch 1749, Loss: 0.00000002063922, Improvement: 0.00000000198403, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000003450273, Improvement: 0.00000001386351, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1751
Epoch 1751, Loss: 0.00000003227675, Improvement: -0.00000000222598, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1752
Epoch 1752, Loss: 0.00000002325438, Improvement: -0.00000000902236, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1753
Epoch 1753, Loss: 0.00000002629095, Improvement: 0.00000000303656, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1754
Epoch 1754, Loss: 0.00000002351698, Improvement: -0.00000000277396, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1755
Epoch 1755, Loss: 0.00000002652778, Improvement: 0.00000000301080, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1756
Epoch 1756, Loss: 0.00000002247480, Improvement: -0.00000000405299, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1757
Epoch 1757, Loss: 0.00000001748450, Improvement: -0.00000000499029, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1758
Epoch 1758, Loss: 0.00000001480852, Improvement: -0.00000000267598, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1759
Epoch 1759, Loss: 0.00000001356570, Improvement: -0.00000000124282, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1760
Epoch 1760, Loss: 0.00000001400038, Improvement: 0.00000000043468, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1761
Epoch 1761, Loss: 0.00000001438678, Improvement: 0.00000000038641, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1762
Epoch 1762, Loss: 0.00000001723168, Improvement: 0.00000000284490, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1763
Epoch 1763, Loss: 0.00000001578712, Improvement: -0.00000000144456, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1764
Epoch 1764, Loss: 0.00000001613150, Improvement: 0.00000000034438, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1765
Epoch 1765, Loss: 0.00000001731290, Improvement: 0.00000000118140, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1766
Epoch 1766, Loss: 0.00000001607782, Improvement: -0.00000000123508, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1767
Epoch 1767, Loss: 0.00000001682922, Improvement: 0.00000000075139, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1768
Epoch 1768, Loss: 0.00000001912935, Improvement: 0.00000000230014, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1769
Epoch 1769, Loss: 0.00000006300518, Improvement: 0.00000004387583, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1770
Epoch 1770, Loss: 0.00000004632122, Improvement: -0.00000001668396, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1771
Epoch 1771, Loss: 0.00000002797255, Improvement: -0.00000001834867, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1772
Epoch 1772, Loss: 0.00000002182564, Improvement: -0.00000000614690, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1773
Epoch 1773, Loss: 0.00000002004268, Improvement: -0.00000000178297, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1774
Epoch 1774, Loss: 0.00000001643051, Improvement: -0.00000000361216, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1775
Epoch 1775, Loss: 0.00000001502501, Improvement: -0.00000000140550, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1776
Epoch 1776, Loss: 0.00000001428371, Improvement: -0.00000000074130, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1777
Epoch 1777, Loss: 0.00000001388752, Improvement: -0.00000000039619, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1778
Epoch 1778, Loss: 0.00000001536663, Improvement: 0.00000000147911, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1779
Epoch 1779, Loss: 0.00000001419761, Improvement: -0.00000000116902, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1780
Epoch 1780, Loss: 0.00000001280217, Improvement: -0.00000000139545, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1781
Epoch 1781, Loss: 0.00000001255566, Improvement: -0.00000000024651, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1782
Epoch 1782, Loss: 0.00000001261972, Improvement: 0.00000000006406, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1783
Epoch 1783, Loss: 0.00000001266980, Improvement: 0.00000000005007, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1784
Epoch 1784, Loss: 0.00000001258561, Improvement: -0.00000000008418, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1785
Epoch 1785, Loss: 0.00000001276311, Improvement: 0.00000000017750, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1786
Epoch 1786, Loss: 0.00000001354159, Improvement: 0.00000000077848, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1787
Epoch 1787, Loss: 0.00000001307724, Improvement: -0.00000000046435, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1788
Epoch 1788, Loss: 0.00000001367886, Improvement: 0.00000000060162, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1789
Epoch 1789, Loss: 0.00000001494049, Improvement: 0.00000000126163, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1790
Epoch 1790, Loss: 0.00000001554000, Improvement: 0.00000000059951, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1791
Epoch 1791, Loss: 0.00000001626752, Improvement: 0.00000000072752, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1792
Epoch 1792, Loss: 0.00000001550871, Improvement: -0.00000000075880, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1793
Epoch 1793, Loss: 0.00000001422809, Improvement: -0.00000000128062, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1794
Epoch 1794, Loss: 0.00000001578316, Improvement: 0.00000000155507, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1795
Epoch 1795, Loss: 0.00000001833725, Improvement: 0.00000000255409, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1796
Epoch 1796, Loss: 0.00000002317344, Improvement: 0.00000000483619, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1797
Epoch 1797, Loss: 0.00000002296065, Improvement: -0.00000000021279, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1798
Epoch 1798, Loss: 0.00000002812740, Improvement: 0.00000000516675, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1799
Epoch 1799, Loss: 0.00000002869020, Improvement: 0.00000000056280, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000002580865, Improvement: -0.00000000288155, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1801
Epoch 1801, Loss: 0.00000001755312, Improvement: -0.00000000825553, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1802
Epoch 1802, Loss: 0.00000001600930, Improvement: -0.00000000154382, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1803
Epoch 1803, Loss: 0.00000002314301, Improvement: 0.00000000713371, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1804
Epoch 1804, Loss: 0.00000002315883, Improvement: 0.00000000001582, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1805
Epoch 1805, Loss: 0.00000003882890, Improvement: 0.00000001567006, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1806
Epoch 1806, Loss: 0.00000004404143, Improvement: 0.00000000521254, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1807
Epoch 1807, Loss: 0.00000004536326, Improvement: 0.00000000132183, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1808
Epoch 1808, Loss: 0.00000004046022, Improvement: -0.00000000490304, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1809
Epoch 1809, Loss: 0.00000003090020, Improvement: -0.00000000956002, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1810
Epoch 1810, Loss: 0.00000002579111, Improvement: -0.00000000510910, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1811
Epoch 1811, Loss: 0.00000001798734, Improvement: -0.00000000780376, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1812
Epoch 1812, Loss: 0.00000001736222, Improvement: -0.00000000062513, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1813
Epoch 1813, Loss: 0.00000001550717, Improvement: -0.00000000185505, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1814
Epoch 1814, Loss: 0.00000001332221, Improvement: -0.00000000218496, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1815
Epoch 1815, Loss: 0.00000001322718, Improvement: -0.00000000009503, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1816
Epoch 1816, Loss: 0.00000001327695, Improvement: 0.00000000004977, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1817
Epoch 1817, Loss: 0.00000001458802, Improvement: 0.00000000131107, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1818
Epoch 1818, Loss: 0.00000001327453, Improvement: -0.00000000131350, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1819
Epoch 1819, Loss: 0.00000001312120, Improvement: -0.00000000015333, Best Loss: 0.00000000783866 in Epoch 1741
Epoch 1820
A best model at epoch 1820 has been saved with training error 0.00000000783175.
Epoch 1820, Loss: 0.00000001277677, Improvement: -0.00000000034443, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1821
Epoch 1821, Loss: 0.00000001272949, Improvement: -0.00000000004728, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1822
Epoch 1822, Loss: 0.00000001376046, Improvement: 0.00000000103098, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1823
Epoch 1823, Loss: 0.00000001419082, Improvement: 0.00000000043036, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1824
Epoch 1824, Loss: 0.00000001402642, Improvement: -0.00000000016440, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1825
Epoch 1825, Loss: 0.00000001372913, Improvement: -0.00000000029729, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1826
Epoch 1826, Loss: 0.00000001337059, Improvement: -0.00000000035854, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1827
Epoch 1827, Loss: 0.00000001328127, Improvement: -0.00000000008932, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1828
Epoch 1828, Loss: 0.00000001475284, Improvement: 0.00000000147157, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1829
Epoch 1829, Loss: 0.00000001934466, Improvement: 0.00000000459182, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1830
Epoch 1830, Loss: 0.00000004176903, Improvement: 0.00000002242437, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1831
Epoch 1831, Loss: 0.00000004095504, Improvement: -0.00000000081399, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1832
Epoch 1832, Loss: 0.00000003448930, Improvement: -0.00000000646573, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1833
Epoch 1833, Loss: 0.00000003765489, Improvement: 0.00000000316559, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1834
Epoch 1834, Loss: 0.00000003899387, Improvement: 0.00000000133898, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1835
Epoch 1835, Loss: 0.00000002741086, Improvement: -0.00000001158300, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1836
Epoch 1836, Loss: 0.00000001560717, Improvement: -0.00000001180369, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1837
Epoch 1837, Loss: 0.00000001789254, Improvement: 0.00000000228537, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1838
Epoch 1838, Loss: 0.00000001598501, Improvement: -0.00000000190753, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1839
Epoch 1839, Loss: 0.00000001356440, Improvement: -0.00000000242061, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1840
Epoch 1840, Loss: 0.00000001295428, Improvement: -0.00000000061013, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1841
Epoch 1841, Loss: 0.00000001346003, Improvement: 0.00000000050576, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1842
Epoch 1842, Loss: 0.00000001300737, Improvement: -0.00000000045266, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1843
Epoch 1843, Loss: 0.00000001252190, Improvement: -0.00000000048547, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1844
Epoch 1844, Loss: 0.00000001302415, Improvement: 0.00000000050225, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1845
Epoch 1845, Loss: 0.00000001670222, Improvement: 0.00000000367807, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1846
Epoch 1846, Loss: 0.00000001888649, Improvement: 0.00000000218428, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1847
Epoch 1847, Loss: 0.00000001979741, Improvement: 0.00000000091092, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1848
Epoch 1848, Loss: 0.00000001580262, Improvement: -0.00000000399479, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1849
Epoch 1849, Loss: 0.00000001519967, Improvement: -0.00000000060295, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000001575054, Improvement: 0.00000000055087, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1851
Epoch 1851, Loss: 0.00000002683256, Improvement: 0.00000001108202, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1852
Epoch 1852, Loss: 0.00000002772407, Improvement: 0.00000000089151, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1853
Epoch 1853, Loss: 0.00000002895453, Improvement: 0.00000000123046, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1854
Epoch 1854, Loss: 0.00000002714081, Improvement: -0.00000000181372, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1855
Epoch 1855, Loss: 0.00000002351420, Improvement: -0.00000000362662, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1856
Epoch 1856, Loss: 0.00000002317877, Improvement: -0.00000000033543, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1857
Epoch 1857, Loss: 0.00000001630621, Improvement: -0.00000000687256, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1858
Epoch 1858, Loss: 0.00000001451702, Improvement: -0.00000000178919, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1859
Epoch 1859, Loss: 0.00000001348485, Improvement: -0.00000000103217, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1860
Epoch 1860, Loss: 0.00000001435582, Improvement: 0.00000000087097, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1861
Epoch 1861, Loss: 0.00000001573095, Improvement: 0.00000000137513, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1862
Epoch 1862, Loss: 0.00000002050225, Improvement: 0.00000000477130, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1863
Epoch 1863, Loss: 0.00000002785801, Improvement: 0.00000000735576, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1864
Epoch 1864, Loss: 0.00000003806671, Improvement: 0.00000001020870, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1865
Epoch 1865, Loss: 0.00000002329251, Improvement: -0.00000001477421, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1866
Epoch 1866, Loss: 0.00000002972637, Improvement: 0.00000000643386, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1867
Epoch 1867, Loss: 0.00000002514562, Improvement: -0.00000000458075, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1868
Epoch 1868, Loss: 0.00000001935001, Improvement: -0.00000000579562, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1869
Epoch 1869, Loss: 0.00000001562196, Improvement: -0.00000000372804, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1870
Epoch 1870, Loss: 0.00000001473607, Improvement: -0.00000000088590, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1871
Epoch 1871, Loss: 0.00000001463448, Improvement: -0.00000000010158, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1872
Epoch 1872, Loss: 0.00000001360934, Improvement: -0.00000000102514, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1873
Epoch 1873, Loss: 0.00000001392725, Improvement: 0.00000000031791, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1874
Epoch 1874, Loss: 0.00000001493756, Improvement: 0.00000000101032, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1875
Epoch 1875, Loss: 0.00000001695122, Improvement: 0.00000000201365, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1876
Epoch 1876, Loss: 0.00000002593571, Improvement: 0.00000000898449, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1877
Epoch 1877, Loss: 0.00000002067477, Improvement: -0.00000000526095, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1878
Epoch 1878, Loss: 0.00000002727825, Improvement: 0.00000000660348, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1879
Epoch 1879, Loss: 0.00000003047389, Improvement: 0.00000000319564, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1880
Epoch 1880, Loss: 0.00000003773426, Improvement: 0.00000000726037, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1881
Epoch 1881, Loss: 0.00000002994053, Improvement: -0.00000000779373, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1882
Epoch 1882, Loss: 0.00000002948688, Improvement: -0.00000000045364, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1883
Epoch 1883, Loss: 0.00000003075245, Improvement: 0.00000000126556, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1884
Epoch 1884, Loss: 0.00000002176480, Improvement: -0.00000000898765, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1885
Epoch 1885, Loss: 0.00000002461955, Improvement: 0.00000000285476, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1886
Epoch 1886, Loss: 0.00000002152362, Improvement: -0.00000000309594, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1887
Epoch 1887, Loss: 0.00000001801774, Improvement: -0.00000000350587, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1888
Epoch 1888, Loss: 0.00000001417121, Improvement: -0.00000000384653, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1889
Epoch 1889, Loss: 0.00000001488339, Improvement: 0.00000000071218, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1890
Epoch 1890, Loss: 0.00000001314711, Improvement: -0.00000000173628, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1891
Epoch 1891, Loss: 0.00000001254868, Improvement: -0.00000000059843, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1892
Epoch 1892, Loss: 0.00000001297393, Improvement: 0.00000000042525, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1893
Epoch 1893, Loss: 0.00000001241865, Improvement: -0.00000000055528, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1894
Epoch 1894, Loss: 0.00000001298956, Improvement: 0.00000000057091, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1895
Epoch 1895, Loss: 0.00000001238510, Improvement: -0.00000000060446, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1896
Epoch 1896, Loss: 0.00000001246196, Improvement: 0.00000000007686, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1897
Epoch 1897, Loss: 0.00000001509269, Improvement: 0.00000000263073, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1898
Epoch 1898, Loss: 0.00000001376682, Improvement: -0.00000000132587, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1899
Epoch 1899, Loss: 0.00000001472368, Improvement: 0.00000000095686, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000001649950, Improvement: 0.00000000177583, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1901
Epoch 1901, Loss: 0.00000002879906, Improvement: 0.00000001229955, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1902
Epoch 1902, Loss: 0.00000004686832, Improvement: 0.00000001806926, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1903
Epoch 1903, Loss: 0.00000003376920, Improvement: -0.00000001309912, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1904
Epoch 1904, Loss: 0.00000002583863, Improvement: -0.00000000793058, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1905
Epoch 1905, Loss: 0.00000002355298, Improvement: -0.00000000228565, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1906
Epoch 1906, Loss: 0.00000002841293, Improvement: 0.00000000485995, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1907
Epoch 1907, Loss: 0.00000002007822, Improvement: -0.00000000833471, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1908
Epoch 1908, Loss: 0.00000002966212, Improvement: 0.00000000958390, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1909
Epoch 1909, Loss: 0.00000002957855, Improvement: -0.00000000008357, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1910
Epoch 1910, Loss: 0.00000002729174, Improvement: -0.00000000228680, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1911
Epoch 1911, Loss: 0.00000001813806, Improvement: -0.00000000915369, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1912
Epoch 1912, Loss: 0.00000001607466, Improvement: -0.00000000206340, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1913
Epoch 1913, Loss: 0.00000001521287, Improvement: -0.00000000086178, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1914
Epoch 1914, Loss: 0.00000001387186, Improvement: -0.00000000134101, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1915
Epoch 1915, Loss: 0.00000001716145, Improvement: 0.00000000328958, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1916
Epoch 1916, Loss: 0.00000002573617, Improvement: 0.00000000857473, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1917
Epoch 1917, Loss: 0.00000002195427, Improvement: -0.00000000378190, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1918
Epoch 1918, Loss: 0.00000002079678, Improvement: -0.00000000115749, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1919
Epoch 1919, Loss: 0.00000001826647, Improvement: -0.00000000253031, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1920
Epoch 1920, Loss: 0.00000001887715, Improvement: 0.00000000061068, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1921
Epoch 1921, Loss: 0.00000001693132, Improvement: -0.00000000194583, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1922
Epoch 1922, Loss: 0.00000001910080, Improvement: 0.00000000216948, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1923
Epoch 1923, Loss: 0.00000001699116, Improvement: -0.00000000210964, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1924
Epoch 1924, Loss: 0.00000001381189, Improvement: -0.00000000317928, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1925
Epoch 1925, Loss: 0.00000002556673, Improvement: 0.00000001175484, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1926
Epoch 1926, Loss: 0.00000002454103, Improvement: -0.00000000102570, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1927
Epoch 1927, Loss: 0.00000001994751, Improvement: -0.00000000459351, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1928
Epoch 1928, Loss: 0.00000002106156, Improvement: 0.00000000111405, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1929
Epoch 1929, Loss: 0.00000002908481, Improvement: 0.00000000802325, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1930
Epoch 1930, Loss: 0.00000001940551, Improvement: -0.00000000967930, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1931
Epoch 1931, Loss: 0.00000001544973, Improvement: -0.00000000395578, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1932
Epoch 1932, Loss: 0.00000001556121, Improvement: 0.00000000011148, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1933
Epoch 1933, Loss: 0.00000001611662, Improvement: 0.00000000055541, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1934
Epoch 1934, Loss: 0.00000001395659, Improvement: -0.00000000216003, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1935
Epoch 1935, Loss: 0.00000001973135, Improvement: 0.00000000577476, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1936
Epoch 1936, Loss: 0.00000001944899, Improvement: -0.00000000028236, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1937
Epoch 1937, Loss: 0.00000001486643, Improvement: -0.00000000458256, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1938
Epoch 1938, Loss: 0.00000001791905, Improvement: 0.00000000305261, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1939
Epoch 1939, Loss: 0.00000001588187, Improvement: -0.00000000203717, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1940
Epoch 1940, Loss: 0.00000001400994, Improvement: -0.00000000187193, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1941
Epoch 1941, Loss: 0.00000001268452, Improvement: -0.00000000132542, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1942
Epoch 1942, Loss: 0.00000001668706, Improvement: 0.00000000400255, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1943
Epoch 1943, Loss: 0.00000001747383, Improvement: 0.00000000078676, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1944
Epoch 1944, Loss: 0.00000002634390, Improvement: 0.00000000887008, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1945
Epoch 1945, Loss: 0.00000002111664, Improvement: -0.00000000522727, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1946
Epoch 1946, Loss: 0.00000003061220, Improvement: 0.00000000949556, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1947
Epoch 1947, Loss: 0.00000004540149, Improvement: 0.00000001478929, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1948
Epoch 1948, Loss: 0.00000003576983, Improvement: -0.00000000963166, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1949
Epoch 1949, Loss: 0.00000002143687, Improvement: -0.00000001433295, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000001841667, Improvement: -0.00000000302020, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1951
Epoch 1951, Loss: 0.00000001779934, Improvement: -0.00000000061733, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1952
Epoch 1952, Loss: 0.00000001534642, Improvement: -0.00000000245292, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1953
Epoch 1953, Loss: 0.00000001327482, Improvement: -0.00000000207161, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1954
Epoch 1954, Loss: 0.00000001179799, Improvement: -0.00000000147683, Best Loss: 0.00000000783175 in Epoch 1820
Epoch 1955
A best model at epoch 1955 has been saved with training error 0.00000000674099.
Epoch 1955, Loss: 0.00000001150949, Improvement: -0.00000000028849, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1956
Epoch 1956, Loss: 0.00000001130122, Improvement: -0.00000000020827, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1957
Epoch 1957, Loss: 0.00000001167981, Improvement: 0.00000000037859, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1958
Epoch 1958, Loss: 0.00000001208505, Improvement: 0.00000000040524, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1959
Epoch 1959, Loss: 0.00000001266807, Improvement: 0.00000000058301, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1960
Epoch 1960, Loss: 0.00000001222863, Improvement: -0.00000000043944, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1961
Epoch 1961, Loss: 0.00000001289746, Improvement: 0.00000000066883, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1962
Epoch 1962, Loss: 0.00000001778032, Improvement: 0.00000000488286, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1963
Epoch 1963, Loss: 0.00000001850094, Improvement: 0.00000000072062, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1964
Epoch 1964, Loss: 0.00000001562982, Improvement: -0.00000000287112, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1965
Epoch 1965, Loss: 0.00000001354840, Improvement: -0.00000000208142, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1966
Epoch 1966, Loss: 0.00000002084838, Improvement: 0.00000000729999, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1967
Epoch 1967, Loss: 0.00000002706509, Improvement: 0.00000000621670, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1968
Epoch 1968, Loss: 0.00000002531364, Improvement: -0.00000000175145, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1969
Epoch 1969, Loss: 0.00000002144772, Improvement: -0.00000000386592, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1970
Epoch 1970, Loss: 0.00000001621234, Improvement: -0.00000000523538, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1971
Epoch 1971, Loss: 0.00000001719690, Improvement: 0.00000000098456, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1972
Epoch 1972, Loss: 0.00000001898042, Improvement: 0.00000000178352, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1973
Epoch 1973, Loss: 0.00000002141927, Improvement: 0.00000000243885, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1974
Epoch 1974, Loss: 0.00000002007392, Improvement: -0.00000000134535, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1975
Epoch 1975, Loss: 0.00000001809519, Improvement: -0.00000000197873, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1976
Epoch 1976, Loss: 0.00000001923951, Improvement: 0.00000000114432, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1977
Epoch 1977, Loss: 0.00000003185807, Improvement: 0.00000001261856, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1978
Epoch 1978, Loss: 0.00000006452655, Improvement: 0.00000003266849, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1979
Epoch 1979, Loss: 0.00000005329879, Improvement: -0.00000001122776, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1980
Epoch 1980, Loss: 0.00000002412740, Improvement: -0.00000002917139, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1981
Epoch 1981, Loss: 0.00000001476053, Improvement: -0.00000000936687, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1982
Epoch 1982, Loss: 0.00000001199792, Improvement: -0.00000000276261, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1983
Epoch 1983, Loss: 0.00000001122619, Improvement: -0.00000000077173, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1984
Epoch 1984, Loss: 0.00000001109654, Improvement: -0.00000000012964, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1985
Epoch 1985, Loss: 0.00000001117672, Improvement: 0.00000000008017, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1986
Epoch 1986, Loss: 0.00000001117105, Improvement: -0.00000000000566, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1987
Epoch 1987, Loss: 0.00000001091621, Improvement: -0.00000000025485, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1988
Epoch 1988, Loss: 0.00000001088261, Improvement: -0.00000000003359, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1989
Epoch 1989, Loss: 0.00000001082462, Improvement: -0.00000000005800, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1990
Epoch 1990, Loss: 0.00000001100983, Improvement: 0.00000000018521, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1991
Epoch 1991, Loss: 0.00000001102793, Improvement: 0.00000000001811, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1992
Epoch 1992, Loss: 0.00000001108265, Improvement: 0.00000000005472, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1993
Epoch 1993, Loss: 0.00000001153049, Improvement: 0.00000000044784, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1994
Epoch 1994, Loss: 0.00000001130879, Improvement: -0.00000000022171, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1995
Epoch 1995, Loss: 0.00000001137402, Improvement: 0.00000000006523, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1996
Epoch 1996, Loss: 0.00000001161894, Improvement: 0.00000000024492, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1997
Epoch 1997, Loss: 0.00000001102685, Improvement: -0.00000000059209, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1998
Epoch 1998, Loss: 0.00000001095316, Improvement: -0.00000000007369, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 1999
Epoch 1999, Loss: 0.00000001089129, Improvement: -0.00000000006187, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000001101111, Improvement: 0.00000000011982, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2001
Epoch 2001, Loss: 0.00000001136077, Improvement: 0.00000000034966, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2002
Epoch 2002, Loss: 0.00000001196831, Improvement: 0.00000000060754, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2003
Epoch 2003, Loss: 0.00000001175019, Improvement: -0.00000000021812, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2004
Epoch 2004, Loss: 0.00000001211580, Improvement: 0.00000000036561, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2005
Epoch 2005, Loss: 0.00000001252974, Improvement: 0.00000000041394, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2006
Epoch 2006, Loss: 0.00000001310656, Improvement: 0.00000000057682, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2007
Epoch 2007, Loss: 0.00000001373529, Improvement: 0.00000000062873, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2008
Epoch 2008, Loss: 0.00000001409080, Improvement: 0.00000000035552, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2009
Epoch 2009, Loss: 0.00000001629717, Improvement: 0.00000000220637, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2010
Epoch 2010, Loss: 0.00000002630397, Improvement: 0.00000001000680, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2011
Epoch 2011, Loss: 0.00000003439020, Improvement: 0.00000000808623, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2012
Epoch 2012, Loss: 0.00000002423871, Improvement: -0.00000001015149, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2013
Epoch 2013, Loss: 0.00000001631331, Improvement: -0.00000000792540, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2014
Epoch 2014, Loss: 0.00000001921975, Improvement: 0.00000000290645, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2015
Epoch 2015, Loss: 0.00000001777221, Improvement: -0.00000000144754, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2016
Epoch 2016, Loss: 0.00000001413734, Improvement: -0.00000000363487, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2017
Epoch 2017, Loss: 0.00000001233890, Improvement: -0.00000000179844, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2018
Epoch 2018, Loss: 0.00000001172956, Improvement: -0.00000000060934, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2019
Epoch 2019, Loss: 0.00000001140243, Improvement: -0.00000000032713, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2020
Epoch 2020, Loss: 0.00000001189748, Improvement: 0.00000000049505, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2021
Epoch 2021, Loss: 0.00000001259348, Improvement: 0.00000000069600, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2022
Epoch 2022, Loss: 0.00000001264946, Improvement: 0.00000000005597, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2023
Epoch 2023, Loss: 0.00000001574684, Improvement: 0.00000000309739, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2024
Epoch 2024, Loss: 0.00000001796972, Improvement: 0.00000000222288, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2025
Epoch 2025, Loss: 0.00000001330475, Improvement: -0.00000000466497, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2026
Epoch 2026, Loss: 0.00000001588227, Improvement: 0.00000000257752, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2027
Epoch 2027, Loss: 0.00000002060877, Improvement: 0.00000000472650, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2028
Epoch 2028, Loss: 0.00000001771751, Improvement: -0.00000000289126, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2029
Epoch 2029, Loss: 0.00000001823849, Improvement: 0.00000000052098, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2030
Epoch 2030, Loss: 0.00000001928023, Improvement: 0.00000000104174, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2031
Epoch 2031, Loss: 0.00000001533725, Improvement: -0.00000000394298, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2032
Epoch 2032, Loss: 0.00000001582287, Improvement: 0.00000000048562, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2033
Epoch 2033, Loss: 0.00000001664094, Improvement: 0.00000000081808, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2034
Epoch 2034, Loss: 0.00000002370092, Improvement: 0.00000000705998, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2035
Epoch 2035, Loss: 0.00000002498097, Improvement: 0.00000000128005, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2036
Epoch 2036, Loss: 0.00000003456124, Improvement: 0.00000000958027, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2037
Epoch 2037, Loss: 0.00000004364181, Improvement: 0.00000000908057, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2038
Epoch 2038, Loss: 0.00000002267344, Improvement: -0.00000002096837, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2039
Epoch 2039, Loss: 0.00000002363003, Improvement: 0.00000000095659, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2040
Epoch 2040, Loss: 0.00000002729942, Improvement: 0.00000000366939, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2041
Epoch 2041, Loss: 0.00000002962148, Improvement: 0.00000000232206, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2042
Epoch 2042, Loss: 0.00000002897975, Improvement: -0.00000000064173, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2043
Epoch 2043, Loss: 0.00000003121672, Improvement: 0.00000000223698, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2044
Epoch 2044, Loss: 0.00000002589926, Improvement: -0.00000000531746, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2045
Epoch 2045, Loss: 0.00000001416506, Improvement: -0.00000001173420, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2046
Epoch 2046, Loss: 0.00000001268226, Improvement: -0.00000000148280, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2047
Epoch 2047, Loss: 0.00000001185622, Improvement: -0.00000000082604, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2048
Epoch 2048, Loss: 0.00000001099315, Improvement: -0.00000000086308, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2049
Epoch 2049, Loss: 0.00000001090117, Improvement: -0.00000000009198, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000001112553, Improvement: 0.00000000022436, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2051
Epoch 2051, Loss: 0.00000001085092, Improvement: -0.00000000027461, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2052
Epoch 2052, Loss: 0.00000001080431, Improvement: -0.00000000004662, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2053
Epoch 2053, Loss: 0.00000001124168, Improvement: 0.00000000043737, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2054
Epoch 2054, Loss: 0.00000001126823, Improvement: 0.00000000002655, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2055
Epoch 2055, Loss: 0.00000001121338, Improvement: -0.00000000005486, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2056
Epoch 2056, Loss: 0.00000001164390, Improvement: 0.00000000043052, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2057
Epoch 2057, Loss: 0.00000001138536, Improvement: -0.00000000025853, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2058
Epoch 2058, Loss: 0.00000001226323, Improvement: 0.00000000087787, Best Loss: 0.00000000674099 in Epoch 1955
Epoch 2059
A best model at epoch 2059 has been saved with training error 0.00000000660550.
Epoch 2059, Loss: 0.00000001217221, Improvement: -0.00000000009102, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2060
Epoch 2060, Loss: 0.00000001430825, Improvement: 0.00000000213604, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2061
Epoch 2061, Loss: 0.00000001429469, Improvement: -0.00000000001355, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2062
Epoch 2062, Loss: 0.00000001410355, Improvement: -0.00000000019114, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2063
Epoch 2063, Loss: 0.00000001790933, Improvement: 0.00000000380577, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2064
Epoch 2064, Loss: 0.00000001517724, Improvement: -0.00000000273209, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2065
Epoch 2065, Loss: 0.00000001474171, Improvement: -0.00000000043553, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2066
Epoch 2066, Loss: 0.00000001574026, Improvement: 0.00000000099855, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2067
Epoch 2067, Loss: 0.00000002998111, Improvement: 0.00000001424086, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2068
Epoch 2068, Loss: 0.00000004918153, Improvement: 0.00000001920041, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2069
Epoch 2069, Loss: 0.00000006098939, Improvement: 0.00000001180786, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2070
Epoch 2070, Loss: 0.00000005705930, Improvement: -0.00000000393009, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2071
Epoch 2071, Loss: 0.00000002874285, Improvement: -0.00000002831645, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2072
Epoch 2072, Loss: 0.00000001660584, Improvement: -0.00000001213701, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2073
Epoch 2073, Loss: 0.00000001189290, Improvement: -0.00000000471293, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2074
Epoch 2074, Loss: 0.00000001076702, Improvement: -0.00000000112589, Best Loss: 0.00000000660550 in Epoch 2059
Epoch 2075
A best model at epoch 2075 has been saved with training error 0.00000000645225.
Epoch 2075, Loss: 0.00000001077873, Improvement: 0.00000000001171, Best Loss: 0.00000000645225 in Epoch 2075
Epoch 2076
Epoch 2076, Loss: 0.00000001068821, Improvement: -0.00000000009052, Best Loss: 0.00000000645225 in Epoch 2075
Epoch 2077
A best model at epoch 2077 has been saved with training error 0.00000000640569.
Epoch 2077, Loss: 0.00000001053914, Improvement: -0.00000000014907, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2078
Epoch 2078, Loss: 0.00000001040049, Improvement: -0.00000000013865, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2079
Epoch 2079, Loss: 0.00000001047902, Improvement: 0.00000000007853, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2080
Epoch 2080, Loss: 0.00000001045576, Improvement: -0.00000000002326, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2081
Epoch 2081, Loss: 0.00000001075449, Improvement: 0.00000000029873, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2082
Epoch 2082, Loss: 0.00000001121573, Improvement: 0.00000000046124, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2083
Epoch 2083, Loss: 0.00000001076500, Improvement: -0.00000000045073, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2084
Epoch 2084, Loss: 0.00000001054060, Improvement: -0.00000000022441, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2085
Epoch 2085, Loss: 0.00000001083928, Improvement: 0.00000000029868, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2086
Epoch 2086, Loss: 0.00000001080586, Improvement: -0.00000000003343, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2087
Epoch 2087, Loss: 0.00000001076137, Improvement: -0.00000000004448, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2088
Epoch 2088, Loss: 0.00000001092861, Improvement: 0.00000000016723, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2089
Epoch 2089, Loss: 0.00000001123289, Improvement: 0.00000000030428, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2090
Epoch 2090, Loss: 0.00000001220889, Improvement: 0.00000000097600, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2091
Epoch 2091, Loss: 0.00000001121536, Improvement: -0.00000000099353, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2092
Epoch 2092, Loss: 0.00000001127299, Improvement: 0.00000000005764, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2093
Epoch 2093, Loss: 0.00000001124554, Improvement: -0.00000000002746, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2094
Epoch 2094, Loss: 0.00000001143822, Improvement: 0.00000000019268, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2095
Epoch 2095, Loss: 0.00000001297431, Improvement: 0.00000000153609, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2096
Epoch 2096, Loss: 0.00000001251320, Improvement: -0.00000000046111, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2097
Epoch 2097, Loss: 0.00000001132993, Improvement: -0.00000000118327, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2098
Epoch 2098, Loss: 0.00000001162240, Improvement: 0.00000000029247, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2099
Epoch 2099, Loss: 0.00000001206490, Improvement: 0.00000000044250, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000001277579, Improvement: 0.00000000071089, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2101
Epoch 2101, Loss: 0.00000001171469, Improvement: -0.00000000106110, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2102
Epoch 2102, Loss: 0.00000001182421, Improvement: 0.00000000010951, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2103
Epoch 2103, Loss: 0.00000001104435, Improvement: -0.00000000077985, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2104
Epoch 2104, Loss: 0.00000001396305, Improvement: 0.00000000291869, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2105
Epoch 2105, Loss: 0.00000001364138, Improvement: -0.00000000032167, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2106
Epoch 2106, Loss: 0.00000001274089, Improvement: -0.00000000090049, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2107
Epoch 2107, Loss: 0.00000001454152, Improvement: 0.00000000180063, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2108
Epoch 2108, Loss: 0.00000001266191, Improvement: -0.00000000187961, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2109
Epoch 2109, Loss: 0.00000001271074, Improvement: 0.00000000004883, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2110
Epoch 2110, Loss: 0.00000001246428, Improvement: -0.00000000024646, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2111
Epoch 2111, Loss: 0.00000001236981, Improvement: -0.00000000009447, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2112
Epoch 2112, Loss: 0.00000001453665, Improvement: 0.00000000216684, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2113
Epoch 2113, Loss: 0.00000002600085, Improvement: 0.00000001146420, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2114
Epoch 2114, Loss: 0.00000002270807, Improvement: -0.00000000329278, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2115
Epoch 2115, Loss: 0.00000001603067, Improvement: -0.00000000667741, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2116
Epoch 2116, Loss: 0.00000001320479, Improvement: -0.00000000282587, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2117
Epoch 2117, Loss: 0.00000001472683, Improvement: 0.00000000152203, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2118
Epoch 2118, Loss: 0.00000001418523, Improvement: -0.00000000054159, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2119
Epoch 2119, Loss: 0.00000001251094, Improvement: -0.00000000167429, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2120
Epoch 2120, Loss: 0.00000001138986, Improvement: -0.00000000112109, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2121
Epoch 2121, Loss: 0.00000001117815, Improvement: -0.00000000021170, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2122
Epoch 2122, Loss: 0.00000001239714, Improvement: 0.00000000121898, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2123
Epoch 2123, Loss: 0.00000002112226, Improvement: 0.00000000872513, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2124
Epoch 2124, Loss: 0.00000004198454, Improvement: 0.00000002086227, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2125
Epoch 2125, Loss: 0.00000002863208, Improvement: -0.00000001335246, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2126
Epoch 2126, Loss: 0.00000001898070, Improvement: -0.00000000965138, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2127
Epoch 2127, Loss: 0.00000001447590, Improvement: -0.00000000450479, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2128
Epoch 2128, Loss: 0.00000001281867, Improvement: -0.00000000165724, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2129
Epoch 2129, Loss: 0.00000001281964, Improvement: 0.00000000000097, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2130
Epoch 2130, Loss: 0.00000001182309, Improvement: -0.00000000099655, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2131
Epoch 2131, Loss: 0.00000001417774, Improvement: 0.00000000235465, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2132
Epoch 2132, Loss: 0.00000001632263, Improvement: 0.00000000214489, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2133
Epoch 2133, Loss: 0.00000001759387, Improvement: 0.00000000127124, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2134
Epoch 2134, Loss: 0.00000001660707, Improvement: -0.00000000098681, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2135
Epoch 2135, Loss: 0.00000001681329, Improvement: 0.00000000020622, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2136
Epoch 2136, Loss: 0.00000002343766, Improvement: 0.00000000662436, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2137
Epoch 2137, Loss: 0.00000001996199, Improvement: -0.00000000347567, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2138
Epoch 2138, Loss: 0.00000001647021, Improvement: -0.00000000349178, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2139
Epoch 2139, Loss: 0.00000001468192, Improvement: -0.00000000178829, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2140
Epoch 2140, Loss: 0.00000001347787, Improvement: -0.00000000120405, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2141
Epoch 2141, Loss: 0.00000001673094, Improvement: 0.00000000325307, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2142
Epoch 2142, Loss: 0.00000003380427, Improvement: 0.00000001707333, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2143
Epoch 2143, Loss: 0.00000002561532, Improvement: -0.00000000818895, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2144
Epoch 2144, Loss: 0.00000002408339, Improvement: -0.00000000153192, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2145
Epoch 2145, Loss: 0.00000002619005, Improvement: 0.00000000210666, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2146
Epoch 2146, Loss: 0.00000003491794, Improvement: 0.00000000872789, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2147
Epoch 2147, Loss: 0.00000005245873, Improvement: 0.00000001754079, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2148
Epoch 2148, Loss: 0.00000005438481, Improvement: 0.00000000192608, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2149
Epoch 2149, Loss: 0.00000003525249, Improvement: -0.00000001913232, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000001841341, Improvement: -0.00000001683908, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2151
Epoch 2151, Loss: 0.00000001223129, Improvement: -0.00000000618212, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2152
Epoch 2152, Loss: 0.00000001140768, Improvement: -0.00000000082361, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2153
Epoch 2153, Loss: 0.00000001047555, Improvement: -0.00000000093213, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2154
Epoch 2154, Loss: 0.00000001021920, Improvement: -0.00000000025635, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2155
Epoch 2155, Loss: 0.00000001030967, Improvement: 0.00000000009047, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2156
Epoch 2156, Loss: 0.00000001016038, Improvement: -0.00000000014929, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2157
Epoch 2157, Loss: 0.00000001013804, Improvement: -0.00000000002234, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2158
Epoch 2158, Loss: 0.00000001027614, Improvement: 0.00000000013810, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2159
Epoch 2159, Loss: 0.00000001023179, Improvement: -0.00000000004434, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2160
Epoch 2160, Loss: 0.00000001032728, Improvement: 0.00000000009549, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2161
Epoch 2161, Loss: 0.00000001012568, Improvement: -0.00000000020161, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2162
Epoch 2162, Loss: 0.00000001012628, Improvement: 0.00000000000060, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2163
Epoch 2163, Loss: 0.00000001022587, Improvement: 0.00000000009959, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2164
Epoch 2164, Loss: 0.00000001013609, Improvement: -0.00000000008978, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2165
Epoch 2165, Loss: 0.00000001027776, Improvement: 0.00000000014167, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2166
Epoch 2166, Loss: 0.00000001021330, Improvement: -0.00000000006446, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2167
Epoch 2167, Loss: 0.00000001017288, Improvement: -0.00000000004042, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2168
Epoch 2168, Loss: 0.00000001063312, Improvement: 0.00000000046024, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2169
Epoch 2169, Loss: 0.00000001061428, Improvement: -0.00000000001884, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2170
Epoch 2170, Loss: 0.00000001046202, Improvement: -0.00000000015226, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2171
Epoch 2171, Loss: 0.00000001034626, Improvement: -0.00000000011576, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2172
Epoch 2172, Loss: 0.00000001043030, Improvement: 0.00000000008404, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2173
Epoch 2173, Loss: 0.00000001056971, Improvement: 0.00000000013941, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2174
Epoch 2174, Loss: 0.00000001036225, Improvement: -0.00000000020746, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2175
Epoch 2175, Loss: 0.00000001057250, Improvement: 0.00000000021025, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2176
Epoch 2176, Loss: 0.00000001129997, Improvement: 0.00000000072747, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2177
Epoch 2177, Loss: 0.00000001439881, Improvement: 0.00000000309883, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2178
Epoch 2178, Loss: 0.00000001448398, Improvement: 0.00000000008517, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2179
Epoch 2179, Loss: 0.00000001690737, Improvement: 0.00000000242339, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2180
Epoch 2180, Loss: 0.00000003005780, Improvement: 0.00000001315043, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2181
Epoch 2181, Loss: 0.00000002199781, Improvement: -0.00000000805999, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2182
Epoch 2182, Loss: 0.00000002285164, Improvement: 0.00000000085383, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2183
Epoch 2183, Loss: 0.00000001630094, Improvement: -0.00000000655070, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2184
Epoch 2184, Loss: 0.00000001469933, Improvement: -0.00000000160161, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2185
Epoch 2185, Loss: 0.00000001761623, Improvement: 0.00000000291689, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2186
Epoch 2186, Loss: 0.00000001310693, Improvement: -0.00000000450930, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2187
Epoch 2187, Loss: 0.00000001241359, Improvement: -0.00000000069334, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2188
Epoch 2188, Loss: 0.00000001200403, Improvement: -0.00000000040956, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2189
Epoch 2189, Loss: 0.00000001114628, Improvement: -0.00000000085774, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2190
Epoch 2190, Loss: 0.00000001042619, Improvement: -0.00000000072010, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2191
Epoch 2191, Loss: 0.00000001061829, Improvement: 0.00000000019211, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2192
Epoch 2192, Loss: 0.00000001117818, Improvement: 0.00000000055988, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2193
Epoch 2193, Loss: 0.00000001132966, Improvement: 0.00000000015148, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2194
Epoch 2194, Loss: 0.00000001506646, Improvement: 0.00000000373680, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2195
Epoch 2195, Loss: 0.00000002040225, Improvement: 0.00000000533579, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2196
Epoch 2196, Loss: 0.00000002265262, Improvement: 0.00000000225037, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2197
Epoch 2197, Loss: 0.00000002211026, Improvement: -0.00000000054236, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2198
Epoch 2198, Loss: 0.00000001411087, Improvement: -0.00000000799939, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2199
Epoch 2199, Loss: 0.00000001393254, Improvement: -0.00000000017833, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000001155970, Improvement: -0.00000000237284, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2201
Epoch 2201, Loss: 0.00000001410206, Improvement: 0.00000000254236, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2202
Epoch 2202, Loss: 0.00000002170600, Improvement: 0.00000000760394, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2203
Epoch 2203, Loss: 0.00000002799978, Improvement: 0.00000000629378, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2204
Epoch 2204, Loss: 0.00000002383212, Improvement: -0.00000000416766, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2205
Epoch 2205, Loss: 0.00000001742319, Improvement: -0.00000000640893, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2206
Epoch 2206, Loss: 0.00000001452330, Improvement: -0.00000000289989, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2207
Epoch 2207, Loss: 0.00000001453332, Improvement: 0.00000000001002, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2208
Epoch 2208, Loss: 0.00000001539780, Improvement: 0.00000000086448, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2209
Epoch 2209, Loss: 0.00000002000214, Improvement: 0.00000000460434, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2210
Epoch 2210, Loss: 0.00000001674877, Improvement: -0.00000000325337, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2211
Epoch 2211, Loss: 0.00000004086591, Improvement: 0.00000002411714, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2212
Epoch 2212, Loss: 0.00000004569211, Improvement: 0.00000000482619, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2213
Epoch 2213, Loss: 0.00000007377617, Improvement: 0.00000002808407, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2214
Epoch 2214, Loss: 0.00000004334114, Improvement: -0.00000003043503, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2215
Epoch 2215, Loss: 0.00000002021693, Improvement: -0.00000002312421, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2216
Epoch 2216, Loss: 0.00000001439970, Improvement: -0.00000000581723, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2217
Epoch 2217, Loss: 0.00000001267333, Improvement: -0.00000000172637, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2218
Epoch 2218, Loss: 0.00000001129499, Improvement: -0.00000000137834, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2219
Epoch 2219, Loss: 0.00000001010180, Improvement: -0.00000000119319, Best Loss: 0.00000000640569 in Epoch 2077
Epoch 2220
A best model at epoch 2220 has been saved with training error 0.00000000590538.
Epoch 2220, Loss: 0.00000000995199, Improvement: -0.00000000014981, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2221
Epoch 2221, Loss: 0.00000000997719, Improvement: 0.00000000002520, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2222
Epoch 2222, Loss: 0.00000000986828, Improvement: -0.00000000010891, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2223
Epoch 2223, Loss: 0.00000000977856, Improvement: -0.00000000008972, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2224
Epoch 2224, Loss: 0.00000000985249, Improvement: 0.00000000007393, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2225
Epoch 2225, Loss: 0.00000000995840, Improvement: 0.00000000010591, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2226
Epoch 2226, Loss: 0.00000000996937, Improvement: 0.00000000001098, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2227
Epoch 2227, Loss: 0.00000000986504, Improvement: -0.00000000010433, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2228
Epoch 2228, Loss: 0.00000001013496, Improvement: 0.00000000026991, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2229
Epoch 2229, Loss: 0.00000001011834, Improvement: -0.00000000001661, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2230
Epoch 2230, Loss: 0.00000001045217, Improvement: 0.00000000033383, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2231
Epoch 2231, Loss: 0.00000001037281, Improvement: -0.00000000007937, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2232
Epoch 2232, Loss: 0.00000001004504, Improvement: -0.00000000032776, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2233
Epoch 2233, Loss: 0.00000000990044, Improvement: -0.00000000014460, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2234
Epoch 2234, Loss: 0.00000000989948, Improvement: -0.00000000000096, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2235
Epoch 2235, Loss: 0.00000001018347, Improvement: 0.00000000028399, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2236
Epoch 2236, Loss: 0.00000001029919, Improvement: 0.00000000011573, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2237
Epoch 2237, Loss: 0.00000001052441, Improvement: 0.00000000022521, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2238
Epoch 2238, Loss: 0.00000001025959, Improvement: -0.00000000026482, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2239
Epoch 2239, Loss: 0.00000001004768, Improvement: -0.00000000021191, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2240
Epoch 2240, Loss: 0.00000001011801, Improvement: 0.00000000007034, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2241
Epoch 2241, Loss: 0.00000001008846, Improvement: -0.00000000002955, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2242
Epoch 2242, Loss: 0.00000001016978, Improvement: 0.00000000008132, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2243
Epoch 2243, Loss: 0.00000001039830, Improvement: 0.00000000022852, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2244
Epoch 2244, Loss: 0.00000001139814, Improvement: 0.00000000099984, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2245
Epoch 2245, Loss: 0.00000001082899, Improvement: -0.00000000056915, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2246
Epoch 2246, Loss: 0.00000001121710, Improvement: 0.00000000038811, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2247
Epoch 2247, Loss: 0.00000001120156, Improvement: -0.00000000001554, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2248
Epoch 2248, Loss: 0.00000001215568, Improvement: 0.00000000095411, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2249
Epoch 2249, Loss: 0.00000001641514, Improvement: 0.00000000425946, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000002219981, Improvement: 0.00000000578468, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2251
Epoch 2251, Loss: 0.00000002438297, Improvement: 0.00000000218315, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2252
Epoch 2252, Loss: 0.00000002099219, Improvement: -0.00000000339078, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2253
Epoch 2253, Loss: 0.00000001474357, Improvement: -0.00000000624862, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2254
Epoch 2254, Loss: 0.00000001164856, Improvement: -0.00000000309501, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2255
Epoch 2255, Loss: 0.00000001048351, Improvement: -0.00000000116505, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2256
Epoch 2256, Loss: 0.00000001010139, Improvement: -0.00000000038212, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2257
Epoch 2257, Loss: 0.00000000991815, Improvement: -0.00000000018324, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2258
Epoch 2258, Loss: 0.00000000988304, Improvement: -0.00000000003511, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2259
Epoch 2259, Loss: 0.00000001043383, Improvement: 0.00000000055079, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2260
Epoch 2260, Loss: 0.00000001177422, Improvement: 0.00000000134039, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2261
Epoch 2261, Loss: 0.00000001196257, Improvement: 0.00000000018835, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2262
Epoch 2262, Loss: 0.00000001504634, Improvement: 0.00000000308377, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2263
Epoch 2263, Loss: 0.00000001203733, Improvement: -0.00000000300901, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2264
Epoch 2264, Loss: 0.00000001453050, Improvement: 0.00000000249317, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2265
Epoch 2265, Loss: 0.00000001742620, Improvement: 0.00000000289571, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2266
Epoch 2266, Loss: 0.00000001944944, Improvement: 0.00000000202324, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2267
Epoch 2267, Loss: 0.00000003190087, Improvement: 0.00000001245143, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2268
Epoch 2268, Loss: 0.00000002348643, Improvement: -0.00000000841444, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2269
Epoch 2269, Loss: 0.00000001608010, Improvement: -0.00000000740633, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2270
Epoch 2270, Loss: 0.00000001518144, Improvement: -0.00000000089866, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2271
Epoch 2271, Loss: 0.00000001610553, Improvement: 0.00000000092409, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2272
Epoch 2272, Loss: 0.00000002424036, Improvement: 0.00000000813482, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2273
Epoch 2273, Loss: 0.00000002720091, Improvement: 0.00000000296056, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2274
Epoch 2274, Loss: 0.00000002007421, Improvement: -0.00000000712670, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2275
Epoch 2275, Loss: 0.00000002103733, Improvement: 0.00000000096312, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2276
Epoch 2276, Loss: 0.00000001720897, Improvement: -0.00000000382836, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2277
Epoch 2277, Loss: 0.00000001738500, Improvement: 0.00000000017602, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2278
Epoch 2278, Loss: 0.00000001930018, Improvement: 0.00000000191519, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2279
Epoch 2279, Loss: 0.00000001954973, Improvement: 0.00000000024955, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2280
Epoch 2280, Loss: 0.00000001974995, Improvement: 0.00000000020022, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2281
Epoch 2281, Loss: 0.00000001600349, Improvement: -0.00000000374646, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2282
Epoch 2282, Loss: 0.00000002155300, Improvement: 0.00000000554951, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2283
Epoch 2283, Loss: 0.00000002000081, Improvement: -0.00000000155219, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2284
Epoch 2284, Loss: 0.00000001432367, Improvement: -0.00000000567714, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2285
Epoch 2285, Loss: 0.00000001388241, Improvement: -0.00000000044126, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2286
Epoch 2286, Loss: 0.00000001541147, Improvement: 0.00000000152907, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2287
Epoch 2287, Loss: 0.00000001774469, Improvement: 0.00000000233322, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2288
Epoch 2288, Loss: 0.00000001522429, Improvement: -0.00000000252040, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2289
Epoch 2289, Loss: 0.00000002085871, Improvement: 0.00000000563442, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2290
Epoch 2290, Loss: 0.00000002090312, Improvement: 0.00000000004442, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2291
Epoch 2291, Loss: 0.00000001799737, Improvement: -0.00000000290575, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2292
Epoch 2292, Loss: 0.00000002028438, Improvement: 0.00000000228701, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2293
Epoch 2293, Loss: 0.00000003365104, Improvement: 0.00000001336667, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2294
Epoch 2294, Loss: 0.00000003537542, Improvement: 0.00000000172437, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2295
Epoch 2295, Loss: 0.00000003080485, Improvement: -0.00000000457057, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2296
Epoch 2296, Loss: 0.00000002657595, Improvement: -0.00000000422890, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2297
Epoch 2297, Loss: 0.00000002256913, Improvement: -0.00000000400682, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2298
Epoch 2298, Loss: 0.00000001492422, Improvement: -0.00000000764490, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2299
Epoch 2299, Loss: 0.00000001693788, Improvement: 0.00000000201366, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000001322182, Improvement: -0.00000000371606, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2301
Epoch 2301, Loss: 0.00000001143591, Improvement: -0.00000000178591, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2302
Epoch 2302, Loss: 0.00000001184199, Improvement: 0.00000000040608, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2303
Epoch 2303, Loss: 0.00000001025297, Improvement: -0.00000000158901, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2304
Epoch 2304, Loss: 0.00000001089375, Improvement: 0.00000000064078, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2305
Epoch 2305, Loss: 0.00000001023210, Improvement: -0.00000000066165, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2306
Epoch 2306, Loss: 0.00000001114899, Improvement: 0.00000000091689, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2307
Epoch 2307, Loss: 0.00000001109670, Improvement: -0.00000000005229, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2308
Epoch 2308, Loss: 0.00000001214058, Improvement: 0.00000000104388, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2309
Epoch 2309, Loss: 0.00000001075792, Improvement: -0.00000000138265, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2310
Epoch 2310, Loss: 0.00000001057827, Improvement: -0.00000000017965, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2311
Epoch 2311, Loss: 0.00000001097650, Improvement: 0.00000000039823, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2312
Epoch 2312, Loss: 0.00000001016812, Improvement: -0.00000000080838, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2313
Epoch 2313, Loss: 0.00000001006834, Improvement: -0.00000000009978, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2314
Epoch 2314, Loss: 0.00000001034840, Improvement: 0.00000000028006, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2315
Epoch 2315, Loss: 0.00000001022798, Improvement: -0.00000000012041, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2316
Epoch 2316, Loss: 0.00000001032620, Improvement: 0.00000000009821, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2317
Epoch 2317, Loss: 0.00000001109831, Improvement: 0.00000000077211, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2318
Epoch 2318, Loss: 0.00000001042296, Improvement: -0.00000000067534, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2319
Epoch 2319, Loss: 0.00000001121726, Improvement: 0.00000000079430, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2320
Epoch 2320, Loss: 0.00000001032246, Improvement: -0.00000000089480, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2321
Epoch 2321, Loss: 0.00000001085987, Improvement: 0.00000000053741, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2322
Epoch 2322, Loss: 0.00000001108088, Improvement: 0.00000000022100, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2323
Epoch 2323, Loss: 0.00000001200477, Improvement: 0.00000000092389, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2324
Epoch 2324, Loss: 0.00000001716359, Improvement: 0.00000000515882, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2325
Epoch 2325, Loss: 0.00000001939801, Improvement: 0.00000000223441, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2326
Epoch 2326, Loss: 0.00000002509695, Improvement: 0.00000000569895, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2327
Epoch 2327, Loss: 0.00000002148902, Improvement: -0.00000000360793, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2328
Epoch 2328, Loss: 0.00000001888722, Improvement: -0.00000000260179, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2329
Epoch 2329, Loss: 0.00000001776271, Improvement: -0.00000000112452, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2330
Epoch 2330, Loss: 0.00000002084898, Improvement: 0.00000000308627, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2331
Epoch 2331, Loss: 0.00000003388843, Improvement: 0.00000001303946, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2332
Epoch 2332, Loss: 0.00000003242647, Improvement: -0.00000000146197, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2333
Epoch 2333, Loss: 0.00000002328242, Improvement: -0.00000000914405, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2334
Epoch 2334, Loss: 0.00000002302012, Improvement: -0.00000000026230, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2335
Epoch 2335, Loss: 0.00000001701542, Improvement: -0.00000000600470, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2336
Epoch 2336, Loss: 0.00000001344597, Improvement: -0.00000000356945, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2337
Epoch 2337, Loss: 0.00000001123216, Improvement: -0.00000000221381, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2338
Epoch 2338, Loss: 0.00000001070975, Improvement: -0.00000000052240, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2339
Epoch 2339, Loss: 0.00000001015108, Improvement: -0.00000000055867, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2340
Epoch 2340, Loss: 0.00000000973085, Improvement: -0.00000000042023, Best Loss: 0.00000000590538 in Epoch 2220
Epoch 2341
A best model at epoch 2341 has been saved with training error 0.00000000544570.
Epoch 2341, Loss: 0.00000000976718, Improvement: 0.00000000003633, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2342
Epoch 2342, Loss: 0.00000000970383, Improvement: -0.00000000006335, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2343
Epoch 2343, Loss: 0.00000000982353, Improvement: 0.00000000011969, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2344
Epoch 2344, Loss: 0.00000001021094, Improvement: 0.00000000038741, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2345
Epoch 2345, Loss: 0.00000001060839, Improvement: 0.00000000039745, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2346
Epoch 2346, Loss: 0.00000001047779, Improvement: -0.00000000013061, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2347
Epoch 2347, Loss: 0.00000001019345, Improvement: -0.00000000028433, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2348
Epoch 2348, Loss: 0.00000001076888, Improvement: 0.00000000057542, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2349
Epoch 2349, Loss: 0.00000001136318, Improvement: 0.00000000059431, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000001416239, Improvement: 0.00000000279921, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2351
Epoch 2351, Loss: 0.00000001249783, Improvement: -0.00000000166456, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2352
Epoch 2352, Loss: 0.00000001173651, Improvement: -0.00000000076132, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2353
Epoch 2353, Loss: 0.00000001250348, Improvement: 0.00000000076698, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2354
Epoch 2354, Loss: 0.00000001187292, Improvement: -0.00000000063057, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2355
Epoch 2355, Loss: 0.00000001762960, Improvement: 0.00000000575668, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2356
Epoch 2356, Loss: 0.00000002254921, Improvement: 0.00000000491961, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2357
Epoch 2357, Loss: 0.00000002920872, Improvement: 0.00000000665951, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2358
Epoch 2358, Loss: 0.00000003066907, Improvement: 0.00000000146034, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2359
Epoch 2359, Loss: 0.00000002340562, Improvement: -0.00000000726344, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2360
Epoch 2360, Loss: 0.00000002414225, Improvement: 0.00000000073662, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2361
Epoch 2361, Loss: 0.00000003188132, Improvement: 0.00000000773907, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2362
Epoch 2362, Loss: 0.00000002645088, Improvement: -0.00000000543044, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2363
Epoch 2363, Loss: 0.00000001502431, Improvement: -0.00000001142657, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2364
Epoch 2364, Loss: 0.00000001215618, Improvement: -0.00000000286814, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2365
Epoch 2365, Loss: 0.00000001043772, Improvement: -0.00000000171846, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2366
Epoch 2366, Loss: 0.00000000983964, Improvement: -0.00000000059808, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2367
Epoch 2367, Loss: 0.00000000951147, Improvement: -0.00000000032818, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2368
Epoch 2368, Loss: 0.00000000956527, Improvement: 0.00000000005380, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2369
Epoch 2369, Loss: 0.00000000975306, Improvement: 0.00000000018779, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2370
Epoch 2370, Loss: 0.00000001009335, Improvement: 0.00000000034030, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2371
Epoch 2371, Loss: 0.00000001029960, Improvement: 0.00000000020624, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2372
Epoch 2372, Loss: 0.00000001026769, Improvement: -0.00000000003191, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2373
Epoch 2373, Loss: 0.00000001027053, Improvement: 0.00000000000285, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2374
Epoch 2374, Loss: 0.00000001002033, Improvement: -0.00000000025020, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2375
Epoch 2375, Loss: 0.00000001071101, Improvement: 0.00000000069068, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2376
Epoch 2376, Loss: 0.00000001081452, Improvement: 0.00000000010351, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2377
Epoch 2377, Loss: 0.00000001119898, Improvement: 0.00000000038445, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2378
Epoch 2378, Loss: 0.00000001058129, Improvement: -0.00000000061769, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2379
Epoch 2379, Loss: 0.00000001062897, Improvement: 0.00000000004769, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2380
Epoch 2380, Loss: 0.00000001234573, Improvement: 0.00000000171675, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2381
Epoch 2381, Loss: 0.00000001582500, Improvement: 0.00000000347928, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2382
Epoch 2382, Loss: 0.00000001967933, Improvement: 0.00000000385432, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2383
Epoch 2383, Loss: 0.00000001922363, Improvement: -0.00000000045570, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2384
Epoch 2384, Loss: 0.00000002758866, Improvement: 0.00000000836503, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2385
Epoch 2385, Loss: 0.00000002974451, Improvement: 0.00000000215586, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2386
Epoch 2386, Loss: 0.00000003117508, Improvement: 0.00000000143057, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2387
Epoch 2387, Loss: 0.00000003279543, Improvement: 0.00000000162035, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2388
Epoch 2388, Loss: 0.00000002740861, Improvement: -0.00000000538681, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2389
Epoch 2389, Loss: 0.00000001660958, Improvement: -0.00000001079903, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2390
Epoch 2390, Loss: 0.00000001213947, Improvement: -0.00000000447012, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2391
Epoch 2391, Loss: 0.00000001121160, Improvement: -0.00000000092786, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2392
Epoch 2392, Loss: 0.00000001063684, Improvement: -0.00000000057476, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2393
Epoch 2393, Loss: 0.00000001020913, Improvement: -0.00000000042771, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2394
Epoch 2394, Loss: 0.00000001024283, Improvement: 0.00000000003370, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2395
Epoch 2395, Loss: 0.00000001014585, Improvement: -0.00000000009698, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2396
Epoch 2396, Loss: 0.00000001001894, Improvement: -0.00000000012691, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2397
Epoch 2397, Loss: 0.00000001003290, Improvement: 0.00000000001396, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2398
Epoch 2398, Loss: 0.00000001101222, Improvement: 0.00000000097932, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2399
Epoch 2399, Loss: 0.00000001703479, Improvement: 0.00000000602257, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000002098698, Improvement: 0.00000000395220, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2401
Epoch 2401, Loss: 0.00000001600415, Improvement: -0.00000000498283, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2402
Epoch 2402, Loss: 0.00000001803241, Improvement: 0.00000000202826, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2403
Epoch 2403, Loss: 0.00000001552758, Improvement: -0.00000000250483, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2404
Epoch 2404, Loss: 0.00000001720652, Improvement: 0.00000000167894, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2405
Epoch 2405, Loss: 0.00000001657355, Improvement: -0.00000000063296, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2406
Epoch 2406, Loss: 0.00000001636047, Improvement: -0.00000000021308, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2407
Epoch 2407, Loss: 0.00000001818785, Improvement: 0.00000000182738, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2408
Epoch 2408, Loss: 0.00000003847908, Improvement: 0.00000002029123, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2409
Epoch 2409, Loss: 0.00000004993974, Improvement: 0.00000001146066, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2410
Epoch 2410, Loss: 0.00000003164022, Improvement: -0.00000001829952, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2411
Epoch 2411, Loss: 0.00000003464227, Improvement: 0.00000000300205, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2412
Epoch 2412, Loss: 0.00000001265257, Improvement: -0.00000002198970, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2413
Epoch 2413, Loss: 0.00000001060805, Improvement: -0.00000000204452, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2414
Epoch 2414, Loss: 0.00000000961232, Improvement: -0.00000000099572, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2415
Epoch 2415, Loss: 0.00000000951718, Improvement: -0.00000000009514, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2416
Epoch 2416, Loss: 0.00000000934310, Improvement: -0.00000000017408, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2417
Epoch 2417, Loss: 0.00000000933456, Improvement: -0.00000000000854, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2418
Epoch 2418, Loss: 0.00000000934445, Improvement: 0.00000000000990, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2419
Epoch 2419, Loss: 0.00000000924524, Improvement: -0.00000000009921, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2420
Epoch 2420, Loss: 0.00000000929946, Improvement: 0.00000000005422, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2421
Epoch 2421, Loss: 0.00000000929011, Improvement: -0.00000000000935, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2422
Epoch 2422, Loss: 0.00000000926622, Improvement: -0.00000000002389, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2423
Epoch 2423, Loss: 0.00000000925636, Improvement: -0.00000000000986, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2424
Epoch 2424, Loss: 0.00000000923383, Improvement: -0.00000000002253, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2425
Epoch 2425, Loss: 0.00000000932883, Improvement: 0.00000000009500, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2426
Epoch 2426, Loss: 0.00000000925020, Improvement: -0.00000000007863, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2427
Epoch 2427, Loss: 0.00000000929077, Improvement: 0.00000000004057, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2428
Epoch 2428, Loss: 0.00000000927845, Improvement: -0.00000000001233, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2429
Epoch 2429, Loss: 0.00000000938377, Improvement: 0.00000000010532, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2430
Epoch 2430, Loss: 0.00000000950125, Improvement: 0.00000000011747, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2431
Epoch 2431, Loss: 0.00000000985257, Improvement: 0.00000000035132, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2432
Epoch 2432, Loss: 0.00000001391866, Improvement: 0.00000000406609, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2433
Epoch 2433, Loss: 0.00000002543382, Improvement: 0.00000001151516, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2434
Epoch 2434, Loss: 0.00000004188424, Improvement: 0.00000001645042, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2435
Epoch 2435, Loss: 0.00000001920343, Improvement: -0.00000002268081, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2436
Epoch 2436, Loss: 0.00000001529153, Improvement: -0.00000000391190, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2437
Epoch 2437, Loss: 0.00000001235976, Improvement: -0.00000000293178, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2438
Epoch 2438, Loss: 0.00000001028677, Improvement: -0.00000000207299, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2439
Epoch 2439, Loss: 0.00000001034174, Improvement: 0.00000000005497, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2440
Epoch 2440, Loss: 0.00000001051630, Improvement: 0.00000000017457, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2441
Epoch 2441, Loss: 0.00000001015374, Improvement: -0.00000000036256, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2442
Epoch 2442, Loss: 0.00000000966923, Improvement: -0.00000000048451, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2443
Epoch 2443, Loss: 0.00000001001471, Improvement: 0.00000000034547, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2444
Epoch 2444, Loss: 0.00000001269136, Improvement: 0.00000000267666, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2445
Epoch 2445, Loss: 0.00000001181698, Improvement: -0.00000000087438, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2446
Epoch 2446, Loss: 0.00000001305999, Improvement: 0.00000000124300, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2447
Epoch 2447, Loss: 0.00000001522686, Improvement: 0.00000000216687, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2448
Epoch 2448, Loss: 0.00000001418015, Improvement: -0.00000000104671, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2449
Epoch 2449, Loss: 0.00000001258073, Improvement: -0.00000000159943, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000001794320, Improvement: 0.00000000536247, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2451
Epoch 2451, Loss: 0.00000001928023, Improvement: 0.00000000133703, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2452
Epoch 2452, Loss: 0.00000001872330, Improvement: -0.00000000055693, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2453
Epoch 2453, Loss: 0.00000001484294, Improvement: -0.00000000388036, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2454
Epoch 2454, Loss: 0.00000001227072, Improvement: -0.00000000257222, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2455
Epoch 2455, Loss: 0.00000001090821, Improvement: -0.00000000136251, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2456
Epoch 2456, Loss: 0.00000000972792, Improvement: -0.00000000118029, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2457
Epoch 2457, Loss: 0.00000000953696, Improvement: -0.00000000019096, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2458
Epoch 2458, Loss: 0.00000000987097, Improvement: 0.00000000033401, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2459
Epoch 2459, Loss: 0.00000001020603, Improvement: 0.00000000033506, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2460
Epoch 2460, Loss: 0.00000000980289, Improvement: -0.00000000040315, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2461
Epoch 2461, Loss: 0.00000000954999, Improvement: -0.00000000025290, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2462
Epoch 2462, Loss: 0.00000000995284, Improvement: 0.00000000040285, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2463
Epoch 2463, Loss: 0.00000001051641, Improvement: 0.00000000056358, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2464
Epoch 2464, Loss: 0.00000001178647, Improvement: 0.00000000127006, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2465
Epoch 2465, Loss: 0.00000001702546, Improvement: 0.00000000523900, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2466
Epoch 2466, Loss: 0.00000002576637, Improvement: 0.00000000874091, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2467
Epoch 2467, Loss: 0.00000002905496, Improvement: 0.00000000328858, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2468
Epoch 2468, Loss: 0.00000002306759, Improvement: -0.00000000598736, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2469
Epoch 2469, Loss: 0.00000001501654, Improvement: -0.00000000805105, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2470
Epoch 2470, Loss: 0.00000001282511, Improvement: -0.00000000219143, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2471
Epoch 2471, Loss: 0.00000001167082, Improvement: -0.00000000115429, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2472
Epoch 2472, Loss: 0.00000001322839, Improvement: 0.00000000155757, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2473
Epoch 2473, Loss: 0.00000001134371, Improvement: -0.00000000188468, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2474
Epoch 2474, Loss: 0.00000000963683, Improvement: -0.00000000170688, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2475
Epoch 2475, Loss: 0.00000000934977, Improvement: -0.00000000028707, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2476
Epoch 2476, Loss: 0.00000000933247, Improvement: -0.00000000001729, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2477
Epoch 2477, Loss: 0.00000000976048, Improvement: 0.00000000042800, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2478
Epoch 2478, Loss: 0.00000001156125, Improvement: 0.00000000180077, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2479
Epoch 2479, Loss: 0.00000001104905, Improvement: -0.00000000051220, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2480
Epoch 2480, Loss: 0.00000001354215, Improvement: 0.00000000249309, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2481
Epoch 2481, Loss: 0.00000001557503, Improvement: 0.00000000203288, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2482
Epoch 2482, Loss: 0.00000002691139, Improvement: 0.00000001133636, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2483
Epoch 2483, Loss: 0.00000002632311, Improvement: -0.00000000058828, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2484
Epoch 2484, Loss: 0.00000002357139, Improvement: -0.00000000275172, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2485
Epoch 2485, Loss: 0.00000002296046, Improvement: -0.00000000061092, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2486
Epoch 2486, Loss: 0.00000002835746, Improvement: 0.00000000539699, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2487
Epoch 2487, Loss: 0.00000002641107, Improvement: -0.00000000194638, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2488
Epoch 2488, Loss: 0.00000001382606, Improvement: -0.00000001258502, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2489
Epoch 2489, Loss: 0.00000001486634, Improvement: 0.00000000104028, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2490
Epoch 2490, Loss: 0.00000001755208, Improvement: 0.00000000268574, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2491
Epoch 2491, Loss: 0.00000001773949, Improvement: 0.00000000018741, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2492
Epoch 2492, Loss: 0.00000001528184, Improvement: -0.00000000245765, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2493
Epoch 2493, Loss: 0.00000001673638, Improvement: 0.00000000145453, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2494
Epoch 2494, Loss: 0.00000001294692, Improvement: -0.00000000378946, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2495
Epoch 2495, Loss: 0.00000001293597, Improvement: -0.00000000001095, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2496
Epoch 2496, Loss: 0.00000001169426, Improvement: -0.00000000124171, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2497
Epoch 2497, Loss: 0.00000001131510, Improvement: -0.00000000037916, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2498
Epoch 2498, Loss: 0.00000001110640, Improvement: -0.00000000020870, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2499
Epoch 2499, Loss: 0.00000001051098, Improvement: -0.00000000059542, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000001060119, Improvement: 0.00000000009021, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2501
Epoch 2501, Loss: 0.00000001024022, Improvement: -0.00000000036097, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2502
Epoch 2502, Loss: 0.00000001154300, Improvement: 0.00000000130277, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2503
Epoch 2503, Loss: 0.00000001391777, Improvement: 0.00000000237478, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2504
Epoch 2504, Loss: 0.00000001168838, Improvement: -0.00000000222940, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2505
Epoch 2505, Loss: 0.00000001046569, Improvement: -0.00000000122269, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2506
Epoch 2506, Loss: 0.00000001138195, Improvement: 0.00000000091627, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2507
Epoch 2507, Loss: 0.00000001086113, Improvement: -0.00000000052082, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2508
Epoch 2508, Loss: 0.00000001394441, Improvement: 0.00000000308328, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2509
Epoch 2509, Loss: 0.00000002111198, Improvement: 0.00000000716758, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2510
Epoch 2510, Loss: 0.00000001686823, Improvement: -0.00000000424375, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2511
Epoch 2511, Loss: 0.00000001363659, Improvement: -0.00000000323164, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2512
Epoch 2512, Loss: 0.00000002145033, Improvement: 0.00000000781374, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2513
Epoch 2513, Loss: 0.00000003911560, Improvement: 0.00000001766526, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2514
Epoch 2514, Loss: 0.00000002238594, Improvement: -0.00000001672966, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2515
Epoch 2515, Loss: 0.00000001799982, Improvement: -0.00000000438612, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2516
Epoch 2516, Loss: 0.00000001473962, Improvement: -0.00000000326020, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2517
Epoch 2517, Loss: 0.00000001137509, Improvement: -0.00000000336453, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2518
Epoch 2518, Loss: 0.00000000996242, Improvement: -0.00000000141267, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2519
Epoch 2519, Loss: 0.00000001030759, Improvement: 0.00000000034517, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2520
Epoch 2520, Loss: 0.00000000998226, Improvement: -0.00000000032533, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2521
Epoch 2521, Loss: 0.00000001105190, Improvement: 0.00000000106964, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2522
Epoch 2522, Loss: 0.00000001562280, Improvement: 0.00000000457090, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2523
Epoch 2523, Loss: 0.00000001602718, Improvement: 0.00000000040438, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2524
Epoch 2524, Loss: 0.00000001353959, Improvement: -0.00000000248759, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2525
Epoch 2525, Loss: 0.00000001133750, Improvement: -0.00000000220209, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2526
Epoch 2526, Loss: 0.00000001461188, Improvement: 0.00000000327438, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2527
Epoch 2527, Loss: 0.00000001306035, Improvement: -0.00000000155153, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2528
Epoch 2528, Loss: 0.00000001094937, Improvement: -0.00000000211098, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2529
Epoch 2529, Loss: 0.00000000999192, Improvement: -0.00000000095745, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2530
Epoch 2530, Loss: 0.00000000999033, Improvement: -0.00000000000158, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2531
Epoch 2531, Loss: 0.00000001168826, Improvement: 0.00000000169793, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2532
Epoch 2532, Loss: 0.00000001305991, Improvement: 0.00000000137165, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2533
Epoch 2533, Loss: 0.00000001372784, Improvement: 0.00000000066793, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2534
Epoch 2534, Loss: 0.00000002000075, Improvement: 0.00000000627291, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2535
Epoch 2535, Loss: 0.00000002982926, Improvement: 0.00000000982851, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2536
Epoch 2536, Loss: 0.00000003965664, Improvement: 0.00000000982738, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2537
Epoch 2537, Loss: 0.00000003204496, Improvement: -0.00000000761168, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2538
Epoch 2538, Loss: 0.00000002810716, Improvement: -0.00000000393780, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2539
Epoch 2539, Loss: 0.00000001436833, Improvement: -0.00000001373883, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2540
Epoch 2540, Loss: 0.00000001049900, Improvement: -0.00000000386933, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2541
Epoch 2541, Loss: 0.00000000958225, Improvement: -0.00000000091675, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2542
Epoch 2542, Loss: 0.00000000943098, Improvement: -0.00000000015127, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2543
Epoch 2543, Loss: 0.00000000928655, Improvement: -0.00000000014443, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2544
Epoch 2544, Loss: 0.00000001006312, Improvement: 0.00000000077657, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2545
Epoch 2545, Loss: 0.00000001059639, Improvement: 0.00000000053327, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2546
Epoch 2546, Loss: 0.00000001019663, Improvement: -0.00000000039976, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2547
Epoch 2547, Loss: 0.00000000981860, Improvement: -0.00000000037803, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2548
Epoch 2548, Loss: 0.00000000927608, Improvement: -0.00000000054251, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2549
Epoch 2549, Loss: 0.00000000939289, Improvement: 0.00000000011681, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000001009520, Improvement: 0.00000000070230, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2551
Epoch 2551, Loss: 0.00000001002673, Improvement: -0.00000000006847, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2552
Epoch 2552, Loss: 0.00000000941099, Improvement: -0.00000000061573, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2553
Epoch 2553, Loss: 0.00000000936263, Improvement: -0.00000000004836, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2554
Epoch 2554, Loss: 0.00000000927788, Improvement: -0.00000000008475, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2555
Epoch 2555, Loss: 0.00000001175971, Improvement: 0.00000000248183, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2556
Epoch 2556, Loss: 0.00000001116791, Improvement: -0.00000000059180, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2557
Epoch 2557, Loss: 0.00000000960681, Improvement: -0.00000000156110, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2558
Epoch 2558, Loss: 0.00000001026588, Improvement: 0.00000000065908, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2559
Epoch 2559, Loss: 0.00000001596392, Improvement: 0.00000000569804, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2560
Epoch 2560, Loss: 0.00000001423804, Improvement: -0.00000000172588, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2561
Epoch 2561, Loss: 0.00000001188006, Improvement: -0.00000000235798, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2562
Epoch 2562, Loss: 0.00000001452006, Improvement: 0.00000000264000, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2563
Epoch 2563, Loss: 0.00000001545558, Improvement: 0.00000000093552, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2564
Epoch 2564, Loss: 0.00000001153641, Improvement: -0.00000000391918, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2565
Epoch 2565, Loss: 0.00000001088635, Improvement: -0.00000000065006, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2566
Epoch 2566, Loss: 0.00000001074912, Improvement: -0.00000000013723, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2567
Epoch 2567, Loss: 0.00000001015316, Improvement: -0.00000000059596, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2568
Epoch 2568, Loss: 0.00000001244177, Improvement: 0.00000000228861, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2569
Epoch 2569, Loss: 0.00000001172734, Improvement: -0.00000000071443, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2570
Epoch 2570, Loss: 0.00000001102756, Improvement: -0.00000000069978, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2571
Epoch 2571, Loss: 0.00000001199310, Improvement: 0.00000000096555, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2572
Epoch 2572, Loss: 0.00000001898061, Improvement: 0.00000000698751, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2573
Epoch 2573, Loss: 0.00000004459052, Improvement: 0.00000002560991, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2574
Epoch 2574, Loss: 0.00000003219816, Improvement: -0.00000001239236, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2575
Epoch 2575, Loss: 0.00000001744980, Improvement: -0.00000001474836, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2576
Epoch 2576, Loss: 0.00000001210278, Improvement: -0.00000000534702, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2577
Epoch 2577, Loss: 0.00000000982704, Improvement: -0.00000000227575, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2578
Epoch 2578, Loss: 0.00000000941941, Improvement: -0.00000000040763, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2579
Epoch 2579, Loss: 0.00000000911498, Improvement: -0.00000000030443, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2580
Epoch 2580, Loss: 0.00000000903984, Improvement: -0.00000000007514, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2581
Epoch 2581, Loss: 0.00000000882856, Improvement: -0.00000000021128, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2582
Epoch 2582, Loss: 0.00000000884408, Improvement: 0.00000000001552, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2583
Epoch 2583, Loss: 0.00000000885367, Improvement: 0.00000000000959, Best Loss: 0.00000000544570 in Epoch 2341
Epoch 2584
A best model at epoch 2584 has been saved with training error 0.00000000506068.
A best model at epoch 2584 has been saved with training error 0.00000000480304.
Epoch 2584, Loss: 0.00000000899595, Improvement: 0.00000000014228, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2585
Epoch 2585, Loss: 0.00000000891632, Improvement: -0.00000000007963, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2586
Epoch 2586, Loss: 0.00000000893302, Improvement: 0.00000000001669, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2587
Epoch 2587, Loss: 0.00000000912729, Improvement: 0.00000000019427, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2588
Epoch 2588, Loss: 0.00000000903088, Improvement: -0.00000000009640, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2589
Epoch 2589, Loss: 0.00000000919197, Improvement: 0.00000000016109, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2590
Epoch 2590, Loss: 0.00000000964274, Improvement: 0.00000000045076, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2591
Epoch 2591, Loss: 0.00000001076765, Improvement: 0.00000000112492, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2592
Epoch 2592, Loss: 0.00000001008406, Improvement: -0.00000000068359, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2593
Epoch 2593, Loss: 0.00000001028986, Improvement: 0.00000000020579, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2594
Epoch 2594, Loss: 0.00000001037710, Improvement: 0.00000000008724, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2595
Epoch 2595, Loss: 0.00000001125822, Improvement: 0.00000000088111, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2596
Epoch 2596, Loss: 0.00000001012493, Improvement: -0.00000000113329, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2597
Epoch 2597, Loss: 0.00000001025958, Improvement: 0.00000000013465, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2598
Epoch 2598, Loss: 0.00000001291430, Improvement: 0.00000000265472, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2599
Epoch 2599, Loss: 0.00000001270164, Improvement: -0.00000000021266, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000001632555, Improvement: 0.00000000362391, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2601
Epoch 2601, Loss: 0.00000001608261, Improvement: -0.00000000024293, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2602
Epoch 2602, Loss: 0.00000001330745, Improvement: -0.00000000277516, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2603
Epoch 2603, Loss: 0.00000001156347, Improvement: -0.00000000174398, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2604
Epoch 2604, Loss: 0.00000001369985, Improvement: 0.00000000213638, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2605
Epoch 2605, Loss: 0.00000001380672, Improvement: 0.00000000010687, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2606
Epoch 2606, Loss: 0.00000001459490, Improvement: 0.00000000078818, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2607
Epoch 2607, Loss: 0.00000001408297, Improvement: -0.00000000051193, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2608
Epoch 2608, Loss: 0.00000002411541, Improvement: 0.00000001003244, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2609
Epoch 2609, Loss: 0.00000002011151, Improvement: -0.00000000400390, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2610
Epoch 2610, Loss: 0.00000001725942, Improvement: -0.00000000285209, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2611
Epoch 2611, Loss: 0.00000002097983, Improvement: 0.00000000372042, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2612
Epoch 2612, Loss: 0.00000002160281, Improvement: 0.00000000062298, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2613
Epoch 2613, Loss: 0.00000002017004, Improvement: -0.00000000143277, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2614
Epoch 2614, Loss: 0.00000001841864, Improvement: -0.00000000175141, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2615
Epoch 2615, Loss: 0.00000001261626, Improvement: -0.00000000580238, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2616
Epoch 2616, Loss: 0.00000001139355, Improvement: -0.00000000122270, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2617
Epoch 2617, Loss: 0.00000001052320, Improvement: -0.00000000087036, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2618
Epoch 2618, Loss: 0.00000000966620, Improvement: -0.00000000085700, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2619
Epoch 2619, Loss: 0.00000000970750, Improvement: 0.00000000004130, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2620
Epoch 2620, Loss: 0.00000000977107, Improvement: 0.00000000006357, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2621
Epoch 2621, Loss: 0.00000000988988, Improvement: 0.00000000011881, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2622
Epoch 2622, Loss: 0.00000001203084, Improvement: 0.00000000214096, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2623
Epoch 2623, Loss: 0.00000001293004, Improvement: 0.00000000089920, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2624
Epoch 2624, Loss: 0.00000001064858, Improvement: -0.00000000228146, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2625
Epoch 2625, Loss: 0.00000002374019, Improvement: 0.00000001309161, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2626
Epoch 2626, Loss: 0.00000002573541, Improvement: 0.00000000199522, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2627
Epoch 2627, Loss: 0.00000001746399, Improvement: -0.00000000827142, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2628
Epoch 2628, Loss: 0.00000001528887, Improvement: -0.00000000217513, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2629
Epoch 2629, Loss: 0.00000001403572, Improvement: -0.00000000125315, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2630
Epoch 2630, Loss: 0.00000001359274, Improvement: -0.00000000044298, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2631
Epoch 2631, Loss: 0.00000001516097, Improvement: 0.00000000156823, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2632
Epoch 2632, Loss: 0.00000001546467, Improvement: 0.00000000030369, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2633
Epoch 2633, Loss: 0.00000001028217, Improvement: -0.00000000518250, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2634
Epoch 2634, Loss: 0.00000001081922, Improvement: 0.00000000053705, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2635
Epoch 2635, Loss: 0.00000001163516, Improvement: 0.00000000081594, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2636
Epoch 2636, Loss: 0.00000001196374, Improvement: 0.00000000032858, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2637
Epoch 2637, Loss: 0.00000001622192, Improvement: 0.00000000425818, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2638
Epoch 2638, Loss: 0.00000001333660, Improvement: -0.00000000288532, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2639
Epoch 2639, Loss: 0.00000001309054, Improvement: -0.00000000024606, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2640
Epoch 2640, Loss: 0.00000001431494, Improvement: 0.00000000122440, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2641
Epoch 2641, Loss: 0.00000001638167, Improvement: 0.00000000206673, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2642
Epoch 2642, Loss: 0.00000001448974, Improvement: -0.00000000189193, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2643
Epoch 2643, Loss: 0.00000003033133, Improvement: 0.00000001584159, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2644
Epoch 2644, Loss: 0.00000004172249, Improvement: 0.00000001139116, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2645
Epoch 2645, Loss: 0.00000003433676, Improvement: -0.00000000738573, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2646
Epoch 2646, Loss: 0.00000002072484, Improvement: -0.00000001361192, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2647
Epoch 2647, Loss: 0.00000001339337, Improvement: -0.00000000733147, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2648
Epoch 2648, Loss: 0.00000001058002, Improvement: -0.00000000281335, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2649
Epoch 2649, Loss: 0.00000000950088, Improvement: -0.00000000107913, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000000908095, Improvement: -0.00000000041994, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2651
Epoch 2651, Loss: 0.00000000911859, Improvement: 0.00000000003764, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2652
Epoch 2652, Loss: 0.00000000877834, Improvement: -0.00000000034025, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2653
Epoch 2653, Loss: 0.00000000886339, Improvement: 0.00000000008505, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2654
Epoch 2654, Loss: 0.00000000902540, Improvement: 0.00000000016201, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2655
Epoch 2655, Loss: 0.00000000896604, Improvement: -0.00000000005936, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2656
Epoch 2656, Loss: 0.00000000906826, Improvement: 0.00000000010223, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2657
Epoch 2657, Loss: 0.00000000990395, Improvement: 0.00000000083568, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2658
Epoch 2658, Loss: 0.00000000998400, Improvement: 0.00000000008006, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2659
Epoch 2659, Loss: 0.00000001027106, Improvement: 0.00000000028706, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2660
Epoch 2660, Loss: 0.00000000989734, Improvement: -0.00000000037372, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2661
Epoch 2661, Loss: 0.00000000988969, Improvement: -0.00000000000765, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2662
Epoch 2662, Loss: 0.00000000961079, Improvement: -0.00000000027890, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2663
Epoch 2663, Loss: 0.00000000907643, Improvement: -0.00000000053436, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2664
Epoch 2664, Loss: 0.00000000919171, Improvement: 0.00000000011528, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2665
Epoch 2665, Loss: 0.00000000939994, Improvement: 0.00000000020823, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2666
Epoch 2666, Loss: 0.00000001009152, Improvement: 0.00000000069158, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2667
Epoch 2667, Loss: 0.00000001063414, Improvement: 0.00000000054263, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2668
Epoch 2668, Loss: 0.00000001027988, Improvement: -0.00000000035427, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2669
Epoch 2669, Loss: 0.00000001195317, Improvement: 0.00000000167329, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2670
Epoch 2670, Loss: 0.00000001730652, Improvement: 0.00000000535335, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2671
Epoch 2671, Loss: 0.00000001223149, Improvement: -0.00000000507503, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2672
Epoch 2672, Loss: 0.00000001100542, Improvement: -0.00000000122608, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2673
Epoch 2673, Loss: 0.00000001046683, Improvement: -0.00000000053859, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2674
Epoch 2674, Loss: 0.00000001036339, Improvement: -0.00000000010344, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2675
Epoch 2675, Loss: 0.00000001225699, Improvement: 0.00000000189360, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2676
Epoch 2676, Loss: 0.00000001449747, Improvement: 0.00000000224047, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2677
Epoch 2677, Loss: 0.00000001562458, Improvement: 0.00000000112711, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2678
Epoch 2678, Loss: 0.00000002048065, Improvement: 0.00000000485607, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2679
Epoch 2679, Loss: 0.00000001483567, Improvement: -0.00000000564499, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2680
Epoch 2680, Loss: 0.00000001218092, Improvement: -0.00000000265474, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2681
Epoch 2681, Loss: 0.00000001305218, Improvement: 0.00000000087126, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2682
Epoch 2682, Loss: 0.00000001194292, Improvement: -0.00000000110926, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2683
Epoch 2683, Loss: 0.00000001278486, Improvement: 0.00000000084194, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2684
Epoch 2684, Loss: 0.00000001236297, Improvement: -0.00000000042188, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2685
Epoch 2685, Loss: 0.00000001054279, Improvement: -0.00000000182018, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2686
Epoch 2686, Loss: 0.00000000955407, Improvement: -0.00000000098872, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2687
Epoch 2687, Loss: 0.00000000991389, Improvement: 0.00000000035983, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2688
Epoch 2688, Loss: 0.00000000898521, Improvement: -0.00000000092869, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2689
Epoch 2689, Loss: 0.00000000894120, Improvement: -0.00000000004401, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2690
Epoch 2690, Loss: 0.00000001092007, Improvement: 0.00000000197888, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2691
Epoch 2691, Loss: 0.00000001212643, Improvement: 0.00000000120635, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2692
Epoch 2692, Loss: 0.00000001037846, Improvement: -0.00000000174796, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2693
Epoch 2693, Loss: 0.00000001503788, Improvement: 0.00000000465941, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2694
Epoch 2694, Loss: 0.00000002114864, Improvement: 0.00000000611076, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2695
Epoch 2695, Loss: 0.00000002139516, Improvement: 0.00000000024652, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2696
Epoch 2696, Loss: 0.00000002508489, Improvement: 0.00000000368972, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2697
Epoch 2697, Loss: 0.00000002515219, Improvement: 0.00000000006731, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2698
Epoch 2698, Loss: 0.00000002043458, Improvement: -0.00000000471762, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2699
Epoch 2699, Loss: 0.00000001331187, Improvement: -0.00000000712271, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000001209616, Improvement: -0.00000000121571, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2701
Epoch 2701, Loss: 0.00000001054447, Improvement: -0.00000000155169, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2702
Epoch 2702, Loss: 0.00000000929924, Improvement: -0.00000000124524, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2703
Epoch 2703, Loss: 0.00000000917605, Improvement: -0.00000000012319, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2704
Epoch 2704, Loss: 0.00000001008751, Improvement: 0.00000000091146, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2705
Epoch 2705, Loss: 0.00000001037462, Improvement: 0.00000000028711, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2706
Epoch 2706, Loss: 0.00000001149306, Improvement: 0.00000000111843, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2707
Epoch 2707, Loss: 0.00000001261717, Improvement: 0.00000000112411, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2708
Epoch 2708, Loss: 0.00000001721502, Improvement: 0.00000000459786, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2709
Epoch 2709, Loss: 0.00000002867295, Improvement: 0.00000001145793, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2710
Epoch 2710, Loss: 0.00000002511393, Improvement: -0.00000000355902, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2711
Epoch 2711, Loss: 0.00000001906528, Improvement: -0.00000000604866, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2712
Epoch 2712, Loss: 0.00000001377408, Improvement: -0.00000000529120, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2713
Epoch 2713, Loss: 0.00000001122458, Improvement: -0.00000000254949, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2714
Epoch 2714, Loss: 0.00000000998610, Improvement: -0.00000000123849, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2715
Epoch 2715, Loss: 0.00000000974542, Improvement: -0.00000000024068, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2716
Epoch 2716, Loss: 0.00000000909791, Improvement: -0.00000000064751, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2717
Epoch 2717, Loss: 0.00000000911944, Improvement: 0.00000000002153, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2718
Epoch 2718, Loss: 0.00000000957216, Improvement: 0.00000000045272, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2719
Epoch 2719, Loss: 0.00000000946474, Improvement: -0.00000000010742, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2720
Epoch 2720, Loss: 0.00000000978741, Improvement: 0.00000000032267, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2721
Epoch 2721, Loss: 0.00000000998893, Improvement: 0.00000000020152, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2722
Epoch 2722, Loss: 0.00000001129584, Improvement: 0.00000000130691, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2723
Epoch 2723, Loss: 0.00000001248529, Improvement: 0.00000000118945, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2724
Epoch 2724, Loss: 0.00000001207012, Improvement: -0.00000000041517, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2725
Epoch 2725, Loss: 0.00000001271335, Improvement: 0.00000000064323, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2726
Epoch 2726, Loss: 0.00000001627760, Improvement: 0.00000000356426, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2727
Epoch 2727, Loss: 0.00000001739866, Improvement: 0.00000000112105, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2728
Epoch 2728, Loss: 0.00000001505441, Improvement: -0.00000000234424, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2729
Epoch 2729, Loss: 0.00000001659285, Improvement: 0.00000000153844, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2730
Epoch 2730, Loss: 0.00000001373489, Improvement: -0.00000000285797, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2731
Epoch 2731, Loss: 0.00000001230423, Improvement: -0.00000000143066, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2732
Epoch 2732, Loss: 0.00000001103443, Improvement: -0.00000000126980, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2733
Epoch 2733, Loss: 0.00000001008464, Improvement: -0.00000000094979, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2734
Epoch 2734, Loss: 0.00000001107705, Improvement: 0.00000000099241, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2735
Epoch 2735, Loss: 0.00000001426663, Improvement: 0.00000000318958, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2736
Epoch 2736, Loss: 0.00000001811435, Improvement: 0.00000000384772, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2737
Epoch 2737, Loss: 0.00000002155239, Improvement: 0.00000000343803, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2738
Epoch 2738, Loss: 0.00000003313002, Improvement: 0.00000001157763, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2739
Epoch 2739, Loss: 0.00000004081138, Improvement: 0.00000000768137, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2740
Epoch 2740, Loss: 0.00000004088959, Improvement: 0.00000000007821, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2741
Epoch 2741, Loss: 0.00000002345800, Improvement: -0.00000001743159, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2742
Epoch 2742, Loss: 0.00000001281505, Improvement: -0.00000001064295, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2743
Epoch 2743, Loss: 0.00000001019105, Improvement: -0.00000000262400, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2744
Epoch 2744, Loss: 0.00000000919512, Improvement: -0.00000000099593, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2745
Epoch 2745, Loss: 0.00000000882055, Improvement: -0.00000000037457, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2746
Epoch 2746, Loss: 0.00000000858093, Improvement: -0.00000000023963, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2747
Epoch 2747, Loss: 0.00000000854450, Improvement: -0.00000000003643, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2748
Epoch 2748, Loss: 0.00000000869833, Improvement: 0.00000000015383, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2749
Epoch 2749, Loss: 0.00000000860449, Improvement: -0.00000000009384, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000000861249, Improvement: 0.00000000000800, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2751
Epoch 2751, Loss: 0.00000000859837, Improvement: -0.00000000001412, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2752
Epoch 2752, Loss: 0.00000000851743, Improvement: -0.00000000008095, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2753
Epoch 2753, Loss: 0.00000000849715, Improvement: -0.00000000002028, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2754
Epoch 2754, Loss: 0.00000000845405, Improvement: -0.00000000004310, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2755
Epoch 2755, Loss: 0.00000000859367, Improvement: 0.00000000013962, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2756
Epoch 2756, Loss: 0.00000000850576, Improvement: -0.00000000008791, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2757
Epoch 2757, Loss: 0.00000000873548, Improvement: 0.00000000022972, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2758
Epoch 2758, Loss: 0.00000000865712, Improvement: -0.00000000007836, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2759
Epoch 2759, Loss: 0.00000000863557, Improvement: -0.00000000002155, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2760
Epoch 2760, Loss: 0.00000000869247, Improvement: 0.00000000005690, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2761
Epoch 2761, Loss: 0.00000000867198, Improvement: -0.00000000002049, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2762
Epoch 2762, Loss: 0.00000000870728, Improvement: 0.00000000003529, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2763
Epoch 2763, Loss: 0.00000000909748, Improvement: 0.00000000039020, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2764
Epoch 2764, Loss: 0.00000000913219, Improvement: 0.00000000003472, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2765
Epoch 2765, Loss: 0.00000000913587, Improvement: 0.00000000000367, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2766
Epoch 2766, Loss: 0.00000000949348, Improvement: 0.00000000035761, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2767
Epoch 2767, Loss: 0.00000001094262, Improvement: 0.00000000144914, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2768
Epoch 2768, Loss: 0.00000000929472, Improvement: -0.00000000164790, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2769
Epoch 2769, Loss: 0.00000001009585, Improvement: 0.00000000080113, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2770
Epoch 2770, Loss: 0.00000000975550, Improvement: -0.00000000034035, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2771
Epoch 2771, Loss: 0.00000001033357, Improvement: 0.00000000057807, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2772
Epoch 2772, Loss: 0.00000001175445, Improvement: 0.00000000142088, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2773
Epoch 2773, Loss: 0.00000001274361, Improvement: 0.00000000098916, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2774
Epoch 2774, Loss: 0.00000001228279, Improvement: -0.00000000046082, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2775
Epoch 2775, Loss: 0.00000001213276, Improvement: -0.00000000015003, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2776
Epoch 2776, Loss: 0.00000001436914, Improvement: 0.00000000223639, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2777
Epoch 2777, Loss: 0.00000002101258, Improvement: 0.00000000664344, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2778
Epoch 2778, Loss: 0.00000002810978, Improvement: 0.00000000709720, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2779
Epoch 2779, Loss: 0.00000002737456, Improvement: -0.00000000073522, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2780
Epoch 2780, Loss: 0.00000001700778, Improvement: -0.00000001036677, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2781
Epoch 2781, Loss: 0.00000001182873, Improvement: -0.00000000517906, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2782
Epoch 2782, Loss: 0.00000000939704, Improvement: -0.00000000243169, Best Loss: 0.00000000480304 in Epoch 2584
Epoch 2783
A best model at epoch 2783 has been saved with training error 0.00000000478647.
Epoch 2783, Loss: 0.00000000939921, Improvement: 0.00000000000217, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2784
Epoch 2784, Loss: 0.00000000945888, Improvement: 0.00000000005967, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2785
Epoch 2785, Loss: 0.00000000926019, Improvement: -0.00000000019870, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2786
Epoch 2786, Loss: 0.00000001112506, Improvement: 0.00000000186487, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2787
Epoch 2787, Loss: 0.00000001147528, Improvement: 0.00000000035023, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2788
Epoch 2788, Loss: 0.00000001132342, Improvement: -0.00000000015186, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2789
Epoch 2789, Loss: 0.00000001353834, Improvement: 0.00000000221492, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2790
Epoch 2790, Loss: 0.00000001662863, Improvement: 0.00000000309030, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2791
Epoch 2791, Loss: 0.00000001243184, Improvement: -0.00000000419679, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2792
Epoch 2792, Loss: 0.00000001343191, Improvement: 0.00000000100007, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2793
Epoch 2793, Loss: 0.00000001422097, Improvement: 0.00000000078906, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2794
Epoch 2794, Loss: 0.00000001576911, Improvement: 0.00000000154814, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2795
Epoch 2795, Loss: 0.00000002624439, Improvement: 0.00000001047529, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2796
Epoch 2796, Loss: 0.00000002308737, Improvement: -0.00000000315702, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2797
Epoch 2797, Loss: 0.00000001585684, Improvement: -0.00000000723053, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2798
Epoch 2798, Loss: 0.00000001424047, Improvement: -0.00000000161637, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2799
Epoch 2799, Loss: 0.00000001367899, Improvement: -0.00000000056147, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000001173543, Improvement: -0.00000000194356, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2801
Epoch 2801, Loss: 0.00000001423733, Improvement: 0.00000000250190, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2802
Epoch 2802, Loss: 0.00000001741634, Improvement: 0.00000000317902, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2803
Epoch 2803, Loss: 0.00000001841559, Improvement: 0.00000000099925, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2804
Epoch 2804, Loss: 0.00000002216627, Improvement: 0.00000000375068, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2805
Epoch 2805, Loss: 0.00000003503282, Improvement: 0.00000001286654, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2806
Epoch 2806, Loss: 0.00000002754040, Improvement: -0.00000000749241, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2807
Epoch 2807, Loss: 0.00000001771778, Improvement: -0.00000000982262, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2808
Epoch 2808, Loss: 0.00000001180929, Improvement: -0.00000000590849, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2809
Epoch 2809, Loss: 0.00000001058980, Improvement: -0.00000000121949, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2810
Epoch 2810, Loss: 0.00000000936332, Improvement: -0.00000000122648, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2811
Epoch 2811, Loss: 0.00000000892915, Improvement: -0.00000000043416, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2812
Epoch 2812, Loss: 0.00000000891000, Improvement: -0.00000000001916, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2813
Epoch 2813, Loss: 0.00000000996301, Improvement: 0.00000000105301, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2814
Epoch 2814, Loss: 0.00000000927690, Improvement: -0.00000000068611, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2815
Epoch 2815, Loss: 0.00000000897098, Improvement: -0.00000000030592, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2816
Epoch 2816, Loss: 0.00000000886240, Improvement: -0.00000000010859, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2817
Epoch 2817, Loss: 0.00000000889775, Improvement: 0.00000000003535, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2818
Epoch 2818, Loss: 0.00000000903692, Improvement: 0.00000000013917, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2819
Epoch 2819, Loss: 0.00000000917853, Improvement: 0.00000000014161, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2820
Epoch 2820, Loss: 0.00000000913271, Improvement: -0.00000000004582, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2821
Epoch 2821, Loss: 0.00000000897881, Improvement: -0.00000000015390, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2822
Epoch 2822, Loss: 0.00000000899499, Improvement: 0.00000000001618, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2823
Epoch 2823, Loss: 0.00000000858008, Improvement: -0.00000000041491, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2824
Epoch 2824, Loss: 0.00000000902205, Improvement: 0.00000000044197, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2825
Epoch 2825, Loss: 0.00000000942925, Improvement: 0.00000000040719, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2826
Epoch 2826, Loss: 0.00000000924138, Improvement: -0.00000000018786, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2827
Epoch 2827, Loss: 0.00000000987838, Improvement: 0.00000000063699, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2828
Epoch 2828, Loss: 0.00000001060990, Improvement: 0.00000000073152, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2829
Epoch 2829, Loss: 0.00000001107663, Improvement: 0.00000000046672, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2830
Epoch 2830, Loss: 0.00000001112559, Improvement: 0.00000000004896, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2831
Epoch 2831, Loss: 0.00000001766157, Improvement: 0.00000000653598, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2832
Epoch 2832, Loss: 0.00000001830220, Improvement: 0.00000000064063, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2833
Epoch 2833, Loss: 0.00000002895924, Improvement: 0.00000001065704, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2834
Epoch 2834, Loss: 0.00000002025808, Improvement: -0.00000000870116, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2835
Epoch 2835, Loss: 0.00000001343282, Improvement: -0.00000000682526, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2836
Epoch 2836, Loss: 0.00000001238319, Improvement: -0.00000000104963, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2837
Epoch 2837, Loss: 0.00000001235501, Improvement: -0.00000000002818, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2838
Epoch 2838, Loss: 0.00000001340705, Improvement: 0.00000000105204, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2839
Epoch 2839, Loss: 0.00000001122963, Improvement: -0.00000000217742, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2840
Epoch 2840, Loss: 0.00000001242554, Improvement: 0.00000000119591, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2841
Epoch 2841, Loss: 0.00000001058997, Improvement: -0.00000000183557, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2842
Epoch 2842, Loss: 0.00000001042965, Improvement: -0.00000000016032, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2843
Epoch 2843, Loss: 0.00000001150773, Improvement: 0.00000000107807, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2844
Epoch 2844, Loss: 0.00000001223520, Improvement: 0.00000000072748, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2845
Epoch 2845, Loss: 0.00000001185983, Improvement: -0.00000000037538, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2846
Epoch 2846, Loss: 0.00000001235802, Improvement: 0.00000000049820, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2847
Epoch 2847, Loss: 0.00000001085684, Improvement: -0.00000000150119, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2848
Epoch 2848, Loss: 0.00000001418112, Improvement: 0.00000000332428, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2849
Epoch 2849, Loss: 0.00000002530695, Improvement: 0.00000001112583, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000002816627, Improvement: 0.00000000285931, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2851
Epoch 2851, Loss: 0.00000002035403, Improvement: -0.00000000781223, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2852
Epoch 2852, Loss: 0.00000001364552, Improvement: -0.00000000670851, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2853
Epoch 2853, Loss: 0.00000000918492, Improvement: -0.00000000446060, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2854
Epoch 2854, Loss: 0.00000000871884, Improvement: -0.00000000046608, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2855
Epoch 2855, Loss: 0.00000000852608, Improvement: -0.00000000019276, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2856
Epoch 2856, Loss: 0.00000000845546, Improvement: -0.00000000007062, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2857
Epoch 2857, Loss: 0.00000000852272, Improvement: 0.00000000006725, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2858
Epoch 2858, Loss: 0.00000000867301, Improvement: 0.00000000015029, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2859
Epoch 2859, Loss: 0.00000000877930, Improvement: 0.00000000010629, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2860
Epoch 2860, Loss: 0.00000000881500, Improvement: 0.00000000003570, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2861
Epoch 2861, Loss: 0.00000000994056, Improvement: 0.00000000112556, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2862
Epoch 2862, Loss: 0.00000000999462, Improvement: 0.00000000005405, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2863
Epoch 2863, Loss: 0.00000001234895, Improvement: 0.00000000235433, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2864
Epoch 2864, Loss: 0.00000001152254, Improvement: -0.00000000082641, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2865
Epoch 2865, Loss: 0.00000001124632, Improvement: -0.00000000027622, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2866
Epoch 2866, Loss: 0.00000001238005, Improvement: 0.00000000113374, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2867
Epoch 2867, Loss: 0.00000001419727, Improvement: 0.00000000181721, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2868
Epoch 2868, Loss: 0.00000001250872, Improvement: -0.00000000168855, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2869
Epoch 2869, Loss: 0.00000001493763, Improvement: 0.00000000242891, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2870
Epoch 2870, Loss: 0.00000001652749, Improvement: 0.00000000158987, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2871
Epoch 2871, Loss: 0.00000001597414, Improvement: -0.00000000055336, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2872
Epoch 2872, Loss: 0.00000001643319, Improvement: 0.00000000045905, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2873
Epoch 2873, Loss: 0.00000001424733, Improvement: -0.00000000218586, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2874
Epoch 2874, Loss: 0.00000001222331, Improvement: -0.00000000202402, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2875
Epoch 2875, Loss: 0.00000001396380, Improvement: 0.00000000174050, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2876
Epoch 2876, Loss: 0.00000001467135, Improvement: 0.00000000070755, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2877
Epoch 2877, Loss: 0.00000001177924, Improvement: -0.00000000289211, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2878
Epoch 2878, Loss: 0.00000001143057, Improvement: -0.00000000034867, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2879
Epoch 2879, Loss: 0.00000001026728, Improvement: -0.00000000116329, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2880
Epoch 2880, Loss: 0.00000001180089, Improvement: 0.00000000153361, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2881
Epoch 2881, Loss: 0.00000001851601, Improvement: 0.00000000671512, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2882
Epoch 2882, Loss: 0.00000001429342, Improvement: -0.00000000422258, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2883
Epoch 2883, Loss: 0.00000001175645, Improvement: -0.00000000253698, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2884
Epoch 2884, Loss: 0.00000002127402, Improvement: 0.00000000951757, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2885
Epoch 2885, Loss: 0.00000002824473, Improvement: 0.00000000697072, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2886
Epoch 2886, Loss: 0.00000003491832, Improvement: 0.00000000667359, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2887
Epoch 2887, Loss: 0.00000002901322, Improvement: -0.00000000590511, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2888
Epoch 2888, Loss: 0.00000001926471, Improvement: -0.00000000974851, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2889
Epoch 2889, Loss: 0.00000001354550, Improvement: -0.00000000571921, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2890
Epoch 2890, Loss: 0.00000001123016, Improvement: -0.00000000231534, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2891
Epoch 2891, Loss: 0.00000000973264, Improvement: -0.00000000149752, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2892
Epoch 2892, Loss: 0.00000000925339, Improvement: -0.00000000047925, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2893
Epoch 2893, Loss: 0.00000000862202, Improvement: -0.00000000063138, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2894
Epoch 2894, Loss: 0.00000000850986, Improvement: -0.00000000011216, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2895
Epoch 2895, Loss: 0.00000000849744, Improvement: -0.00000000001242, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2896
Epoch 2896, Loss: 0.00000000880540, Improvement: 0.00000000030796, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2897
Epoch 2897, Loss: 0.00000000860303, Improvement: -0.00000000020237, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2898
Epoch 2898, Loss: 0.00000000853713, Improvement: -0.00000000006590, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2899
Epoch 2899, Loss: 0.00000000904303, Improvement: 0.00000000050590, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000000912953, Improvement: 0.00000000008650, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2901
Epoch 2901, Loss: 0.00000000903958, Improvement: -0.00000000008995, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2902
Epoch 2902, Loss: 0.00000000943779, Improvement: 0.00000000039821, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2903
Epoch 2903, Loss: 0.00000001041713, Improvement: 0.00000000097934, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2904
Epoch 2904, Loss: 0.00000001076821, Improvement: 0.00000000035108, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2905
Epoch 2905, Loss: 0.00000001023168, Improvement: -0.00000000053653, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2906
Epoch 2906, Loss: 0.00000001306003, Improvement: 0.00000000282835, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2907
Epoch 2907, Loss: 0.00000001741960, Improvement: 0.00000000435957, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2908
Epoch 2908, Loss: 0.00000001866477, Improvement: 0.00000000124517, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2909
Epoch 2909, Loss: 0.00000001384000, Improvement: -0.00000000482477, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2910
Epoch 2910, Loss: 0.00000001102599, Improvement: -0.00000000281401, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2911
Epoch 2911, Loss: 0.00000000973261, Improvement: -0.00000000129337, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2912
Epoch 2912, Loss: 0.00000000881785, Improvement: -0.00000000091476, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2913
Epoch 2913, Loss: 0.00000000988355, Improvement: 0.00000000106570, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2914
Epoch 2914, Loss: 0.00000001080978, Improvement: 0.00000000092623, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2915
Epoch 2915, Loss: 0.00000000938163, Improvement: -0.00000000142815, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2916
Epoch 2916, Loss: 0.00000000924916, Improvement: -0.00000000013248, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2917
Epoch 2917, Loss: 0.00000000951511, Improvement: 0.00000000026595, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2918
Epoch 2918, Loss: 0.00000001171551, Improvement: 0.00000000220040, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2919
Epoch 2919, Loss: 0.00000001192953, Improvement: 0.00000000021402, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2920
Epoch 2920, Loss: 0.00000001601932, Improvement: 0.00000000408979, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2921
Epoch 2921, Loss: 0.00000002112032, Improvement: 0.00000000510100, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2922
Epoch 2922, Loss: 0.00000002128047, Improvement: 0.00000000016015, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2923
Epoch 2923, Loss: 0.00000002102308, Improvement: -0.00000000025739, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2924
Epoch 2924, Loss: 0.00000002018305, Improvement: -0.00000000084003, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2925
Epoch 2925, Loss: 0.00000002000342, Improvement: -0.00000000017963, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2926
Epoch 2926, Loss: 0.00000001340513, Improvement: -0.00000000659830, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2927
Epoch 2927, Loss: 0.00000001080609, Improvement: -0.00000000259903, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2928
Epoch 2928, Loss: 0.00000001028106, Improvement: -0.00000000052503, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2929
Epoch 2929, Loss: 0.00000000940874, Improvement: -0.00000000087232, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2930
Epoch 2930, Loss: 0.00000000967667, Improvement: 0.00000000026792, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2931
Epoch 2931, Loss: 0.00000000987316, Improvement: 0.00000000019649, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2932
Epoch 2932, Loss: 0.00000001124367, Improvement: 0.00000000137051, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2933
Epoch 2933, Loss: 0.00000001112613, Improvement: -0.00000000011754, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2934
Epoch 2934, Loss: 0.00000001040646, Improvement: -0.00000000071967, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2935
Epoch 2935, Loss: 0.00000000975801, Improvement: -0.00000000064845, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2936
Epoch 2936, Loss: 0.00000001040506, Improvement: 0.00000000064704, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2937
Epoch 2937, Loss: 0.00000001053659, Improvement: 0.00000000013153, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2938
Epoch 2938, Loss: 0.00000000999368, Improvement: -0.00000000054291, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2939
Epoch 2939, Loss: 0.00000001606244, Improvement: 0.00000000606876, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2940
Epoch 2940, Loss: 0.00000001904157, Improvement: 0.00000000297912, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2941
Epoch 2941, Loss: 0.00000001995363, Improvement: 0.00000000091206, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2942
Epoch 2942, Loss: 0.00000002050831, Improvement: 0.00000000055468, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2943
Epoch 2943, Loss: 0.00000002666732, Improvement: 0.00000000615901, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2944
Epoch 2944, Loss: 0.00000001705592, Improvement: -0.00000000961141, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2945
Epoch 2945, Loss: 0.00000001497906, Improvement: -0.00000000207686, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2946
Epoch 2946, Loss: 0.00000001556133, Improvement: 0.00000000058227, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2947
Epoch 2947, Loss: 0.00000001172167, Improvement: -0.00000000383965, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2948
Epoch 2948, Loss: 0.00000001011916, Improvement: -0.00000000160252, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2949
Epoch 2949, Loss: 0.00000001060320, Improvement: 0.00000000048405, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000000930769, Improvement: -0.00000000129552, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2951
Epoch 2951, Loss: 0.00000000928163, Improvement: -0.00000000002605, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2952
Epoch 2952, Loss: 0.00000000902758, Improvement: -0.00000000025405, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2953
Epoch 2953, Loss: 0.00000000881237, Improvement: -0.00000000021521, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2954
Epoch 2954, Loss: 0.00000000835745, Improvement: -0.00000000045492, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2955
Epoch 2955, Loss: 0.00000000832211, Improvement: -0.00000000003535, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2956
Epoch 2956, Loss: 0.00000000830411, Improvement: -0.00000000001799, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2957
Epoch 2957, Loss: 0.00000000856473, Improvement: 0.00000000026061, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2958
Epoch 2958, Loss: 0.00000001020106, Improvement: 0.00000000163633, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2959
Epoch 2959, Loss: 0.00000000970687, Improvement: -0.00000000049419, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2960
Epoch 2960, Loss: 0.00000000945001, Improvement: -0.00000000025686, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2961
Epoch 2961, Loss: 0.00000000999141, Improvement: 0.00000000054140, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2962
Epoch 2962, Loss: 0.00000001015931, Improvement: 0.00000000016789, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2963
Epoch 2963, Loss: 0.00000001956927, Improvement: 0.00000000940996, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2964
Epoch 2964, Loss: 0.00000001823641, Improvement: -0.00000000133285, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2965
Epoch 2965, Loss: 0.00000001328789, Improvement: -0.00000000494853, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2966
Epoch 2966, Loss: 0.00000001129800, Improvement: -0.00000000198988, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2967
Epoch 2967, Loss: 0.00000001073837, Improvement: -0.00000000055963, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2968
Epoch 2968, Loss: 0.00000001501286, Improvement: 0.00000000427449, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2969
Epoch 2969, Loss: 0.00000001398841, Improvement: -0.00000000102446, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2970
Epoch 2970, Loss: 0.00000001032050, Improvement: -0.00000000366791, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2971
Epoch 2971, Loss: 0.00000001178209, Improvement: 0.00000000146160, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2972
Epoch 2972, Loss: 0.00000001125320, Improvement: -0.00000000052889, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2973
Epoch 2973, Loss: 0.00000001076821, Improvement: -0.00000000048499, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2974
Epoch 2974, Loss: 0.00000001026235, Improvement: -0.00000000050587, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2975
Epoch 2975, Loss: 0.00000001195152, Improvement: 0.00000000168918, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2976
Epoch 2976, Loss: 0.00000001357534, Improvement: 0.00000000162381, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2977
Epoch 2977, Loss: 0.00000001293830, Improvement: -0.00000000063704, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2978
Epoch 2978, Loss: 0.00000001078268, Improvement: -0.00000000215562, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2979
Epoch 2979, Loss: 0.00000001044018, Improvement: -0.00000000034250, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2980
Epoch 2980, Loss: 0.00000001090532, Improvement: 0.00000000046514, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2981
Epoch 2981, Loss: 0.00000001033994, Improvement: -0.00000000056537, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2982
Epoch 2982, Loss: 0.00000002011473, Improvement: 0.00000000977479, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2983
Epoch 2983, Loss: 0.00000002339289, Improvement: 0.00000000327816, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2984
Epoch 2984, Loss: 0.00000002184508, Improvement: -0.00000000154781, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2985
Epoch 2985, Loss: 0.00000002440253, Improvement: 0.00000000255744, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2986
Epoch 2986, Loss: 0.00000001789088, Improvement: -0.00000000651165, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2987
Epoch 2987, Loss: 0.00000001708876, Improvement: -0.00000000080212, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2988
Epoch 2988, Loss: 0.00000002030697, Improvement: 0.00000000321821, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2989
Epoch 2989, Loss: 0.00000001432776, Improvement: -0.00000000597921, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2990
Epoch 2990, Loss: 0.00000001163672, Improvement: -0.00000000269103, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2991
Epoch 2991, Loss: 0.00000001012939, Improvement: -0.00000000150733, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2992
Epoch 2992, Loss: 0.00000000889042, Improvement: -0.00000000123897, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2993
Epoch 2993, Loss: 0.00000000901111, Improvement: 0.00000000012069, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2994
Epoch 2994, Loss: 0.00000000881248, Improvement: -0.00000000019864, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2995
Epoch 2995, Loss: 0.00000000889798, Improvement: 0.00000000008550, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2996
Epoch 2996, Loss: 0.00000000870033, Improvement: -0.00000000019764, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2997
Epoch 2997, Loss: 0.00000000943881, Improvement: 0.00000000073847, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2998
Epoch 2998, Loss: 0.00000000974512, Improvement: 0.00000000030632, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 2999
Epoch 2999, Loss: 0.00000001177302, Improvement: 0.00000000202790, Best Loss: 0.00000000478647 in Epoch 2783
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000001039190, Improvement: -0.00000000138112, Best Loss: 0.00000000478647 in Epoch 2783
