The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.001155536.
A best model at epoch 1 has been saved with training error 0.000131584.
A best model at epoch 1 has been saved with training error 0.000069130.
Epoch 1, Loss: 0.001537698, Improvement: 0.001537698, Best Loss: 0.000069130 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.000058323.
A best model at epoch 2 has been saved with training error 0.000043436.
Epoch 2, Loss: 0.000092127, Improvement: -0.001445571, Best Loss: 0.000043436 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.000041485.
Epoch 3, Loss: 0.000058123, Improvement: -0.000034004, Best Loss: 0.000041485 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.000038295.
A best model at epoch 4 has been saved with training error 0.000035492.
Epoch 4, Loss: 0.000050404, Improvement: -0.000007719, Best Loss: 0.000035492 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.000033127.
A best model at epoch 5 has been saved with training error 0.000022576.
Epoch 5, Loss: 0.000049310, Improvement: -0.000001093, Best Loss: 0.000022576 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.000048755, Improvement: -0.000000555, Best Loss: 0.000022576 in Epoch 5
Epoch 7
Epoch 7, Loss: 0.000048217, Improvement: -0.000000538, Best Loss: 0.000022576 in Epoch 5
Epoch 8
Epoch 8, Loss: 0.000047677, Improvement: -0.000000540, Best Loss: 0.000022576 in Epoch 5
Epoch 9
Epoch 9, Loss: 0.000047096, Improvement: -0.000000581, Best Loss: 0.000022576 in Epoch 5
Epoch 10
Epoch 10, Loss: 0.000046466, Improvement: -0.000000630, Best Loss: 0.000022576 in Epoch 5
Epoch 11
Epoch 11, Loss: 0.000045814, Improvement: -0.000000653, Best Loss: 0.000022576 in Epoch 5
Epoch 12
Epoch 12, Loss: 0.000045088, Improvement: -0.000000726, Best Loss: 0.000022576 in Epoch 5
Epoch 13
Epoch 13, Loss: 0.000044323, Improvement: -0.000000765, Best Loss: 0.000022576 in Epoch 5
Epoch 14
Epoch 14, Loss: 0.000043516, Improvement: -0.000000807, Best Loss: 0.000022576 in Epoch 5
Epoch 15
Epoch 15, Loss: 0.000042586, Improvement: -0.000000930, Best Loss: 0.000022576 in Epoch 5
Epoch 16
Epoch 16, Loss: 0.000041612, Improvement: -0.000000974, Best Loss: 0.000022576 in Epoch 5
Epoch 17
Epoch 17, Loss: 0.000040561, Improvement: -0.000001051, Best Loss: 0.000022576 in Epoch 5
Epoch 18
A best model at epoch 18 has been saved with training error 0.000021204.
Epoch 18, Loss: 0.000039370, Improvement: -0.000001191, Best Loss: 0.000021204 in Epoch 18
Epoch 19
A best model at epoch 19 has been saved with training error 0.000017633.
Epoch 19, Loss: 0.000038061, Improvement: -0.000001309, Best Loss: 0.000017633 in Epoch 19
Epoch 20
Epoch 20, Loss: 0.000036631, Improvement: -0.000001431, Best Loss: 0.000017633 in Epoch 19
Epoch 21
Epoch 21, Loss: 0.000035110, Improvement: -0.000001521, Best Loss: 0.000017633 in Epoch 19
Epoch 22
A best model at epoch 22 has been saved with training error 0.000017624.
A best model at epoch 22 has been saved with training error 0.000016941.
Epoch 22, Loss: 0.000033408, Improvement: -0.000001702, Best Loss: 0.000016941 in Epoch 22
Epoch 23
Epoch 23, Loss: 0.000031575, Improvement: -0.000001833, Best Loss: 0.000016941 in Epoch 22
Epoch 24
Epoch 24, Loss: 0.000029614, Improvement: -0.000001961, Best Loss: 0.000016941 in Epoch 22
Epoch 25
A best model at epoch 25 has been saved with training error 0.000013984.
Epoch 25, Loss: 0.000027535, Improvement: -0.000002080, Best Loss: 0.000013984 in Epoch 25
Epoch 26
Epoch 26, Loss: 0.000025321, Improvement: -0.000002214, Best Loss: 0.000013984 in Epoch 25
Epoch 27
A best model at epoch 27 has been saved with training error 0.000011884.
Epoch 27, Loss: 0.000023105, Improvement: -0.000002216, Best Loss: 0.000011884 in Epoch 27
Epoch 28
Epoch 28, Loss: 0.000020945, Improvement: -0.000002160, Best Loss: 0.000011884 in Epoch 27
Epoch 29
A best model at epoch 29 has been saved with training error 0.000010078.
Epoch 29, Loss: 0.000018808, Improvement: -0.000002136, Best Loss: 0.000010078 in Epoch 29
Epoch 30
Epoch 30, Loss: 0.000016821, Improvement: -0.000001988, Best Loss: 0.000010078 in Epoch 29
Epoch 31
A best model at epoch 31 has been saved with training error 0.000008925.
Epoch 31, Loss: 0.000014876, Improvement: -0.000001944, Best Loss: 0.000008925 in Epoch 31
Epoch 32
A best model at epoch 32 has been saved with training error 0.000008229.
Epoch 32, Loss: 0.000012930, Improvement: -0.000001947, Best Loss: 0.000008229 in Epoch 32
Epoch 33
A best model at epoch 33 has been saved with training error 0.000007545.
A best model at epoch 33 has been saved with training error 0.000006806.
Epoch 33, Loss: 0.000011048, Improvement: -0.000001881, Best Loss: 0.000006806 in Epoch 33
Epoch 34
A best model at epoch 34 has been saved with training error 0.000005672.
Epoch 34, Loss: 0.000009115, Improvement: -0.000001933, Best Loss: 0.000005672 in Epoch 34
Epoch 35
A best model at epoch 35 has been saved with training error 0.000005010.
A best model at epoch 35 has been saved with training error 0.000004716.
A best model at epoch 35 has been saved with training error 0.000003883.
Epoch 35, Loss: 0.000007414, Improvement: -0.000001701, Best Loss: 0.000003883 in Epoch 35
Epoch 36
A best model at epoch 36 has been saved with training error 0.000002753.
Epoch 36, Loss: 0.000005756, Improvement: -0.000001658, Best Loss: 0.000002753 in Epoch 36
Epoch 37
Epoch 37, Loss: 0.000004420, Improvement: -0.000001336, Best Loss: 0.000002753 in Epoch 36
Epoch 38
A best model at epoch 38 has been saved with training error 0.000002595.
A best model at epoch 38 has been saved with training error 0.000002371.
A best model at epoch 38 has been saved with training error 0.000002313.
Epoch 38, Loss: 0.000003374, Improvement: -0.000001045, Best Loss: 0.000002313 in Epoch 38
Epoch 39
A best model at epoch 39 has been saved with training error 0.000001922.
A best model at epoch 39 has been saved with training error 0.000001650.
Epoch 39, Loss: 0.000002631, Improvement: -0.000000744, Best Loss: 0.000001650 in Epoch 39
Epoch 40
A best model at epoch 40 has been saved with training error 0.000001373.
A best model at epoch 40 has been saved with training error 0.000001139.
Epoch 40, Loss: 0.000002116, Improvement: -0.000000514, Best Loss: 0.000001139 in Epoch 40
Epoch 41
A best model at epoch 41 has been saved with training error 0.000001134.
Epoch 41, Loss: 0.000001760, Improvement: -0.000000357, Best Loss: 0.000001134 in Epoch 41
Epoch 42
A best model at epoch 42 has been saved with training error 0.000001070.
A best model at epoch 42 has been saved with training error 0.000001047.
A best model at epoch 42 has been saved with training error 0.000000975.
Epoch 42, Loss: 0.000001524, Improvement: -0.000000235, Best Loss: 0.000000975 in Epoch 42
Epoch 43
A best model at epoch 43 has been saved with training error 0.000000873.
Epoch 43, Loss: 0.000001349, Improvement: -0.000000175, Best Loss: 0.000000873 in Epoch 43
Epoch 44
A best model at epoch 44 has been saved with training error 0.000000766.
Epoch 44, Loss: 0.000001220, Improvement: -0.000000130, Best Loss: 0.000000766 in Epoch 44
Epoch 45
A best model at epoch 45 has been saved with training error 0.000000669.
Epoch 45, Loss: 0.000001104, Improvement: -0.000000115, Best Loss: 0.000000669 in Epoch 45
Epoch 46
A best model at epoch 46 has been saved with training error 0.000000633.
A best model at epoch 46 has been saved with training error 0.000000599.
Epoch 46, Loss: 0.000001018, Improvement: -0.000000087, Best Loss: 0.000000599 in Epoch 46
Epoch 47
A best model at epoch 47 has been saved with training error 0.000000521.
Epoch 47, Loss: 0.000000938, Improvement: -0.000000080, Best Loss: 0.000000521 in Epoch 47
Epoch 48
A best model at epoch 48 has been saved with training error 0.000000430.
Epoch 48, Loss: 0.000000872, Improvement: -0.000000066, Best Loss: 0.000000430 in Epoch 48
Epoch 49
Epoch 49, Loss: 0.000000814, Improvement: -0.000000057, Best Loss: 0.000000430 in Epoch 48
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.000000764, Improvement: -0.000000050, Best Loss: 0.000000430 in Epoch 48
Epoch 51
A best model at epoch 51 has been saved with training error 0.000000407.
Epoch 51, Loss: 0.000000716, Improvement: -0.000000049, Best Loss: 0.000000407 in Epoch 51
Epoch 52
Epoch 52, Loss: 0.000000673, Improvement: -0.000000043, Best Loss: 0.000000407 in Epoch 51
Epoch 53
Epoch 53, Loss: 0.000000636, Improvement: -0.000000037, Best Loss: 0.000000407 in Epoch 51
Epoch 54
Epoch 54, Loss: 0.000000599, Improvement: -0.000000037, Best Loss: 0.000000407 in Epoch 51
Epoch 55
Epoch 55, Loss: 0.000000569, Improvement: -0.000000031, Best Loss: 0.000000407 in Epoch 51
Epoch 56
Epoch 56, Loss: 0.000000539, Improvement: -0.000000029, Best Loss: 0.000000407 in Epoch 51
Epoch 57
A best model at epoch 57 has been saved with training error 0.000000397.
A best model at epoch 57 has been saved with training error 0.000000368.
A best model at epoch 57 has been saved with training error 0.000000319.
Epoch 57, Loss: 0.000000511, Improvement: -0.000000028, Best Loss: 0.000000319 in Epoch 57
Epoch 58
Epoch 58, Loss: 0.000000485, Improvement: -0.000000025, Best Loss: 0.000000319 in Epoch 57
Epoch 59
Epoch 59, Loss: 0.000000461, Improvement: -0.000000024, Best Loss: 0.000000319 in Epoch 57
Epoch 60
A best model at epoch 60 has been saved with training error 0.000000315.
Epoch 60, Loss: 0.000000440, Improvement: -0.000000021, Best Loss: 0.000000315 in Epoch 60
Epoch 61
A best model at epoch 61 has been saved with training error 0.000000304.
A best model at epoch 61 has been saved with training error 0.000000302.
Epoch 61, Loss: 0.000000421, Improvement: -0.000000019, Best Loss: 0.000000302 in Epoch 61
Epoch 62
Epoch 62, Loss: 0.000000404, Improvement: -0.000000018, Best Loss: 0.000000302 in Epoch 61
Epoch 63
Epoch 63, Loss: 0.000000386, Improvement: -0.000000018, Best Loss: 0.000000302 in Epoch 61
Epoch 64
A best model at epoch 64 has been saved with training error 0.000000224.
Epoch 64, Loss: 0.000000369, Improvement: -0.000000017, Best Loss: 0.000000224 in Epoch 64
Epoch 65
Epoch 65, Loss: 0.000000354, Improvement: -0.000000015, Best Loss: 0.000000224 in Epoch 64
Epoch 66
A best model at epoch 66 has been saved with training error 0.000000224.
A best model at epoch 66 has been saved with training error 0.000000217.
Epoch 66, Loss: 0.000000340, Improvement: -0.000000014, Best Loss: 0.000000217 in Epoch 66
Epoch 67
Epoch 67, Loss: 0.000000328, Improvement: -0.000000012, Best Loss: 0.000000217 in Epoch 66
Epoch 68
Epoch 68, Loss: 0.000000317, Improvement: -0.000000012, Best Loss: 0.000000217 in Epoch 66
Epoch 69
Epoch 69, Loss: 0.000000307, Improvement: -0.000000010, Best Loss: 0.000000217 in Epoch 66
Epoch 70
Epoch 70, Loss: 0.000000297, Improvement: -0.000000009, Best Loss: 0.000000217 in Epoch 66
Epoch 71
Epoch 71, Loss: 0.000000289, Improvement: -0.000000009, Best Loss: 0.000000217 in Epoch 66
Epoch 72
A best model at epoch 72 has been saved with training error 0.000000210.
Epoch 72, Loss: 0.000000282, Improvement: -0.000000007, Best Loss: 0.000000210 in Epoch 72
Epoch 73
A best model at epoch 73 has been saved with training error 0.000000206.
A best model at epoch 73 has been saved with training error 0.000000203.
Epoch 73, Loss: 0.000000273, Improvement: -0.000000009, Best Loss: 0.000000203 in Epoch 73
Epoch 74
A best model at epoch 74 has been saved with training error 0.000000201.
Epoch 74, Loss: 0.000000266, Improvement: -0.000000006, Best Loss: 0.000000201 in Epoch 74
Epoch 75
A best model at epoch 75 has been saved with training error 0.000000178.
Epoch 75, Loss: 0.000000261, Improvement: -0.000000006, Best Loss: 0.000000178 in Epoch 75
Epoch 76
Epoch 76, Loss: 0.000000256, Improvement: -0.000000005, Best Loss: 0.000000178 in Epoch 75
Epoch 77
Epoch 77, Loss: 0.000000250, Improvement: -0.000000006, Best Loss: 0.000000178 in Epoch 75
Epoch 78
A best model at epoch 78 has been saved with training error 0.000000174.
Epoch 78, Loss: 0.000000246, Improvement: -0.000000004, Best Loss: 0.000000174 in Epoch 78
Epoch 79
A best model at epoch 79 has been saved with training error 0.000000146.
Epoch 79, Loss: 0.000000242, Improvement: -0.000000004, Best Loss: 0.000000146 in Epoch 79
Epoch 80
Epoch 80, Loss: 0.000000238, Improvement: -0.000000004, Best Loss: 0.000000146 in Epoch 79
Epoch 81
Epoch 81, Loss: 0.000000235, Improvement: -0.000000003, Best Loss: 0.000000146 in Epoch 79
Epoch 82
Epoch 82, Loss: 0.000000232, Improvement: -0.000000003, Best Loss: 0.000000146 in Epoch 79
Epoch 83
Epoch 83, Loss: 0.000000229, Improvement: -0.000000003, Best Loss: 0.000000146 in Epoch 79
Epoch 84
Epoch 84, Loss: 0.000000226, Improvement: -0.000000003, Best Loss: 0.000000146 in Epoch 79
Epoch 85
Epoch 85, Loss: 0.000000224, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 86
Epoch 86, Loss: 0.000000222, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 87
Epoch 87, Loss: 0.000000220, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 88
Epoch 88, Loss: 0.000000218, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 89
Epoch 89, Loss: 0.000000217, Improvement: -0.000000001, Best Loss: 0.000000146 in Epoch 79
Epoch 90
Epoch 90, Loss: 0.000000215, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 91
Epoch 91, Loss: 0.000000213, Improvement: -0.000000001, Best Loss: 0.000000146 in Epoch 79
Epoch 92
Epoch 92, Loss: 0.000000212, Improvement: -0.000000002, Best Loss: 0.000000146 in Epoch 79
Epoch 93
Epoch 93, Loss: 0.000000211, Improvement: -0.000000001, Best Loss: 0.000000146 in Epoch 79
Epoch 94
Epoch 94, Loss: 0.000000210, Improvement: -0.000000001, Best Loss: 0.000000146 in Epoch 79
Epoch 95
Epoch 95, Loss: 0.000000209, Improvement: -0.000000001, Best Loss: 0.000000146 in Epoch 79
Epoch 96
A best model at epoch 96 has been saved with training error 0.000000144.
Epoch 96, Loss: 0.000000207, Improvement: -0.000000002, Best Loss: 0.000000144 in Epoch 96
Epoch 97
Epoch 97, Loss: 0.000000206, Improvement: -0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 98
Epoch 98, Loss: 0.000000205, Improvement: -0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 99
Epoch 99, Loss: 0.000000204, Improvement: -0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.000000203, Improvement: -0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 101
Epoch 101, Loss: 0.000000203, Improvement: -0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 102
Epoch 102, Loss: 0.000000204, Improvement: 0.000000001, Best Loss: 0.000000144 in Epoch 96
Epoch 103
A best model at epoch 103 has been saved with training error 0.000000124.
Epoch 103, Loss: 0.000000201, Improvement: -0.000000003, Best Loss: 0.000000124 in Epoch 103
Epoch 104
Epoch 104, Loss: 0.000000200, Improvement: -0.000000001, Best Loss: 0.000000124 in Epoch 103
Epoch 105
Epoch 105, Loss: 0.000000200, Improvement: -0.000000000, Best Loss: 0.000000124 in Epoch 103
Epoch 106
Epoch 106, Loss: 0.000000198, Improvement: -0.000000002, Best Loss: 0.000000124 in Epoch 103
Epoch 107
Epoch 107, Loss: 0.000000198, Improvement: -0.000000000, Best Loss: 0.000000124 in Epoch 103
Epoch 108
Epoch 108, Loss: 0.000000198, Improvement: 0.000000000, Best Loss: 0.000000124 in Epoch 103
Epoch 109
Epoch 109, Loss: 0.000000197, Improvement: -0.000000001, Best Loss: 0.000000124 in Epoch 103
Epoch 110
A best model at epoch 110 has been saved with training error 0.000000123.
Epoch 110, Loss: 0.000000197, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 111
Epoch 111, Loss: 0.000000196, Improvement: -0.000000001, Best Loss: 0.000000123 in Epoch 110
Epoch 112
Epoch 112, Loss: 0.000000196, Improvement: 0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 113
Epoch 113, Loss: 0.000000194, Improvement: -0.000000002, Best Loss: 0.000000123 in Epoch 110
Epoch 114
Epoch 114, Loss: 0.000000194, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 115
Epoch 115, Loss: 0.000000195, Improvement: 0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 116
Epoch 116, Loss: 0.000000193, Improvement: -0.000000001, Best Loss: 0.000000123 in Epoch 110
Epoch 117
Epoch 117, Loss: 0.000000193, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 118
Epoch 118, Loss: 0.000000193, Improvement: 0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 119
Epoch 119, Loss: 0.000000193, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 120
Epoch 120, Loss: 0.000000192, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 121
Epoch 121, Loss: 0.000000192, Improvement: -0.000000001, Best Loss: 0.000000123 in Epoch 110
Epoch 122
Epoch 122, Loss: 0.000000191, Improvement: -0.000000000, Best Loss: 0.000000123 in Epoch 110
Epoch 123
A best model at epoch 123 has been saved with training error 0.000000122.
Epoch 123, Loss: 0.000000191, Improvement: -0.000000000, Best Loss: 0.000000122 in Epoch 123
Epoch 124
Epoch 124, Loss: 0.000000191, Improvement: -0.000000000, Best Loss: 0.000000122 in Epoch 123
Epoch 125
Epoch 125, Loss: 0.000000193, Improvement: 0.000000002, Best Loss: 0.000000122 in Epoch 123
Epoch 126
A best model at epoch 126 has been saved with training error 0.000000103.
Epoch 126, Loss: 0.000000192, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 127
Epoch 127, Loss: 0.000000191, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 128
Epoch 128, Loss: 0.000000190, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 129
Epoch 129, Loss: 0.000000189, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 130
Epoch 130, Loss: 0.000000189, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 131
Epoch 131, Loss: 0.000000190, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 132
Epoch 132, Loss: 0.000000190, Improvement: 0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 133
Epoch 133, Loss: 0.000000189, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 134
Epoch 134, Loss: 0.000000189, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 135
Epoch 135, Loss: 0.000000190, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 136
Epoch 136, Loss: 0.000000188, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 137
Epoch 137, Loss: 0.000000189, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 138
Epoch 138, Loss: 0.000000187, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 139
Epoch 139, Loss: 0.000000187, Improvement: 0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 140
Epoch 140, Loss: 0.000000190, Improvement: 0.000000003, Best Loss: 0.000000103 in Epoch 126
Epoch 141
Epoch 141, Loss: 0.000000190, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 142
Epoch 142, Loss: 0.000000191, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 143
Epoch 143, Loss: 0.000000189, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 144
Epoch 144, Loss: 0.000000187, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 145
Epoch 145, Loss: 0.000000187, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 146
Epoch 146, Loss: 0.000000188, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 147
Epoch 147, Loss: 0.000000187, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 148
Epoch 148, Loss: 0.000000187, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 149
Epoch 149, Loss: 0.000000189, Improvement: 0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.000000190, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 151
Epoch 151, Loss: 0.000000195, Improvement: 0.000000005, Best Loss: 0.000000103 in Epoch 126
Epoch 152
Epoch 152, Loss: 0.000000220, Improvement: 0.000000025, Best Loss: 0.000000103 in Epoch 126
Epoch 153
Epoch 153, Loss: 0.000000206, Improvement: -0.000000014, Best Loss: 0.000000103 in Epoch 126
Epoch 154
Epoch 154, Loss: 0.000000191, Improvement: -0.000000015, Best Loss: 0.000000103 in Epoch 126
Epoch 155
Epoch 155, Loss: 0.000000186, Improvement: -0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 156
Epoch 156, Loss: 0.000000190, Improvement: 0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 157
Epoch 157, Loss: 0.000000191, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 158
Epoch 158, Loss: 0.000000187, Improvement: -0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 159
Epoch 159, Loss: 0.000000186, Improvement: -0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 160
Epoch 160, Loss: 0.000000190, Improvement: 0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 161
Epoch 161, Loss: 0.000000191, Improvement: 0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 162
Epoch 162, Loss: 0.000000189, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 163
Epoch 163, Loss: 0.000000189, Improvement: -0.000000000, Best Loss: 0.000000103 in Epoch 126
Epoch 164
Epoch 164, Loss: 0.000000187, Improvement: -0.000000002, Best Loss: 0.000000103 in Epoch 126
Epoch 165
Epoch 165, Loss: 0.000000188, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 166
Epoch 166, Loss: 0.000000197, Improvement: 0.000000009, Best Loss: 0.000000103 in Epoch 126
Epoch 167
Epoch 167, Loss: 0.000000284, Improvement: 0.000000086, Best Loss: 0.000000103 in Epoch 126
Epoch 168
Epoch 168, Loss: 0.000000489, Improvement: 0.000000205, Best Loss: 0.000000103 in Epoch 126
Epoch 169
Epoch 169, Loss: 0.000000350, Improvement: -0.000000139, Best Loss: 0.000000103 in Epoch 126
Epoch 170
Epoch 170, Loss: 0.000000344, Improvement: -0.000000006, Best Loss: 0.000000103 in Epoch 126
Epoch 171
Epoch 171, Loss: 0.000000240, Improvement: -0.000000103, Best Loss: 0.000000103 in Epoch 126
Epoch 172
Epoch 172, Loss: 0.000000192, Improvement: -0.000000049, Best Loss: 0.000000103 in Epoch 126
Epoch 173
Epoch 173, Loss: 0.000000199, Improvement: 0.000000008, Best Loss: 0.000000103 in Epoch 126
Epoch 174
Epoch 174, Loss: 0.000000192, Improvement: -0.000000008, Best Loss: 0.000000103 in Epoch 126
Epoch 175
Epoch 175, Loss: 0.000000184, Improvement: -0.000000007, Best Loss: 0.000000103 in Epoch 126
Epoch 176
Epoch 176, Loss: 0.000000186, Improvement: 0.000000001, Best Loss: 0.000000103 in Epoch 126
Epoch 177
Epoch 177, Loss: 0.000000202, Improvement: 0.000000017, Best Loss: 0.000000103 in Epoch 126
Epoch 178
Epoch 178, Loss: 0.000000190, Improvement: -0.000000013, Best Loss: 0.000000103 in Epoch 126
Epoch 179
Epoch 179, Loss: 0.000000203, Improvement: 0.000000014, Best Loss: 0.000000103 in Epoch 126
Epoch 180
Epoch 180, Loss: 0.000000207, Improvement: 0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 181
Epoch 181, Loss: 0.000000409, Improvement: 0.000000202, Best Loss: 0.000000103 in Epoch 126
Epoch 182
Epoch 182, Loss: 0.000000479, Improvement: 0.000000070, Best Loss: 0.000000103 in Epoch 126
Epoch 183
Epoch 183, Loss: 0.000000371, Improvement: -0.000000109, Best Loss: 0.000000103 in Epoch 126
Epoch 184
Epoch 184, Loss: 0.000000216, Improvement: -0.000000154, Best Loss: 0.000000103 in Epoch 126
Epoch 185
Epoch 185, Loss: 0.000000192, Improvement: -0.000000024, Best Loss: 0.000000103 in Epoch 126
Epoch 186
Epoch 186, Loss: 0.000000187, Improvement: -0.000000004, Best Loss: 0.000000103 in Epoch 126
Epoch 187
Epoch 187, Loss: 0.000000525, Improvement: 0.000000337, Best Loss: 0.000000103 in Epoch 126
Epoch 188
Epoch 188, Loss: 0.000000280, Improvement: -0.000000244, Best Loss: 0.000000103 in Epoch 126
Epoch 189
Epoch 189, Loss: 0.000000192, Improvement: -0.000000088, Best Loss: 0.000000103 in Epoch 126
Epoch 190
Epoch 190, Loss: 0.000000206, Improvement: 0.000000013, Best Loss: 0.000000103 in Epoch 126
Epoch 191
Epoch 191, Loss: 0.000000202, Improvement: -0.000000003, Best Loss: 0.000000103 in Epoch 126
Epoch 192
Epoch 192, Loss: 0.000000189, Improvement: -0.000000014, Best Loss: 0.000000103 in Epoch 126
Epoch 193
Epoch 193, Loss: 0.000000195, Improvement: 0.000000006, Best Loss: 0.000000103 in Epoch 126
Epoch 194
Epoch 194, Loss: 0.000000412, Improvement: 0.000000217, Best Loss: 0.000000103 in Epoch 126
Epoch 195
Epoch 195, Loss: 0.000001249, Improvement: 0.000000837, Best Loss: 0.000000103 in Epoch 126
Epoch 196
Epoch 196, Loss: 0.000000388, Improvement: -0.000000861, Best Loss: 0.000000103 in Epoch 126
Epoch 197
Epoch 197, Loss: 0.000000290, Improvement: -0.000000098, Best Loss: 0.000000103 in Epoch 126
Epoch 198
Epoch 198, Loss: 0.000000217, Improvement: -0.000000073, Best Loss: 0.000000103 in Epoch 126
Epoch 199
A best model at epoch 199 has been saved with training error 0.000000098.
Epoch 199, Loss: 0.000000187, Improvement: -0.000000030, Best Loss: 0.000000098 in Epoch 199
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.000000183, Improvement: -0.000000004, Best Loss: 0.000000098 in Epoch 199
Epoch 201
Epoch 201, Loss: 0.000000183, Improvement: 0.000000000, Best Loss: 0.000000098 in Epoch 199
Epoch 202
Epoch 202, Loss: 0.000000182, Improvement: -0.000000001, Best Loss: 0.000000098 in Epoch 199
Epoch 203
A best model at epoch 203 has been saved with training error 0.000000096.
Epoch 203, Loss: 0.000000183, Improvement: 0.000000001, Best Loss: 0.000000096 in Epoch 203
Epoch 204
Epoch 204, Loss: 0.000000183, Improvement: 0.000000000, Best Loss: 0.000000096 in Epoch 203
Epoch 205
A best model at epoch 205 has been saved with training error 0.000000090.
Epoch 205, Loss: 0.000000183, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 206
Epoch 206, Loss: 0.000000183, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 207
Epoch 207, Loss: 0.000000183, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 208
Epoch 208, Loss: 0.000000184, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 209
Epoch 209, Loss: 0.000000184, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 210
Epoch 210, Loss: 0.000000198, Improvement: 0.000000014, Best Loss: 0.000000090 in Epoch 205
Epoch 211
Epoch 211, Loss: 0.000000393, Improvement: 0.000000196, Best Loss: 0.000000090 in Epoch 205
Epoch 212
Epoch 212, Loss: 0.000001832, Improvement: 0.000001439, Best Loss: 0.000000090 in Epoch 205
Epoch 213
Epoch 213, Loss: 0.000000499, Improvement: -0.000001333, Best Loss: 0.000000090 in Epoch 205
Epoch 214
Epoch 214, Loss: 0.000000269, Improvement: -0.000000230, Best Loss: 0.000000090 in Epoch 205
Epoch 215
Epoch 215, Loss: 0.000000289, Improvement: 0.000000020, Best Loss: 0.000000090 in Epoch 205
Epoch 216
Epoch 216, Loss: 0.000000245, Improvement: -0.000000044, Best Loss: 0.000000090 in Epoch 205
Epoch 217
Epoch 217, Loss: 0.000000211, Improvement: -0.000000034, Best Loss: 0.000000090 in Epoch 205
Epoch 218
Epoch 218, Loss: 0.000000187, Improvement: -0.000000023, Best Loss: 0.000000090 in Epoch 205
Epoch 219
Epoch 219, Loss: 0.000000184, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 220
Epoch 220, Loss: 0.000000185, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 221
Epoch 221, Loss: 0.000000182, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 222
Epoch 222, Loss: 0.000000183, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 223
Epoch 223, Loss: 0.000000201, Improvement: 0.000000018, Best Loss: 0.000000090 in Epoch 205
Epoch 224
Epoch 224, Loss: 0.000001517, Improvement: 0.000001315, Best Loss: 0.000000090 in Epoch 205
Epoch 225
Epoch 225, Loss: 0.000000665, Improvement: -0.000000851, Best Loss: 0.000000090 in Epoch 205
Epoch 226
Epoch 226, Loss: 0.000000265, Improvement: -0.000000401, Best Loss: 0.000000090 in Epoch 205
Epoch 227
Epoch 227, Loss: 0.000000206, Improvement: -0.000000059, Best Loss: 0.000000090 in Epoch 205
Epoch 228
Epoch 228, Loss: 0.000000187, Improvement: -0.000000019, Best Loss: 0.000000090 in Epoch 205
Epoch 229
Epoch 229, Loss: 0.000000184, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 230
Epoch 230, Loss: 0.000000181, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 231
Epoch 231, Loss: 0.000000180, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 232
Epoch 232, Loss: 0.000000180, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 233
Epoch 233, Loss: 0.000000181, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 234
Epoch 234, Loss: 0.000000183, Improvement: 0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 235
Epoch 235, Loss: 0.000000182, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 236
Epoch 236, Loss: 0.000000182, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 237
Epoch 237, Loss: 0.000000185, Improvement: 0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 238
Epoch 238, Loss: 0.000000935, Improvement: 0.000000750, Best Loss: 0.000000090 in Epoch 205
Epoch 239
Epoch 239, Loss: 0.000000284, Improvement: -0.000000651, Best Loss: 0.000000090 in Epoch 205
Epoch 240
Epoch 240, Loss: 0.000000230, Improvement: -0.000000053, Best Loss: 0.000000090 in Epoch 205
Epoch 241
Epoch 241, Loss: 0.000000188, Improvement: -0.000000042, Best Loss: 0.000000090 in Epoch 205
Epoch 242
Epoch 242, Loss: 0.000000183, Improvement: -0.000000005, Best Loss: 0.000000090 in Epoch 205
Epoch 243
Epoch 243, Loss: 0.000000181, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 244
Epoch 244, Loss: 0.000000187, Improvement: 0.000000006, Best Loss: 0.000000090 in Epoch 205
Epoch 245
Epoch 245, Loss: 0.000000182, Improvement: -0.000000005, Best Loss: 0.000000090 in Epoch 205
Epoch 246
Epoch 246, Loss: 0.000000478, Improvement: 0.000000297, Best Loss: 0.000000090 in Epoch 205
Epoch 247
Epoch 247, Loss: 0.000001498, Improvement: 0.000001019, Best Loss: 0.000000090 in Epoch 205
Epoch 248
Epoch 248, Loss: 0.000000313, Improvement: -0.000001184, Best Loss: 0.000000090 in Epoch 205
Epoch 249
Epoch 249, Loss: 0.000000363, Improvement: 0.000000050, Best Loss: 0.000000090 in Epoch 205
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.000000192, Improvement: -0.000000170, Best Loss: 0.000000090 in Epoch 205
Epoch 251
Epoch 251, Loss: 0.000000189, Improvement: -0.000000004, Best Loss: 0.000000090 in Epoch 205
Epoch 252
Epoch 252, Loss: 0.000000183, Improvement: -0.000000006, Best Loss: 0.000000090 in Epoch 205
Epoch 253
Epoch 253, Loss: 0.000000181, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 254
Epoch 254, Loss: 0.000000181, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 255
Epoch 255, Loss: 0.000000184, Improvement: 0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 256
Epoch 256, Loss: 0.000000194, Improvement: 0.000000010, Best Loss: 0.000000090 in Epoch 205
Epoch 257
Epoch 257, Loss: 0.000000201, Improvement: 0.000000006, Best Loss: 0.000000090 in Epoch 205
Epoch 258
Epoch 258, Loss: 0.000000189, Improvement: -0.000000012, Best Loss: 0.000000090 in Epoch 205
Epoch 259
Epoch 259, Loss: 0.000001430, Improvement: 0.000001241, Best Loss: 0.000000090 in Epoch 205
Epoch 260
Epoch 260, Loss: 0.000000586, Improvement: -0.000000844, Best Loss: 0.000000090 in Epoch 205
Epoch 261
Epoch 261, Loss: 0.000000247, Improvement: -0.000000339, Best Loss: 0.000000090 in Epoch 205
Epoch 262
Epoch 262, Loss: 0.000000186, Improvement: -0.000000061, Best Loss: 0.000000090 in Epoch 205
Epoch 263
Epoch 263, Loss: 0.000000181, Improvement: -0.000000005, Best Loss: 0.000000090 in Epoch 205
Epoch 264
Epoch 264, Loss: 0.000000180, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 265
Epoch 265, Loss: 0.000000179, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 266
Epoch 266, Loss: 0.000000178, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 267
Epoch 267, Loss: 0.000000178, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 268
Epoch 268, Loss: 0.000000180, Improvement: 0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 269
Epoch 269, Loss: 0.000000178, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 270
Epoch 270, Loss: 0.000000200, Improvement: 0.000000022, Best Loss: 0.000000090 in Epoch 205
Epoch 271
Epoch 271, Loss: 0.000000406, Improvement: 0.000000206, Best Loss: 0.000000090 in Epoch 205
Epoch 272
Epoch 272, Loss: 0.000001427, Improvement: 0.000001020, Best Loss: 0.000000090 in Epoch 205
Epoch 273
Epoch 273, Loss: 0.000000290, Improvement: -0.000001137, Best Loss: 0.000000090 in Epoch 205
Epoch 274
Epoch 274, Loss: 0.000000230, Improvement: -0.000000059, Best Loss: 0.000000090 in Epoch 205
Epoch 275
Epoch 275, Loss: 0.000000243, Improvement: 0.000000013, Best Loss: 0.000000090 in Epoch 205
Epoch 276
Epoch 276, Loss: 0.000000217, Improvement: -0.000000026, Best Loss: 0.000000090 in Epoch 205
Epoch 277
Epoch 277, Loss: 0.000000182, Improvement: -0.000000035, Best Loss: 0.000000090 in Epoch 205
Epoch 278
Epoch 278, Loss: 0.000000178, Improvement: -0.000000004, Best Loss: 0.000000090 in Epoch 205
Epoch 279
Epoch 279, Loss: 0.000000177, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 280
Epoch 280, Loss: 0.000000177, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 281
Epoch 281, Loss: 0.000000179, Improvement: 0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 282
Epoch 282, Loss: 0.000000181, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 283
Epoch 283, Loss: 0.000000181, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 284
Epoch 284, Loss: 0.000000186, Improvement: 0.000000004, Best Loss: 0.000000090 in Epoch 205
Epoch 285
Epoch 285, Loss: 0.000000515, Improvement: 0.000000329, Best Loss: 0.000000090 in Epoch 205
Epoch 286
Epoch 286, Loss: 0.000000775, Improvement: 0.000000260, Best Loss: 0.000000090 in Epoch 205
Epoch 287
Epoch 287, Loss: 0.000000332, Improvement: -0.000000443, Best Loss: 0.000000090 in Epoch 205
Epoch 288
Epoch 288, Loss: 0.000000193, Improvement: -0.000000139, Best Loss: 0.000000090 in Epoch 205
Epoch 289
Epoch 289, Loss: 0.000000180, Improvement: -0.000000013, Best Loss: 0.000000090 in Epoch 205
Epoch 290
Epoch 290, Loss: 0.000000178, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 291
Epoch 291, Loss: 0.000000176, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 292
Epoch 292, Loss: 0.000000178, Improvement: 0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 293
Epoch 293, Loss: 0.000000184, Improvement: 0.000000006, Best Loss: 0.000000090 in Epoch 205
Epoch 294
Epoch 294, Loss: 0.000000181, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 295
Epoch 295, Loss: 0.000000181, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 296
Epoch 296, Loss: 0.000000178, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 297
Epoch 297, Loss: 0.000000176, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 298
Epoch 298, Loss: 0.000000179, Improvement: 0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 299
Epoch 299, Loss: 0.000001298, Improvement: 0.000001119, Best Loss: 0.000000090 in Epoch 205
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.000000647, Improvement: -0.000000651, Best Loss: 0.000000090 in Epoch 205
Epoch 301
Epoch 301, Loss: 0.000000283, Improvement: -0.000000364, Best Loss: 0.000000090 in Epoch 205
Epoch 302
Epoch 302, Loss: 0.000000232, Improvement: -0.000000051, Best Loss: 0.000000090 in Epoch 205
Epoch 303
Epoch 303, Loss: 0.000000196, Improvement: -0.000000036, Best Loss: 0.000000090 in Epoch 205
Epoch 304
Epoch 304, Loss: 0.000000178, Improvement: -0.000000018, Best Loss: 0.000000090 in Epoch 205
Epoch 305
Epoch 305, Loss: 0.000000175, Improvement: -0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 306
Epoch 306, Loss: 0.000000174, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 307
Epoch 307, Loss: 0.000000175, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 308
Epoch 308, Loss: 0.000000175, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 309
Epoch 309, Loss: 0.000000175, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 310
Epoch 310, Loss: 0.000000175, Improvement: 0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 311
Epoch 311, Loss: 0.000000180, Improvement: 0.000000005, Best Loss: 0.000000090 in Epoch 205
Epoch 312
Epoch 312, Loss: 0.000000178, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 313
Epoch 313, Loss: 0.000000224, Improvement: 0.000000046, Best Loss: 0.000000090 in Epoch 205
Epoch 314
Epoch 314, Loss: 0.000001231, Improvement: 0.000001007, Best Loss: 0.000000090 in Epoch 205
Epoch 315
Epoch 315, Loss: 0.000000415, Improvement: -0.000000816, Best Loss: 0.000000090 in Epoch 205
Epoch 316
Epoch 316, Loss: 0.000000202, Improvement: -0.000000213, Best Loss: 0.000000090 in Epoch 205
Epoch 317
Epoch 317, Loss: 0.000000188, Improvement: -0.000000014, Best Loss: 0.000000090 in Epoch 205
Epoch 318
Epoch 318, Loss: 0.000000178, Improvement: -0.000000010, Best Loss: 0.000000090 in Epoch 205
Epoch 319
Epoch 319, Loss: 0.000000177, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 320
Epoch 320, Loss: 0.000000175, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 321
Epoch 321, Loss: 0.000000175, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 322
Epoch 322, Loss: 0.000000174, Improvement: -0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 323
Epoch 323, Loss: 0.000000175, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 324
Epoch 324, Loss: 0.000000174, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 325
Epoch 325, Loss: 0.000000176, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 326
Epoch 326, Loss: 0.000000173, Improvement: -0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 327
Epoch 327, Loss: 0.000000174, Improvement: 0.000000001, Best Loss: 0.000000090 in Epoch 205
Epoch 328
Epoch 328, Loss: 0.000000174, Improvement: -0.000000000, Best Loss: 0.000000090 in Epoch 205
Epoch 329
Epoch 329, Loss: 0.000000179, Improvement: 0.000000005, Best Loss: 0.000000090 in Epoch 205
Epoch 330
Epoch 330, Loss: 0.000000888, Improvement: 0.000000709, Best Loss: 0.000000090 in Epoch 205
Epoch 331
Epoch 331, Loss: 0.000000470, Improvement: -0.000000418, Best Loss: 0.000000090 in Epoch 205
Epoch 332
Epoch 332, Loss: 0.000000187, Improvement: -0.000000283, Best Loss: 0.000000090 in Epoch 205
Epoch 333
Epoch 333, Loss: 0.000000181, Improvement: -0.000000006, Best Loss: 0.000000090 in Epoch 205
Epoch 334
Epoch 334, Loss: 0.000000177, Improvement: -0.000000004, Best Loss: 0.000000090 in Epoch 205
Epoch 335
Epoch 335, Loss: 0.000000180, Improvement: 0.000000003, Best Loss: 0.000000090 in Epoch 205
Epoch 336
Epoch 336, Loss: 0.000000181, Improvement: 0.000000002, Best Loss: 0.000000090 in Epoch 205
Epoch 337
slurmstepd: error: *** JOB 8035771 ON a100-03 CANCELLED AT 2024-11-19T19:06:54 ***
