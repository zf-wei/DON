The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00006547976955.
A best model at epoch 1 has been saved with training error 0.00003559409015.
Epoch 1, Loss: 0.00115825169214, Improvement: 0.00115825169214, Best Loss: 0.00003559409015 in Epoch 1
Epoch 2
Epoch 2, Loss: 0.00009153584106, Improvement: -0.00106671585108, Best Loss: 0.00003559409015 in Epoch 1
Epoch 3
Epoch 3, Loss: 0.00005933562061, Improvement: -0.00003220022045, Best Loss: 0.00003559409015 in Epoch 1
Epoch 4
A best model at epoch 4 has been saved with training error 0.00003450986696.
A best model at epoch 4 has been saved with training error 0.00003231092705.
Epoch 4, Loss: 0.00005300984594, Improvement: -0.00000632577467, Best Loss: 0.00003231092705 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.00002769592356.
Epoch 5, Loss: 0.00005216217623, Improvement: -0.00000084766971, Best Loss: 0.00002769592356 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.00005208181647, Improvement: -0.00000008035977, Best Loss: 0.00002769592356 in Epoch 5
Epoch 7
Epoch 7, Loss: 0.00005205444377, Improvement: -0.00000002737270, Best Loss: 0.00002769592356 in Epoch 5
Epoch 8
Epoch 8, Loss: 0.00005204575009, Improvement: -0.00000000869368, Best Loss: 0.00002769592356 in Epoch 5
Epoch 9
Epoch 9, Loss: 0.00005201059776, Improvement: -0.00000003515233, Best Loss: 0.00002769592356 in Epoch 5
Epoch 10
Epoch 10, Loss: 0.00005199325042, Improvement: -0.00000001734734, Best Loss: 0.00002769592356 in Epoch 5
Epoch 11
Epoch 11, Loss: 0.00005197334249, Improvement: -0.00000001990793, Best Loss: 0.00002769592356 in Epoch 5
Epoch 12
Epoch 12, Loss: 0.00005195436315, Improvement: -0.00000001897934, Best Loss: 0.00002769592356 in Epoch 5
Epoch 13
Epoch 13, Loss: 0.00005193524757, Improvement: -0.00000001911558, Best Loss: 0.00002769592356 in Epoch 5
Epoch 14
Epoch 14, Loss: 0.00005191631062, Improvement: -0.00000001893695, Best Loss: 0.00002769592356 in Epoch 5
Epoch 15
Epoch 15, Loss: 0.00005188987398, Improvement: -0.00000002643665, Best Loss: 0.00002769592356 in Epoch 5
Epoch 16
Epoch 16, Loss: 0.00005186337530, Improvement: -0.00000002649867, Best Loss: 0.00002769592356 in Epoch 5
Epoch 17
Epoch 17, Loss: 0.00005184190086, Improvement: -0.00000002147444, Best Loss: 0.00002769592356 in Epoch 5
Epoch 18
Epoch 18, Loss: 0.00005181904125, Improvement: -0.00000002285960, Best Loss: 0.00002769592356 in Epoch 5
Epoch 19
A best model at epoch 19 has been saved with training error 0.00002322775799.
Epoch 19, Loss: 0.00005179955378, Improvement: -0.00000001948747, Best Loss: 0.00002322775799 in Epoch 19
Epoch 20
Epoch 20, Loss: 0.00005176011291, Improvement: -0.00000003944087, Best Loss: 0.00002322775799 in Epoch 19
Epoch 21
Epoch 21, Loss: 0.00005172593246, Improvement: -0.00000003418045, Best Loss: 0.00002322775799 in Epoch 19
Epoch 22
Epoch 22, Loss: 0.00005169548094, Improvement: -0.00000003045152, Best Loss: 0.00002322775799 in Epoch 19
Epoch 23
Epoch 23, Loss: 0.00005165966541, Improvement: -0.00000003581554, Best Loss: 0.00002322775799 in Epoch 19
Epoch 24
Epoch 24, Loss: 0.00005161457475, Improvement: -0.00000004509066, Best Loss: 0.00002322775799 in Epoch 19
Epoch 25
Epoch 25, Loss: 0.00005157341548, Improvement: -0.00000004115927, Best Loss: 0.00002322775799 in Epoch 19
Epoch 26
Epoch 26, Loss: 0.00005151700207, Improvement: -0.00000005641341, Best Loss: 0.00002322775799 in Epoch 19
Epoch 27
Epoch 27, Loss: 0.00005147225547, Improvement: -0.00000004474659, Best Loss: 0.00002322775799 in Epoch 19
Epoch 28
Epoch 28, Loss: 0.00005140765988, Improvement: -0.00000006459559, Best Loss: 0.00002322775799 in Epoch 19
Epoch 29
Epoch 29, Loss: 0.00005133589466, Improvement: -0.00000007176523, Best Loss: 0.00002322775799 in Epoch 19
Epoch 30
Epoch 30, Loss: 0.00005127287714, Improvement: -0.00000006301752, Best Loss: 0.00002322775799 in Epoch 19
Epoch 31
Epoch 31, Loss: 0.00005118612880, Improvement: -0.00000008674833, Best Loss: 0.00002322775799 in Epoch 19
Epoch 32
Epoch 32, Loss: 0.00005106861008, Improvement: -0.00000011751872, Best Loss: 0.00002322775799 in Epoch 19
Epoch 33
Epoch 33, Loss: 0.00005096172245, Improvement: -0.00000010688764, Best Loss: 0.00002322775799 in Epoch 19
Epoch 34
Epoch 34, Loss: 0.00005082675034, Improvement: -0.00000013497211, Best Loss: 0.00002322775799 in Epoch 19
Epoch 35
Epoch 35, Loss: 0.00005067097100, Improvement: -0.00000015577934, Best Loss: 0.00002322775799 in Epoch 19
Epoch 36
Epoch 36, Loss: 0.00005049899419, Improvement: -0.00000017197681, Best Loss: 0.00002322775799 in Epoch 19
Epoch 37
Epoch 37, Loss: 0.00005028036503, Improvement: -0.00000021862916, Best Loss: 0.00002322775799 in Epoch 19
Epoch 38
Epoch 38, Loss: 0.00005001938089, Improvement: -0.00000026098414, Best Loss: 0.00002322775799 in Epoch 19
Epoch 39
Epoch 39, Loss: 0.00004970735263, Improvement: -0.00000031202826, Best Loss: 0.00002322775799 in Epoch 19
Epoch 40
Epoch 40, Loss: 0.00004931517451, Improvement: -0.00000039217812, Best Loss: 0.00002322775799 in Epoch 19
Epoch 41
Epoch 41, Loss: 0.00004883278334, Improvement: -0.00000048239117, Best Loss: 0.00002322775799 in Epoch 19
Epoch 42
Epoch 42, Loss: 0.00004822107676, Improvement: -0.00000061170658, Best Loss: 0.00002322775799 in Epoch 19
Epoch 43
Epoch 43, Loss: 0.00004740929126, Improvement: -0.00000081178550, Best Loss: 0.00002322775799 in Epoch 19
Epoch 44
Epoch 44, Loss: 0.00004639709714, Improvement: -0.00000101219412, Best Loss: 0.00002322775799 in Epoch 19
Epoch 45
Epoch 45, Loss: 0.00004510378676, Improvement: -0.00000129331038, Best Loss: 0.00002322775799 in Epoch 19
Epoch 46
Epoch 46, Loss: 0.00004345752895, Improvement: -0.00000164625781, Best Loss: 0.00002322775799 in Epoch 19
Epoch 47
A best model at epoch 47 has been saved with training error 0.00002205064266.
Epoch 47, Loss: 0.00004159250157, Improvement: -0.00000186502739, Best Loss: 0.00002205064266 in Epoch 47
Epoch 48
Epoch 48, Loss: 0.00003961497951, Improvement: -0.00000197752206, Best Loss: 0.00002205064266 in Epoch 47
Epoch 49
Epoch 49, Loss: 0.00003811587067, Improvement: -0.00000149910884, Best Loss: 0.00002205064266 in Epoch 47
Epoch 50
A best model at epoch 50 has been saved with training error 0.00001901247742.
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00003743535599, Improvement: -0.00000068051468, Best Loss: 0.00001901247742 in Epoch 50
Epoch 51
Epoch 51, Loss: 0.00003725418719, Improvement: -0.00000018116880, Best Loss: 0.00001901247742 in Epoch 50
Epoch 52
A best model at epoch 52 has been saved with training error 0.00001793794945.
Epoch 52, Loss: 0.00003719611987, Improvement: -0.00000005806733, Best Loss: 0.00001793794945 in Epoch 52
Epoch 53
Epoch 53, Loss: 0.00003715187968, Improvement: -0.00000004424019, Best Loss: 0.00001793794945 in Epoch 52
Epoch 54
Epoch 54, Loss: 0.00003712882390, Improvement: -0.00000002305578, Best Loss: 0.00001793794945 in Epoch 52
Epoch 55
Epoch 55, Loss: 0.00003711296185, Improvement: -0.00000001586204, Best Loss: 0.00001793794945 in Epoch 52
Epoch 56
Epoch 56, Loss: 0.00003710310639, Improvement: -0.00000000985547, Best Loss: 0.00001793794945 in Epoch 52
Epoch 57
Epoch 57, Loss: 0.00003709691255, Improvement: -0.00000000619384, Best Loss: 0.00001793794945 in Epoch 52
Epoch 58
Epoch 58, Loss: 0.00003709382290, Improvement: -0.00000000308964, Best Loss: 0.00001793794945 in Epoch 52
Epoch 59
Epoch 59, Loss: 0.00003710261699, Improvement: 0.00000000879409, Best Loss: 0.00001793794945 in Epoch 52
Epoch 60
Epoch 60, Loss: 0.00003708703889, Improvement: -0.00000001557810, Best Loss: 0.00001793794945 in Epoch 52
Epoch 61
Epoch 61, Loss: 0.00003708568483, Improvement: -0.00000000135406, Best Loss: 0.00001793794945 in Epoch 52
Epoch 62
Epoch 62, Loss: 0.00003708317245, Improvement: -0.00000000251239, Best Loss: 0.00001793794945 in Epoch 52
Epoch 63
Epoch 63, Loss: 0.00003708150525, Improvement: -0.00000000166719, Best Loss: 0.00001793794945 in Epoch 52
Epoch 64
Epoch 64, Loss: 0.00003707960832, Improvement: -0.00000000189693, Best Loss: 0.00001793794945 in Epoch 52
Epoch 65
Epoch 65, Loss: 0.00003707675778, Improvement: -0.00000000285054, Best Loss: 0.00001793794945 in Epoch 52
Epoch 66
Epoch 66, Loss: 0.00003707450796, Improvement: -0.00000000224982, Best Loss: 0.00001793794945 in Epoch 52
Epoch 67
Epoch 67, Loss: 0.00003707471888, Improvement: 0.00000000021091, Best Loss: 0.00001793794945 in Epoch 52
Epoch 68
Epoch 68, Loss: 0.00003707458418, Improvement: -0.00000000013470, Best Loss: 0.00001793794945 in Epoch 52
Epoch 69
Epoch 69, Loss: 0.00003707675414, Improvement: 0.00000000216996, Best Loss: 0.00001793794945 in Epoch 52
Epoch 70
Epoch 70, Loss: 0.00003706660500, Improvement: -0.00000001014914, Best Loss: 0.00001793794945 in Epoch 52
Epoch 71
Epoch 71, Loss: 0.00003706938060, Improvement: 0.00000000277560, Best Loss: 0.00001793794945 in Epoch 52
Epoch 72
Epoch 72, Loss: 0.00003706423977, Improvement: -0.00000000514083, Best Loss: 0.00001793794945 in Epoch 52
Epoch 73
Epoch 73, Loss: 0.00003706792413, Improvement: 0.00000000368436, Best Loss: 0.00001793794945 in Epoch 52
Epoch 74
Epoch 74, Loss: 0.00003706584293, Improvement: -0.00000000208120, Best Loss: 0.00001793794945 in Epoch 52
Epoch 75
Epoch 75, Loss: 0.00003705910340, Improvement: -0.00000000673954, Best Loss: 0.00001793794945 in Epoch 52
Epoch 76
Epoch 76, Loss: 0.00003706094885, Improvement: 0.00000000184546, Best Loss: 0.00001793794945 in Epoch 52
Epoch 77
Epoch 77, Loss: 0.00003705320487, Improvement: -0.00000000774398, Best Loss: 0.00001793794945 in Epoch 52
Epoch 78
Epoch 78, Loss: 0.00003705238760, Improvement: -0.00000000081727, Best Loss: 0.00001793794945 in Epoch 52
Epoch 79
Epoch 79, Loss: 0.00003704916808, Improvement: -0.00000000321952, Best Loss: 0.00001793794945 in Epoch 52
Epoch 80
Epoch 80, Loss: 0.00003704966794, Improvement: 0.00000000049986, Best Loss: 0.00001793794945 in Epoch 52
Epoch 81
A best model at epoch 81 has been saved with training error 0.00001758327380.
Epoch 81, Loss: 0.00003704707233, Improvement: -0.00000000259561, Best Loss: 0.00001758327380 in Epoch 81
Epoch 82
Epoch 82, Loss: 0.00003704096389, Improvement: -0.00000000610844, Best Loss: 0.00001758327380 in Epoch 81
Epoch 83
Epoch 83, Loss: 0.00003703835864, Improvement: -0.00000000260525, Best Loss: 0.00001758327380 in Epoch 81
Epoch 84
Epoch 84, Loss: 0.00003703643879, Improvement: -0.00000000191985, Best Loss: 0.00001758327380 in Epoch 81
Epoch 85
Epoch 85, Loss: 0.00003703458524, Improvement: -0.00000000185355, Best Loss: 0.00001758327380 in Epoch 81
Epoch 86
A best model at epoch 86 has been saved with training error 0.00001738823812.
Epoch 86, Loss: 0.00003704405881, Improvement: 0.00000000947357, Best Loss: 0.00001738823812 in Epoch 86
Epoch 87
Epoch 87, Loss: 0.00003704007331, Improvement: -0.00000000398550, Best Loss: 0.00001738823812 in Epoch 86
Epoch 88
Epoch 88, Loss: 0.00003702431823, Improvement: -0.00000001575509, Best Loss: 0.00001738823812 in Epoch 86
Epoch 89
Epoch 89, Loss: 0.00003702552031, Improvement: 0.00000000120208, Best Loss: 0.00001738823812 in Epoch 86
Epoch 90
Epoch 90, Loss: 0.00003701848873, Improvement: -0.00000000703158, Best Loss: 0.00001738823812 in Epoch 86
Epoch 91
Epoch 91, Loss: 0.00003701703326, Improvement: -0.00000000145546, Best Loss: 0.00001738823812 in Epoch 86
Epoch 92
Epoch 92, Loss: 0.00003701560599, Improvement: -0.00000000142727, Best Loss: 0.00001738823812 in Epoch 86
Epoch 93
Epoch 93, Loss: 0.00003702146114, Improvement: 0.00000000585514, Best Loss: 0.00001738823812 in Epoch 86
Epoch 94
Epoch 94, Loss: 0.00003700983716, Improvement: -0.00000001162398, Best Loss: 0.00001738823812 in Epoch 86
Epoch 95
Epoch 95, Loss: 0.00003700437155, Improvement: -0.00000000546561, Best Loss: 0.00001738823812 in Epoch 86
Epoch 96
Epoch 96, Loss: 0.00003700005409, Improvement: -0.00000000431746, Best Loss: 0.00001738823812 in Epoch 86
Epoch 97
Epoch 97, Loss: 0.00003699864801, Improvement: -0.00000000140608, Best Loss: 0.00001738823812 in Epoch 86
Epoch 98
Epoch 98, Loss: 0.00003699457366, Improvement: -0.00000000407435, Best Loss: 0.00001738823812 in Epoch 86
Epoch 99
Epoch 99, Loss: 0.00003699354402, Improvement: -0.00000000102964, Best Loss: 0.00001738823812 in Epoch 86
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00003698429491, Improvement: -0.00000000924911, Best Loss: 0.00001738823812 in Epoch 86
Epoch 101
Epoch 101, Loss: 0.00003698544897, Improvement: 0.00000000115406, Best Loss: 0.00001738823812 in Epoch 86
Epoch 102
Epoch 102, Loss: 0.00003697638585, Improvement: -0.00000000906311, Best Loss: 0.00001738823812 in Epoch 86
Epoch 103
Epoch 103, Loss: 0.00003697003167, Improvement: -0.00000000635418, Best Loss: 0.00001738823812 in Epoch 86
Epoch 104
Epoch 104, Loss: 0.00003696723534, Improvement: -0.00000000279633, Best Loss: 0.00001738823812 in Epoch 86
Epoch 105
Epoch 105, Loss: 0.00003696571639, Improvement: -0.00000000151895, Best Loss: 0.00001738823812 in Epoch 86
Epoch 106
Epoch 106, Loss: 0.00003696362655, Improvement: -0.00000000208984, Best Loss: 0.00001738823812 in Epoch 86
Epoch 107
Epoch 107, Loss: 0.00003695806245, Improvement: -0.00000000556411, Best Loss: 0.00001738823812 in Epoch 86
Epoch 108
Epoch 108, Loss: 0.00003695286041, Improvement: -0.00000000520204, Best Loss: 0.00001738823812 in Epoch 86
Epoch 109
Epoch 109, Loss: 0.00003693694061, Improvement: -0.00000001591980, Best Loss: 0.00001738823812 in Epoch 86
Epoch 110
Epoch 110, Loss: 0.00003693339895, Improvement: -0.00000000354166, Best Loss: 0.00001738823812 in Epoch 86
Epoch 111
Epoch 111, Loss: 0.00003693021517, Improvement: -0.00000000318378, Best Loss: 0.00001738823812 in Epoch 86
Epoch 112
Epoch 112, Loss: 0.00003692103082, Improvement: -0.00000000918435, Best Loss: 0.00001738823812 in Epoch 86
Epoch 113
Epoch 113, Loss: 0.00003690998556, Improvement: -0.00000001104527, Best Loss: 0.00001738823812 in Epoch 86
Epoch 114
Epoch 114, Loss: 0.00003690091917, Improvement: -0.00000000906639, Best Loss: 0.00001738823812 in Epoch 86
Epoch 115
Epoch 115, Loss: 0.00003689075083, Improvement: -0.00000001016833, Best Loss: 0.00001738823812 in Epoch 86
Epoch 116
Epoch 116, Loss: 0.00003689057157, Improvement: -0.00000000017926, Best Loss: 0.00001738823812 in Epoch 86
Epoch 117
Epoch 117, Loss: 0.00003687815370, Improvement: -0.00000001241788, Best Loss: 0.00001738823812 in Epoch 86
Epoch 118
Epoch 118, Loss: 0.00003686989266, Improvement: -0.00000000826103, Best Loss: 0.00001738823812 in Epoch 86
Epoch 119
Epoch 119, Loss: 0.00003685778820, Improvement: -0.00000001210446, Best Loss: 0.00001738823812 in Epoch 86
Epoch 120
Epoch 120, Loss: 0.00003684676803, Improvement: -0.00000001102017, Best Loss: 0.00001738823812 in Epoch 86
Epoch 121
Epoch 121, Loss: 0.00003683002087, Improvement: -0.00000001674716, Best Loss: 0.00001738823812 in Epoch 86
Epoch 122
Epoch 122, Loss: 0.00003681291419, Improvement: -0.00000001710669, Best Loss: 0.00001738823812 in Epoch 86
Epoch 123
Epoch 123, Loss: 0.00003679990741, Improvement: -0.00000001300677, Best Loss: 0.00001738823812 in Epoch 86
Epoch 124
Epoch 124, Loss: 0.00003680668206, Improvement: 0.00000000677464, Best Loss: 0.00001738823812 in Epoch 86
Epoch 125
Epoch 125, Loss: 0.00003677283139, Improvement: -0.00000003385067, Best Loss: 0.00001738823812 in Epoch 86
Epoch 126
Epoch 126, Loss: 0.00003676188762, Improvement: -0.00000001094377, Best Loss: 0.00001738823812 in Epoch 86
Epoch 127
Epoch 127, Loss: 0.00003672926232, Improvement: -0.00000003262530, Best Loss: 0.00001738823812 in Epoch 86
Epoch 128
Epoch 128, Loss: 0.00003670920059, Improvement: -0.00000002006173, Best Loss: 0.00001738823812 in Epoch 86
Epoch 129
Epoch 129, Loss: 0.00003670342585, Improvement: -0.00000000577475, Best Loss: 0.00001738823812 in Epoch 86
Epoch 130
Epoch 130, Loss: 0.00003664991254, Improvement: -0.00000005351330, Best Loss: 0.00001738823812 in Epoch 86
Epoch 131
Epoch 131, Loss: 0.00003663414973, Improvement: -0.00000001576282, Best Loss: 0.00001738823812 in Epoch 86
Epoch 132
Epoch 132, Loss: 0.00003659584136, Improvement: -0.00000003830837, Best Loss: 0.00001738823812 in Epoch 86
Epoch 133
Epoch 133, Loss: 0.00003654657030, Improvement: -0.00000004927106, Best Loss: 0.00001738823812 in Epoch 86
Epoch 134
Epoch 134, Loss: 0.00003651205607, Improvement: -0.00000003451423, Best Loss: 0.00001738823812 in Epoch 86
Epoch 135
Epoch 135, Loss: 0.00003645435572, Improvement: -0.00000005770034, Best Loss: 0.00001738823812 in Epoch 86
Epoch 136
Epoch 136, Loss: 0.00003641880712, Improvement: -0.00000003554860, Best Loss: 0.00001738823812 in Epoch 86
Epoch 137
Epoch 137, Loss: 0.00003634998147, Improvement: -0.00000006882565, Best Loss: 0.00001738823812 in Epoch 86
Epoch 138
Epoch 138, Loss: 0.00003631012896, Improvement: -0.00000003985251, Best Loss: 0.00001738823812 in Epoch 86
Epoch 139
Epoch 139, Loss: 0.00003619605277, Improvement: -0.00000011407619, Best Loss: 0.00001738823812 in Epoch 86
Epoch 140
Epoch 140, Loss: 0.00003609758214, Improvement: -0.00000009847063, Best Loss: 0.00001738823812 in Epoch 86
Epoch 141
Epoch 141, Loss: 0.00003596569150, Improvement: -0.00000013189065, Best Loss: 0.00001738823812 in Epoch 86
Epoch 142
Epoch 142, Loss: 0.00003582066047, Improvement: -0.00000014503103, Best Loss: 0.00001738823812 in Epoch 86
Epoch 143
Epoch 143, Loss: 0.00003563111231, Improvement: -0.00000018954815, Best Loss: 0.00001738823812 in Epoch 86
Epoch 144
Epoch 144, Loss: 0.00003535290571, Improvement: -0.00000027820661, Best Loss: 0.00001738823812 in Epoch 86
Epoch 145
Epoch 145, Loss: 0.00003505648338, Improvement: -0.00000029642233, Best Loss: 0.00001738823812 in Epoch 86
Epoch 146
A best model at epoch 146 has been saved with training error 0.00001724047252.
Epoch 146, Loss: 0.00003464611955, Improvement: -0.00000041036383, Best Loss: 0.00001724047252 in Epoch 146
Epoch 147
Epoch 147, Loss: 0.00003407190543, Improvement: -0.00000057421412, Best Loss: 0.00001724047252 in Epoch 146
Epoch 148
Epoch 148, Loss: 0.00003318845838, Improvement: -0.00000088344705, Best Loss: 0.00001724047252 in Epoch 146
Epoch 149
Epoch 149, Loss: 0.00003194979563, Improvement: -0.00000123866275, Best Loss: 0.00001724047252 in Epoch 146
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00002992213076, Improvement: -0.00000202766487, Best Loss: 0.00001724047252 in Epoch 146
Epoch 151
A best model at epoch 151 has been saved with training error 0.00001539135701.
Epoch 151, Loss: 0.00002757471311, Improvement: -0.00000234741765, Best Loss: 0.00001539135701 in Epoch 151
Epoch 152
Epoch 152, Loss: 0.00002565454497, Improvement: -0.00000192016814, Best Loss: 0.00001539135701 in Epoch 151
Epoch 153
A best model at epoch 153 has been saved with training error 0.00001365318531.
A best model at epoch 153 has been saved with training error 0.00001343014810.
Epoch 153, Loss: 0.00002336013149, Improvement: -0.00000229441348, Best Loss: 0.00001343014810 in Epoch 153
Epoch 154
Epoch 154, Loss: 0.00002135840186, Improvement: -0.00000200172963, Best Loss: 0.00001343014810 in Epoch 153
Epoch 155
A best model at epoch 155 has been saved with training error 0.00001204351975.
Epoch 155, Loss: 0.00002135408240, Improvement: -0.00000000431946, Best Loss: 0.00001204351975 in Epoch 155
Epoch 156
Epoch 156, Loss: 0.00001887309863, Improvement: -0.00000248098377, Best Loss: 0.00001204351975 in Epoch 155
Epoch 157
Epoch 157, Loss: 0.00002188982471, Improvement: 0.00000301672608, Best Loss: 0.00001204351975 in Epoch 155
Epoch 158
A best model at epoch 158 has been saved with training error 0.00001003545640.
Epoch 158, Loss: 0.00001814987468, Improvement: -0.00000373995003, Best Loss: 0.00001003545640 in Epoch 158
Epoch 159
Epoch 159, Loss: 0.00001780156763, Improvement: -0.00000034830705, Best Loss: 0.00001003545640 in Epoch 158
Epoch 160
A best model at epoch 160 has been saved with training error 0.00000999542499.
A best model at epoch 160 has been saved with training error 0.00000901584917.
A best model at epoch 160 has been saved with training error 0.00000688742784.
Epoch 160, Loss: 0.00001576771715, Improvement: -0.00000203385048, Best Loss: 0.00000688742784 in Epoch 160
Epoch 161
Epoch 161, Loss: 0.00001485427224, Improvement: -0.00000091344491, Best Loss: 0.00000688742784 in Epoch 160
Epoch 162
Epoch 162, Loss: 0.00001620353069, Improvement: 0.00000134925845, Best Loss: 0.00000688742784 in Epoch 160
Epoch 163
Epoch 163, Loss: 0.00002097203633, Improvement: 0.00000476850564, Best Loss: 0.00000688742784 in Epoch 160
Epoch 164
Epoch 164, Loss: 0.00001692801761, Improvement: -0.00000404401871, Best Loss: 0.00000688742784 in Epoch 160
Epoch 165
A best model at epoch 165 has been saved with training error 0.00000603757280.
Epoch 165, Loss: 0.00001431015990, Improvement: -0.00000261785772, Best Loss: 0.00000603757280 in Epoch 165
Epoch 166
Epoch 166, Loss: 0.00001386125259, Improvement: -0.00000044890730, Best Loss: 0.00000603757280 in Epoch 165
Epoch 167
Epoch 167, Loss: 0.00001433308903, Improvement: 0.00000047183644, Best Loss: 0.00000603757280 in Epoch 165
Epoch 168
Epoch 168, Loss: 0.00001452972256, Improvement: 0.00000019663353, Best Loss: 0.00000603757280 in Epoch 165
Epoch 169
Epoch 169, Loss: 0.00001492174920, Improvement: 0.00000039202664, Best Loss: 0.00000603757280 in Epoch 165
Epoch 170
Epoch 170, Loss: 0.00001578620131, Improvement: 0.00000086445211, Best Loss: 0.00000603757280 in Epoch 165
Epoch 171
Epoch 171, Loss: 0.00001469097197, Improvement: -0.00000109522935, Best Loss: 0.00000603757280 in Epoch 165
Epoch 172
Epoch 172, Loss: 0.00001433461221, Improvement: -0.00000035635976, Best Loss: 0.00000603757280 in Epoch 165
Epoch 173
A best model at epoch 173 has been saved with training error 0.00000569896429.
Epoch 173, Loss: 0.00001570521065, Improvement: 0.00000137059844, Best Loss: 0.00000569896429 in Epoch 173
Epoch 174
Epoch 174, Loss: 0.00001468512974, Improvement: -0.00000102008091, Best Loss: 0.00000569896429 in Epoch 173
Epoch 175
Epoch 175, Loss: 0.00001397750339, Improvement: -0.00000070762635, Best Loss: 0.00000569896429 in Epoch 173
Epoch 176
Epoch 176, Loss: 0.00001381413122, Improvement: -0.00000016337217, Best Loss: 0.00000569896429 in Epoch 173
Epoch 177
Epoch 177, Loss: 0.00001499725195, Improvement: 0.00000118312073, Best Loss: 0.00000569896429 in Epoch 173
Epoch 178
Epoch 178, Loss: 0.00001413408716, Improvement: -0.00000086316479, Best Loss: 0.00000569896429 in Epoch 173
Epoch 179
Epoch 179, Loss: 0.00001655850183, Improvement: 0.00000242441467, Best Loss: 0.00000569896429 in Epoch 173
Epoch 180
Epoch 180, Loss: 0.00001659274617, Improvement: 0.00000003424434, Best Loss: 0.00000569896429 in Epoch 173
Epoch 181
Epoch 181, Loss: 0.00001483657029, Improvement: -0.00000175617588, Best Loss: 0.00000569896429 in Epoch 173
Epoch 182
Epoch 182, Loss: 0.00001383224917, Improvement: -0.00000100432112, Best Loss: 0.00000569896429 in Epoch 173
Epoch 183
Epoch 183, Loss: 0.00001300470792, Improvement: -0.00000082754125, Best Loss: 0.00000569896429 in Epoch 173
Epoch 184
Epoch 184, Loss: 0.00001339066239, Improvement: 0.00000038595447, Best Loss: 0.00000569896429 in Epoch 173
Epoch 185
Epoch 185, Loss: 0.00001361292848, Improvement: 0.00000022226609, Best Loss: 0.00000569896429 in Epoch 173
Epoch 186
Epoch 186, Loss: 0.00001329324778, Improvement: -0.00000031968070, Best Loss: 0.00000569896429 in Epoch 173
Epoch 187
Epoch 187, Loss: 0.00001338032725, Improvement: 0.00000008707948, Best Loss: 0.00000569896429 in Epoch 173
Epoch 188
Epoch 188, Loss: 0.00001369218780, Improvement: 0.00000031186055, Best Loss: 0.00000569896429 in Epoch 173
Epoch 189
Epoch 189, Loss: 0.00001425174673, Improvement: 0.00000055955893, Best Loss: 0.00000569896429 in Epoch 173
Epoch 190
Epoch 190, Loss: 0.00001404143491, Improvement: -0.00000021031183, Best Loss: 0.00000569896429 in Epoch 173
Epoch 191
Epoch 191, Loss: 0.00001402484818, Improvement: -0.00000001658673, Best Loss: 0.00000569896429 in Epoch 173
Epoch 192
Epoch 192, Loss: 0.00001686172050, Improvement: 0.00000283687232, Best Loss: 0.00000569896429 in Epoch 173
Epoch 193
Epoch 193, Loss: 0.00001379876317, Improvement: -0.00000306295733, Best Loss: 0.00000569896429 in Epoch 173
Epoch 194
Epoch 194, Loss: 0.00001267844264, Improvement: -0.00000112032053, Best Loss: 0.00000569896429 in Epoch 173
Epoch 195
Epoch 195, Loss: 0.00001292995310, Improvement: 0.00000025151046, Best Loss: 0.00000569896429 in Epoch 173
Epoch 196
Epoch 196, Loss: 0.00001263972968, Improvement: -0.00000029022342, Best Loss: 0.00000569896429 in Epoch 173
Epoch 197
Epoch 197, Loss: 0.00001281241869, Improvement: 0.00000017268901, Best Loss: 0.00000569896429 in Epoch 173
Epoch 198
Epoch 198, Loss: 0.00001288455874, Improvement: 0.00000007214005, Best Loss: 0.00000569896429 in Epoch 173
Epoch 199
Epoch 199, Loss: 0.00001522410635, Improvement: 0.00000233954761, Best Loss: 0.00000569896429 in Epoch 173
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00001627244560, Improvement: 0.00000104833925, Best Loss: 0.00000569896429 in Epoch 173
Epoch 201
Epoch 201, Loss: 0.00001322593776, Improvement: -0.00000304650785, Best Loss: 0.00000569896429 in Epoch 173
Epoch 202
Epoch 202, Loss: 0.00001214371887, Improvement: -0.00000108221889, Best Loss: 0.00000569896429 in Epoch 173
Epoch 203
Epoch 203, Loss: 0.00001237612023, Improvement: 0.00000023240136, Best Loss: 0.00000569896429 in Epoch 173
Epoch 204
Epoch 204, Loss: 0.00001212306479, Improvement: -0.00000025305544, Best Loss: 0.00000569896429 in Epoch 173
Epoch 205
Epoch 205, Loss: 0.00001218512559, Improvement: 0.00000006206080, Best Loss: 0.00000569896429 in Epoch 173
Epoch 206
Epoch 206, Loss: 0.00001201621135, Improvement: -0.00000016891424, Best Loss: 0.00000569896429 in Epoch 173
Epoch 207
Epoch 207, Loss: 0.00001190883390, Improvement: -0.00000010737745, Best Loss: 0.00000569896429 in Epoch 173
Epoch 208
Epoch 208, Loss: 0.00001210996111, Improvement: 0.00000020112720, Best Loss: 0.00000569896429 in Epoch 173
Epoch 209
Epoch 209, Loss: 0.00001195724633, Improvement: -0.00000015271478, Best Loss: 0.00000569896429 in Epoch 173
Epoch 210
A best model at epoch 210 has been saved with training error 0.00000476737659.
Epoch 210, Loss: 0.00001176269905, Improvement: -0.00000019454728, Best Loss: 0.00000476737659 in Epoch 210
Epoch 211
Epoch 211, Loss: 0.00001180097483, Improvement: 0.00000003827579, Best Loss: 0.00000476737659 in Epoch 210
Epoch 212
Epoch 212, Loss: 0.00001175697046, Improvement: -0.00000004400438, Best Loss: 0.00000476737659 in Epoch 210
Epoch 213
Epoch 213, Loss: 0.00001160284276, Improvement: -0.00000015412770, Best Loss: 0.00000476737659 in Epoch 210
Epoch 214
Epoch 214, Loss: 0.00001173237140, Improvement: 0.00000012952864, Best Loss: 0.00000476737659 in Epoch 210
Epoch 215
Epoch 215, Loss: 0.00001186276331, Improvement: 0.00000013039191, Best Loss: 0.00000476737659 in Epoch 210
Epoch 216
Epoch 216, Loss: 0.00001209369811, Improvement: 0.00000023093480, Best Loss: 0.00000476737659 in Epoch 210
Epoch 217
Epoch 217, Loss: 0.00001170229518, Improvement: -0.00000039140293, Best Loss: 0.00000476737659 in Epoch 210
Epoch 218
Epoch 218, Loss: 0.00001138046907, Improvement: -0.00000032182611, Best Loss: 0.00000476737659 in Epoch 210
Epoch 219
Epoch 219, Loss: 0.00001131465531, Improvement: -0.00000006581377, Best Loss: 0.00000476737659 in Epoch 210
Epoch 220
Epoch 220, Loss: 0.00001155299924, Improvement: 0.00000023834393, Best Loss: 0.00000476737659 in Epoch 210
Epoch 221
Epoch 221, Loss: 0.00001134208501, Improvement: -0.00000021091423, Best Loss: 0.00000476737659 in Epoch 210
Epoch 222
Epoch 222, Loss: 0.00001181850535, Improvement: 0.00000047642034, Best Loss: 0.00000476737659 in Epoch 210
Epoch 223
Epoch 223, Loss: 0.00001159444469, Improvement: -0.00000022406066, Best Loss: 0.00000476737659 in Epoch 210
Epoch 224
Epoch 224, Loss: 0.00001114397492, Improvement: -0.00000045046977, Best Loss: 0.00000476737659 in Epoch 210
Epoch 225
Epoch 225, Loss: 0.00001102472211, Improvement: -0.00000011925281, Best Loss: 0.00000476737659 in Epoch 210
Epoch 226
Epoch 226, Loss: 0.00001099379804, Improvement: -0.00000003092407, Best Loss: 0.00000476737659 in Epoch 210
Epoch 227
Epoch 227, Loss: 0.00001051009428, Improvement: -0.00000048370375, Best Loss: 0.00000476737659 in Epoch 210
Epoch 228
Epoch 228, Loss: 0.00001115455370, Improvement: 0.00000064445942, Best Loss: 0.00000476737659 in Epoch 210
Epoch 229
Epoch 229, Loss: 0.00001098618163, Improvement: -0.00000016837207, Best Loss: 0.00000476737659 in Epoch 210
Epoch 230
Epoch 230, Loss: 0.00001172272960, Improvement: 0.00000073654796, Best Loss: 0.00000476737659 in Epoch 210
Epoch 231
Epoch 231, Loss: 0.00001124339371, Improvement: -0.00000047933588, Best Loss: 0.00000476737659 in Epoch 210
Epoch 232
Epoch 232, Loss: 0.00001125556751, Improvement: 0.00000001217379, Best Loss: 0.00000476737659 in Epoch 210
Epoch 233
Epoch 233, Loss: 0.00001093488661, Improvement: -0.00000032068090, Best Loss: 0.00000476737659 in Epoch 210
Epoch 234
Epoch 234, Loss: 0.00001028227935, Improvement: -0.00000065260726, Best Loss: 0.00000476737659 in Epoch 210
Epoch 235
Epoch 235, Loss: 0.00001085289291, Improvement: 0.00000057061357, Best Loss: 0.00000476737659 in Epoch 210
Epoch 236
Epoch 236, Loss: 0.00001046684943, Improvement: -0.00000038604348, Best Loss: 0.00000476737659 in Epoch 210
Epoch 237
Epoch 237, Loss: 0.00001043543305, Improvement: -0.00000003141638, Best Loss: 0.00000476737659 in Epoch 210
Epoch 238
Epoch 238, Loss: 0.00000951501418, Improvement: -0.00000092041887, Best Loss: 0.00000476737659 in Epoch 210
Epoch 239
Epoch 239, Loss: 0.00000956133190, Improvement: 0.00000004631772, Best Loss: 0.00000476737659 in Epoch 210
Epoch 240
Epoch 240, Loss: 0.00001020525306, Improvement: 0.00000064392116, Best Loss: 0.00000476737659 in Epoch 210
Epoch 241
Epoch 241, Loss: 0.00000926450653, Improvement: -0.00000094074653, Best Loss: 0.00000476737659 in Epoch 210
Epoch 242
Epoch 242, Loss: 0.00000850030990, Improvement: -0.00000076419662, Best Loss: 0.00000476737659 in Epoch 210
Epoch 243
A best model at epoch 243 has been saved with training error 0.00000469976476.
A best model at epoch 243 has been saved with training error 0.00000467090194.
Epoch 243, Loss: 0.00000808228594, Improvement: -0.00000041802396, Best Loss: 0.00000467090194 in Epoch 243
Epoch 244
A best model at epoch 244 has been saved with training error 0.00000457949955.
Epoch 244, Loss: 0.00000876125177, Improvement: 0.00000067896583, Best Loss: 0.00000457949955 in Epoch 244
Epoch 245
A best model at epoch 245 has been saved with training error 0.00000436660093.
Epoch 245, Loss: 0.00000782112372, Improvement: -0.00000094012805, Best Loss: 0.00000436660093 in Epoch 245
Epoch 246
Epoch 246, Loss: 0.00000990952403, Improvement: 0.00000208840031, Best Loss: 0.00000436660093 in Epoch 245
Epoch 247
Epoch 247, Loss: 0.00000711858224, Improvement: -0.00000279094179, Best Loss: 0.00000436660093 in Epoch 245
Epoch 248
A best model at epoch 248 has been saved with training error 0.00000408311098.
Epoch 248, Loss: 0.00000585531227, Improvement: -0.00000126326997, Best Loss: 0.00000408311098 in Epoch 248
Epoch 249
A best model at epoch 249 has been saved with training error 0.00000351364974.
A best model at epoch 249 has been saved with training error 0.00000323034624.
A best model at epoch 249 has been saved with training error 0.00000309267648.
Epoch 249, Loss: 0.00000519397988, Improvement: -0.00000066133239, Best Loss: 0.00000309267648 in Epoch 249
Epoch 250
A best model at epoch 250 has been saved with training error 0.00000296401981.
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000506571812, Improvement: -0.00000012826176, Best Loss: 0.00000296401981 in Epoch 250
Epoch 251
A best model at epoch 251 has been saved with training error 0.00000286081081.
Epoch 251, Loss: 0.00000538846703, Improvement: 0.00000032274891, Best Loss: 0.00000286081081 in Epoch 251
Epoch 252
Epoch 252, Loss: 0.00000619326366, Improvement: 0.00000080479664, Best Loss: 0.00000286081081 in Epoch 251
Epoch 253
Epoch 253, Loss: 0.00000655923531, Improvement: 0.00000036597164, Best Loss: 0.00000286081081 in Epoch 251
Epoch 254
A best model at epoch 254 has been saved with training error 0.00000272827924.
A best model at epoch 254 has been saved with training error 0.00000242820738.
Epoch 254, Loss: 0.00000419339331, Improvement: -0.00000236584200, Best Loss: 0.00000242820738 in Epoch 254
Epoch 255
A best model at epoch 255 has been saved with training error 0.00000191365348.
Epoch 255, Loss: 0.00000546432519, Improvement: 0.00000127093189, Best Loss: 0.00000191365348 in Epoch 255
Epoch 256
Epoch 256, Loss: 0.00001089881268, Improvement: 0.00000543448749, Best Loss: 0.00000191365348 in Epoch 255
Epoch 257
A best model at epoch 257 has been saved with training error 0.00000191355616.
Epoch 257, Loss: 0.00000452081849, Improvement: -0.00000637799419, Best Loss: 0.00000191355616 in Epoch 257
Epoch 258
Epoch 258, Loss: 0.00000341495668, Improvement: -0.00000110586182, Best Loss: 0.00000191355616 in Epoch 257
Epoch 259
Epoch 259, Loss: 0.00000291281909, Improvement: -0.00000050213758, Best Loss: 0.00000191355616 in Epoch 257
Epoch 260
A best model at epoch 260 has been saved with training error 0.00000155288296.
Epoch 260, Loss: 0.00000261784429, Improvement: -0.00000029497481, Best Loss: 0.00000155288296 in Epoch 260
Epoch 261
Epoch 261, Loss: 0.00000244035897, Improvement: -0.00000017748532, Best Loss: 0.00000155288296 in Epoch 260
Epoch 262
A best model at epoch 262 has been saved with training error 0.00000151458494.
Epoch 262, Loss: 0.00000238134044, Improvement: -0.00000005901853, Best Loss: 0.00000151458494 in Epoch 262
Epoch 263
Epoch 263, Loss: 0.00000243295297, Improvement: 0.00000005161253, Best Loss: 0.00000151458494 in Epoch 262
Epoch 264
Epoch 264, Loss: 0.00000289390189, Improvement: 0.00000046094892, Best Loss: 0.00000151458494 in Epoch 262
Epoch 265
A best model at epoch 265 has been saved with training error 0.00000137098596.
Epoch 265, Loss: 0.00000328864577, Improvement: 0.00000039474388, Best Loss: 0.00000137098596 in Epoch 265
Epoch 266
Epoch 266, Loss: 0.00000405036263, Improvement: 0.00000076171685, Best Loss: 0.00000137098596 in Epoch 265
Epoch 267
Epoch 267, Loss: 0.00000294332356, Improvement: -0.00000110703907, Best Loss: 0.00000137098596 in Epoch 265
Epoch 268
Epoch 268, Loss: 0.00000802970794, Improvement: 0.00000508638439, Best Loss: 0.00000137098596 in Epoch 265
Epoch 269
Epoch 269, Loss: 0.00001020493766, Improvement: 0.00000217522971, Best Loss: 0.00000137098596 in Epoch 265
Epoch 270
Epoch 270, Loss: 0.00000333028365, Improvement: -0.00000687465400, Best Loss: 0.00000137098596 in Epoch 265
Epoch 271
Epoch 271, Loss: 0.00000264005707, Improvement: -0.00000069022659, Best Loss: 0.00000137098596 in Epoch 265
Epoch 272
Epoch 272, Loss: 0.00000233045781, Improvement: -0.00000030959926, Best Loss: 0.00000137098596 in Epoch 265
Epoch 273
Epoch 273, Loss: 0.00000212273251, Improvement: -0.00000020772530, Best Loss: 0.00000137098596 in Epoch 265
Epoch 274
A best model at epoch 274 has been saved with training error 0.00000134620370.
A best model at epoch 274 has been saved with training error 0.00000126593613.
Epoch 274, Loss: 0.00000217373428, Improvement: 0.00000005100177, Best Loss: 0.00000126593613 in Epoch 274
Epoch 275
A best model at epoch 275 has been saved with training error 0.00000119431991.
A best model at epoch 275 has been saved with training error 0.00000103852267.
Epoch 275, Loss: 0.00000196514584, Improvement: -0.00000020858844, Best Loss: 0.00000103852267 in Epoch 275
Epoch 276
Epoch 276, Loss: 0.00000187406858, Improvement: -0.00000009107726, Best Loss: 0.00000103852267 in Epoch 275
Epoch 277
Epoch 277, Loss: 0.00000198894035, Improvement: 0.00000011487177, Best Loss: 0.00000103852267 in Epoch 275
Epoch 278
Epoch 278, Loss: 0.00000226651022, Improvement: 0.00000027756988, Best Loss: 0.00000103852267 in Epoch 275
Epoch 279
Epoch 279, Loss: 0.00000327059676, Improvement: 0.00000100408654, Best Loss: 0.00000103852267 in Epoch 275
Epoch 280
Epoch 280, Loss: 0.00000227857602, Improvement: -0.00000099202074, Best Loss: 0.00000103852267 in Epoch 275
Epoch 281
Epoch 281, Loss: 0.00000235262075, Improvement: 0.00000007404473, Best Loss: 0.00000103852267 in Epoch 275
Epoch 282
Epoch 282, Loss: 0.00000221902611, Improvement: -0.00000013359464, Best Loss: 0.00000103852267 in Epoch 275
Epoch 283
Epoch 283, Loss: 0.00000330176334, Improvement: 0.00000108273723, Best Loss: 0.00000103852267 in Epoch 275
Epoch 284
Epoch 284, Loss: 0.00000367960440, Improvement: 0.00000037784106, Best Loss: 0.00000103852267 in Epoch 275
Epoch 285
Epoch 285, Loss: 0.00000408348602, Improvement: 0.00000040388162, Best Loss: 0.00000103852267 in Epoch 275
Epoch 286
Epoch 286, Loss: 0.00000411272688, Improvement: 0.00000002924086, Best Loss: 0.00000103852267 in Epoch 275
Epoch 287
Epoch 287, Loss: 0.00000339746979, Improvement: -0.00000071525709, Best Loss: 0.00000103852267 in Epoch 275
Epoch 288
Epoch 288, Loss: 0.00000374637146, Improvement: 0.00000034890167, Best Loss: 0.00000103852267 in Epoch 275
Epoch 289
Epoch 289, Loss: 0.00000262634194, Improvement: -0.00000112002952, Best Loss: 0.00000103852267 in Epoch 275
Epoch 290
Epoch 290, Loss: 0.00000214627385, Improvement: -0.00000048006809, Best Loss: 0.00000103852267 in Epoch 275
Epoch 291
Epoch 291, Loss: 0.00000201943162, Improvement: -0.00000012684223, Best Loss: 0.00000103852267 in Epoch 275
Epoch 292
Epoch 292, Loss: 0.00000182763385, Improvement: -0.00000019179777, Best Loss: 0.00000103852267 in Epoch 275
Epoch 293
Epoch 293, Loss: 0.00000185148423, Improvement: 0.00000002385038, Best Loss: 0.00000103852267 in Epoch 275
Epoch 294
Epoch 294, Loss: 0.00000177208673, Improvement: -0.00000007939751, Best Loss: 0.00000103852267 in Epoch 275
Epoch 295
Epoch 295, Loss: 0.00000185508451, Improvement: 0.00000008299778, Best Loss: 0.00000103852267 in Epoch 275
Epoch 296
Epoch 296, Loss: 0.00000190188137, Improvement: 0.00000004679686, Best Loss: 0.00000103852267 in Epoch 275
Epoch 297
Epoch 297, Loss: 0.00000237880121, Improvement: 0.00000047691984, Best Loss: 0.00000103852267 in Epoch 275
Epoch 298
A best model at epoch 298 has been saved with training error 0.00000101069486.
Epoch 298, Loss: 0.00000221779566, Improvement: -0.00000016100555, Best Loss: 0.00000101069486 in Epoch 298
Epoch 299
Epoch 299, Loss: 0.00000218895084, Improvement: -0.00000002884482, Best Loss: 0.00000101069486 in Epoch 298
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000249153137, Improvement: 0.00000030258053, Best Loss: 0.00000101069486 in Epoch 298
Epoch 301
Epoch 301, Loss: 0.00000296767804, Improvement: 0.00000047614667, Best Loss: 0.00000101069486 in Epoch 298
Epoch 302
Epoch 302, Loss: 0.00000241374148, Improvement: -0.00000055393656, Best Loss: 0.00000101069486 in Epoch 298
Epoch 303
Epoch 303, Loss: 0.00000286115179, Improvement: 0.00000044741032, Best Loss: 0.00000101069486 in Epoch 298
Epoch 304
Epoch 304, Loss: 0.00000482714266, Improvement: 0.00000196599087, Best Loss: 0.00000101069486 in Epoch 298
Epoch 305
Epoch 305, Loss: 0.00000370419547, Improvement: -0.00000112294720, Best Loss: 0.00000101069486 in Epoch 298
Epoch 306
Epoch 306, Loss: 0.00000359417410, Improvement: -0.00000011002137, Best Loss: 0.00000101069486 in Epoch 298
Epoch 307
Epoch 307, Loss: 0.00000442612831, Improvement: 0.00000083195421, Best Loss: 0.00000101069486 in Epoch 298
Epoch 308
Epoch 308, Loss: 0.00000230344434, Improvement: -0.00000212268397, Best Loss: 0.00000101069486 in Epoch 298
Epoch 309
Epoch 309, Loss: 0.00000196739588, Improvement: -0.00000033604846, Best Loss: 0.00000101069486 in Epoch 298
Epoch 310
Epoch 310, Loss: 0.00000191754374, Improvement: -0.00000004985213, Best Loss: 0.00000101069486 in Epoch 298
Epoch 311
Epoch 311, Loss: 0.00000220263921, Improvement: 0.00000028509546, Best Loss: 0.00000101069486 in Epoch 298
Epoch 312
Epoch 312, Loss: 0.00000322074818, Improvement: 0.00000101810897, Best Loss: 0.00000101069486 in Epoch 298
Epoch 313
Epoch 313, Loss: 0.00000261141885, Improvement: -0.00000060932933, Best Loss: 0.00000101069486 in Epoch 298
Epoch 314
Epoch 314, Loss: 0.00000235406472, Improvement: -0.00000025735413, Best Loss: 0.00000101069486 in Epoch 298
Epoch 315
Epoch 315, Loss: 0.00000174497805, Improvement: -0.00000060908667, Best Loss: 0.00000101069486 in Epoch 298
Epoch 316
Epoch 316, Loss: 0.00000158051913, Improvement: -0.00000016445892, Best Loss: 0.00000101069486 in Epoch 298
Epoch 317
Epoch 317, Loss: 0.00000157263927, Improvement: -0.00000000787986, Best Loss: 0.00000101069486 in Epoch 298
Epoch 318
Epoch 318, Loss: 0.00000196695439, Improvement: 0.00000039431512, Best Loss: 0.00000101069486 in Epoch 298
Epoch 319
Epoch 319, Loss: 0.00000201651844, Improvement: 0.00000004956405, Best Loss: 0.00000101069486 in Epoch 298
Epoch 320
Epoch 320, Loss: 0.00000222570371, Improvement: 0.00000020918528, Best Loss: 0.00000101069486 in Epoch 298
Epoch 321
Epoch 321, Loss: 0.00000332465086, Improvement: 0.00000109894715, Best Loss: 0.00000101069486 in Epoch 298
Epoch 322
Epoch 322, Loss: 0.00000211838887, Improvement: -0.00000120626199, Best Loss: 0.00000101069486 in Epoch 298
Epoch 323
Epoch 323, Loss: 0.00000186345598, Improvement: -0.00000025493289, Best Loss: 0.00000101069486 in Epoch 298
Epoch 324
Epoch 324, Loss: 0.00000191430935, Improvement: 0.00000005085337, Best Loss: 0.00000101069486 in Epoch 298
Epoch 325
Epoch 325, Loss: 0.00000199290909, Improvement: 0.00000007859974, Best Loss: 0.00000101069486 in Epoch 298
Epoch 326
Epoch 326, Loss: 0.00000175247547, Improvement: -0.00000024043362, Best Loss: 0.00000101069486 in Epoch 298
Epoch 327
Epoch 327, Loss: 0.00000303487587, Improvement: 0.00000128240040, Best Loss: 0.00000101069486 in Epoch 298
Epoch 328
Epoch 328, Loss: 0.00000694913747, Improvement: 0.00000391426159, Best Loss: 0.00000101069486 in Epoch 298
Epoch 329
Epoch 329, Loss: 0.00000233752604, Improvement: -0.00000461161143, Best Loss: 0.00000101069486 in Epoch 298
Epoch 330
A best model at epoch 330 has been saved with training error 0.00000087352527.
A best model at epoch 330 has been saved with training error 0.00000083789735.
Epoch 330, Loss: 0.00000172252710, Improvement: -0.00000061499894, Best Loss: 0.00000083789735 in Epoch 330
Epoch 331
A best model at epoch 331 has been saved with training error 0.00000080594259.
Epoch 331, Loss: 0.00000148084322, Improvement: -0.00000024168388, Best Loss: 0.00000080594259 in Epoch 331
Epoch 332
Epoch 332, Loss: 0.00000156541892, Improvement: 0.00000008457570, Best Loss: 0.00000080594259 in Epoch 331
Epoch 333
Epoch 333, Loss: 0.00000148093692, Improvement: -0.00000008448200, Best Loss: 0.00000080594259 in Epoch 331
Epoch 334
Epoch 334, Loss: 0.00000149313284, Improvement: 0.00000001219591, Best Loss: 0.00000080594259 in Epoch 331
Epoch 335
Epoch 335, Loss: 0.00000138792828, Improvement: -0.00000010520455, Best Loss: 0.00000080594259 in Epoch 331
Epoch 336
A best model at epoch 336 has been saved with training error 0.00000080031856.
Epoch 336, Loss: 0.00000136681057, Improvement: -0.00000002111771, Best Loss: 0.00000080031856 in Epoch 336
Epoch 337
Epoch 337, Loss: 0.00000146179442, Improvement: 0.00000009498385, Best Loss: 0.00000080031856 in Epoch 336
Epoch 338
Epoch 338, Loss: 0.00000145310535, Improvement: -0.00000000868907, Best Loss: 0.00000080031856 in Epoch 336
Epoch 339
Epoch 339, Loss: 0.00000151508865, Improvement: 0.00000006198330, Best Loss: 0.00000080031856 in Epoch 336
Epoch 340
Epoch 340, Loss: 0.00000142728553, Improvement: -0.00000008780312, Best Loss: 0.00000080031856 in Epoch 336
Epoch 341
Epoch 341, Loss: 0.00000145421353, Improvement: 0.00000002692800, Best Loss: 0.00000080031856 in Epoch 336
Epoch 342
Epoch 342, Loss: 0.00000233967600, Improvement: 0.00000088546248, Best Loss: 0.00000080031856 in Epoch 336
Epoch 343
Epoch 343, Loss: 0.00000185925872, Improvement: -0.00000048041728, Best Loss: 0.00000080031856 in Epoch 336
Epoch 344
Epoch 344, Loss: 0.00000137908120, Improvement: -0.00000048017752, Best Loss: 0.00000080031856 in Epoch 336
Epoch 345
Epoch 345, Loss: 0.00000137079640, Improvement: -0.00000000828479, Best Loss: 0.00000080031856 in Epoch 336
Epoch 346
Epoch 346, Loss: 0.00000129267763, Improvement: -0.00000007811877, Best Loss: 0.00000080031856 in Epoch 336
Epoch 347
Epoch 347, Loss: 0.00000127830053, Improvement: -0.00000001437710, Best Loss: 0.00000080031856 in Epoch 336
Epoch 348
Epoch 348, Loss: 0.00000125154175, Improvement: -0.00000002675878, Best Loss: 0.00000080031856 in Epoch 336
Epoch 349
A best model at epoch 349 has been saved with training error 0.00000074215291.
Epoch 349, Loss: 0.00000125149888, Improvement: -0.00000000004287, Best Loss: 0.00000074215291 in Epoch 349
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000130866911, Improvement: 0.00000005717023, Best Loss: 0.00000074215291 in Epoch 349
Epoch 351
Epoch 351, Loss: 0.00000150591722, Improvement: 0.00000019724812, Best Loss: 0.00000074215291 in Epoch 349
Epoch 352
Epoch 352, Loss: 0.00000147456163, Improvement: -0.00000003135560, Best Loss: 0.00000074215291 in Epoch 349
Epoch 353
Epoch 353, Loss: 0.00000143687926, Improvement: -0.00000003768236, Best Loss: 0.00000074215291 in Epoch 349
Epoch 354
Epoch 354, Loss: 0.00000169417147, Improvement: 0.00000025729221, Best Loss: 0.00000074215291 in Epoch 349
Epoch 355
A best model at epoch 355 has been saved with training error 0.00000066696185.
Epoch 355, Loss: 0.00000152862205, Improvement: -0.00000016554942, Best Loss: 0.00000066696185 in Epoch 355
Epoch 356
Epoch 356, Loss: 0.00000137468019, Improvement: -0.00000015394185, Best Loss: 0.00000066696185 in Epoch 355
Epoch 357
Epoch 357, Loss: 0.00000127082501, Improvement: -0.00000010385518, Best Loss: 0.00000066696185 in Epoch 355
Epoch 358
Epoch 358, Loss: 0.00000134544661, Improvement: 0.00000007462160, Best Loss: 0.00000066696185 in Epoch 355
Epoch 359
Epoch 359, Loss: 0.00000125067763, Improvement: -0.00000009476898, Best Loss: 0.00000066696185 in Epoch 355
Epoch 360
Epoch 360, Loss: 0.00000140014686, Improvement: 0.00000014946923, Best Loss: 0.00000066696185 in Epoch 355
Epoch 361
Epoch 361, Loss: 0.00000158240847, Improvement: 0.00000018226161, Best Loss: 0.00000066696185 in Epoch 355
Epoch 362
Epoch 362, Loss: 0.00000188975001, Improvement: 0.00000030734154, Best Loss: 0.00000066696185 in Epoch 355
Epoch 363
Epoch 363, Loss: 0.00000186131934, Improvement: -0.00000002843067, Best Loss: 0.00000066696185 in Epoch 355
Epoch 364
Epoch 364, Loss: 0.00000152066385, Improvement: -0.00000034065550, Best Loss: 0.00000066696185 in Epoch 355
Epoch 365
Epoch 365, Loss: 0.00000161746628, Improvement: 0.00000009680243, Best Loss: 0.00000066696185 in Epoch 355
Epoch 366
Epoch 366, Loss: 0.00000246003980, Improvement: 0.00000084257352, Best Loss: 0.00000066696185 in Epoch 355
Epoch 367
Epoch 367, Loss: 0.00000437691043, Improvement: 0.00000191687063, Best Loss: 0.00000066696185 in Epoch 355
Epoch 368
Epoch 368, Loss: 0.00000238924712, Improvement: -0.00000198766331, Best Loss: 0.00000066696185 in Epoch 355
Epoch 369
Epoch 369, Loss: 0.00000155178610, Improvement: -0.00000083746102, Best Loss: 0.00000066696185 in Epoch 355
Epoch 370
Epoch 370, Loss: 0.00000205299348, Improvement: 0.00000050120738, Best Loss: 0.00000066696185 in Epoch 355
Epoch 371
Epoch 371, Loss: 0.00000211415979, Improvement: 0.00000006116631, Best Loss: 0.00000066696185 in Epoch 355
Epoch 372
Epoch 372, Loss: 0.00000145195225, Improvement: -0.00000066220754, Best Loss: 0.00000066696185 in Epoch 355
Epoch 373
Epoch 373, Loss: 0.00000131406733, Improvement: -0.00000013788492, Best Loss: 0.00000066696185 in Epoch 355
Epoch 374
Epoch 374, Loss: 0.00000123525860, Improvement: -0.00000007880873, Best Loss: 0.00000066696185 in Epoch 355
Epoch 375
Epoch 375, Loss: 0.00000126160068, Improvement: 0.00000002634208, Best Loss: 0.00000066696185 in Epoch 355
Epoch 376
Epoch 376, Loss: 0.00000112879171, Improvement: -0.00000013280897, Best Loss: 0.00000066696185 in Epoch 355
Epoch 377
Epoch 377, Loss: 0.00000115347978, Improvement: 0.00000002468807, Best Loss: 0.00000066696185 in Epoch 355
Epoch 378
Epoch 378, Loss: 0.00000118607869, Improvement: 0.00000003259891, Best Loss: 0.00000066696185 in Epoch 355
Epoch 379
Epoch 379, Loss: 0.00000190054042, Improvement: 0.00000071446172, Best Loss: 0.00000066696185 in Epoch 355
Epoch 380
Epoch 380, Loss: 0.00000219551244, Improvement: 0.00000029497202, Best Loss: 0.00000066696185 in Epoch 355
Epoch 381
Epoch 381, Loss: 0.00000157854803, Improvement: -0.00000061696441, Best Loss: 0.00000066696185 in Epoch 355
Epoch 382
Epoch 382, Loss: 0.00000123157047, Improvement: -0.00000034697756, Best Loss: 0.00000066696185 in Epoch 355
Epoch 383
Epoch 383, Loss: 0.00000139142803, Improvement: 0.00000015985756, Best Loss: 0.00000066696185 in Epoch 355
Epoch 384
Epoch 384, Loss: 0.00000165039794, Improvement: 0.00000025896991, Best Loss: 0.00000066696185 in Epoch 355
Epoch 385
A best model at epoch 385 has been saved with training error 0.00000066194747.
Epoch 385, Loss: 0.00000138643771, Improvement: -0.00000026396022, Best Loss: 0.00000066194747 in Epoch 385
Epoch 386
Epoch 386, Loss: 0.00000163581390, Improvement: 0.00000024937619, Best Loss: 0.00000066194747 in Epoch 385
Epoch 387
Epoch 387, Loss: 0.00000160546024, Improvement: -0.00000003035366, Best Loss: 0.00000066194747 in Epoch 385
Epoch 388
Epoch 388, Loss: 0.00000133650507, Improvement: -0.00000026895517, Best Loss: 0.00000066194747 in Epoch 385
Epoch 389
Epoch 389, Loss: 0.00000169897865, Improvement: 0.00000036247358, Best Loss: 0.00000066194747 in Epoch 385
Epoch 390
Epoch 390, Loss: 0.00000157274392, Improvement: -0.00000012623473, Best Loss: 0.00000066194747 in Epoch 385
Epoch 391
Epoch 391, Loss: 0.00000135002865, Improvement: -0.00000022271527, Best Loss: 0.00000066194747 in Epoch 385
Epoch 392
Epoch 392, Loss: 0.00000153989164, Improvement: 0.00000018986300, Best Loss: 0.00000066194747 in Epoch 385
Epoch 393
Epoch 393, Loss: 0.00000113379198, Improvement: -0.00000040609967, Best Loss: 0.00000066194747 in Epoch 385
Epoch 394
Epoch 394, Loss: 0.00000107899028, Improvement: -0.00000005480169, Best Loss: 0.00000066194747 in Epoch 385
Epoch 395
Epoch 395, Loss: 0.00000103393276, Improvement: -0.00000004505753, Best Loss: 0.00000066194747 in Epoch 385
Epoch 396
Epoch 396, Loss: 0.00000104839864, Improvement: 0.00000001446589, Best Loss: 0.00000066194747 in Epoch 385
Epoch 397
Epoch 397, Loss: 0.00000109133071, Improvement: 0.00000004293207, Best Loss: 0.00000066194747 in Epoch 385
Epoch 398
Epoch 398, Loss: 0.00000109051074, Improvement: -0.00000000081997, Best Loss: 0.00000066194747 in Epoch 385
Epoch 399
Epoch 399, Loss: 0.00000122532797, Improvement: 0.00000013481723, Best Loss: 0.00000066194747 in Epoch 385
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000262577892, Improvement: 0.00000140045095, Best Loss: 0.00000066194747 in Epoch 385
Epoch 401
Epoch 401, Loss: 0.00000350279112, Improvement: 0.00000087701220, Best Loss: 0.00000066194747 in Epoch 385
Epoch 402
Epoch 402, Loss: 0.00000320270388, Improvement: -0.00000030008724, Best Loss: 0.00000066194747 in Epoch 385
Epoch 403
Epoch 403, Loss: 0.00000210179016, Improvement: -0.00000110091372, Best Loss: 0.00000066194747 in Epoch 385
Epoch 404
Epoch 404, Loss: 0.00000219385839, Improvement: 0.00000009206823, Best Loss: 0.00000066194747 in Epoch 385
Epoch 405
Epoch 405, Loss: 0.00000147304221, Improvement: -0.00000072081618, Best Loss: 0.00000066194747 in Epoch 385
Epoch 406
Epoch 406, Loss: 0.00000114415856, Improvement: -0.00000032888365, Best Loss: 0.00000066194747 in Epoch 385
Epoch 407
Epoch 407, Loss: 0.00000103376379, Improvement: -0.00000011039477, Best Loss: 0.00000066194747 in Epoch 385
Epoch 408
A best model at epoch 408 has been saved with training error 0.00000062947828.
Epoch 408, Loss: 0.00000095187794, Improvement: -0.00000008188586, Best Loss: 0.00000062947828 in Epoch 408
Epoch 409
A best model at epoch 409 has been saved with training error 0.00000060426709.
A best model at epoch 409 has been saved with training error 0.00000057294233.
A best model at epoch 409 has been saved with training error 0.00000055562236.
Epoch 409, Loss: 0.00000092042038, Improvement: -0.00000003145755, Best Loss: 0.00000055562236 in Epoch 409
Epoch 410
Epoch 410, Loss: 0.00000092457395, Improvement: 0.00000000415357, Best Loss: 0.00000055562236 in Epoch 409
Epoch 411
Epoch 411, Loss: 0.00000105745432, Improvement: 0.00000013288036, Best Loss: 0.00000055562236 in Epoch 409
Epoch 412
Epoch 412, Loss: 0.00000098134769, Improvement: -0.00000007610663, Best Loss: 0.00000055562236 in Epoch 409
Epoch 413
Epoch 413, Loss: 0.00000090689013, Improvement: -0.00000007445755, Best Loss: 0.00000055562236 in Epoch 409
Epoch 414
Epoch 414, Loss: 0.00000091895227, Improvement: 0.00000001206214, Best Loss: 0.00000055562236 in Epoch 409
Epoch 415
Epoch 415, Loss: 0.00000091055530, Improvement: -0.00000000839697, Best Loss: 0.00000055562236 in Epoch 409
Epoch 416
Epoch 416, Loss: 0.00000102690782, Improvement: 0.00000011635252, Best Loss: 0.00000055562236 in Epoch 409
Epoch 417
Epoch 417, Loss: 0.00000100098809, Improvement: -0.00000002591974, Best Loss: 0.00000055562236 in Epoch 409
Epoch 418
Epoch 418, Loss: 0.00000098257536, Improvement: -0.00000001841273, Best Loss: 0.00000055562236 in Epoch 409
Epoch 419
Epoch 419, Loss: 0.00000100832084, Improvement: 0.00000002574548, Best Loss: 0.00000055562236 in Epoch 409
Epoch 420
Epoch 420, Loss: 0.00000096635945, Improvement: -0.00000004196138, Best Loss: 0.00000055562236 in Epoch 409
Epoch 421
Epoch 421, Loss: 0.00000094597936, Improvement: -0.00000002038009, Best Loss: 0.00000055562236 in Epoch 409
Epoch 422
Epoch 422, Loss: 0.00000120330345, Improvement: 0.00000025732409, Best Loss: 0.00000055562236 in Epoch 409
Epoch 423
Epoch 423, Loss: 0.00000155230507, Improvement: 0.00000034900162, Best Loss: 0.00000055562236 in Epoch 409
Epoch 424
Epoch 424, Loss: 0.00000268717154, Improvement: 0.00000113486647, Best Loss: 0.00000055562236 in Epoch 409
Epoch 425
Epoch 425, Loss: 0.00000211113002, Improvement: -0.00000057604153, Best Loss: 0.00000055562236 in Epoch 409
Epoch 426
Epoch 426, Loss: 0.00000179127345, Improvement: -0.00000031985657, Best Loss: 0.00000055562236 in Epoch 409
Epoch 427
Epoch 427, Loss: 0.00000162404557, Improvement: -0.00000016722788, Best Loss: 0.00000055562236 in Epoch 409
Epoch 428
Epoch 428, Loss: 0.00000126053759, Improvement: -0.00000036350798, Best Loss: 0.00000055562236 in Epoch 409
Epoch 429
Epoch 429, Loss: 0.00000101749909, Improvement: -0.00000024303850, Best Loss: 0.00000055562236 in Epoch 409
Epoch 430
Epoch 430, Loss: 0.00000104468885, Improvement: 0.00000002718976, Best Loss: 0.00000055562236 in Epoch 409
Epoch 431
Epoch 431, Loss: 0.00000095038239, Improvement: -0.00000009430646, Best Loss: 0.00000055562236 in Epoch 409
Epoch 432
Epoch 432, Loss: 0.00000100922009, Improvement: 0.00000005883770, Best Loss: 0.00000055562236 in Epoch 409
Epoch 433
Epoch 433, Loss: 0.00000151530083, Improvement: 0.00000050608074, Best Loss: 0.00000055562236 in Epoch 409
Epoch 434
Epoch 434, Loss: 0.00000146147022, Improvement: -0.00000005383061, Best Loss: 0.00000055562236 in Epoch 409
Epoch 435
Epoch 435, Loss: 0.00000181442319, Improvement: 0.00000035295298, Best Loss: 0.00000055562236 in Epoch 409
Epoch 436
Epoch 436, Loss: 0.00000164210654, Improvement: -0.00000017231665, Best Loss: 0.00000055562236 in Epoch 409
Epoch 437
Epoch 437, Loss: 0.00000124557909, Improvement: -0.00000039652745, Best Loss: 0.00000055562236 in Epoch 409
Epoch 438
Epoch 438, Loss: 0.00000176953962, Improvement: 0.00000052396053, Best Loss: 0.00000055562236 in Epoch 409
Epoch 439
Epoch 439, Loss: 0.00000125174721, Improvement: -0.00000051779241, Best Loss: 0.00000055562236 in Epoch 409
Epoch 440
Epoch 440, Loss: 0.00000096146432, Improvement: -0.00000029028288, Best Loss: 0.00000055562236 in Epoch 409
Epoch 441
Epoch 441, Loss: 0.00000086022856, Improvement: -0.00000010123576, Best Loss: 0.00000055562236 in Epoch 409
Epoch 442
Epoch 442, Loss: 0.00000110714173, Improvement: 0.00000024691316, Best Loss: 0.00000055562236 in Epoch 409
Epoch 443
Epoch 443, Loss: 0.00000110391895, Improvement: -0.00000000322277, Best Loss: 0.00000055562236 in Epoch 409
Epoch 444
Epoch 444, Loss: 0.00000114842764, Improvement: 0.00000004450869, Best Loss: 0.00000055562236 in Epoch 409
Epoch 445
Epoch 445, Loss: 0.00000133618144, Improvement: 0.00000018775380, Best Loss: 0.00000055562236 in Epoch 409
Epoch 446
Epoch 446, Loss: 0.00000106369221, Improvement: -0.00000027248923, Best Loss: 0.00000055562236 in Epoch 409
Epoch 447
Epoch 447, Loss: 0.00000098014505, Improvement: -0.00000008354716, Best Loss: 0.00000055562236 in Epoch 409
Epoch 448
Epoch 448, Loss: 0.00000100532097, Improvement: 0.00000002517592, Best Loss: 0.00000055562236 in Epoch 409
Epoch 449
Epoch 449, Loss: 0.00000141871781, Improvement: 0.00000041339684, Best Loss: 0.00000055562236 in Epoch 409
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000172658281, Improvement: 0.00000030786500, Best Loss: 0.00000055562236 in Epoch 409
Epoch 451
Epoch 451, Loss: 0.00000128495245, Improvement: -0.00000044163036, Best Loss: 0.00000055562236 in Epoch 409
Epoch 452
Epoch 452, Loss: 0.00000115032887, Improvement: -0.00000013462357, Best Loss: 0.00000055562236 in Epoch 409
Epoch 453
Epoch 453, Loss: 0.00000206230443, Improvement: 0.00000091197556, Best Loss: 0.00000055562236 in Epoch 409
Epoch 454
Epoch 454, Loss: 0.00000193353877, Improvement: -0.00000012876566, Best Loss: 0.00000055562236 in Epoch 409
Epoch 455
Epoch 455, Loss: 0.00000131243405, Improvement: -0.00000062110472, Best Loss: 0.00000055562236 in Epoch 409
Epoch 456
A best model at epoch 456 has been saved with training error 0.00000054585018.
Epoch 456, Loss: 0.00000110195214, Improvement: -0.00000021048190, Best Loss: 0.00000054585018 in Epoch 456
Epoch 457
Epoch 457, Loss: 0.00000093293156, Improvement: -0.00000016902059, Best Loss: 0.00000054585018 in Epoch 456
Epoch 458
Epoch 458, Loss: 0.00000097779170, Improvement: 0.00000004486014, Best Loss: 0.00000054585018 in Epoch 456
Epoch 459
Epoch 459, Loss: 0.00000134904558, Improvement: 0.00000037125388, Best Loss: 0.00000054585018 in Epoch 456
Epoch 460
Epoch 460, Loss: 0.00000120033332, Improvement: -0.00000014871226, Best Loss: 0.00000054585018 in Epoch 456
Epoch 461
Epoch 461, Loss: 0.00000126308968, Improvement: 0.00000006275637, Best Loss: 0.00000054585018 in Epoch 456
Epoch 462
Epoch 462, Loss: 0.00000109978447, Improvement: -0.00000016330521, Best Loss: 0.00000054585018 in Epoch 456
Epoch 463
Epoch 463, Loss: 0.00000150026068, Improvement: 0.00000040047621, Best Loss: 0.00000054585018 in Epoch 456
Epoch 464
Epoch 464, Loss: 0.00000149494065, Improvement: -0.00000000532002, Best Loss: 0.00000054585018 in Epoch 456
Epoch 465
Epoch 465, Loss: 0.00000203283872, Improvement: 0.00000053789807, Best Loss: 0.00000054585018 in Epoch 456
Epoch 466
Epoch 466, Loss: 0.00000189864385, Improvement: -0.00000013419487, Best Loss: 0.00000054585018 in Epoch 456
Epoch 467
Epoch 467, Loss: 0.00000203806252, Improvement: 0.00000013941867, Best Loss: 0.00000054585018 in Epoch 456
Epoch 468
Epoch 468, Loss: 0.00000215238240, Improvement: 0.00000011431988, Best Loss: 0.00000054585018 in Epoch 456
Epoch 469
Epoch 469, Loss: 0.00000147015465, Improvement: -0.00000068222775, Best Loss: 0.00000054585018 in Epoch 456
Epoch 470
Epoch 470, Loss: 0.00000110864588, Improvement: -0.00000036150876, Best Loss: 0.00000054585018 in Epoch 456
Epoch 471
Epoch 471, Loss: 0.00000094979686, Improvement: -0.00000015884902, Best Loss: 0.00000054585018 in Epoch 456
Epoch 472
Epoch 472, Loss: 0.00000093146365, Improvement: -0.00000001833321, Best Loss: 0.00000054585018 in Epoch 456
Epoch 473
Epoch 473, Loss: 0.00000084828873, Improvement: -0.00000008317492, Best Loss: 0.00000054585018 in Epoch 456
Epoch 474
Epoch 474, Loss: 0.00000154146609, Improvement: 0.00000069317736, Best Loss: 0.00000054585018 in Epoch 456
Epoch 475
Epoch 475, Loss: 0.00000104540017, Improvement: -0.00000049606593, Best Loss: 0.00000054585018 in Epoch 456
Epoch 476
Epoch 476, Loss: 0.00000100766867, Improvement: -0.00000003773150, Best Loss: 0.00000054585018 in Epoch 456
Epoch 477
Epoch 477, Loss: 0.00000095779171, Improvement: -0.00000004987695, Best Loss: 0.00000054585018 in Epoch 456
Epoch 478
Epoch 478, Loss: 0.00000077113360, Improvement: -0.00000018665811, Best Loss: 0.00000054585018 in Epoch 456
Epoch 479
A best model at epoch 479 has been saved with training error 0.00000054432280.
Epoch 479, Loss: 0.00000082991930, Improvement: 0.00000005878570, Best Loss: 0.00000054432280 in Epoch 479
Epoch 480
A best model at epoch 480 has been saved with training error 0.00000053345894.
Epoch 480, Loss: 0.00000076331079, Improvement: -0.00000006660852, Best Loss: 0.00000053345894 in Epoch 480
Epoch 481
Epoch 481, Loss: 0.00000081065701, Improvement: 0.00000004734622, Best Loss: 0.00000053345894 in Epoch 480
Epoch 482
Epoch 482, Loss: 0.00000109306273, Improvement: 0.00000028240572, Best Loss: 0.00000053345894 in Epoch 480
Epoch 483
Epoch 483, Loss: 0.00000107206375, Improvement: -0.00000002099898, Best Loss: 0.00000053345894 in Epoch 480
Epoch 484
Epoch 484, Loss: 0.00000110926424, Improvement: 0.00000003720049, Best Loss: 0.00000053345894 in Epoch 480
Epoch 485
Epoch 485, Loss: 0.00000089470895, Improvement: -0.00000021455529, Best Loss: 0.00000053345894 in Epoch 480
Epoch 486
A best model at epoch 486 has been saved with training error 0.00000046674728.
Epoch 486, Loss: 0.00000090353131, Improvement: 0.00000000882236, Best Loss: 0.00000046674728 in Epoch 486
Epoch 487
Epoch 487, Loss: 0.00000081748350, Improvement: -0.00000008604781, Best Loss: 0.00000046674728 in Epoch 486
Epoch 488
Epoch 488, Loss: 0.00000079951686, Improvement: -0.00000001796664, Best Loss: 0.00000046674728 in Epoch 486
Epoch 489
Epoch 489, Loss: 0.00000082732014, Improvement: 0.00000002780327, Best Loss: 0.00000046674728 in Epoch 486
Epoch 490
Epoch 490, Loss: 0.00000113282352, Improvement: 0.00000030550339, Best Loss: 0.00000046674728 in Epoch 486
Epoch 491
Epoch 491, Loss: 0.00000155524612, Improvement: 0.00000042242260, Best Loss: 0.00000046674728 in Epoch 486
Epoch 492
Epoch 492, Loss: 0.00000130253798, Improvement: -0.00000025270814, Best Loss: 0.00000046674728 in Epoch 486
Epoch 493
Epoch 493, Loss: 0.00000155060411, Improvement: 0.00000024806613, Best Loss: 0.00000046674728 in Epoch 486
Epoch 494
Epoch 494, Loss: 0.00000173621719, Improvement: 0.00000018561308, Best Loss: 0.00000046674728 in Epoch 486
Epoch 495
Epoch 495, Loss: 0.00000166886046, Improvement: -0.00000006735673, Best Loss: 0.00000046674728 in Epoch 486
Epoch 496
Epoch 496, Loss: 0.00000183829724, Improvement: 0.00000016943677, Best Loss: 0.00000046674728 in Epoch 486
Epoch 497
Epoch 497, Loss: 0.00000268905523, Improvement: 0.00000085075799, Best Loss: 0.00000046674728 in Epoch 486
Epoch 498
Epoch 498, Loss: 0.00000160128212, Improvement: -0.00000108777311, Best Loss: 0.00000046674728 in Epoch 486
Epoch 499
Epoch 499, Loss: 0.00000110263357, Improvement: -0.00000049864855, Best Loss: 0.00000046674728 in Epoch 486
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000082409533, Improvement: -0.00000027853824, Best Loss: 0.00000046674728 in Epoch 486
Epoch 501
Epoch 501, Loss: 0.00000068919415, Improvement: -0.00000013490118, Best Loss: 0.00000046674728 in Epoch 486
Epoch 502
Epoch 502, Loss: 0.00000064933439, Improvement: -0.00000003985976, Best Loss: 0.00000046674728 in Epoch 486
Epoch 503
A best model at epoch 503 has been saved with training error 0.00000046308023.
Epoch 503, Loss: 0.00000065883352, Improvement: 0.00000000949913, Best Loss: 0.00000046308023 in Epoch 503
Epoch 504
A best model at epoch 504 has been saved with training error 0.00000046048817.
Epoch 504, Loss: 0.00000066166247, Improvement: 0.00000000282894, Best Loss: 0.00000046048817 in Epoch 504
Epoch 505
Epoch 505, Loss: 0.00000065121664, Improvement: -0.00000001044583, Best Loss: 0.00000046048817 in Epoch 504
Epoch 506
A best model at epoch 506 has been saved with training error 0.00000036701277.
Epoch 506, Loss: 0.00000062979165, Improvement: -0.00000002142499, Best Loss: 0.00000036701277 in Epoch 506
Epoch 507
Epoch 507, Loss: 0.00000069251154, Improvement: 0.00000006271989, Best Loss: 0.00000036701277 in Epoch 506
Epoch 508
Epoch 508, Loss: 0.00000067829534, Improvement: -0.00000001421620, Best Loss: 0.00000036701277 in Epoch 506
Epoch 509
Epoch 509, Loss: 0.00000062403150, Improvement: -0.00000005426384, Best Loss: 0.00000036701277 in Epoch 506
Epoch 510
Epoch 510, Loss: 0.00000062022416, Improvement: -0.00000000380735, Best Loss: 0.00000036701277 in Epoch 506
Epoch 511
Epoch 511, Loss: 0.00000071622491, Improvement: 0.00000009600075, Best Loss: 0.00000036701277 in Epoch 506
Epoch 512
Epoch 512, Loss: 0.00000080821887, Improvement: 0.00000009199396, Best Loss: 0.00000036701277 in Epoch 506
Epoch 513
Epoch 513, Loss: 0.00000064895328, Improvement: -0.00000015926559, Best Loss: 0.00000036701277 in Epoch 506
Epoch 514
Epoch 514, Loss: 0.00000063662577, Improvement: -0.00000001232751, Best Loss: 0.00000036701277 in Epoch 506
Epoch 515
Epoch 515, Loss: 0.00000072514422, Improvement: 0.00000008851846, Best Loss: 0.00000036701277 in Epoch 506
Epoch 516
Epoch 516, Loss: 0.00000141901354, Improvement: 0.00000069386932, Best Loss: 0.00000036701277 in Epoch 506
Epoch 517
Epoch 517, Loss: 0.00000092534094, Improvement: -0.00000049367260, Best Loss: 0.00000036701277 in Epoch 506
Epoch 518
Epoch 518, Loss: 0.00000168094081, Improvement: 0.00000075559987, Best Loss: 0.00000036701277 in Epoch 506
Epoch 519
Epoch 519, Loss: 0.00000208793130, Improvement: 0.00000040699048, Best Loss: 0.00000036701277 in Epoch 506
Epoch 520
Epoch 520, Loss: 0.00000140814857, Improvement: -0.00000067978273, Best Loss: 0.00000036701277 in Epoch 506
Epoch 521
Epoch 521, Loss: 0.00000099765393, Improvement: -0.00000041049464, Best Loss: 0.00000036701277 in Epoch 506
Epoch 522
Epoch 522, Loss: 0.00000072096714, Improvement: -0.00000027668679, Best Loss: 0.00000036701277 in Epoch 506
Epoch 523
Epoch 523, Loss: 0.00000077485365, Improvement: 0.00000005388652, Best Loss: 0.00000036701277 in Epoch 506
Epoch 524
Epoch 524, Loss: 0.00000080199378, Improvement: 0.00000002714012, Best Loss: 0.00000036701277 in Epoch 506
Epoch 525
Epoch 525, Loss: 0.00000085729993, Improvement: 0.00000005530615, Best Loss: 0.00000036701277 in Epoch 506
Epoch 526
Epoch 526, Loss: 0.00000074539086, Improvement: -0.00000011190906, Best Loss: 0.00000036701277 in Epoch 506
Epoch 527
Epoch 527, Loss: 0.00000067876943, Improvement: -0.00000006662143, Best Loss: 0.00000036701277 in Epoch 506
Epoch 528
Epoch 528, Loss: 0.00000068399507, Improvement: 0.00000000522564, Best Loss: 0.00000036701277 in Epoch 506
Epoch 529
Epoch 529, Loss: 0.00000065801446, Improvement: -0.00000002598061, Best Loss: 0.00000036701277 in Epoch 506
Epoch 530
Epoch 530, Loss: 0.00000063239468, Improvement: -0.00000002561978, Best Loss: 0.00000036701277 in Epoch 506
Epoch 531
Epoch 531, Loss: 0.00000064008789, Improvement: 0.00000000769321, Best Loss: 0.00000036701277 in Epoch 506
Epoch 532
Epoch 532, Loss: 0.00000061627639, Improvement: -0.00000002381150, Best Loss: 0.00000036701277 in Epoch 506
Epoch 533
Epoch 533, Loss: 0.00000062251407, Improvement: 0.00000000623768, Best Loss: 0.00000036701277 in Epoch 506
Epoch 534
Epoch 534, Loss: 0.00000062245465, Improvement: -0.00000000005942, Best Loss: 0.00000036701277 in Epoch 506
Epoch 535
Epoch 535, Loss: 0.00000069554100, Improvement: 0.00000007308635, Best Loss: 0.00000036701277 in Epoch 506
Epoch 536
Epoch 536, Loss: 0.00000076980236, Improvement: 0.00000007426135, Best Loss: 0.00000036701277 in Epoch 506
Epoch 537
Epoch 537, Loss: 0.00000197799795, Improvement: 0.00000120819560, Best Loss: 0.00000036701277 in Epoch 506
Epoch 538
Epoch 538, Loss: 0.00000146535638, Improvement: -0.00000051264157, Best Loss: 0.00000036701277 in Epoch 506
Epoch 539
Epoch 539, Loss: 0.00000077037924, Improvement: -0.00000069497713, Best Loss: 0.00000036701277 in Epoch 506
Epoch 540
Epoch 540, Loss: 0.00000092302654, Improvement: 0.00000015264729, Best Loss: 0.00000036701277 in Epoch 506
Epoch 541
Epoch 541, Loss: 0.00000077952606, Improvement: -0.00000014350047, Best Loss: 0.00000036701277 in Epoch 506
Epoch 542
Epoch 542, Loss: 0.00000067074245, Improvement: -0.00000010878361, Best Loss: 0.00000036701277 in Epoch 506
Epoch 543
Epoch 543, Loss: 0.00000068339377, Improvement: 0.00000001265132, Best Loss: 0.00000036701277 in Epoch 506
Epoch 544
Epoch 544, Loss: 0.00000071688631, Improvement: 0.00000003349254, Best Loss: 0.00000036701277 in Epoch 506
Epoch 545
A best model at epoch 545 has been saved with training error 0.00000032745078.
Epoch 545, Loss: 0.00000085624252, Improvement: 0.00000013935621, Best Loss: 0.00000032745078 in Epoch 545
Epoch 546
Epoch 546, Loss: 0.00000077565094, Improvement: -0.00000008059158, Best Loss: 0.00000032745078 in Epoch 545
Epoch 547
Epoch 547, Loss: 0.00000088189670, Improvement: 0.00000010624576, Best Loss: 0.00000032745078 in Epoch 545
Epoch 548
Epoch 548, Loss: 0.00000074709473, Improvement: -0.00000013480197, Best Loss: 0.00000032745078 in Epoch 545
Epoch 549
Epoch 549, Loss: 0.00000113138883, Improvement: 0.00000038429410, Best Loss: 0.00000032745078 in Epoch 545
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000108169619, Improvement: -0.00000004969264, Best Loss: 0.00000032745078 in Epoch 545
Epoch 551
Epoch 551, Loss: 0.00000084571366, Improvement: -0.00000023598253, Best Loss: 0.00000032745078 in Epoch 545
Epoch 552
Epoch 552, Loss: 0.00000071331746, Improvement: -0.00000013239620, Best Loss: 0.00000032745078 in Epoch 545
Epoch 553
Epoch 553, Loss: 0.00000070070244, Improvement: -0.00000001261502, Best Loss: 0.00000032745078 in Epoch 545
Epoch 554
Epoch 554, Loss: 0.00000080290787, Improvement: 0.00000010220543, Best Loss: 0.00000032745078 in Epoch 545
Epoch 555
Epoch 555, Loss: 0.00000067175620, Improvement: -0.00000013115167, Best Loss: 0.00000032745078 in Epoch 545
Epoch 556
Epoch 556, Loss: 0.00000066213461, Improvement: -0.00000000962159, Best Loss: 0.00000032745078 in Epoch 545
Epoch 557
Epoch 557, Loss: 0.00000066486247, Improvement: 0.00000000272786, Best Loss: 0.00000032745078 in Epoch 545
Epoch 558
Epoch 558, Loss: 0.00000086560445, Improvement: 0.00000020074198, Best Loss: 0.00000032745078 in Epoch 545
Epoch 559
Epoch 559, Loss: 0.00000126795359, Improvement: 0.00000040234914, Best Loss: 0.00000032745078 in Epoch 545
Epoch 560
Epoch 560, Loss: 0.00000103506421, Improvement: -0.00000023288938, Best Loss: 0.00000032745078 in Epoch 545
Epoch 561
Epoch 561, Loss: 0.00000139838278, Improvement: 0.00000036331857, Best Loss: 0.00000032745078 in Epoch 545
Epoch 562
Epoch 562, Loss: 0.00000147420552, Improvement: 0.00000007582273, Best Loss: 0.00000032745078 in Epoch 545
Epoch 563
Epoch 563, Loss: 0.00000129197813, Improvement: -0.00000018222739, Best Loss: 0.00000032745078 in Epoch 545
Epoch 564
Epoch 564, Loss: 0.00000086075101, Improvement: -0.00000043122712, Best Loss: 0.00000032745078 in Epoch 545
Epoch 565
Epoch 565, Loss: 0.00000077767394, Improvement: -0.00000008307708, Best Loss: 0.00000032745078 in Epoch 545
Epoch 566
Epoch 566, Loss: 0.00000060428733, Improvement: -0.00000017338661, Best Loss: 0.00000032745078 in Epoch 545
Epoch 567
Epoch 567, Loss: 0.00000060692096, Improvement: 0.00000000263363, Best Loss: 0.00000032745078 in Epoch 545
Epoch 568
Epoch 568, Loss: 0.00000056193874, Improvement: -0.00000004498221, Best Loss: 0.00000032745078 in Epoch 545
Epoch 569
Epoch 569, Loss: 0.00000055171914, Improvement: -0.00000001021960, Best Loss: 0.00000032745078 in Epoch 545
Epoch 570
Epoch 570, Loss: 0.00000055049436, Improvement: -0.00000000122478, Best Loss: 0.00000032745078 in Epoch 545
Epoch 571
Epoch 571, Loss: 0.00000055298362, Improvement: 0.00000000248926, Best Loss: 0.00000032745078 in Epoch 545
Epoch 572
Epoch 572, Loss: 0.00000054191184, Improvement: -0.00000001107178, Best Loss: 0.00000032745078 in Epoch 545
Epoch 573
Epoch 573, Loss: 0.00000056395874, Improvement: 0.00000002204690, Best Loss: 0.00000032745078 in Epoch 545
Epoch 574
Epoch 574, Loss: 0.00000059784928, Improvement: 0.00000003389054, Best Loss: 0.00000032745078 in Epoch 545
Epoch 575
Epoch 575, Loss: 0.00000058784611, Improvement: -0.00000001000318, Best Loss: 0.00000032745078 in Epoch 545
Epoch 576
Epoch 576, Loss: 0.00000060358362, Improvement: 0.00000001573752, Best Loss: 0.00000032745078 in Epoch 545
Epoch 577
Epoch 577, Loss: 0.00000059198557, Improvement: -0.00000001159805, Best Loss: 0.00000032745078 in Epoch 545
Epoch 578
Epoch 578, Loss: 0.00000071157427, Improvement: 0.00000011958870, Best Loss: 0.00000032745078 in Epoch 545
Epoch 579
Epoch 579, Loss: 0.00000086319056, Improvement: 0.00000015161629, Best Loss: 0.00000032745078 in Epoch 545
Epoch 580
Epoch 580, Loss: 0.00000132018937, Improvement: 0.00000045699881, Best Loss: 0.00000032745078 in Epoch 545
Epoch 581
Epoch 581, Loss: 0.00000119461319, Improvement: -0.00000012557618, Best Loss: 0.00000032745078 in Epoch 545
Epoch 582
Epoch 582, Loss: 0.00000084864080, Improvement: -0.00000034597240, Best Loss: 0.00000032745078 in Epoch 545
Epoch 583
Epoch 583, Loss: 0.00000074078487, Improvement: -0.00000010785592, Best Loss: 0.00000032745078 in Epoch 545
Epoch 584
Epoch 584, Loss: 0.00000089035977, Improvement: 0.00000014957489, Best Loss: 0.00000032745078 in Epoch 545
Epoch 585
Epoch 585, Loss: 0.00000103275285, Improvement: 0.00000014239309, Best Loss: 0.00000032745078 in Epoch 545
Epoch 586
Epoch 586, Loss: 0.00000129613463, Improvement: 0.00000026338177, Best Loss: 0.00000032745078 in Epoch 545
Epoch 587
Epoch 587, Loss: 0.00000112612333, Improvement: -0.00000017001129, Best Loss: 0.00000032745078 in Epoch 545
Epoch 588
Epoch 588, Loss: 0.00000071883502, Improvement: -0.00000040728832, Best Loss: 0.00000032745078 in Epoch 545
Epoch 589
Epoch 589, Loss: 0.00000068213074, Improvement: -0.00000003670427, Best Loss: 0.00000032745078 in Epoch 545
Epoch 590
Epoch 590, Loss: 0.00000082892256, Improvement: 0.00000014679182, Best Loss: 0.00000032745078 in Epoch 545
Epoch 591
Epoch 591, Loss: 0.00000076297045, Improvement: -0.00000006595211, Best Loss: 0.00000032745078 in Epoch 545
Epoch 592
Epoch 592, Loss: 0.00000075059719, Improvement: -0.00000001237326, Best Loss: 0.00000032745078 in Epoch 545
Epoch 593
Epoch 593, Loss: 0.00000121689673, Improvement: 0.00000046629955, Best Loss: 0.00000032745078 in Epoch 545
Epoch 594
Epoch 594, Loss: 0.00000160547518, Improvement: 0.00000038857845, Best Loss: 0.00000032745078 in Epoch 545
Epoch 595
Epoch 595, Loss: 0.00000118806346, Improvement: -0.00000041741172, Best Loss: 0.00000032745078 in Epoch 545
Epoch 596
Epoch 596, Loss: 0.00000105377333, Improvement: -0.00000013429013, Best Loss: 0.00000032745078 in Epoch 545
Epoch 597
Epoch 597, Loss: 0.00000090715647, Improvement: -0.00000014661686, Best Loss: 0.00000032745078 in Epoch 545
Epoch 598
Epoch 598, Loss: 0.00000066611541, Improvement: -0.00000024104106, Best Loss: 0.00000032745078 in Epoch 545
Epoch 599
Epoch 599, Loss: 0.00000058928943, Improvement: -0.00000007682598, Best Loss: 0.00000032745078 in Epoch 545
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000052244930, Improvement: -0.00000006684013, Best Loss: 0.00000032745078 in Epoch 545
Epoch 601
Epoch 601, Loss: 0.00000052001555, Improvement: -0.00000000243375, Best Loss: 0.00000032745078 in Epoch 545
Epoch 602
Epoch 602, Loss: 0.00000052808650, Improvement: 0.00000000807095, Best Loss: 0.00000032745078 in Epoch 545
Epoch 603
Epoch 603, Loss: 0.00000052984172, Improvement: 0.00000000175521, Best Loss: 0.00000032745078 in Epoch 545
Epoch 604
Epoch 604, Loss: 0.00000054993707, Improvement: 0.00000002009535, Best Loss: 0.00000032745078 in Epoch 545
Epoch 605
Epoch 605, Loss: 0.00000071090828, Improvement: 0.00000016097122, Best Loss: 0.00000032745078 in Epoch 545
Epoch 606
Epoch 606, Loss: 0.00000100658520, Improvement: 0.00000029567692, Best Loss: 0.00000032745078 in Epoch 545
Epoch 607
Epoch 607, Loss: 0.00000099471144, Improvement: -0.00000001187376, Best Loss: 0.00000032745078 in Epoch 545
Epoch 608
Epoch 608, Loss: 0.00000061728757, Improvement: -0.00000037742387, Best Loss: 0.00000032745078 in Epoch 545
Epoch 609
Epoch 609, Loss: 0.00000055398397, Improvement: -0.00000006330359, Best Loss: 0.00000032745078 in Epoch 545
Epoch 610
Epoch 610, Loss: 0.00000057025683, Improvement: 0.00000001627285, Best Loss: 0.00000032745078 in Epoch 545
Epoch 611
Epoch 611, Loss: 0.00000056865519, Improvement: -0.00000000160164, Best Loss: 0.00000032745078 in Epoch 545
Epoch 612
Epoch 612, Loss: 0.00000050473021, Improvement: -0.00000006392498, Best Loss: 0.00000032745078 in Epoch 545
Epoch 613
Epoch 613, Loss: 0.00000055663355, Improvement: 0.00000005190335, Best Loss: 0.00000032745078 in Epoch 545
Epoch 614
Epoch 614, Loss: 0.00000057231315, Improvement: 0.00000001567959, Best Loss: 0.00000032745078 in Epoch 545
Epoch 615
Epoch 615, Loss: 0.00000057220120, Improvement: -0.00000000011195, Best Loss: 0.00000032745078 in Epoch 545
Epoch 616
Epoch 616, Loss: 0.00000064504358, Improvement: 0.00000007284238, Best Loss: 0.00000032745078 in Epoch 545
Epoch 617
Epoch 617, Loss: 0.00000059986662, Improvement: -0.00000004517696, Best Loss: 0.00000032745078 in Epoch 545
Epoch 618
Epoch 618, Loss: 0.00000058813777, Improvement: -0.00000001172884, Best Loss: 0.00000032745078 in Epoch 545
Epoch 619
Epoch 619, Loss: 0.00000085193239, Improvement: 0.00000026379462, Best Loss: 0.00000032745078 in Epoch 545
Epoch 620
Epoch 620, Loss: 0.00000097849986, Improvement: 0.00000012656746, Best Loss: 0.00000032745078 in Epoch 545
Epoch 621
Epoch 621, Loss: 0.00000074020819, Improvement: -0.00000023829167, Best Loss: 0.00000032745078 in Epoch 545
Epoch 622
Epoch 622, Loss: 0.00000127461291, Improvement: 0.00000053440473, Best Loss: 0.00000032745078 in Epoch 545
Epoch 623
Epoch 623, Loss: 0.00000115382508, Improvement: -0.00000012078784, Best Loss: 0.00000032745078 in Epoch 545
Epoch 624
Epoch 624, Loss: 0.00000070793709, Improvement: -0.00000044588799, Best Loss: 0.00000032745078 in Epoch 545
Epoch 625
Epoch 625, Loss: 0.00000057438916, Improvement: -0.00000013354793, Best Loss: 0.00000032745078 in Epoch 545
Epoch 626
Epoch 626, Loss: 0.00000055342837, Improvement: -0.00000002096080, Best Loss: 0.00000032745078 in Epoch 545
Epoch 627
Epoch 627, Loss: 0.00000048170499, Improvement: -0.00000007172337, Best Loss: 0.00000032745078 in Epoch 545
Epoch 628
Epoch 628, Loss: 0.00000049147104, Improvement: 0.00000000976605, Best Loss: 0.00000032745078 in Epoch 545
Epoch 629
Epoch 629, Loss: 0.00000047705550, Improvement: -0.00000001441555, Best Loss: 0.00000032745078 in Epoch 545
Epoch 630
Epoch 630, Loss: 0.00000048846019, Improvement: 0.00000001140469, Best Loss: 0.00000032745078 in Epoch 545
Epoch 631
Epoch 631, Loss: 0.00000047649489, Improvement: -0.00000001196530, Best Loss: 0.00000032745078 in Epoch 545
Epoch 632
Epoch 632, Loss: 0.00000047547857, Improvement: -0.00000000101632, Best Loss: 0.00000032745078 in Epoch 545
Epoch 633
Epoch 633, Loss: 0.00000050479334, Improvement: 0.00000002931477, Best Loss: 0.00000032745078 in Epoch 545
Epoch 634
Epoch 634, Loss: 0.00000078221613, Improvement: 0.00000027742279, Best Loss: 0.00000032745078 in Epoch 545
Epoch 635
Epoch 635, Loss: 0.00000072218013, Improvement: -0.00000006003600, Best Loss: 0.00000032745078 in Epoch 545
Epoch 636
Epoch 636, Loss: 0.00000073708827, Improvement: 0.00000001490814, Best Loss: 0.00000032745078 in Epoch 545
Epoch 637
A best model at epoch 637 has been saved with training error 0.00000030313197.
Epoch 637, Loss: 0.00000054237292, Improvement: -0.00000019471535, Best Loss: 0.00000030313197 in Epoch 637
Epoch 638
Epoch 638, Loss: 0.00000057177804, Improvement: 0.00000002940512, Best Loss: 0.00000030313197 in Epoch 637
Epoch 639
Epoch 639, Loss: 0.00000134855244, Improvement: 0.00000077677440, Best Loss: 0.00000030313197 in Epoch 637
Epoch 640
Epoch 640, Loss: 0.00000167355896, Improvement: 0.00000032500652, Best Loss: 0.00000030313197 in Epoch 637
Epoch 641
Epoch 641, Loss: 0.00000088332274, Improvement: -0.00000079023622, Best Loss: 0.00000030313197 in Epoch 637
Epoch 642
Epoch 642, Loss: 0.00000061227789, Improvement: -0.00000027104486, Best Loss: 0.00000030313197 in Epoch 637
Epoch 643
Epoch 643, Loss: 0.00000052441302, Improvement: -0.00000008786486, Best Loss: 0.00000030313197 in Epoch 637
Epoch 644
Epoch 644, Loss: 0.00000049921465, Improvement: -0.00000002519837, Best Loss: 0.00000030313197 in Epoch 637
Epoch 645
A best model at epoch 645 has been saved with training error 0.00000028792718.
Epoch 645, Loss: 0.00000047395613, Improvement: -0.00000002525852, Best Loss: 0.00000028792718 in Epoch 645
Epoch 646
Epoch 646, Loss: 0.00000047795274, Improvement: 0.00000000399661, Best Loss: 0.00000028792718 in Epoch 645
Epoch 647
Epoch 647, Loss: 0.00000046572595, Improvement: -0.00000001222680, Best Loss: 0.00000028792718 in Epoch 645
Epoch 648
Epoch 648, Loss: 0.00000045810712, Improvement: -0.00000000761883, Best Loss: 0.00000028792718 in Epoch 645
Epoch 649
Epoch 649, Loss: 0.00000044885391, Improvement: -0.00000000925320, Best Loss: 0.00000028792718 in Epoch 645
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000047326637, Improvement: 0.00000002441246, Best Loss: 0.00000028792718 in Epoch 645
Epoch 651
Epoch 651, Loss: 0.00000045585383, Improvement: -0.00000001741254, Best Loss: 0.00000028792718 in Epoch 645
Epoch 652
Epoch 652, Loss: 0.00000045779198, Improvement: 0.00000000193815, Best Loss: 0.00000028792718 in Epoch 645
Epoch 653
Epoch 653, Loss: 0.00000046111199, Improvement: 0.00000000332001, Best Loss: 0.00000028792718 in Epoch 645
Epoch 654
Epoch 654, Loss: 0.00000053855180, Improvement: 0.00000007743981, Best Loss: 0.00000028792718 in Epoch 645
Epoch 655
Epoch 655, Loss: 0.00000051680817, Improvement: -0.00000002174364, Best Loss: 0.00000028792718 in Epoch 645
Epoch 656
Epoch 656, Loss: 0.00000084404490, Improvement: 0.00000032723673, Best Loss: 0.00000028792718 in Epoch 645
Epoch 657
Epoch 657, Loss: 0.00000070357572, Improvement: -0.00000014046918, Best Loss: 0.00000028792718 in Epoch 645
Epoch 658
Epoch 658, Loss: 0.00000061895180, Improvement: -0.00000008462391, Best Loss: 0.00000028792718 in Epoch 645
Epoch 659
Epoch 659, Loss: 0.00000055282440, Improvement: -0.00000006612740, Best Loss: 0.00000028792718 in Epoch 645
Epoch 660
Epoch 660, Loss: 0.00000052052617, Improvement: -0.00000003229823, Best Loss: 0.00000028792718 in Epoch 645
Epoch 661
Epoch 661, Loss: 0.00000052852045, Improvement: 0.00000000799427, Best Loss: 0.00000028792718 in Epoch 645
Epoch 662
Epoch 662, Loss: 0.00000058855062, Improvement: 0.00000006003017, Best Loss: 0.00000028792718 in Epoch 645
Epoch 663
Epoch 663, Loss: 0.00000050749008, Improvement: -0.00000008106053, Best Loss: 0.00000028792718 in Epoch 645
Epoch 664
Epoch 664, Loss: 0.00000047605863, Improvement: -0.00000003143145, Best Loss: 0.00000028792718 in Epoch 645
Epoch 665
Epoch 665, Loss: 0.00000047480169, Improvement: -0.00000000125694, Best Loss: 0.00000028792718 in Epoch 645
Epoch 666
Epoch 666, Loss: 0.00000045734589, Improvement: -0.00000001745580, Best Loss: 0.00000028792718 in Epoch 645
Epoch 667
Epoch 667, Loss: 0.00000051961604, Improvement: 0.00000006227015, Best Loss: 0.00000028792718 in Epoch 645
Epoch 668
Epoch 668, Loss: 0.00000054390264, Improvement: 0.00000002428661, Best Loss: 0.00000028792718 in Epoch 645
Epoch 669
Epoch 669, Loss: 0.00000066095860, Improvement: 0.00000011705595, Best Loss: 0.00000028792718 in Epoch 645
Epoch 670
Epoch 670, Loss: 0.00000083182863, Improvement: 0.00000017087004, Best Loss: 0.00000028792718 in Epoch 645
Epoch 671
Epoch 671, Loss: 0.00000113913073, Improvement: 0.00000030730210, Best Loss: 0.00000028792718 in Epoch 645
Epoch 672
Epoch 672, Loss: 0.00000099938223, Improvement: -0.00000013974850, Best Loss: 0.00000028792718 in Epoch 645
Epoch 673
Epoch 673, Loss: 0.00000085227365, Improvement: -0.00000014710858, Best Loss: 0.00000028792718 in Epoch 645
Epoch 674
Epoch 674, Loss: 0.00000088577035, Improvement: 0.00000003349670, Best Loss: 0.00000028792718 in Epoch 645
Epoch 675
Epoch 675, Loss: 0.00000108503879, Improvement: 0.00000019926844, Best Loss: 0.00000028792718 in Epoch 645
Epoch 676
Epoch 676, Loss: 0.00000086334497, Improvement: -0.00000022169383, Best Loss: 0.00000028792718 in Epoch 645
Epoch 677
Epoch 677, Loss: 0.00000061373916, Improvement: -0.00000024960580, Best Loss: 0.00000028792718 in Epoch 645
Epoch 678
Epoch 678, Loss: 0.00000047178515, Improvement: -0.00000014195401, Best Loss: 0.00000028792718 in Epoch 645
Epoch 679
Epoch 679, Loss: 0.00000047444756, Improvement: 0.00000000266241, Best Loss: 0.00000028792718 in Epoch 645
Epoch 680
Epoch 680, Loss: 0.00000045656115, Improvement: -0.00000001788640, Best Loss: 0.00000028792718 in Epoch 645
Epoch 681
Epoch 681, Loss: 0.00000044618516, Improvement: -0.00000001037599, Best Loss: 0.00000028792718 in Epoch 645
Epoch 682
Epoch 682, Loss: 0.00000044807812, Improvement: 0.00000000189296, Best Loss: 0.00000028792718 in Epoch 645
Epoch 683
Epoch 683, Loss: 0.00000047547978, Improvement: 0.00000002740166, Best Loss: 0.00000028792718 in Epoch 645
Epoch 684
Epoch 684, Loss: 0.00000058453893, Improvement: 0.00000010905915, Best Loss: 0.00000028792718 in Epoch 645
Epoch 685
Epoch 685, Loss: 0.00000059932357, Improvement: 0.00000001478464, Best Loss: 0.00000028792718 in Epoch 645
Epoch 686
Epoch 686, Loss: 0.00000062229463, Improvement: 0.00000002297106, Best Loss: 0.00000028792718 in Epoch 645
Epoch 687
Epoch 687, Loss: 0.00000052405179, Improvement: -0.00000009824284, Best Loss: 0.00000028792718 in Epoch 645
Epoch 688
Epoch 688, Loss: 0.00000051644511, Improvement: -0.00000000760669, Best Loss: 0.00000028792718 in Epoch 645
Epoch 689
Epoch 689, Loss: 0.00000058902180, Improvement: 0.00000007257670, Best Loss: 0.00000028792718 in Epoch 645
Epoch 690
Epoch 690, Loss: 0.00000132402093, Improvement: 0.00000073499913, Best Loss: 0.00000028792718 in Epoch 645
Epoch 691
Epoch 691, Loss: 0.00000160199650, Improvement: 0.00000027797557, Best Loss: 0.00000028792718 in Epoch 645
Epoch 692
Epoch 692, Loss: 0.00000079423453, Improvement: -0.00000080776197, Best Loss: 0.00000028792718 in Epoch 645
Epoch 693
Epoch 693, Loss: 0.00000062035079, Improvement: -0.00000017388374, Best Loss: 0.00000028792718 in Epoch 645
Epoch 694
Epoch 694, Loss: 0.00000051531367, Improvement: -0.00000010503712, Best Loss: 0.00000028792718 in Epoch 645
Epoch 695
Epoch 695, Loss: 0.00000054584586, Improvement: 0.00000003053219, Best Loss: 0.00000028792718 in Epoch 645
Epoch 696
Epoch 696, Loss: 0.00000061266177, Improvement: 0.00000006681592, Best Loss: 0.00000028792718 in Epoch 645
Epoch 697
Epoch 697, Loss: 0.00000053384125, Improvement: -0.00000007882052, Best Loss: 0.00000028792718 in Epoch 645
Epoch 698
Epoch 698, Loss: 0.00000054707259, Improvement: 0.00000001323134, Best Loss: 0.00000028792718 in Epoch 645
Epoch 699
Epoch 699, Loss: 0.00000058610178, Improvement: 0.00000003902919, Best Loss: 0.00000028792718 in Epoch 645
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000053923279, Improvement: -0.00000004686899, Best Loss: 0.00000028792718 in Epoch 645
Epoch 701
Epoch 701, Loss: 0.00000083756562, Improvement: 0.00000029833282, Best Loss: 0.00000028792718 in Epoch 645
Epoch 702
Epoch 702, Loss: 0.00000068107945, Improvement: -0.00000015648617, Best Loss: 0.00000028792718 in Epoch 645
Epoch 703
Epoch 703, Loss: 0.00000085512478, Improvement: 0.00000017404533, Best Loss: 0.00000028792718 in Epoch 645
Epoch 704
Epoch 704, Loss: 0.00000059631371, Improvement: -0.00000025881107, Best Loss: 0.00000028792718 in Epoch 645
Epoch 705
Epoch 705, Loss: 0.00000046242272, Improvement: -0.00000013389099, Best Loss: 0.00000028792718 in Epoch 645
Epoch 706
Epoch 706, Loss: 0.00000045908852, Improvement: -0.00000000333419, Best Loss: 0.00000028792718 in Epoch 645
Epoch 707
Epoch 707, Loss: 0.00000045550305, Improvement: -0.00000000358548, Best Loss: 0.00000028792718 in Epoch 645
Epoch 708
Epoch 708, Loss: 0.00000044695745, Improvement: -0.00000000854560, Best Loss: 0.00000028792718 in Epoch 645
Epoch 709
Epoch 709, Loss: 0.00000045223682, Improvement: 0.00000000527937, Best Loss: 0.00000028792718 in Epoch 645
Epoch 710
Epoch 710, Loss: 0.00000050292303, Improvement: 0.00000005068621, Best Loss: 0.00000028792718 in Epoch 645
Epoch 711
Epoch 711, Loss: 0.00000091337081, Improvement: 0.00000041044778, Best Loss: 0.00000028792718 in Epoch 645
Epoch 712
Epoch 712, Loss: 0.00000079151659, Improvement: -0.00000012185422, Best Loss: 0.00000028792718 in Epoch 645
Epoch 713
Epoch 713, Loss: 0.00000075526002, Improvement: -0.00000003625657, Best Loss: 0.00000028792718 in Epoch 645
Epoch 714
Epoch 714, Loss: 0.00000077276398, Improvement: 0.00000001750396, Best Loss: 0.00000028792718 in Epoch 645
Epoch 715
Epoch 715, Loss: 0.00000064055825, Improvement: -0.00000013220573, Best Loss: 0.00000028792718 in Epoch 645
Epoch 716
Epoch 716, Loss: 0.00000053348213, Improvement: -0.00000010707612, Best Loss: 0.00000028792718 in Epoch 645
Epoch 717
Epoch 717, Loss: 0.00000049853225, Improvement: -0.00000003494987, Best Loss: 0.00000028792718 in Epoch 645
Epoch 718
Epoch 718, Loss: 0.00000048358625, Improvement: -0.00000001494601, Best Loss: 0.00000028792718 in Epoch 645
Epoch 719
Epoch 719, Loss: 0.00000041867333, Improvement: -0.00000006491292, Best Loss: 0.00000028792718 in Epoch 645
Epoch 720
A best model at epoch 720 has been saved with training error 0.00000028118217.
Epoch 720, Loss: 0.00000041005443, Improvement: -0.00000000861890, Best Loss: 0.00000028118217 in Epoch 720
Epoch 721
Epoch 721, Loss: 0.00000039962983, Improvement: -0.00000001042460, Best Loss: 0.00000028118217 in Epoch 720
Epoch 722
Epoch 722, Loss: 0.00000040020356, Improvement: 0.00000000057373, Best Loss: 0.00000028118217 in Epoch 720
Epoch 723
A best model at epoch 723 has been saved with training error 0.00000025728011.
Epoch 723, Loss: 0.00000047374591, Improvement: 0.00000007354235, Best Loss: 0.00000025728011 in Epoch 723
Epoch 724
Epoch 724, Loss: 0.00000061029011, Improvement: 0.00000013654420, Best Loss: 0.00000025728011 in Epoch 723
Epoch 725
Epoch 725, Loss: 0.00000064876636, Improvement: 0.00000003847625, Best Loss: 0.00000025728011 in Epoch 723
Epoch 726
Epoch 726, Loss: 0.00000113455509, Improvement: 0.00000048578872, Best Loss: 0.00000025728011 in Epoch 723
Epoch 727
Epoch 727, Loss: 0.00000147737717, Improvement: 0.00000034282209, Best Loss: 0.00000025728011 in Epoch 723
Epoch 728
Epoch 728, Loss: 0.00000089367395, Improvement: -0.00000058370322, Best Loss: 0.00000025728011 in Epoch 723
Epoch 729
Epoch 729, Loss: 0.00000064135573, Improvement: -0.00000025231823, Best Loss: 0.00000025728011 in Epoch 723
Epoch 730
Epoch 730, Loss: 0.00000060094208, Improvement: -0.00000004041364, Best Loss: 0.00000025728011 in Epoch 723
Epoch 731
Epoch 731, Loss: 0.00000045410166, Improvement: -0.00000014684042, Best Loss: 0.00000025728011 in Epoch 723
Epoch 732
Epoch 732, Loss: 0.00000041587574, Improvement: -0.00000003822592, Best Loss: 0.00000025728011 in Epoch 723
Epoch 733
A best model at epoch 733 has been saved with training error 0.00000025096332.
Epoch 733, Loss: 0.00000042515051, Improvement: 0.00000000927476, Best Loss: 0.00000025096332 in Epoch 733
Epoch 734
Epoch 734, Loss: 0.00000041426506, Improvement: -0.00000001088544, Best Loss: 0.00000025096332 in Epoch 733
Epoch 735
Epoch 735, Loss: 0.00000045931987, Improvement: 0.00000004505480, Best Loss: 0.00000025096332 in Epoch 733
Epoch 736
Epoch 736, Loss: 0.00000071419536, Improvement: 0.00000025487549, Best Loss: 0.00000025096332 in Epoch 733
Epoch 737
Epoch 737, Loss: 0.00000085825740, Improvement: 0.00000014406204, Best Loss: 0.00000025096332 in Epoch 733
Epoch 738
Epoch 738, Loss: 0.00000057838478, Improvement: -0.00000027987263, Best Loss: 0.00000025096332 in Epoch 733
Epoch 739
Epoch 739, Loss: 0.00000058041948, Improvement: 0.00000000203470, Best Loss: 0.00000025096332 in Epoch 733
Epoch 740
Epoch 740, Loss: 0.00000049109185, Improvement: -0.00000008932762, Best Loss: 0.00000025096332 in Epoch 733
Epoch 741
Epoch 741, Loss: 0.00000051171952, Improvement: 0.00000002062767, Best Loss: 0.00000025096332 in Epoch 733
Epoch 742
Epoch 742, Loss: 0.00000049557225, Improvement: -0.00000001614727, Best Loss: 0.00000025096332 in Epoch 733
Epoch 743
Epoch 743, Loss: 0.00000051754536, Improvement: 0.00000002197311, Best Loss: 0.00000025096332 in Epoch 733
Epoch 744
A best model at epoch 744 has been saved with training error 0.00000024635588.
Epoch 744, Loss: 0.00000044526136, Improvement: -0.00000007228400, Best Loss: 0.00000024635588 in Epoch 744
Epoch 745
Epoch 745, Loss: 0.00000042584765, Improvement: -0.00000001941370, Best Loss: 0.00000024635588 in Epoch 744
Epoch 746
Epoch 746, Loss: 0.00000043566927, Improvement: 0.00000000982162, Best Loss: 0.00000024635588 in Epoch 744
Epoch 747
Epoch 747, Loss: 0.00000046224462, Improvement: 0.00000002657535, Best Loss: 0.00000024635588 in Epoch 744
Epoch 748
Epoch 748, Loss: 0.00000041665976, Improvement: -0.00000004558486, Best Loss: 0.00000024635588 in Epoch 744
Epoch 749
Epoch 749, Loss: 0.00000042751278, Improvement: 0.00000001085303, Best Loss: 0.00000024635588 in Epoch 744
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000042786091, Improvement: 0.00000000034813, Best Loss: 0.00000024635588 in Epoch 744
Epoch 751
Epoch 751, Loss: 0.00000047270966, Improvement: 0.00000004484874, Best Loss: 0.00000024635588 in Epoch 744
Epoch 752
Epoch 752, Loss: 0.00000050406153, Improvement: 0.00000003135187, Best Loss: 0.00000024635588 in Epoch 744
Epoch 753
Epoch 753, Loss: 0.00000063284462, Improvement: 0.00000012878309, Best Loss: 0.00000024635588 in Epoch 744
Epoch 754
Epoch 754, Loss: 0.00000077119756, Improvement: 0.00000013835295, Best Loss: 0.00000024635588 in Epoch 744
Epoch 755
Epoch 755, Loss: 0.00000071244795, Improvement: -0.00000005874961, Best Loss: 0.00000024635588 in Epoch 744
Epoch 756
Epoch 756, Loss: 0.00000061121765, Improvement: -0.00000010123029, Best Loss: 0.00000024635588 in Epoch 744
Epoch 757
Epoch 757, Loss: 0.00000064048130, Improvement: 0.00000002926365, Best Loss: 0.00000024635588 in Epoch 744
Epoch 758
Epoch 758, Loss: 0.00000062546359, Improvement: -0.00000001501770, Best Loss: 0.00000024635588 in Epoch 744
Epoch 759
Epoch 759, Loss: 0.00000061327814, Improvement: -0.00000001218546, Best Loss: 0.00000024635588 in Epoch 744
Epoch 760
Epoch 760, Loss: 0.00000060357987, Improvement: -0.00000000969826, Best Loss: 0.00000024635588 in Epoch 744
Epoch 761
Epoch 761, Loss: 0.00000071436397, Improvement: 0.00000011078409, Best Loss: 0.00000024635588 in Epoch 744
Epoch 762
Epoch 762, Loss: 0.00000061155651, Improvement: -0.00000010280745, Best Loss: 0.00000024635588 in Epoch 744
Epoch 763
Epoch 763, Loss: 0.00000060742932, Improvement: -0.00000000412720, Best Loss: 0.00000024635588 in Epoch 744
Epoch 764
Epoch 764, Loss: 0.00000052417530, Improvement: -0.00000008325402, Best Loss: 0.00000024635588 in Epoch 744
Epoch 765
Epoch 765, Loss: 0.00000072397214, Improvement: 0.00000019979684, Best Loss: 0.00000024635588 in Epoch 744
Epoch 766
Epoch 766, Loss: 0.00000050971253, Improvement: -0.00000021425961, Best Loss: 0.00000024635588 in Epoch 744
Epoch 767
Epoch 767, Loss: 0.00000064221348, Improvement: 0.00000013250095, Best Loss: 0.00000024635588 in Epoch 744
Epoch 768
Epoch 768, Loss: 0.00000082824683, Improvement: 0.00000018603336, Best Loss: 0.00000024635588 in Epoch 744
Epoch 769
Epoch 769, Loss: 0.00000073778022, Improvement: -0.00000009046661, Best Loss: 0.00000024635588 in Epoch 744
Epoch 770
Epoch 770, Loss: 0.00000065220919, Improvement: -0.00000008557104, Best Loss: 0.00000024635588 in Epoch 744
Epoch 771
Epoch 771, Loss: 0.00000076669205, Improvement: 0.00000011448286, Best Loss: 0.00000024635588 in Epoch 744
Epoch 772
Epoch 772, Loss: 0.00000060798902, Improvement: -0.00000015870303, Best Loss: 0.00000024635588 in Epoch 744
Epoch 773
Epoch 773, Loss: 0.00000044293814, Improvement: -0.00000016505088, Best Loss: 0.00000024635588 in Epoch 744
Epoch 774
Epoch 774, Loss: 0.00000043207001, Improvement: -0.00000001086813, Best Loss: 0.00000024635588 in Epoch 744
Epoch 775
Epoch 775, Loss: 0.00000042204845, Improvement: -0.00000001002155, Best Loss: 0.00000024635588 in Epoch 744
Epoch 776
Epoch 776, Loss: 0.00000041912013, Improvement: -0.00000000292832, Best Loss: 0.00000024635588 in Epoch 744
Epoch 777
Epoch 777, Loss: 0.00000078467720, Improvement: 0.00000036555707, Best Loss: 0.00000024635588 in Epoch 744
Epoch 778
Epoch 778, Loss: 0.00000129390862, Improvement: 0.00000050923143, Best Loss: 0.00000024635588 in Epoch 744
Epoch 779
Epoch 779, Loss: 0.00000089155666, Improvement: -0.00000040235196, Best Loss: 0.00000024635588 in Epoch 744
Epoch 780
Epoch 780, Loss: 0.00000134683962, Improvement: 0.00000045528295, Best Loss: 0.00000024635588 in Epoch 744
Epoch 781
Epoch 781, Loss: 0.00000165910270, Improvement: 0.00000031226308, Best Loss: 0.00000024635588 in Epoch 744
Epoch 782
Epoch 782, Loss: 0.00000129176660, Improvement: -0.00000036733610, Best Loss: 0.00000024635588 in Epoch 744
Epoch 783
Epoch 783, Loss: 0.00000065057161, Improvement: -0.00000064119499, Best Loss: 0.00000024635588 in Epoch 744
Epoch 784
Epoch 784, Loss: 0.00000053082171, Improvement: -0.00000011974990, Best Loss: 0.00000024635588 in Epoch 744
Epoch 785
Epoch 785, Loss: 0.00000048812925, Improvement: -0.00000004269246, Best Loss: 0.00000024635588 in Epoch 744
Epoch 786
Epoch 786, Loss: 0.00000039993177, Improvement: -0.00000008819748, Best Loss: 0.00000024635588 in Epoch 744
Epoch 787
Epoch 787, Loss: 0.00000037825445, Improvement: -0.00000002167732, Best Loss: 0.00000024635588 in Epoch 744
Epoch 788
Epoch 788, Loss: 0.00000037560325, Improvement: -0.00000000265120, Best Loss: 0.00000024635588 in Epoch 744
Epoch 789
Epoch 789, Loss: 0.00000037422360, Improvement: -0.00000000137965, Best Loss: 0.00000024635588 in Epoch 744
Epoch 790
Epoch 790, Loss: 0.00000036767130, Improvement: -0.00000000655230, Best Loss: 0.00000024635588 in Epoch 744
Epoch 791
Epoch 791, Loss: 0.00000037384542, Improvement: 0.00000000617412, Best Loss: 0.00000024635588 in Epoch 744
Epoch 792
A best model at epoch 792 has been saved with training error 0.00000023577901.
Epoch 792, Loss: 0.00000036658286, Improvement: -0.00000000726256, Best Loss: 0.00000023577901 in Epoch 792
Epoch 793
Epoch 793, Loss: 0.00000036403367, Improvement: -0.00000000254919, Best Loss: 0.00000023577901 in Epoch 792
Epoch 794
Epoch 794, Loss: 0.00000036584542, Improvement: 0.00000000181175, Best Loss: 0.00000023577901 in Epoch 792
Epoch 795
Epoch 795, Loss: 0.00000037034497, Improvement: 0.00000000449955, Best Loss: 0.00000023577901 in Epoch 792
Epoch 796
Epoch 796, Loss: 0.00000037850458, Improvement: 0.00000000815961, Best Loss: 0.00000023577901 in Epoch 792
Epoch 797
Epoch 797, Loss: 0.00000039456605, Improvement: 0.00000001606147, Best Loss: 0.00000023577901 in Epoch 792
Epoch 798
Epoch 798, Loss: 0.00000036113085, Improvement: -0.00000003343520, Best Loss: 0.00000023577901 in Epoch 792
Epoch 799
Epoch 799, Loss: 0.00000035777974, Improvement: -0.00000000335112, Best Loss: 0.00000023577901 in Epoch 792
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000036002600, Improvement: 0.00000000224627, Best Loss: 0.00000023577901 in Epoch 792
Epoch 801
Epoch 801, Loss: 0.00000034525926, Improvement: -0.00000001476674, Best Loss: 0.00000023577901 in Epoch 792
Epoch 802
A best model at epoch 802 has been saved with training error 0.00000022086708.
Epoch 802, Loss: 0.00000034975894, Improvement: 0.00000000449967, Best Loss: 0.00000022086708 in Epoch 802
Epoch 803
Epoch 803, Loss: 0.00000036313540, Improvement: 0.00000001337646, Best Loss: 0.00000022086708 in Epoch 802
Epoch 804
Epoch 804, Loss: 0.00000038472774, Improvement: 0.00000002159234, Best Loss: 0.00000022086708 in Epoch 802
Epoch 805
Epoch 805, Loss: 0.00000039749915, Improvement: 0.00000001277141, Best Loss: 0.00000022086708 in Epoch 802
Epoch 806
Epoch 806, Loss: 0.00000038047450, Improvement: -0.00000001702465, Best Loss: 0.00000022086708 in Epoch 802
Epoch 807
Epoch 807, Loss: 0.00000038843613, Improvement: 0.00000000796163, Best Loss: 0.00000022086708 in Epoch 802
Epoch 808
Epoch 808, Loss: 0.00000040576954, Improvement: 0.00000001733341, Best Loss: 0.00000022086708 in Epoch 802
Epoch 809
Epoch 809, Loss: 0.00000040413049, Improvement: -0.00000000163905, Best Loss: 0.00000022086708 in Epoch 802
Epoch 810
Epoch 810, Loss: 0.00000056254698, Improvement: 0.00000015841649, Best Loss: 0.00000022086708 in Epoch 802
Epoch 811
Epoch 811, Loss: 0.00000060472073, Improvement: 0.00000004217375, Best Loss: 0.00000022086708 in Epoch 802
Epoch 812
Epoch 812, Loss: 0.00000057185064, Improvement: -0.00000003287009, Best Loss: 0.00000022086708 in Epoch 802
Epoch 813
Epoch 813, Loss: 0.00000068835258, Improvement: 0.00000011650194, Best Loss: 0.00000022086708 in Epoch 802
Epoch 814
Epoch 814, Loss: 0.00000051897623, Improvement: -0.00000016937634, Best Loss: 0.00000022086708 in Epoch 802
Epoch 815
Epoch 815, Loss: 0.00000047056272, Improvement: -0.00000004841352, Best Loss: 0.00000022086708 in Epoch 802
Epoch 816
Epoch 816, Loss: 0.00000067704518, Improvement: 0.00000020648246, Best Loss: 0.00000022086708 in Epoch 802
Epoch 817
Epoch 817, Loss: 0.00000055473927, Improvement: -0.00000012230591, Best Loss: 0.00000022086708 in Epoch 802
Epoch 818
Epoch 818, Loss: 0.00000040289981, Improvement: -0.00000015183946, Best Loss: 0.00000022086708 in Epoch 802
Epoch 819
Epoch 819, Loss: 0.00000047228203, Improvement: 0.00000006938221, Best Loss: 0.00000022086708 in Epoch 802
Epoch 820
Epoch 820, Loss: 0.00000047949320, Improvement: 0.00000000721117, Best Loss: 0.00000022086708 in Epoch 802
Epoch 821
Epoch 821, Loss: 0.00000054300557, Improvement: 0.00000006351237, Best Loss: 0.00000022086708 in Epoch 802
Epoch 822
Epoch 822, Loss: 0.00000046394933, Improvement: -0.00000007905624, Best Loss: 0.00000022086708 in Epoch 802
Epoch 823
Epoch 823, Loss: 0.00000054738520, Improvement: 0.00000008343587, Best Loss: 0.00000022086708 in Epoch 802
Epoch 824
Epoch 824, Loss: 0.00000064278034, Improvement: 0.00000009539514, Best Loss: 0.00000022086708 in Epoch 802
Epoch 825
Epoch 825, Loss: 0.00000084246570, Improvement: 0.00000019968536, Best Loss: 0.00000022086708 in Epoch 802
Epoch 826
Epoch 826, Loss: 0.00000066976148, Improvement: -0.00000017270422, Best Loss: 0.00000022086708 in Epoch 802
Epoch 827
Epoch 827, Loss: 0.00000057119542, Improvement: -0.00000009856606, Best Loss: 0.00000022086708 in Epoch 802
Epoch 828
Epoch 828, Loss: 0.00000054467795, Improvement: -0.00000002651747, Best Loss: 0.00000022086708 in Epoch 802
Epoch 829
Epoch 829, Loss: 0.00000048281124, Improvement: -0.00000006186670, Best Loss: 0.00000022086708 in Epoch 802
Epoch 830
Epoch 830, Loss: 0.00000047930206, Improvement: -0.00000000350919, Best Loss: 0.00000022086708 in Epoch 802
Epoch 831
Epoch 831, Loss: 0.00000036319153, Improvement: -0.00000011611053, Best Loss: 0.00000022086708 in Epoch 802
Epoch 832
Epoch 832, Loss: 0.00000033069585, Improvement: -0.00000003249568, Best Loss: 0.00000022086708 in Epoch 802
Epoch 833
A best model at epoch 833 has been saved with training error 0.00000021316583.
A best model at epoch 833 has been saved with training error 0.00000017897219.
Epoch 833, Loss: 0.00000031446407, Improvement: -0.00000001623177, Best Loss: 0.00000017897219 in Epoch 833
Epoch 834
Epoch 834, Loss: 0.00000031079953, Improvement: -0.00000000366454, Best Loss: 0.00000017897219 in Epoch 833
Epoch 835
Epoch 835, Loss: 0.00000033910478, Improvement: 0.00000002830525, Best Loss: 0.00000017897219 in Epoch 833
Epoch 836
Epoch 836, Loss: 0.00000048281327, Improvement: 0.00000014370849, Best Loss: 0.00000017897219 in Epoch 833
Epoch 837
Epoch 837, Loss: 0.00000040493173, Improvement: -0.00000007788154, Best Loss: 0.00000017897219 in Epoch 833
Epoch 838
Epoch 838, Loss: 0.00000032533709, Improvement: -0.00000007959463, Best Loss: 0.00000017897219 in Epoch 833
Epoch 839
Epoch 839, Loss: 0.00000030618039, Improvement: -0.00000001915670, Best Loss: 0.00000017897219 in Epoch 833
Epoch 840
Epoch 840, Loss: 0.00000037044614, Improvement: 0.00000006426575, Best Loss: 0.00000017897219 in Epoch 833
Epoch 841
Epoch 841, Loss: 0.00000032905073, Improvement: -0.00000004139541, Best Loss: 0.00000017897219 in Epoch 833
Epoch 842
Epoch 842, Loss: 0.00000031979569, Improvement: -0.00000000925505, Best Loss: 0.00000017897219 in Epoch 833
Epoch 843
Epoch 843, Loss: 0.00000038112488, Improvement: 0.00000006132919, Best Loss: 0.00000017897219 in Epoch 833
Epoch 844
Epoch 844, Loss: 0.00000035233321, Improvement: -0.00000002879167, Best Loss: 0.00000017897219 in Epoch 833
Epoch 845
Epoch 845, Loss: 0.00000035806548, Improvement: 0.00000000573227, Best Loss: 0.00000017897219 in Epoch 833
Epoch 846
Epoch 846, Loss: 0.00000037559360, Improvement: 0.00000001752812, Best Loss: 0.00000017897219 in Epoch 833
Epoch 847
Epoch 847, Loss: 0.00000035368567, Improvement: -0.00000002190793, Best Loss: 0.00000017897219 in Epoch 833
Epoch 848
Epoch 848, Loss: 0.00000037828608, Improvement: 0.00000002460041, Best Loss: 0.00000017897219 in Epoch 833
Epoch 849
Epoch 849, Loss: 0.00000088164524, Improvement: 0.00000050335915, Best Loss: 0.00000017897219 in Epoch 833
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000087891101, Improvement: -0.00000000273423, Best Loss: 0.00000017897219 in Epoch 833
Epoch 851
Epoch 851, Loss: 0.00000060058199, Improvement: -0.00000027832902, Best Loss: 0.00000017897219 in Epoch 833
Epoch 852
Epoch 852, Loss: 0.00000050135352, Improvement: -0.00000009922847, Best Loss: 0.00000017897219 in Epoch 833
Epoch 853
Epoch 853, Loss: 0.00000046697605, Improvement: -0.00000003437746, Best Loss: 0.00000017897219 in Epoch 833
Epoch 854
Epoch 854, Loss: 0.00000045392608, Improvement: -0.00000001304997, Best Loss: 0.00000017897219 in Epoch 833
Epoch 855
Epoch 855, Loss: 0.00000038194305, Improvement: -0.00000007198303, Best Loss: 0.00000017897219 in Epoch 833
Epoch 856
Epoch 856, Loss: 0.00000034319323, Improvement: -0.00000003874982, Best Loss: 0.00000017897219 in Epoch 833
Epoch 857
Epoch 857, Loss: 0.00000029373421, Improvement: -0.00000004945903, Best Loss: 0.00000017897219 in Epoch 833
Epoch 858
A best model at epoch 858 has been saved with training error 0.00000015685291.
Epoch 858, Loss: 0.00000031426858, Improvement: 0.00000002053437, Best Loss: 0.00000015685291 in Epoch 858
Epoch 859
Epoch 859, Loss: 0.00000030342025, Improvement: -0.00000001084832, Best Loss: 0.00000015685291 in Epoch 858
Epoch 860
Epoch 860, Loss: 0.00000028688977, Improvement: -0.00000001653049, Best Loss: 0.00000015685291 in Epoch 858
Epoch 861
Epoch 861, Loss: 0.00000029316799, Improvement: 0.00000000627822, Best Loss: 0.00000015685291 in Epoch 858
Epoch 862
Epoch 862, Loss: 0.00000028491339, Improvement: -0.00000000825461, Best Loss: 0.00000015685291 in Epoch 858
Epoch 863
Epoch 863, Loss: 0.00000027717522, Improvement: -0.00000000773817, Best Loss: 0.00000015685291 in Epoch 858
Epoch 864
Epoch 864, Loss: 0.00000026626766, Improvement: -0.00000001090756, Best Loss: 0.00000015685291 in Epoch 858
Epoch 865
Epoch 865, Loss: 0.00000028908767, Improvement: 0.00000002282001, Best Loss: 0.00000015685291 in Epoch 858
Epoch 866
Epoch 866, Loss: 0.00000026552889, Improvement: -0.00000002355878, Best Loss: 0.00000015685291 in Epoch 858
Epoch 867
Epoch 867, Loss: 0.00000025025006, Improvement: -0.00000001527883, Best Loss: 0.00000015685291 in Epoch 858
Epoch 868
Epoch 868, Loss: 0.00000028735028, Improvement: 0.00000003710022, Best Loss: 0.00000015685291 in Epoch 858
Epoch 869
Epoch 869, Loss: 0.00000026924173, Improvement: -0.00000001810855, Best Loss: 0.00000015685291 in Epoch 858
Epoch 870
Epoch 870, Loss: 0.00000030541663, Improvement: 0.00000003617491, Best Loss: 0.00000015685291 in Epoch 858
Epoch 871
Epoch 871, Loss: 0.00000046049391, Improvement: 0.00000015507727, Best Loss: 0.00000015685291 in Epoch 858
Epoch 872
Epoch 872, Loss: 0.00000061963374, Improvement: 0.00000015913983, Best Loss: 0.00000015685291 in Epoch 858
Epoch 873
Epoch 873, Loss: 0.00000039772659, Improvement: -0.00000022190715, Best Loss: 0.00000015685291 in Epoch 858
Epoch 874
Epoch 874, Loss: 0.00000035686496, Improvement: -0.00000004086163, Best Loss: 0.00000015685291 in Epoch 858
Epoch 875
Epoch 875, Loss: 0.00000037012117, Improvement: 0.00000001325621, Best Loss: 0.00000015685291 in Epoch 858
Epoch 876
Epoch 876, Loss: 0.00000029249222, Improvement: -0.00000007762895, Best Loss: 0.00000015685291 in Epoch 858
Epoch 877
Epoch 877, Loss: 0.00000023804109, Improvement: -0.00000005445114, Best Loss: 0.00000015685291 in Epoch 858
Epoch 878
Epoch 878, Loss: 0.00000026447623, Improvement: 0.00000002643514, Best Loss: 0.00000015685291 in Epoch 858
Epoch 879
Epoch 879, Loss: 0.00000031214933, Improvement: 0.00000004767311, Best Loss: 0.00000015685291 in Epoch 858
Epoch 880
Epoch 880, Loss: 0.00000035200190, Improvement: 0.00000003985257, Best Loss: 0.00000015685291 in Epoch 858
Epoch 881
Epoch 881, Loss: 0.00000028978755, Improvement: -0.00000006221436, Best Loss: 0.00000015685291 in Epoch 858
Epoch 882
Epoch 882, Loss: 0.00000028444340, Improvement: -0.00000000534415, Best Loss: 0.00000015685291 in Epoch 858
Epoch 883
Epoch 883, Loss: 0.00000026621127, Improvement: -0.00000001823213, Best Loss: 0.00000015685291 in Epoch 858
Epoch 884
Epoch 884, Loss: 0.00000025131018, Improvement: -0.00000001490109, Best Loss: 0.00000015685291 in Epoch 858
Epoch 885
A best model at epoch 885 has been saved with training error 0.00000015324996.
Epoch 885, Loss: 0.00000027851217, Improvement: 0.00000002720198, Best Loss: 0.00000015324996 in Epoch 885
Epoch 886
Epoch 886, Loss: 0.00000027599625, Improvement: -0.00000000251591, Best Loss: 0.00000015324996 in Epoch 885
Epoch 887
Epoch 887, Loss: 0.00000038256813, Improvement: 0.00000010657188, Best Loss: 0.00000015324996 in Epoch 885
Epoch 888
Epoch 888, Loss: 0.00000036149248, Improvement: -0.00000002107565, Best Loss: 0.00000015324996 in Epoch 885
Epoch 889
Epoch 889, Loss: 0.00000034009713, Improvement: -0.00000002139535, Best Loss: 0.00000015324996 in Epoch 885
Epoch 890
Epoch 890, Loss: 0.00000032919869, Improvement: -0.00000001089844, Best Loss: 0.00000015324996 in Epoch 885
Epoch 891
Epoch 891, Loss: 0.00000036761755, Improvement: 0.00000003841886, Best Loss: 0.00000015324996 in Epoch 885
Epoch 892
Epoch 892, Loss: 0.00000034872755, Improvement: -0.00000001889000, Best Loss: 0.00000015324996 in Epoch 885
Epoch 893
Epoch 893, Loss: 0.00000033372085, Improvement: -0.00000001500669, Best Loss: 0.00000015324996 in Epoch 885
Epoch 894
Epoch 894, Loss: 0.00000033923932, Improvement: 0.00000000551847, Best Loss: 0.00000015324996 in Epoch 885
Epoch 895
Epoch 895, Loss: 0.00000045844489, Improvement: 0.00000011920557, Best Loss: 0.00000015324996 in Epoch 885
Epoch 896
Epoch 896, Loss: 0.00000036715613, Improvement: -0.00000009128876, Best Loss: 0.00000015324996 in Epoch 885
Epoch 897
Epoch 897, Loss: 0.00000042161103, Improvement: 0.00000005445489, Best Loss: 0.00000015324996 in Epoch 885
Epoch 898
Epoch 898, Loss: 0.00000029459528, Improvement: -0.00000012701574, Best Loss: 0.00000015324996 in Epoch 885
Epoch 899
Epoch 899, Loss: 0.00000033047794, Improvement: 0.00000003588266, Best Loss: 0.00000015324996 in Epoch 885
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000040423519, Improvement: 0.00000007375725, Best Loss: 0.00000015324996 in Epoch 885
Epoch 901
Epoch 901, Loss: 0.00000047175093, Improvement: 0.00000006751575, Best Loss: 0.00000015324996 in Epoch 885
Epoch 902
Epoch 902, Loss: 0.00000040151576, Improvement: -0.00000007023518, Best Loss: 0.00000015324996 in Epoch 885
Epoch 903
Epoch 903, Loss: 0.00000040334576, Improvement: 0.00000000183000, Best Loss: 0.00000015324996 in Epoch 885
Epoch 904
Epoch 904, Loss: 0.00000068697269, Improvement: 0.00000028362693, Best Loss: 0.00000015324996 in Epoch 885
Epoch 905
Epoch 905, Loss: 0.00000080247288, Improvement: 0.00000011550019, Best Loss: 0.00000015324996 in Epoch 885
Epoch 906
Epoch 906, Loss: 0.00000050573129, Improvement: -0.00000029674159, Best Loss: 0.00000015324996 in Epoch 885
Epoch 907
Epoch 907, Loss: 0.00000027536578, Improvement: -0.00000023036551, Best Loss: 0.00000015324996 in Epoch 885
Epoch 908
A best model at epoch 908 has been saved with training error 0.00000015280570.
Epoch 908, Loss: 0.00000023798360, Improvement: -0.00000003738218, Best Loss: 0.00000015280570 in Epoch 908
Epoch 909
Epoch 909, Loss: 0.00000021981736, Improvement: -0.00000001816624, Best Loss: 0.00000015280570 in Epoch 908
Epoch 910
A best model at epoch 910 has been saved with training error 0.00000014124055.
A best model at epoch 910 has been saved with training error 0.00000013809280.
Epoch 910, Loss: 0.00000021210403, Improvement: -0.00000000771332, Best Loss: 0.00000013809280 in Epoch 910
Epoch 911
Epoch 911, Loss: 0.00000021085343, Improvement: -0.00000000125060, Best Loss: 0.00000013809280 in Epoch 910
Epoch 912
A best model at epoch 912 has been saved with training error 0.00000013751773.
Epoch 912, Loss: 0.00000021602280, Improvement: 0.00000000516937, Best Loss: 0.00000013751773 in Epoch 912
Epoch 913
Epoch 913, Loss: 0.00000022444917, Improvement: 0.00000000842637, Best Loss: 0.00000013751773 in Epoch 912
Epoch 914
Epoch 914, Loss: 0.00000021595056, Improvement: -0.00000000849861, Best Loss: 0.00000013751773 in Epoch 912
Epoch 915
Epoch 915, Loss: 0.00000025482712, Improvement: 0.00000003887656, Best Loss: 0.00000013751773 in Epoch 912
Epoch 916
Epoch 916, Loss: 0.00000030877950, Improvement: 0.00000005395238, Best Loss: 0.00000013751773 in Epoch 912
Epoch 917
Epoch 917, Loss: 0.00000050995470, Improvement: 0.00000020117520, Best Loss: 0.00000013751773 in Epoch 912
Epoch 918
Epoch 918, Loss: 0.00000048929381, Improvement: -0.00000002066089, Best Loss: 0.00000013751773 in Epoch 912
Epoch 919
Epoch 919, Loss: 0.00000057101712, Improvement: 0.00000008172330, Best Loss: 0.00000013751773 in Epoch 912
Epoch 920
Epoch 920, Loss: 0.00000052988100, Improvement: -0.00000004113611, Best Loss: 0.00000013751773 in Epoch 912
Epoch 921
Epoch 921, Loss: 0.00000033568281, Improvement: -0.00000019419819, Best Loss: 0.00000013751773 in Epoch 912
Epoch 922
Epoch 922, Loss: 0.00000032904916, Improvement: -0.00000000663365, Best Loss: 0.00000013751773 in Epoch 912
Epoch 923
Epoch 923, Loss: 0.00000026976157, Improvement: -0.00000005928759, Best Loss: 0.00000013751773 in Epoch 912
Epoch 924
Epoch 924, Loss: 0.00000024187229, Improvement: -0.00000002788928, Best Loss: 0.00000013751773 in Epoch 912
Epoch 925
Epoch 925, Loss: 0.00000023197601, Improvement: -0.00000000989628, Best Loss: 0.00000013751773 in Epoch 912
Epoch 926
Epoch 926, Loss: 0.00000025247417, Improvement: 0.00000002049817, Best Loss: 0.00000013751773 in Epoch 912
Epoch 927
Epoch 927, Loss: 0.00000025767520, Improvement: 0.00000000520103, Best Loss: 0.00000013751773 in Epoch 912
Epoch 928
Epoch 928, Loss: 0.00000022901663, Improvement: -0.00000002865857, Best Loss: 0.00000013751773 in Epoch 912
Epoch 929
Epoch 929, Loss: 0.00000022818400, Improvement: -0.00000000083263, Best Loss: 0.00000013751773 in Epoch 912
Epoch 930
A best model at epoch 930 has been saved with training error 0.00000013308836.
Epoch 930, Loss: 0.00000021985537, Improvement: -0.00000000832863, Best Loss: 0.00000013308836 in Epoch 930
Epoch 931
Epoch 931, Loss: 0.00000022901571, Improvement: 0.00000000916034, Best Loss: 0.00000013308836 in Epoch 930
Epoch 932
Epoch 932, Loss: 0.00000022197641, Improvement: -0.00000000703930, Best Loss: 0.00000013308836 in Epoch 930
Epoch 933
Epoch 933, Loss: 0.00000024087983, Improvement: 0.00000001890342, Best Loss: 0.00000013308836 in Epoch 930
Epoch 934
Epoch 934, Loss: 0.00000024090832, Improvement: 0.00000000002849, Best Loss: 0.00000013308836 in Epoch 930
Epoch 935
Epoch 935, Loss: 0.00000024410608, Improvement: 0.00000000319776, Best Loss: 0.00000013308836 in Epoch 930
Epoch 936
Epoch 936, Loss: 0.00000025290978, Improvement: 0.00000000880370, Best Loss: 0.00000013308836 in Epoch 930
Epoch 937
Epoch 937, Loss: 0.00000027359231, Improvement: 0.00000002068252, Best Loss: 0.00000013308836 in Epoch 930
Epoch 938
Epoch 938, Loss: 0.00000035070776, Improvement: 0.00000007711545, Best Loss: 0.00000013308836 in Epoch 930
Epoch 939
Epoch 939, Loss: 0.00000025570048, Improvement: -0.00000009500728, Best Loss: 0.00000013308836 in Epoch 930
Epoch 940
Epoch 940, Loss: 0.00000023328399, Improvement: -0.00000002241649, Best Loss: 0.00000013308836 in Epoch 930
Epoch 941
Epoch 941, Loss: 0.00000027261679, Improvement: 0.00000003933279, Best Loss: 0.00000013308836 in Epoch 930
Epoch 942
Epoch 942, Loss: 0.00000028843655, Improvement: 0.00000001581977, Best Loss: 0.00000013308836 in Epoch 930
Epoch 943
Epoch 943, Loss: 0.00000044524959, Improvement: 0.00000015681303, Best Loss: 0.00000013308836 in Epoch 930
Epoch 944
Epoch 944, Loss: 0.00000053529932, Improvement: 0.00000009004973, Best Loss: 0.00000013308836 in Epoch 930
Epoch 945
Epoch 945, Loss: 0.00000035255266, Improvement: -0.00000018274665, Best Loss: 0.00000013308836 in Epoch 930
Epoch 946
Epoch 946, Loss: 0.00000036710090, Improvement: 0.00000001454824, Best Loss: 0.00000013308836 in Epoch 930
Epoch 947
Epoch 947, Loss: 0.00000043668429, Improvement: 0.00000006958339, Best Loss: 0.00000013308836 in Epoch 930
Epoch 948
Epoch 948, Loss: 0.00000036239671, Improvement: -0.00000007428759, Best Loss: 0.00000013308836 in Epoch 930
Epoch 949
Epoch 949, Loss: 0.00000029516222, Improvement: -0.00000006723448, Best Loss: 0.00000013308836 in Epoch 930
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000042238163, Improvement: 0.00000012721940, Best Loss: 0.00000013308836 in Epoch 930
Epoch 951
Epoch 951, Loss: 0.00000042639174, Improvement: 0.00000000401011, Best Loss: 0.00000013308836 in Epoch 930
Epoch 952
Epoch 952, Loss: 0.00000046298032, Improvement: 0.00000003658859, Best Loss: 0.00000013308836 in Epoch 930
Epoch 953
Epoch 953, Loss: 0.00000067680476, Improvement: 0.00000021382444, Best Loss: 0.00000013308836 in Epoch 930
Epoch 954
Epoch 954, Loss: 0.00000052436580, Improvement: -0.00000015243896, Best Loss: 0.00000013308836 in Epoch 930
Epoch 955
Epoch 955, Loss: 0.00000033189507, Improvement: -0.00000019247073, Best Loss: 0.00000013308836 in Epoch 930
Epoch 956
Epoch 956, Loss: 0.00000026768688, Improvement: -0.00000006420818, Best Loss: 0.00000013308836 in Epoch 930
Epoch 957
Epoch 957, Loss: 0.00000037273032, Improvement: 0.00000010504344, Best Loss: 0.00000013308836 in Epoch 930
Epoch 958
Epoch 958, Loss: 0.00000032632409, Improvement: -0.00000004640624, Best Loss: 0.00000013308836 in Epoch 930
Epoch 959
Epoch 959, Loss: 0.00000049162533, Improvement: 0.00000016530124, Best Loss: 0.00000013308836 in Epoch 930
Epoch 960
Epoch 960, Loss: 0.00000035851593, Improvement: -0.00000013310940, Best Loss: 0.00000013308836 in Epoch 930
Epoch 961
Epoch 961, Loss: 0.00000035649782, Improvement: -0.00000000201811, Best Loss: 0.00000013308836 in Epoch 930
Epoch 962
Epoch 962, Loss: 0.00000042429092, Improvement: 0.00000006779310, Best Loss: 0.00000013308836 in Epoch 930
Epoch 963
Epoch 963, Loss: 0.00000053045477, Improvement: 0.00000010616385, Best Loss: 0.00000013308836 in Epoch 930
Epoch 964
Epoch 964, Loss: 0.00000033204323, Improvement: -0.00000019841154, Best Loss: 0.00000013308836 in Epoch 930
Epoch 965
Epoch 965, Loss: 0.00000033089178, Improvement: -0.00000000115145, Best Loss: 0.00000013308836 in Epoch 930
Epoch 966
Epoch 966, Loss: 0.00000040724459, Improvement: 0.00000007635281, Best Loss: 0.00000013308836 in Epoch 930
Epoch 967
Epoch 967, Loss: 0.00000033269231, Improvement: -0.00000007455228, Best Loss: 0.00000013308836 in Epoch 930
Epoch 968
Epoch 968, Loss: 0.00000021777212, Improvement: -0.00000011492020, Best Loss: 0.00000013308836 in Epoch 930
Epoch 969
Epoch 969, Loss: 0.00000026908544, Improvement: 0.00000005131332, Best Loss: 0.00000013308836 in Epoch 930
Epoch 970
Epoch 970, Loss: 0.00000023062702, Improvement: -0.00000003845842, Best Loss: 0.00000013308836 in Epoch 930
Epoch 971
Epoch 971, Loss: 0.00000022426881, Improvement: -0.00000000635820, Best Loss: 0.00000013308836 in Epoch 930
Epoch 972
Epoch 972, Loss: 0.00000024372337, Improvement: 0.00000001945455, Best Loss: 0.00000013308836 in Epoch 930
Epoch 973
Epoch 973, Loss: 0.00000023840719, Improvement: -0.00000000531617, Best Loss: 0.00000013308836 in Epoch 930
Epoch 974
Epoch 974, Loss: 0.00000021883276, Improvement: -0.00000001957444, Best Loss: 0.00000013308836 in Epoch 930
Epoch 975
Epoch 975, Loss: 0.00000021682319, Improvement: -0.00000000200957, Best Loss: 0.00000013308836 in Epoch 930
Epoch 976
Epoch 976, Loss: 0.00000039904027, Improvement: 0.00000018221708, Best Loss: 0.00000013308836 in Epoch 930
Epoch 977
Epoch 977, Loss: 0.00000055161444, Improvement: 0.00000015257416, Best Loss: 0.00000013308836 in Epoch 930
Epoch 978
Epoch 978, Loss: 0.00000045000638, Improvement: -0.00000010160806, Best Loss: 0.00000013308836 in Epoch 930
Epoch 979
Epoch 979, Loss: 0.00000038793832, Improvement: -0.00000006206806, Best Loss: 0.00000013308836 in Epoch 930
Epoch 980
Epoch 980, Loss: 0.00000030087396, Improvement: -0.00000008706437, Best Loss: 0.00000013308836 in Epoch 930
Epoch 981
Epoch 981, Loss: 0.00000026033238, Improvement: -0.00000004054157, Best Loss: 0.00000013308836 in Epoch 930
Epoch 982
Epoch 982, Loss: 0.00000035301021, Improvement: 0.00000009267782, Best Loss: 0.00000013308836 in Epoch 930
Epoch 983
Epoch 983, Loss: 0.00000044425990, Improvement: 0.00000009124969, Best Loss: 0.00000013308836 in Epoch 930
Epoch 984
Epoch 984, Loss: 0.00000042491329, Improvement: -0.00000001934661, Best Loss: 0.00000013308836 in Epoch 930
Epoch 985
Epoch 985, Loss: 0.00000051014496, Improvement: 0.00000008523167, Best Loss: 0.00000013308836 in Epoch 930
Epoch 986
Epoch 986, Loss: 0.00000032035039, Improvement: -0.00000018979457, Best Loss: 0.00000013308836 in Epoch 930
Epoch 987
Epoch 987, Loss: 0.00000031932959, Improvement: -0.00000000102080, Best Loss: 0.00000013308836 in Epoch 930
Epoch 988
Epoch 988, Loss: 0.00000032399522, Improvement: 0.00000000466563, Best Loss: 0.00000013308836 in Epoch 930
Epoch 989
Epoch 989, Loss: 0.00000039725996, Improvement: 0.00000007326474, Best Loss: 0.00000013308836 in Epoch 930
Epoch 990
Epoch 990, Loss: 0.00000044115948, Improvement: 0.00000004389952, Best Loss: 0.00000013308836 in Epoch 930
Epoch 991
Epoch 991, Loss: 0.00000036577401, Improvement: -0.00000007538546, Best Loss: 0.00000013308836 in Epoch 930
Epoch 992
Epoch 992, Loss: 0.00000023331946, Improvement: -0.00000013245455, Best Loss: 0.00000013308836 in Epoch 930
Epoch 993
Epoch 993, Loss: 0.00000027253435, Improvement: 0.00000003921489, Best Loss: 0.00000013308836 in Epoch 930
Epoch 994
Epoch 994, Loss: 0.00000021533102, Improvement: -0.00000005720334, Best Loss: 0.00000013308836 in Epoch 930
Epoch 995
Epoch 995, Loss: 0.00000021008901, Improvement: -0.00000000524201, Best Loss: 0.00000013308836 in Epoch 930
Epoch 996
A best model at epoch 996 has been saved with training error 0.00000012995845.
Epoch 996, Loss: 0.00000020154268, Improvement: -0.00000000854633, Best Loss: 0.00000012995845 in Epoch 996
Epoch 997
Epoch 997, Loss: 0.00000019409914, Improvement: -0.00000000744354, Best Loss: 0.00000012995845 in Epoch 996
Epoch 998
Epoch 998, Loss: 0.00000021809313, Improvement: 0.00000002399400, Best Loss: 0.00000012995845 in Epoch 996
Epoch 999
Epoch 999, Loss: 0.00000023422680, Improvement: 0.00000001613367, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000020628381, Improvement: -0.00000002794299, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1001
Epoch 1001, Loss: 0.00000024279995, Improvement: 0.00000003651614, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1002
Epoch 1002, Loss: 0.00000026850237, Improvement: 0.00000002570242, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1003
Epoch 1003, Loss: 0.00000031093702, Improvement: 0.00000004243465, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1004
Epoch 1004, Loss: 0.00000027571155, Improvement: -0.00000003522546, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1005
Epoch 1005, Loss: 0.00000027112576, Improvement: -0.00000000458579, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1006
Epoch 1006, Loss: 0.00000026899347, Improvement: -0.00000000213230, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1007
Epoch 1007, Loss: 0.00000027002593, Improvement: 0.00000000103246, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1008
Epoch 1008, Loss: 0.00000029919716, Improvement: 0.00000002917123, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1009
Epoch 1009, Loss: 0.00000029096052, Improvement: -0.00000000823664, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1010
Epoch 1010, Loss: 0.00000039034279, Improvement: 0.00000009938227, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1011
Epoch 1011, Loss: 0.00000042844338, Improvement: 0.00000003810059, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1012
Epoch 1012, Loss: 0.00000063446035, Improvement: 0.00000020601696, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1013
Epoch 1013, Loss: 0.00000037212734, Improvement: -0.00000026233301, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1014
Epoch 1014, Loss: 0.00000031878595, Improvement: -0.00000005334139, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1015
Epoch 1015, Loss: 0.00000028005495, Improvement: -0.00000003873100, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1016
Epoch 1016, Loss: 0.00000028619249, Improvement: 0.00000000613754, Best Loss: 0.00000012995845 in Epoch 996
Epoch 1017
A best model at epoch 1017 has been saved with training error 0.00000012498663.
Epoch 1017, Loss: 0.00000020956685, Improvement: -0.00000007662564, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1018
Epoch 1018, Loss: 0.00000018695765, Improvement: -0.00000002260920, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1019
Epoch 1019, Loss: 0.00000018448503, Improvement: -0.00000000247263, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1020
Epoch 1020, Loss: 0.00000020622432, Improvement: 0.00000002173929, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1021
Epoch 1021, Loss: 0.00000034455748, Improvement: 0.00000013833316, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1022
Epoch 1022, Loss: 0.00000048917091, Improvement: 0.00000014461343, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1023
Epoch 1023, Loss: 0.00000061985081, Improvement: 0.00000013067990, Best Loss: 0.00000012498663 in Epoch 1017
Epoch 1024
A best model at epoch 1024 has been saved with training error 0.00000012387370.
Epoch 1024, Loss: 0.00000033323694, Improvement: -0.00000028661388, Best Loss: 0.00000012387370 in Epoch 1024
Epoch 1025
A best model at epoch 1025 has been saved with training error 0.00000012154152.
Epoch 1025, Loss: 0.00000020248793, Improvement: -0.00000013074900, Best Loss: 0.00000012154152 in Epoch 1025
Epoch 1026
A best model at epoch 1026 has been saved with training error 0.00000012128160.
Epoch 1026, Loss: 0.00000018146275, Improvement: -0.00000002102519, Best Loss: 0.00000012128160 in Epoch 1026
Epoch 1027
Epoch 1027, Loss: 0.00000017515337, Improvement: -0.00000000630938, Best Loss: 0.00000012128160 in Epoch 1026
Epoch 1028
A best model at epoch 1028 has been saved with training error 0.00000011285285.
Epoch 1028, Loss: 0.00000017549654, Improvement: 0.00000000034317, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1029
Epoch 1029, Loss: 0.00000020684552, Improvement: 0.00000003134898, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1030
Epoch 1030, Loss: 0.00000021618907, Improvement: 0.00000000934354, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1031
Epoch 1031, Loss: 0.00000021764831, Improvement: 0.00000000145924, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1032
Epoch 1032, Loss: 0.00000018751343, Improvement: -0.00000003013488, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1033
Epoch 1033, Loss: 0.00000018299728, Improvement: -0.00000000451615, Best Loss: 0.00000011285285 in Epoch 1028
Epoch 1034
A best model at epoch 1034 has been saved with training error 0.00000010259441.
Epoch 1034, Loss: 0.00000019504657, Improvement: 0.00000001204930, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1035
Epoch 1035, Loss: 0.00000032744870, Improvement: 0.00000013240213, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1036
Epoch 1036, Loss: 0.00000035582139, Improvement: 0.00000002837269, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1037
Epoch 1037, Loss: 0.00000026541923, Improvement: -0.00000009040216, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1038
Epoch 1038, Loss: 0.00000034238532, Improvement: 0.00000007696609, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1039
Epoch 1039, Loss: 0.00000046645055, Improvement: 0.00000012406523, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1040
Epoch 1040, Loss: 0.00000029137162, Improvement: -0.00000017507894, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1041
Epoch 1041, Loss: 0.00000023699036, Improvement: -0.00000005438126, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1042
Epoch 1042, Loss: 0.00000027795201, Improvement: 0.00000004096166, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1043
Epoch 1043, Loss: 0.00000025503903, Improvement: -0.00000002291299, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1044
Epoch 1044, Loss: 0.00000034766053, Improvement: 0.00000009262151, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1045
Epoch 1045, Loss: 0.00000030607953, Improvement: -0.00000004158101, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1046
Epoch 1046, Loss: 0.00000019594824, Improvement: -0.00000011013128, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1047
Epoch 1047, Loss: 0.00000019365778, Improvement: -0.00000000229047, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1048
Epoch 1048, Loss: 0.00000027315776, Improvement: 0.00000007949998, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1049
Epoch 1049, Loss: 0.00000034076869, Improvement: 0.00000006761093, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000043131249, Improvement: 0.00000009054380, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1051
Epoch 1051, Loss: 0.00000036447261, Improvement: -0.00000006683988, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1052
Epoch 1052, Loss: 0.00000026989083, Improvement: -0.00000009458178, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1053
Epoch 1053, Loss: 0.00000034424497, Improvement: 0.00000007435414, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1054
Epoch 1054, Loss: 0.00000024616002, Improvement: -0.00000009808495, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1055
Epoch 1055, Loss: 0.00000029617155, Improvement: 0.00000005001153, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1056
Epoch 1056, Loss: 0.00000042530069, Improvement: 0.00000012912914, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1057
Epoch 1057, Loss: 0.00000035439308, Improvement: -0.00000007090761, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1058
Epoch 1058, Loss: 0.00000034441966, Improvement: -0.00000000997341, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1059
Epoch 1059, Loss: 0.00000019594311, Improvement: -0.00000014847655, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1060
Epoch 1060, Loss: 0.00000018438511, Improvement: -0.00000001155800, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1061
Epoch 1061, Loss: 0.00000016208524, Improvement: -0.00000002229987, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1062
Epoch 1062, Loss: 0.00000019940735, Improvement: 0.00000003732211, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1063
Epoch 1063, Loss: 0.00000019922550, Improvement: -0.00000000018185, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1064
Epoch 1064, Loss: 0.00000029020440, Improvement: 0.00000009097890, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1065
Epoch 1065, Loss: 0.00000039576322, Improvement: 0.00000010555882, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1066
Epoch 1066, Loss: 0.00000035740555, Improvement: -0.00000003835767, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1067
Epoch 1067, Loss: 0.00000042927197, Improvement: 0.00000007186642, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1068
Epoch 1068, Loss: 0.00000036699873, Improvement: -0.00000006227324, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1069
Epoch 1069, Loss: 0.00000021808619, Improvement: -0.00000014891254, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1070
Epoch 1070, Loss: 0.00000025729812, Improvement: 0.00000003921193, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1071
Epoch 1071, Loss: 0.00000044958199, Improvement: 0.00000019228386, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1072
Epoch 1072, Loss: 0.00000049220056, Improvement: 0.00000004261857, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1073
Epoch 1073, Loss: 0.00000036967276, Improvement: -0.00000012252780, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1074
Epoch 1074, Loss: 0.00000021276862, Improvement: -0.00000015690414, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1075
Epoch 1075, Loss: 0.00000018567989, Improvement: -0.00000002708874, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1076
Epoch 1076, Loss: 0.00000015315846, Improvement: -0.00000003252142, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1077
Epoch 1077, Loss: 0.00000015351201, Improvement: 0.00000000035355, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1078
Epoch 1078, Loss: 0.00000017391295, Improvement: 0.00000002040094, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1079
Epoch 1079, Loss: 0.00000017995367, Improvement: 0.00000000604072, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1080
Epoch 1080, Loss: 0.00000017094276, Improvement: -0.00000000901091, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1081
Epoch 1081, Loss: 0.00000017721727, Improvement: 0.00000000627451, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1082
Epoch 1082, Loss: 0.00000017105345, Improvement: -0.00000000616382, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1083
Epoch 1083, Loss: 0.00000015996274, Improvement: -0.00000001109071, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1084
Epoch 1084, Loss: 0.00000015862566, Improvement: -0.00000000133708, Best Loss: 0.00000010259441 in Epoch 1034
Epoch 1085
A best model at epoch 1085 has been saved with training error 0.00000010065136.
A best model at epoch 1085 has been saved with training error 0.00000009239716.
Epoch 1085, Loss: 0.00000013800483, Improvement: -0.00000002062083, Best Loss: 0.00000009239716 in Epoch 1085
Epoch 1086
A best model at epoch 1086 has been saved with training error 0.00000008050383.
Epoch 1086, Loss: 0.00000012888885, Improvement: -0.00000000911598, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1087
Epoch 1087, Loss: 0.00000013734650, Improvement: 0.00000000845765, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1088
Epoch 1088, Loss: 0.00000014513795, Improvement: 0.00000000779145, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1089
Epoch 1089, Loss: 0.00000028478534, Improvement: 0.00000013964740, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1090
Epoch 1090, Loss: 0.00000040202520, Improvement: 0.00000011723986, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1091
Epoch 1091, Loss: 0.00000056972416, Improvement: 0.00000016769896, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1092
Epoch 1092, Loss: 0.00000030988088, Improvement: -0.00000025984328, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1093
Epoch 1093, Loss: 0.00000018481199, Improvement: -0.00000012506889, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1094
Epoch 1094, Loss: 0.00000016952363, Improvement: -0.00000001528836, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1095
Epoch 1095, Loss: 0.00000018993188, Improvement: 0.00000002040826, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1096
Epoch 1096, Loss: 0.00000016398849, Improvement: -0.00000002594340, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1097
Epoch 1097, Loss: 0.00000014131175, Improvement: -0.00000002267673, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1098
Epoch 1098, Loss: 0.00000013652918, Improvement: -0.00000000478257, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1099
Epoch 1099, Loss: 0.00000013175459, Improvement: -0.00000000477458, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000013643422, Improvement: 0.00000000467962, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1101
Epoch 1101, Loss: 0.00000015189208, Improvement: 0.00000001545787, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1102
Epoch 1102, Loss: 0.00000015789525, Improvement: 0.00000000600317, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1103
Epoch 1103, Loss: 0.00000022666580, Improvement: 0.00000006877055, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1104
Epoch 1104, Loss: 0.00000023930700, Improvement: 0.00000001264121, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1105
Epoch 1105, Loss: 0.00000029653426, Improvement: 0.00000005722726, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1106
Epoch 1106, Loss: 0.00000035224010, Improvement: 0.00000005570584, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1107
Epoch 1107, Loss: 0.00000041160754, Improvement: 0.00000005936744, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1108
Epoch 1108, Loss: 0.00000033202491, Improvement: -0.00000007958263, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1109
Epoch 1109, Loss: 0.00000029698626, Improvement: -0.00000003503865, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1110
Epoch 1110, Loss: 0.00000021767538, Improvement: -0.00000007931088, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1111
Epoch 1111, Loss: 0.00000022629271, Improvement: 0.00000000861733, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1112
Epoch 1112, Loss: 0.00000021163815, Improvement: -0.00000001465457, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1113
Epoch 1113, Loss: 0.00000018764764, Improvement: -0.00000002399050, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1114
Epoch 1114, Loss: 0.00000016216867, Improvement: -0.00000002547897, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1115
Epoch 1115, Loss: 0.00000014162560, Improvement: -0.00000002054307, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1116
Epoch 1116, Loss: 0.00000013725005, Improvement: -0.00000000437555, Best Loss: 0.00000008050383 in Epoch 1086
Epoch 1117
A best model at epoch 1117 has been saved with training error 0.00000006180157.
Epoch 1117, Loss: 0.00000011734196, Improvement: -0.00000001990809, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1118
Epoch 1118, Loss: 0.00000012110008, Improvement: 0.00000000375812, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1119
Epoch 1119, Loss: 0.00000011732534, Improvement: -0.00000000377474, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1120
Epoch 1120, Loss: 0.00000010558198, Improvement: -0.00000001174336, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1121
Epoch 1121, Loss: 0.00000011260492, Improvement: 0.00000000702294, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1122
Epoch 1122, Loss: 0.00000010451592, Improvement: -0.00000000808899, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1123
Epoch 1123, Loss: 0.00000017350154, Improvement: 0.00000006898562, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1124
Epoch 1124, Loss: 0.00000027379888, Improvement: 0.00000010029734, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1125
Epoch 1125, Loss: 0.00000041823022, Improvement: 0.00000014443134, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1126
Epoch 1126, Loss: 0.00000044013573, Improvement: 0.00000002190551, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1127
Epoch 1127, Loss: 0.00000021933705, Improvement: -0.00000022079868, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1128
Epoch 1128, Loss: 0.00000019625327, Improvement: -0.00000002308378, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1129
Epoch 1129, Loss: 0.00000035214670, Improvement: 0.00000015589343, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1130
Epoch 1130, Loss: 0.00000034877589, Improvement: -0.00000000337082, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1131
Epoch 1131, Loss: 0.00000024647739, Improvement: -0.00000010229850, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1132
Epoch 1132, Loss: 0.00000025050390, Improvement: 0.00000000402651, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1133
Epoch 1133, Loss: 0.00000021152350, Improvement: -0.00000003898040, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1134
Epoch 1134, Loss: 0.00000017315163, Improvement: -0.00000003837187, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1135
Epoch 1135, Loss: 0.00000018312246, Improvement: 0.00000000997083, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1136
Epoch 1136, Loss: 0.00000015635684, Improvement: -0.00000002676561, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1137
Epoch 1137, Loss: 0.00000013408299, Improvement: -0.00000002227386, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1138
Epoch 1138, Loss: 0.00000012439868, Improvement: -0.00000000968431, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1139
Epoch 1139, Loss: 0.00000014097549, Improvement: 0.00000001657681, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1140
Epoch 1140, Loss: 0.00000032308483, Improvement: 0.00000018210934, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1141
Epoch 1141, Loss: 0.00000026676622, Improvement: -0.00000005631861, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1142
Epoch 1142, Loss: 0.00000030041614, Improvement: 0.00000003364992, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1143
Epoch 1143, Loss: 0.00000025472818, Improvement: -0.00000004568796, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1144
Epoch 1144, Loss: 0.00000015134179, Improvement: -0.00000010338639, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1145
Epoch 1145, Loss: 0.00000015742728, Improvement: 0.00000000608549, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1146
Epoch 1146, Loss: 0.00000020437121, Improvement: 0.00000004694393, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1147
Epoch 1147, Loss: 0.00000044068308, Improvement: 0.00000023631186, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1148
Epoch 1148, Loss: 0.00000035659438, Improvement: -0.00000008408870, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1149
Epoch 1149, Loss: 0.00000023282906, Improvement: -0.00000012376532, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000019507183, Improvement: -0.00000003775723, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1151
Epoch 1151, Loss: 0.00000013942936, Improvement: -0.00000005564247, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1152
Epoch 1152, Loss: 0.00000011808619, Improvement: -0.00000002134317, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1153
Epoch 1153, Loss: 0.00000010358267, Improvement: -0.00000001450352, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1154
Epoch 1154, Loss: 0.00000011295345, Improvement: 0.00000000937078, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1155
Epoch 1155, Loss: 0.00000019548727, Improvement: 0.00000008253382, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1156
Epoch 1156, Loss: 0.00000013448945, Improvement: -0.00000006099782, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1157
Epoch 1157, Loss: 0.00000011377225, Improvement: -0.00000002071720, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1158
Epoch 1158, Loss: 0.00000010356191, Improvement: -0.00000001021035, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1159
Epoch 1159, Loss: 0.00000010311693, Improvement: -0.00000000044498, Best Loss: 0.00000006180157 in Epoch 1117
Epoch 1160
A best model at epoch 1160 has been saved with training error 0.00000005992803.
Epoch 1160, Loss: 0.00000009777577, Improvement: -0.00000000534115, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1161
Epoch 1161, Loss: 0.00000011214393, Improvement: 0.00000001436815, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1162
Epoch 1162, Loss: 0.00000018505274, Improvement: 0.00000007290882, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1163
Epoch 1163, Loss: 0.00000033431980, Improvement: 0.00000014926705, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1164
Epoch 1164, Loss: 0.00000030801511, Improvement: -0.00000002630469, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1165
Epoch 1165, Loss: 0.00000021188353, Improvement: -0.00000009613157, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1166
Epoch 1166, Loss: 0.00000014211630, Improvement: -0.00000006976723, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1167
Epoch 1167, Loss: 0.00000014083471, Improvement: -0.00000000128159, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1168
Epoch 1168, Loss: 0.00000011959689, Improvement: -0.00000002123782, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1169
Epoch 1169, Loss: 0.00000012800957, Improvement: 0.00000000841268, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1170
Epoch 1170, Loss: 0.00000012976906, Improvement: 0.00000000175949, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1171
Epoch 1171, Loss: 0.00000012232195, Improvement: -0.00000000744712, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1172
Epoch 1172, Loss: 0.00000012582905, Improvement: 0.00000000350710, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1173
Epoch 1173, Loss: 0.00000023855862, Improvement: 0.00000011272957, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1174
Epoch 1174, Loss: 0.00000025733703, Improvement: 0.00000001877841, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1175
Epoch 1175, Loss: 0.00000014098288, Improvement: -0.00000011635415, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1176
Epoch 1176, Loss: 0.00000009630360, Improvement: -0.00000004467927, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1177
Epoch 1177, Loss: 0.00000010969728, Improvement: 0.00000001339368, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1178
Epoch 1178, Loss: 0.00000009426030, Improvement: -0.00000001543698, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1179
Epoch 1179, Loss: 0.00000010468423, Improvement: 0.00000001042393, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1180
Epoch 1180, Loss: 0.00000011408998, Improvement: 0.00000000940575, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1181
Epoch 1181, Loss: 0.00000009544168, Improvement: -0.00000001864830, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1182
Epoch 1182, Loss: 0.00000012346263, Improvement: 0.00000002802094, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1183
Epoch 1183, Loss: 0.00000015361057, Improvement: 0.00000003014794, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1184
Epoch 1184, Loss: 0.00000026816413, Improvement: 0.00000011455357, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1185
Epoch 1185, Loss: 0.00000037889241, Improvement: 0.00000011072827, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1186
Epoch 1186, Loss: 0.00000022768511, Improvement: -0.00000015120730, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1187
Epoch 1187, Loss: 0.00000012930256, Improvement: -0.00000009838254, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1188
Epoch 1188, Loss: 0.00000011010986, Improvement: -0.00000001919270, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1189
Epoch 1189, Loss: 0.00000019762189, Improvement: 0.00000008751203, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1190
Epoch 1190, Loss: 0.00000023515128, Improvement: 0.00000003752939, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1191
Epoch 1191, Loss: 0.00000019968988, Improvement: -0.00000003546140, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1192
Epoch 1192, Loss: 0.00000030948459, Improvement: 0.00000010979471, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1193
Epoch 1193, Loss: 0.00000067221670, Improvement: 0.00000036273211, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1194
Epoch 1194, Loss: 0.00000031857929, Improvement: -0.00000035363740, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1195
Epoch 1195, Loss: 0.00000014183124, Improvement: -0.00000017674805, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1196
Epoch 1196, Loss: 0.00000013010745, Improvement: -0.00000001172380, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1197
Epoch 1197, Loss: 0.00000016039955, Improvement: 0.00000003029210, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1198
Epoch 1198, Loss: 0.00000016667218, Improvement: 0.00000000627263, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1199
Epoch 1199, Loss: 0.00000010514554, Improvement: -0.00000006152664, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000008916734, Improvement: -0.00000001597819, Best Loss: 0.00000005992803 in Epoch 1160
Epoch 1201
A best model at epoch 1201 has been saved with training error 0.00000005854815.
Epoch 1201, Loss: 0.00000008137861, Improvement: -0.00000000778873, Best Loss: 0.00000005854815 in Epoch 1201
Epoch 1202
A best model at epoch 1202 has been saved with training error 0.00000005344966.
Epoch 1202, Loss: 0.00000007644496, Improvement: -0.00000000493365, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1203
Epoch 1203, Loss: 0.00000007680903, Improvement: 0.00000000036407, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1204
Epoch 1204, Loss: 0.00000007465597, Improvement: -0.00000000215306, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1205
Epoch 1205, Loss: 0.00000007583679, Improvement: 0.00000000118082, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1206
Epoch 1206, Loss: 0.00000008628279, Improvement: 0.00000001044600, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1207
Epoch 1207, Loss: 0.00000008661493, Improvement: 0.00000000033215, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1208
Epoch 1208, Loss: 0.00000010993223, Improvement: 0.00000002331729, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1209
Epoch 1209, Loss: 0.00000010349076, Improvement: -0.00000000644147, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1210
Epoch 1210, Loss: 0.00000011788789, Improvement: 0.00000001439713, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1211
Epoch 1211, Loss: 0.00000009752433, Improvement: -0.00000002036356, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1212
Epoch 1212, Loss: 0.00000011382996, Improvement: 0.00000001630563, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1213
Epoch 1213, Loss: 0.00000013759697, Improvement: 0.00000002376701, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1214
Epoch 1214, Loss: 0.00000031573123, Improvement: 0.00000017813425, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1215
Epoch 1215, Loss: 0.00000030284832, Improvement: -0.00000001288291, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1216
Epoch 1216, Loss: 0.00000031132701, Improvement: 0.00000000847869, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1217
Epoch 1217, Loss: 0.00000032105040, Improvement: 0.00000000972340, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1218
Epoch 1218, Loss: 0.00000016616151, Improvement: -0.00000015488889, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1219
Epoch 1219, Loss: 0.00000010918917, Improvement: -0.00000005697234, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1220
Epoch 1220, Loss: 0.00000008798374, Improvement: -0.00000002120543, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1221
Epoch 1221, Loss: 0.00000008319299, Improvement: -0.00000000479075, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1222
Epoch 1222, Loss: 0.00000007895435, Improvement: -0.00000000423864, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1223
Epoch 1223, Loss: 0.00000008111467, Improvement: 0.00000000216032, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1224
Epoch 1224, Loss: 0.00000009259400, Improvement: 0.00000001147933, Best Loss: 0.00000005344966 in Epoch 1202
Epoch 1225
A best model at epoch 1225 has been saved with training error 0.00000005002319.
Epoch 1225, Loss: 0.00000008144945, Improvement: -0.00000001114455, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1226
Epoch 1226, Loss: 0.00000008592520, Improvement: 0.00000000447575, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1227
Epoch 1227, Loss: 0.00000010720380, Improvement: 0.00000002127860, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1228
Epoch 1228, Loss: 0.00000023423523, Improvement: 0.00000012703143, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1229
Epoch 1229, Loss: 0.00000018471220, Improvement: -0.00000004952303, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1230
Epoch 1230, Loss: 0.00000015057869, Improvement: -0.00000003413352, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1231
Epoch 1231, Loss: 0.00000012785273, Improvement: -0.00000002272596, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1232
Epoch 1232, Loss: 0.00000014229029, Improvement: 0.00000001443757, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1233
Epoch 1233, Loss: 0.00000016060073, Improvement: 0.00000001831043, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1234
Epoch 1234, Loss: 0.00000022211318, Improvement: 0.00000006151246, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1235
Epoch 1235, Loss: 0.00000043024807, Improvement: 0.00000020813488, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1236
Epoch 1236, Loss: 0.00000028919475, Improvement: -0.00000014105331, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1237
Epoch 1237, Loss: 0.00000019542110, Improvement: -0.00000009377366, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1238
Epoch 1238, Loss: 0.00000016990652, Improvement: -0.00000002551458, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1239
Epoch 1239, Loss: 0.00000017415165, Improvement: 0.00000000424513, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1240
Epoch 1240, Loss: 0.00000022890318, Improvement: 0.00000005475152, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1241
Epoch 1241, Loss: 0.00000017910107, Improvement: -0.00000004980211, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1242
Epoch 1242, Loss: 0.00000014800490, Improvement: -0.00000003109617, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1243
Epoch 1243, Loss: 0.00000027530425, Improvement: 0.00000012729935, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1244
Epoch 1244, Loss: 0.00000021516268, Improvement: -0.00000006014157, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1245
Epoch 1245, Loss: 0.00000018055579, Improvement: -0.00000003460688, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1246
Epoch 1246, Loss: 0.00000024102374, Improvement: 0.00000006046795, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1247
Epoch 1247, Loss: 0.00000012916836, Improvement: -0.00000011185538, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1248
Epoch 1248, Loss: 0.00000010746599, Improvement: -0.00000002170237, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1249
Epoch 1249, Loss: 0.00000009661675, Improvement: -0.00000001084924, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000009014093, Improvement: -0.00000000647581, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1251
Epoch 1251, Loss: 0.00000008639741, Improvement: -0.00000000374352, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1252
Epoch 1252, Loss: 0.00000009069054, Improvement: 0.00000000429313, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1253
Epoch 1253, Loss: 0.00000018434580, Improvement: 0.00000009365526, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1254
Epoch 1254, Loss: 0.00000021972996, Improvement: 0.00000003538415, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1255
Epoch 1255, Loss: 0.00000010685275, Improvement: -0.00000011287721, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1256
Epoch 1256, Loss: 0.00000018037313, Improvement: 0.00000007352038, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1257
Epoch 1257, Loss: 0.00000018208613, Improvement: 0.00000000171300, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1258
Epoch 1258, Loss: 0.00000034010449, Improvement: 0.00000015801836, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1259
Epoch 1259, Loss: 0.00000033684891, Improvement: -0.00000000325558, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1260
Epoch 1260, Loss: 0.00000025414554, Improvement: -0.00000008270338, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1261
Epoch 1261, Loss: 0.00000016179836, Improvement: -0.00000009234717, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1262
Epoch 1262, Loss: 0.00000014091926, Improvement: -0.00000002087910, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1263
Epoch 1263, Loss: 0.00000017921410, Improvement: 0.00000003829483, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1264
Epoch 1264, Loss: 0.00000015592080, Improvement: -0.00000002329330, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1265
Epoch 1265, Loss: 0.00000013829506, Improvement: -0.00000001762574, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1266
Epoch 1266, Loss: 0.00000010306124, Improvement: -0.00000003523383, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1267
Epoch 1267, Loss: 0.00000013316346, Improvement: 0.00000003010222, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1268
Epoch 1268, Loss: 0.00000014581772, Improvement: 0.00000001265427, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1269
Epoch 1269, Loss: 0.00000013603080, Improvement: -0.00000000978692, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1270
Epoch 1270, Loss: 0.00000020859292, Improvement: 0.00000007256212, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1271
Epoch 1271, Loss: 0.00000018690304, Improvement: -0.00000002168988, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1272
Epoch 1272, Loss: 0.00000029479649, Improvement: 0.00000010789345, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1273
Epoch 1273, Loss: 0.00000036369722, Improvement: 0.00000006890072, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1274
Epoch 1274, Loss: 0.00000021656539, Improvement: -0.00000014713182, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1275
Epoch 1275, Loss: 0.00000015844137, Improvement: -0.00000005812403, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1276
Epoch 1276, Loss: 0.00000015497368, Improvement: -0.00000000346769, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1277
Epoch 1277, Loss: 0.00000010280771, Improvement: -0.00000005216597, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1278
Epoch 1278, Loss: 0.00000009455856, Improvement: -0.00000000824916, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1279
Epoch 1279, Loss: 0.00000009630576, Improvement: 0.00000000174721, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1280
Epoch 1280, Loss: 0.00000011374949, Improvement: 0.00000001744373, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1281
Epoch 1281, Loss: 0.00000016391375, Improvement: 0.00000005016426, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1282
Epoch 1282, Loss: 0.00000023929069, Improvement: 0.00000007537694, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1283
Epoch 1283, Loss: 0.00000018350765, Improvement: -0.00000005578304, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1284
Epoch 1284, Loss: 0.00000019039997, Improvement: 0.00000000689232, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1285
Epoch 1285, Loss: 0.00000022950634, Improvement: 0.00000003910637, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1286
Epoch 1286, Loss: 0.00000024118943, Improvement: 0.00000001168309, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1287
Epoch 1287, Loss: 0.00000046154381, Improvement: 0.00000022035438, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1288
Epoch 1288, Loss: 0.00000036241740, Improvement: -0.00000009912641, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1289
Epoch 1289, Loss: 0.00000025900501, Improvement: -0.00000010341239, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1290
Epoch 1290, Loss: 0.00000013472929, Improvement: -0.00000012427572, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1291
Epoch 1291, Loss: 0.00000009864347, Improvement: -0.00000003608581, Best Loss: 0.00000005002319 in Epoch 1225
Epoch 1292
A best model at epoch 1292 has been saved with training error 0.00000004791377.
Epoch 1292, Loss: 0.00000008079748, Improvement: -0.00000001784599, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1293
Epoch 1293, Loss: 0.00000007370775, Improvement: -0.00000000708973, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1294
Epoch 1294, Loss: 0.00000007796304, Improvement: 0.00000000425529, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1295
Epoch 1295, Loss: 0.00000008160471, Improvement: 0.00000000364168, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1296
Epoch 1296, Loss: 0.00000007711962, Improvement: -0.00000000448510, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1297
Epoch 1297, Loss: 0.00000009214276, Improvement: 0.00000001502314, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1298
Epoch 1298, Loss: 0.00000010086859, Improvement: 0.00000000872584, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1299
Epoch 1299, Loss: 0.00000009509903, Improvement: -0.00000000576957, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000008939118, Improvement: -0.00000000570784, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1301
Epoch 1301, Loss: 0.00000008921343, Improvement: -0.00000000017775, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1302
Epoch 1302, Loss: 0.00000010818768, Improvement: 0.00000001897425, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1303
Epoch 1303, Loss: 0.00000043584202, Improvement: 0.00000032765435, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1304
Epoch 1304, Loss: 0.00000030328631, Improvement: -0.00000013255572, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1305
Epoch 1305, Loss: 0.00000025772097, Improvement: -0.00000004556533, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1306
Epoch 1306, Loss: 0.00000017973325, Improvement: -0.00000007798772, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1307
Epoch 1307, Loss: 0.00000012426072, Improvement: -0.00000005547254, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1308
Epoch 1308, Loss: 0.00000009060276, Improvement: -0.00000003365796, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1309
Epoch 1309, Loss: 0.00000007559866, Improvement: -0.00000001500410, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1310
Epoch 1310, Loss: 0.00000006611315, Improvement: -0.00000000948551, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1311
Epoch 1311, Loss: 0.00000007823569, Improvement: 0.00000001212254, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1312
Epoch 1312, Loss: 0.00000011700205, Improvement: 0.00000003876636, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1313
Epoch 1313, Loss: 0.00000009330750, Improvement: -0.00000002369454, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1314
Epoch 1314, Loss: 0.00000007071352, Improvement: -0.00000002259399, Best Loss: 0.00000004791377 in Epoch 1292
Epoch 1315
A best model at epoch 1315 has been saved with training error 0.00000004399089.
Epoch 1315, Loss: 0.00000006423011, Improvement: -0.00000000648340, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1316
Epoch 1316, Loss: 0.00000008781286, Improvement: 0.00000002358275, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1317
Epoch 1317, Loss: 0.00000009107153, Improvement: 0.00000000325867, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1318
Epoch 1318, Loss: 0.00000008902839, Improvement: -0.00000000204314, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1319
Epoch 1319, Loss: 0.00000009395128, Improvement: 0.00000000492288, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1320
Epoch 1320, Loss: 0.00000010645770, Improvement: 0.00000001250642, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1321
Epoch 1321, Loss: 0.00000011579232, Improvement: 0.00000000933462, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1322
Epoch 1322, Loss: 0.00000013678401, Improvement: 0.00000002099170, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1323
Epoch 1323, Loss: 0.00000035267212, Improvement: 0.00000021588810, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1324
Epoch 1324, Loss: 0.00000021746538, Improvement: -0.00000013520674, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1325
Epoch 1325, Loss: 0.00000015626376, Improvement: -0.00000006120162, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1326
Epoch 1326, Loss: 0.00000014604922, Improvement: -0.00000001021454, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1327
Epoch 1327, Loss: 0.00000010766795, Improvement: -0.00000003838127, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1328
Epoch 1328, Loss: 0.00000008402285, Improvement: -0.00000002364510, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1329
Epoch 1329, Loss: 0.00000009002627, Improvement: 0.00000000600342, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1330
Epoch 1330, Loss: 0.00000009810330, Improvement: 0.00000000807703, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1331
Epoch 1331, Loss: 0.00000011765863, Improvement: 0.00000001955532, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1332
Epoch 1332, Loss: 0.00000013959857, Improvement: 0.00000002193995, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1333
Epoch 1333, Loss: 0.00000018125072, Improvement: 0.00000004165215, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1334
Epoch 1334, Loss: 0.00000018217721, Improvement: 0.00000000092648, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1335
Epoch 1335, Loss: 0.00000043597688, Improvement: 0.00000025379967, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1336
Epoch 1336, Loss: 0.00000052198858, Improvement: 0.00000008601171, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1337
Epoch 1337, Loss: 0.00000036191258, Improvement: -0.00000016007601, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1338
Epoch 1338, Loss: 0.00000029112467, Improvement: -0.00000007078790, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1339
Epoch 1339, Loss: 0.00000010945173, Improvement: -0.00000018167295, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1340
Epoch 1340, Loss: 0.00000009140078, Improvement: -0.00000001805095, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1341
Epoch 1341, Loss: 0.00000007756288, Improvement: -0.00000001383789, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1342
Epoch 1342, Loss: 0.00000006966366, Improvement: -0.00000000789923, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1343
Epoch 1343, Loss: 0.00000006645427, Improvement: -0.00000000320939, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1344
Epoch 1344, Loss: 0.00000006532670, Improvement: -0.00000000112757, Best Loss: 0.00000004399089 in Epoch 1315
Epoch 1345
A best model at epoch 1345 has been saved with training error 0.00000004229064.
Epoch 1345, Loss: 0.00000006587191, Improvement: 0.00000000054522, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1346
Epoch 1346, Loss: 0.00000007567731, Improvement: 0.00000000980540, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1347
Epoch 1347, Loss: 0.00000010123357, Improvement: 0.00000002555626, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1348
Epoch 1348, Loss: 0.00000011290323, Improvement: 0.00000001166967, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1349
Epoch 1349, Loss: 0.00000011266112, Improvement: -0.00000000024211, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000012194211, Improvement: 0.00000000928098, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1351
Epoch 1351, Loss: 0.00000015047064, Improvement: 0.00000002852853, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1352
Epoch 1352, Loss: 0.00000014378704, Improvement: -0.00000000668360, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1353
Epoch 1353, Loss: 0.00000008856966, Improvement: -0.00000005521738, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1354
Epoch 1354, Loss: 0.00000008483195, Improvement: -0.00000000373771, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1355
Epoch 1355, Loss: 0.00000007410402, Improvement: -0.00000001072793, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1356
Epoch 1356, Loss: 0.00000006632144, Improvement: -0.00000000778258, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1357
Epoch 1357, Loss: 0.00000007601399, Improvement: 0.00000000969255, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1358
Epoch 1358, Loss: 0.00000006856387, Improvement: -0.00000000745013, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1359
Epoch 1359, Loss: 0.00000007526092, Improvement: 0.00000000669705, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1360
Epoch 1360, Loss: 0.00000009222787, Improvement: 0.00000001696695, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1361
Epoch 1361, Loss: 0.00000011127439, Improvement: 0.00000001904652, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1362
Epoch 1362, Loss: 0.00000013363678, Improvement: 0.00000002236239, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1363
Epoch 1363, Loss: 0.00000021697820, Improvement: 0.00000008334142, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1364
Epoch 1364, Loss: 0.00000017708280, Improvement: -0.00000003989540, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1365
Epoch 1365, Loss: 0.00000018983740, Improvement: 0.00000001275460, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1366
Epoch 1366, Loss: 0.00000022124446, Improvement: 0.00000003140707, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1367
Epoch 1367, Loss: 0.00000024929728, Improvement: 0.00000002805281, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1368
Epoch 1368, Loss: 0.00000023068533, Improvement: -0.00000001861194, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1369
Epoch 1369, Loss: 0.00000017900466, Improvement: -0.00000005168067, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1370
Epoch 1370, Loss: 0.00000013848138, Improvement: -0.00000004052328, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1371
Epoch 1371, Loss: 0.00000010804460, Improvement: -0.00000003043678, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1372
Epoch 1372, Loss: 0.00000008094473, Improvement: -0.00000002709987, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1373
Epoch 1373, Loss: 0.00000011205243, Improvement: 0.00000003110769, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1374
Epoch 1374, Loss: 0.00000026549077, Improvement: 0.00000015343835, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1375
Epoch 1375, Loss: 0.00000026068081, Improvement: -0.00000000480997, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1376
Epoch 1376, Loss: 0.00000034116113, Improvement: 0.00000008048033, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1377
Epoch 1377, Loss: 0.00000018058365, Improvement: -0.00000016057748, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1378
Epoch 1378, Loss: 0.00000012606417, Improvement: -0.00000005451949, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1379
Epoch 1379, Loss: 0.00000008434864, Improvement: -0.00000004171553, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1380
Epoch 1380, Loss: 0.00000007228515, Improvement: -0.00000001206348, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1381
Epoch 1381, Loss: 0.00000007671194, Improvement: 0.00000000442679, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1382
Epoch 1382, Loss: 0.00000006550891, Improvement: -0.00000001120303, Best Loss: 0.00000004229064 in Epoch 1345
Epoch 1383
A best model at epoch 1383 has been saved with training error 0.00000004213864.
Epoch 1383, Loss: 0.00000006875749, Improvement: 0.00000000324858, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1384
Epoch 1384, Loss: 0.00000008330772, Improvement: 0.00000001455023, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1385
Epoch 1385, Loss: 0.00000010867704, Improvement: 0.00000002536932, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1386
Epoch 1386, Loss: 0.00000025153668, Improvement: 0.00000014285964, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1387
Epoch 1387, Loss: 0.00000019522297, Improvement: -0.00000005631371, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1388
Epoch 1388, Loss: 0.00000010996483, Improvement: -0.00000008525814, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1389
Epoch 1389, Loss: 0.00000009324259, Improvement: -0.00000001672225, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1390
Epoch 1390, Loss: 0.00000011861423, Improvement: 0.00000002537164, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1391
Epoch 1391, Loss: 0.00000010013917, Improvement: -0.00000001847505, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1392
Epoch 1392, Loss: 0.00000011902187, Improvement: 0.00000001888269, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1393
Epoch 1393, Loss: 0.00000017112757, Improvement: 0.00000005210571, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1394
Epoch 1394, Loss: 0.00000016727431, Improvement: -0.00000000385327, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1395
Epoch 1395, Loss: 0.00000010889221, Improvement: -0.00000005838210, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1396
Epoch 1396, Loss: 0.00000011786341, Improvement: 0.00000000897120, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1397
Epoch 1397, Loss: 0.00000019436885, Improvement: 0.00000007650544, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1398
Epoch 1398, Loss: 0.00000024656678, Improvement: 0.00000005219793, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1399
Epoch 1399, Loss: 0.00000020496402, Improvement: -0.00000004160276, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000025920445, Improvement: 0.00000005424044, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1401
Epoch 1401, Loss: 0.00000030818525, Improvement: 0.00000004898080, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1402
Epoch 1402, Loss: 0.00000050231389, Improvement: 0.00000019412863, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1403
Epoch 1403, Loss: 0.00000025926814, Improvement: -0.00000024304575, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1404
Epoch 1404, Loss: 0.00000015333577, Improvement: -0.00000010593237, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1405
Epoch 1405, Loss: 0.00000014729830, Improvement: -0.00000000603747, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1406
Epoch 1406, Loss: 0.00000024968938, Improvement: 0.00000010239108, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1407
Epoch 1407, Loss: 0.00000020197477, Improvement: -0.00000004771461, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1408
Epoch 1408, Loss: 0.00000012047412, Improvement: -0.00000008150065, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1409
Epoch 1409, Loss: 0.00000008317458, Improvement: -0.00000003729953, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1410
Epoch 1410, Loss: 0.00000007235904, Improvement: -0.00000001081554, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1411
Epoch 1411, Loss: 0.00000007192836, Improvement: -0.00000000043068, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1412
Epoch 1412, Loss: 0.00000007165758, Improvement: -0.00000000027078, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1413
Epoch 1413, Loss: 0.00000007261955, Improvement: 0.00000000096196, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1414
Epoch 1414, Loss: 0.00000007124586, Improvement: -0.00000000137368, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1415
Epoch 1415, Loss: 0.00000009882140, Improvement: 0.00000002757554, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1416
Epoch 1416, Loss: 0.00000011944087, Improvement: 0.00000002061947, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1417
Epoch 1417, Loss: 0.00000009654647, Improvement: -0.00000002289440, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1418
Epoch 1418, Loss: 0.00000008207761, Improvement: -0.00000001446885, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1419
Epoch 1419, Loss: 0.00000007009440, Improvement: -0.00000001198321, Best Loss: 0.00000004213864 in Epoch 1383
Epoch 1420
A best model at epoch 1420 has been saved with training error 0.00000003754984.
Epoch 1420, Loss: 0.00000005847367, Improvement: -0.00000001162074, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1421
Epoch 1421, Loss: 0.00000006246654, Improvement: 0.00000000399287, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1422
Epoch 1422, Loss: 0.00000006519016, Improvement: 0.00000000272362, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1423
Epoch 1423, Loss: 0.00000006312377, Improvement: -0.00000000206638, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1424
Epoch 1424, Loss: 0.00000007192804, Improvement: 0.00000000880427, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1425
Epoch 1425, Loss: 0.00000010418958, Improvement: 0.00000003226154, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1426
Epoch 1426, Loss: 0.00000017374893, Improvement: 0.00000006955935, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1427
Epoch 1427, Loss: 0.00000011134238, Improvement: -0.00000006240656, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1428
Epoch 1428, Loss: 0.00000008285699, Improvement: -0.00000002848538, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1429
Epoch 1429, Loss: 0.00000011354470, Improvement: 0.00000003068771, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1430
Epoch 1430, Loss: 0.00000026473858, Improvement: 0.00000015119388, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1431
Epoch 1431, Loss: 0.00000017419031, Improvement: -0.00000009054827, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1432
Epoch 1432, Loss: 0.00000010899194, Improvement: -0.00000006519837, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1433
Epoch 1433, Loss: 0.00000015193357, Improvement: 0.00000004294162, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1434
Epoch 1434, Loss: 0.00000017063205, Improvement: 0.00000001869848, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1435
Epoch 1435, Loss: 0.00000011210943, Improvement: -0.00000005852262, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1436
Epoch 1436, Loss: 0.00000009661418, Improvement: -0.00000001549525, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1437
Epoch 1437, Loss: 0.00000011507527, Improvement: 0.00000001846109, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1438
Epoch 1438, Loss: 0.00000016201514, Improvement: 0.00000004693987, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1439
Epoch 1439, Loss: 0.00000024072739, Improvement: 0.00000007871226, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1440
Epoch 1440, Loss: 0.00000017061574, Improvement: -0.00000007011165, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1441
Epoch 1441, Loss: 0.00000018023784, Improvement: 0.00000000962211, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1442
Epoch 1442, Loss: 0.00000023548095, Improvement: 0.00000005524311, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1443
Epoch 1443, Loss: 0.00000018585707, Improvement: -0.00000004962388, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1444
Epoch 1444, Loss: 0.00000011758681, Improvement: -0.00000006827026, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1445
Epoch 1445, Loss: 0.00000008870819, Improvement: -0.00000002887862, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1446
Epoch 1446, Loss: 0.00000008658444, Improvement: -0.00000000212375, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1447
Epoch 1447, Loss: 0.00000007637189, Improvement: -0.00000001021255, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1448
Epoch 1448, Loss: 0.00000008461331, Improvement: 0.00000000824142, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1449
Epoch 1449, Loss: 0.00000013310836, Improvement: 0.00000004849505, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000014961415, Improvement: 0.00000001650580, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1451
Epoch 1451, Loss: 0.00000021467168, Improvement: 0.00000006505753, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1452
Epoch 1452, Loss: 0.00000018463052, Improvement: -0.00000003004116, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1453
Epoch 1453, Loss: 0.00000017458932, Improvement: -0.00000001004120, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1454
Epoch 1454, Loss: 0.00000033386616, Improvement: 0.00000015927684, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1455
Epoch 1455, Loss: 0.00000020913987, Improvement: -0.00000012472629, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1456
Epoch 1456, Loss: 0.00000012689668, Improvement: -0.00000008224319, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1457
Epoch 1457, Loss: 0.00000008364550, Improvement: -0.00000004325118, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1458
Epoch 1458, Loss: 0.00000007338181, Improvement: -0.00000001026369, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1459
Epoch 1459, Loss: 0.00000007047922, Improvement: -0.00000000290259, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1460
Epoch 1460, Loss: 0.00000007837080, Improvement: 0.00000000789158, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1461
Epoch 1461, Loss: 0.00000008314663, Improvement: 0.00000000477583, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1462
Epoch 1462, Loss: 0.00000007190149, Improvement: -0.00000001124514, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1463
Epoch 1463, Loss: 0.00000006670712, Improvement: -0.00000000519437, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1464
Epoch 1464, Loss: 0.00000007315796, Improvement: 0.00000000645084, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1465
Epoch 1465, Loss: 0.00000006309798, Improvement: -0.00000001005998, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1466
Epoch 1466, Loss: 0.00000006126106, Improvement: -0.00000000183692, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1467
Epoch 1467, Loss: 0.00000005536556, Improvement: -0.00000000589550, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1468
Epoch 1468, Loss: 0.00000006561893, Improvement: 0.00000001025337, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1469
Epoch 1469, Loss: 0.00000007291900, Improvement: 0.00000000730006, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1470
Epoch 1470, Loss: 0.00000007546368, Improvement: 0.00000000254468, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1471
Epoch 1471, Loss: 0.00000012985013, Improvement: 0.00000005438646, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1472
Epoch 1472, Loss: 0.00000017093957, Improvement: 0.00000004108944, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1473
Epoch 1473, Loss: 0.00000022259191, Improvement: 0.00000005165234, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1474
Epoch 1474, Loss: 0.00000017816063, Improvement: -0.00000004443129, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1475
Epoch 1475, Loss: 0.00000013126859, Improvement: -0.00000004689203, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1476
Epoch 1476, Loss: 0.00000009202162, Improvement: -0.00000003924698, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1477
Epoch 1477, Loss: 0.00000010778769, Improvement: 0.00000001576607, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1478
Epoch 1478, Loss: 0.00000010235301, Improvement: -0.00000000543468, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1479
Epoch 1479, Loss: 0.00000010510419, Improvement: 0.00000000275117, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1480
Epoch 1480, Loss: 0.00000010861373, Improvement: 0.00000000350955, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1481
Epoch 1481, Loss: 0.00000018077068, Improvement: 0.00000007215695, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1482
Epoch 1482, Loss: 0.00000020833694, Improvement: 0.00000002756626, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1483
Epoch 1483, Loss: 0.00000026642873, Improvement: 0.00000005809178, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1484
Epoch 1484, Loss: 0.00000022015817, Improvement: -0.00000004627056, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1485
Epoch 1485, Loss: 0.00000038588334, Improvement: 0.00000016572517, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1486
Epoch 1486, Loss: 0.00000027753293, Improvement: -0.00000010835041, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1487
Epoch 1487, Loss: 0.00000016818020, Improvement: -0.00000010935273, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1488
Epoch 1488, Loss: 0.00000010836756, Improvement: -0.00000005981264, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1489
Epoch 1489, Loss: 0.00000007996618, Improvement: -0.00000002840138, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1490
Epoch 1490, Loss: 0.00000006444149, Improvement: -0.00000001552469, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1491
Epoch 1491, Loss: 0.00000007773432, Improvement: 0.00000001329283, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1492
Epoch 1492, Loss: 0.00000006906136, Improvement: -0.00000000867296, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1493
Epoch 1493, Loss: 0.00000006419698, Improvement: -0.00000000486438, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1494
Epoch 1494, Loss: 0.00000005524674, Improvement: -0.00000000895024, Best Loss: 0.00000003754984 in Epoch 1420
Epoch 1495
A best model at epoch 1495 has been saved with training error 0.00000003740685.
Epoch 1495, Loss: 0.00000005560610, Improvement: 0.00000000035936, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1496
Epoch 1496, Loss: 0.00000006848047, Improvement: 0.00000001287437, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1497
Epoch 1497, Loss: 0.00000007370277, Improvement: 0.00000000522230, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1498
Epoch 1498, Loss: 0.00000006984036, Improvement: -0.00000000386241, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1499
Epoch 1499, Loss: 0.00000006643451, Improvement: -0.00000000340585, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000007194738, Improvement: 0.00000000551288, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1501
Epoch 1501, Loss: 0.00000006465101, Improvement: -0.00000000729638, Best Loss: 0.00000003740685 in Epoch 1495
Epoch 1502
A best model at epoch 1502 has been saved with training error 0.00000003589764.
Epoch 1502, Loss: 0.00000005549469, Improvement: -0.00000000915631, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1503
Epoch 1503, Loss: 0.00000007212545, Improvement: 0.00000001663076, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1504
Epoch 1504, Loss: 0.00000008722946, Improvement: 0.00000001510401, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1505
Epoch 1505, Loss: 0.00000020713180, Improvement: 0.00000011990233, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1506
Epoch 1506, Loss: 0.00000021573155, Improvement: 0.00000000859975, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1507
Epoch 1507, Loss: 0.00000014511057, Improvement: -0.00000007062098, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1508
Epoch 1508, Loss: 0.00000009397336, Improvement: -0.00000005113721, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1509
Epoch 1509, Loss: 0.00000010234220, Improvement: 0.00000000836884, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1510
Epoch 1510, Loss: 0.00000013890059, Improvement: 0.00000003655840, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1511
Epoch 1511, Loss: 0.00000008233037, Improvement: -0.00000005657022, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1512
Epoch 1512, Loss: 0.00000007871194, Improvement: -0.00000000361843, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1513
Epoch 1513, Loss: 0.00000014821634, Improvement: 0.00000006950440, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1514
Epoch 1514, Loss: 0.00000024432762, Improvement: 0.00000009611128, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1515
Epoch 1515, Loss: 0.00000015539684, Improvement: -0.00000008893079, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1516
Epoch 1516, Loss: 0.00000012912678, Improvement: -0.00000002627006, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1517
Epoch 1517, Loss: 0.00000008938843, Improvement: -0.00000003973835, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1518
Epoch 1518, Loss: 0.00000012195982, Improvement: 0.00000003257139, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1519
Epoch 1519, Loss: 0.00000025874747, Improvement: 0.00000013678765, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1520
Epoch 1520, Loss: 0.00000040008549, Improvement: 0.00000014133802, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1521
Epoch 1521, Loss: 0.00000023857900, Improvement: -0.00000016150649, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1522
Epoch 1522, Loss: 0.00000018557613, Improvement: -0.00000005300287, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1523
Epoch 1523, Loss: 0.00000015117850, Improvement: -0.00000003439763, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1524
Epoch 1524, Loss: 0.00000011852873, Improvement: -0.00000003264977, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1525
Epoch 1525, Loss: 0.00000010069918, Improvement: -0.00000001782955, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1526
Epoch 1526, Loss: 0.00000005773796, Improvement: -0.00000004296122, Best Loss: 0.00000003589764 in Epoch 1502
Epoch 1527
A best model at epoch 1527 has been saved with training error 0.00000003340091.
Epoch 1527, Loss: 0.00000005128452, Improvement: -0.00000000645344, Best Loss: 0.00000003340091 in Epoch 1527
Epoch 1528
Epoch 1528, Loss: 0.00000005057405, Improvement: -0.00000000071047, Best Loss: 0.00000003340091 in Epoch 1527
Epoch 1529
Epoch 1529, Loss: 0.00000005189082, Improvement: 0.00000000131676, Best Loss: 0.00000003340091 in Epoch 1527
Epoch 1530
Epoch 1530, Loss: 0.00000004903766, Improvement: -0.00000000285316, Best Loss: 0.00000003340091 in Epoch 1527
Epoch 1531
Epoch 1531, Loss: 0.00000004368681, Improvement: -0.00000000535084, Best Loss: 0.00000003340091 in Epoch 1527
Epoch 1532
A best model at epoch 1532 has been saved with training error 0.00000003066531.
Epoch 1532, Loss: 0.00000004579373, Improvement: 0.00000000210691, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1533
Epoch 1533, Loss: 0.00000005124707, Improvement: 0.00000000545334, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1534
Epoch 1534, Loss: 0.00000005678025, Improvement: 0.00000000553318, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1535
Epoch 1535, Loss: 0.00000006849515, Improvement: 0.00000001171490, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1536
Epoch 1536, Loss: 0.00000007588265, Improvement: 0.00000000738750, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1537
Epoch 1537, Loss: 0.00000006446782, Improvement: -0.00000001141483, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1538
Epoch 1538, Loss: 0.00000007556945, Improvement: 0.00000001110163, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1539
Epoch 1539, Loss: 0.00000008501631, Improvement: 0.00000000944687, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1540
Epoch 1540, Loss: 0.00000010634531, Improvement: 0.00000002132900, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1541
Epoch 1541, Loss: 0.00000026715265, Improvement: 0.00000016080734, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1542
Epoch 1542, Loss: 0.00000022381626, Improvement: -0.00000004333639, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1543
Epoch 1543, Loss: 0.00000012998095, Improvement: -0.00000009383532, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1544
Epoch 1544, Loss: 0.00000006954068, Improvement: -0.00000006044027, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1545
Epoch 1545, Loss: 0.00000007198878, Improvement: 0.00000000244810, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1546
Epoch 1546, Loss: 0.00000005615886, Improvement: -0.00000001582991, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1547
Epoch 1547, Loss: 0.00000005479380, Improvement: -0.00000000136506, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1548
Epoch 1548, Loss: 0.00000005717107, Improvement: 0.00000000237727, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1549
Epoch 1549, Loss: 0.00000005880613, Improvement: 0.00000000163506, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000005642435, Improvement: -0.00000000238177, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1551
Epoch 1551, Loss: 0.00000006980461, Improvement: 0.00000001338026, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1552
Epoch 1552, Loss: 0.00000008225573, Improvement: 0.00000001245112, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1553
Epoch 1553, Loss: 0.00000008921308, Improvement: 0.00000000695735, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1554
Epoch 1554, Loss: 0.00000012772603, Improvement: 0.00000003851295, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1555
Epoch 1555, Loss: 0.00000013804763, Improvement: 0.00000001032160, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1556
Epoch 1556, Loss: 0.00000018895230, Improvement: 0.00000005090468, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1557
Epoch 1557, Loss: 0.00000035601368, Improvement: 0.00000016706138, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1558
Epoch 1558, Loss: 0.00000038470996, Improvement: 0.00000002869628, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1559
Epoch 1559, Loss: 0.00000030137444, Improvement: -0.00000008333552, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1560
Epoch 1560, Loss: 0.00000033048319, Improvement: 0.00000002910875, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1561
Epoch 1561, Loss: 0.00000025950538, Improvement: -0.00000007097781, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1562
Epoch 1562, Loss: 0.00000016038617, Improvement: -0.00000009911922, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1563
Epoch 1563, Loss: 0.00000008244472, Improvement: -0.00000007794144, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1564
Epoch 1564, Loss: 0.00000005902965, Improvement: -0.00000002341507, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1565
Epoch 1565, Loss: 0.00000005609358, Improvement: -0.00000000293607, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1566
Epoch 1566, Loss: 0.00000005213780, Improvement: -0.00000000395578, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1567
Epoch 1567, Loss: 0.00000005100715, Improvement: -0.00000000113066, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1568
Epoch 1568, Loss: 0.00000004760513, Improvement: -0.00000000340202, Best Loss: 0.00000003066531 in Epoch 1532
Epoch 1569
A best model at epoch 1569 has been saved with training error 0.00000003049033.
Epoch 1569, Loss: 0.00000004839771, Improvement: 0.00000000079259, Best Loss: 0.00000003049033 in Epoch 1569
Epoch 1570
Epoch 1570, Loss: 0.00000004920165, Improvement: 0.00000000080394, Best Loss: 0.00000003049033 in Epoch 1569
Epoch 1571
A best model at epoch 1571 has been saved with training error 0.00000002952335.
Epoch 1571, Loss: 0.00000004636704, Improvement: -0.00000000283461, Best Loss: 0.00000002952335 in Epoch 1571
Epoch 1572
Epoch 1572, Loss: 0.00000004384109, Improvement: -0.00000000252594, Best Loss: 0.00000002952335 in Epoch 1571
Epoch 1573
Epoch 1573, Loss: 0.00000004514883, Improvement: 0.00000000130773, Best Loss: 0.00000002952335 in Epoch 1571
Epoch 1574
Epoch 1574, Loss: 0.00000004670867, Improvement: 0.00000000155984, Best Loss: 0.00000002952335 in Epoch 1571
Epoch 1575
Epoch 1575, Loss: 0.00000004742007, Improvement: 0.00000000071140, Best Loss: 0.00000002952335 in Epoch 1571
Epoch 1576
A best model at epoch 1576 has been saved with training error 0.00000002656138.
Epoch 1576, Loss: 0.00000004400432, Improvement: -0.00000000341575, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1577
Epoch 1577, Loss: 0.00000004609801, Improvement: 0.00000000209369, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1578
Epoch 1578, Loss: 0.00000004739105, Improvement: 0.00000000129304, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1579
Epoch 1579, Loss: 0.00000004720739, Improvement: -0.00000000018366, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1580
Epoch 1580, Loss: 0.00000004472024, Improvement: -0.00000000248715, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1581
Epoch 1581, Loss: 0.00000006489351, Improvement: 0.00000002017327, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1582
Epoch 1582, Loss: 0.00000009084900, Improvement: 0.00000002595549, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1583
Epoch 1583, Loss: 0.00000013148425, Improvement: 0.00000004063525, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1584
Epoch 1584, Loss: 0.00000013906106, Improvement: 0.00000000757681, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1585
Epoch 1585, Loss: 0.00000034266896, Improvement: 0.00000020360790, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1586
Epoch 1586, Loss: 0.00000031579917, Improvement: -0.00000002686979, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1587
Epoch 1587, Loss: 0.00000024365413, Improvement: -0.00000007214503, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1588
Epoch 1588, Loss: 0.00000017840412, Improvement: -0.00000006525002, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1589
Epoch 1589, Loss: 0.00000010234403, Improvement: -0.00000007606009, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1590
Epoch 1590, Loss: 0.00000006710171, Improvement: -0.00000003524231, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1591
Epoch 1591, Loss: 0.00000005368454, Improvement: -0.00000001341717, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1592
Epoch 1592, Loss: 0.00000004599675, Improvement: -0.00000000768779, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1593
Epoch 1593, Loss: 0.00000004195111, Improvement: -0.00000000404563, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1594
Epoch 1594, Loss: 0.00000004277149, Improvement: 0.00000000082038, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1595
Epoch 1595, Loss: 0.00000003967903, Improvement: -0.00000000309247, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1596
Epoch 1596, Loss: 0.00000003977937, Improvement: 0.00000000010034, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1597
Epoch 1597, Loss: 0.00000004521877, Improvement: 0.00000000543940, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1598
Epoch 1598, Loss: 0.00000004722576, Improvement: 0.00000000200700, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1599
Epoch 1599, Loss: 0.00000007239959, Improvement: 0.00000002517382, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000022242075, Improvement: 0.00000015002117, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1601
Epoch 1601, Loss: 0.00000032617845, Improvement: 0.00000010375770, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1602
Epoch 1602, Loss: 0.00000014645538, Improvement: -0.00000017972308, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1603
Epoch 1603, Loss: 0.00000008629018, Improvement: -0.00000006016520, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1604
Epoch 1604, Loss: 0.00000006746799, Improvement: -0.00000001882218, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1605
Epoch 1605, Loss: 0.00000005072081, Improvement: -0.00000001674718, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1606
Epoch 1606, Loss: 0.00000004306854, Improvement: -0.00000000765227, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1607
Epoch 1607, Loss: 0.00000004442901, Improvement: 0.00000000136047, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1608
Epoch 1608, Loss: 0.00000005354486, Improvement: 0.00000000911585, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1609
Epoch 1609, Loss: 0.00000005271524, Improvement: -0.00000000082963, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1610
Epoch 1610, Loss: 0.00000008086702, Improvement: 0.00000002815178, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1611
Epoch 1611, Loss: 0.00000008327104, Improvement: 0.00000000240402, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1612
Epoch 1612, Loss: 0.00000016959976, Improvement: 0.00000008632872, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1613
Epoch 1613, Loss: 0.00000027963187, Improvement: 0.00000011003212, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1614
Epoch 1614, Loss: 0.00000012445779, Improvement: -0.00000015517408, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1615
Epoch 1615, Loss: 0.00000007840671, Improvement: -0.00000004605108, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1616
Epoch 1616, Loss: 0.00000006865479, Improvement: -0.00000000975193, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1617
Epoch 1617, Loss: 0.00000007818152, Improvement: 0.00000000952674, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1618
Epoch 1618, Loss: 0.00000006794417, Improvement: -0.00000001023736, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1619
Epoch 1619, Loss: 0.00000006240568, Improvement: -0.00000000553848, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1620
Epoch 1620, Loss: 0.00000005733457, Improvement: -0.00000000507111, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1621
Epoch 1621, Loss: 0.00000007125986, Improvement: 0.00000001392529, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1622
Epoch 1622, Loss: 0.00000020350478, Improvement: 0.00000013224492, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1623
Epoch 1623, Loss: 0.00000016113120, Improvement: -0.00000004237358, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1624
Epoch 1624, Loss: 0.00000014562746, Improvement: -0.00000001550374, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1625
Epoch 1625, Loss: 0.00000015878592, Improvement: 0.00000001315846, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1626
Epoch 1626, Loss: 0.00000010530373, Improvement: -0.00000005348219, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1627
Epoch 1627, Loss: 0.00000033843849, Improvement: 0.00000023313476, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1628
Epoch 1628, Loss: 0.00000028539867, Improvement: -0.00000005303982, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1629
Epoch 1629, Loss: 0.00000011855900, Improvement: -0.00000016683966, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1630
Epoch 1630, Loss: 0.00000007515285, Improvement: -0.00000004340615, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1631
Epoch 1631, Loss: 0.00000005777179, Improvement: -0.00000001738106, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1632
Epoch 1632, Loss: 0.00000005548476, Improvement: -0.00000000228703, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1633
Epoch 1633, Loss: 0.00000006828264, Improvement: 0.00000001279788, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1634
Epoch 1634, Loss: 0.00000005692414, Improvement: -0.00000001135851, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1635
Epoch 1635, Loss: 0.00000004615801, Improvement: -0.00000001076613, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1636
Epoch 1636, Loss: 0.00000004419354, Improvement: -0.00000000196447, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1637
Epoch 1637, Loss: 0.00000005066373, Improvement: 0.00000000647020, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1638
Epoch 1638, Loss: 0.00000005746242, Improvement: 0.00000000679869, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1639
Epoch 1639, Loss: 0.00000005076177, Improvement: -0.00000000670065, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1640
Epoch 1640, Loss: 0.00000005374609, Improvement: 0.00000000298431, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1641
Epoch 1641, Loss: 0.00000007726606, Improvement: 0.00000002351997, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1642
Epoch 1642, Loss: 0.00000008962351, Improvement: 0.00000001235745, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1643
Epoch 1643, Loss: 0.00000005689796, Improvement: -0.00000003272555, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1644
Epoch 1644, Loss: 0.00000005508772, Improvement: -0.00000000181024, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1645
Epoch 1645, Loss: 0.00000009285960, Improvement: 0.00000003777188, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1646
Epoch 1646, Loss: 0.00000013894293, Improvement: 0.00000004608333, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1647
Epoch 1647, Loss: 0.00000011858894, Improvement: -0.00000002035399, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1648
Epoch 1648, Loss: 0.00000013971276, Improvement: 0.00000002112382, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1649
Epoch 1649, Loss: 0.00000014764409, Improvement: 0.00000000793133, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000011356254, Improvement: -0.00000003408155, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1651
Epoch 1651, Loss: 0.00000007023480, Improvement: -0.00000004332774, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1652
Epoch 1652, Loss: 0.00000005554406, Improvement: -0.00000001469074, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1653
Epoch 1653, Loss: 0.00000005318144, Improvement: -0.00000000236262, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1654
Epoch 1654, Loss: 0.00000004612860, Improvement: -0.00000000705284, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1655
Epoch 1655, Loss: 0.00000005972207, Improvement: 0.00000001359347, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1656
Epoch 1656, Loss: 0.00000009846063, Improvement: 0.00000003873856, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1657
Epoch 1657, Loss: 0.00000006173225, Improvement: -0.00000003672838, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1658
Epoch 1658, Loss: 0.00000008210978, Improvement: 0.00000002037753, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1659
Epoch 1659, Loss: 0.00000012760231, Improvement: 0.00000004549253, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1660
Epoch 1660, Loss: 0.00000020848725, Improvement: 0.00000008088494, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1661
Epoch 1661, Loss: 0.00000034135463, Improvement: 0.00000013286738, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1662
Epoch 1662, Loss: 0.00000028660304, Improvement: -0.00000005475159, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1663
Epoch 1663, Loss: 0.00000024937664, Improvement: -0.00000003722640, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1664
Epoch 1664, Loss: 0.00000019618838, Improvement: -0.00000005318827, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1665
Epoch 1665, Loss: 0.00000009904666, Improvement: -0.00000009714171, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1666
Epoch 1666, Loss: 0.00000006167674, Improvement: -0.00000003736993, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1667
Epoch 1667, Loss: 0.00000005466459, Improvement: -0.00000000701215, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1668
Epoch 1668, Loss: 0.00000005174738, Improvement: -0.00000000291721, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1669
Epoch 1669, Loss: 0.00000004642420, Improvement: -0.00000000532318, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1670
Epoch 1670, Loss: 0.00000004751181, Improvement: 0.00000000108761, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1671
Epoch 1671, Loss: 0.00000004651036, Improvement: -0.00000000100146, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1672
Epoch 1672, Loss: 0.00000004217051, Improvement: -0.00000000433984, Best Loss: 0.00000002656138 in Epoch 1576
Epoch 1673
A best model at epoch 1673 has been saved with training error 0.00000002559268.
Epoch 1673, Loss: 0.00000004010687, Improvement: -0.00000000206364, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1674
Epoch 1674, Loss: 0.00000004252084, Improvement: 0.00000000241397, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1675
Epoch 1675, Loss: 0.00000004964985, Improvement: 0.00000000712900, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1676
Epoch 1676, Loss: 0.00000011908957, Improvement: 0.00000006943972, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1677
Epoch 1677, Loss: 0.00000016810203, Improvement: 0.00000004901246, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1678
Epoch 1678, Loss: 0.00000021444724, Improvement: 0.00000004634521, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1679
Epoch 1679, Loss: 0.00000013646952, Improvement: -0.00000007797772, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1680
Epoch 1680, Loss: 0.00000010522735, Improvement: -0.00000003124216, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1681
Epoch 1681, Loss: 0.00000008582497, Improvement: -0.00000001940239, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1682
Epoch 1682, Loss: 0.00000004941037, Improvement: -0.00000003641460, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1683
Epoch 1683, Loss: 0.00000004797572, Improvement: -0.00000000143465, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1684
Epoch 1684, Loss: 0.00000005522273, Improvement: 0.00000000724701, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1685
Epoch 1685, Loss: 0.00000007315047, Improvement: 0.00000001792774, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1686
Epoch 1686, Loss: 0.00000007517402, Improvement: 0.00000000202355, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1687
Epoch 1687, Loss: 0.00000006627546, Improvement: -0.00000000889856, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1688
Epoch 1688, Loss: 0.00000007692337, Improvement: 0.00000001064791, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1689
Epoch 1689, Loss: 0.00000015121960, Improvement: 0.00000007429622, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1690
Epoch 1690, Loss: 0.00000016310016, Improvement: 0.00000001188057, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1691
Epoch 1691, Loss: 0.00000009060510, Improvement: -0.00000007249506, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1692
Epoch 1692, Loss: 0.00000006939984, Improvement: -0.00000002120526, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1693
Epoch 1693, Loss: 0.00000005648966, Improvement: -0.00000001291017, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1694
Epoch 1694, Loss: 0.00000004474920, Improvement: -0.00000001174046, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1695
Epoch 1695, Loss: 0.00000004334910, Improvement: -0.00000000140010, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1696
Epoch 1696, Loss: 0.00000006163419, Improvement: 0.00000001828509, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1697
Epoch 1697, Loss: 0.00000008604617, Improvement: 0.00000002441198, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1698
Epoch 1698, Loss: 0.00000007345617, Improvement: -0.00000001259000, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1699
Epoch 1699, Loss: 0.00000005501618, Improvement: -0.00000001843998, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000005852983, Improvement: 0.00000000351364, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1701
Epoch 1701, Loss: 0.00000013832491, Improvement: 0.00000007979508, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1702
Epoch 1702, Loss: 0.00000031076509, Improvement: 0.00000017244018, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1703
Epoch 1703, Loss: 0.00000018657773, Improvement: -0.00000012418737, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1704
Epoch 1704, Loss: 0.00000009507843, Improvement: -0.00000009149929, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1705
Epoch 1705, Loss: 0.00000007688653, Improvement: -0.00000001819190, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1706
Epoch 1706, Loss: 0.00000006581139, Improvement: -0.00000001107514, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1707
Epoch 1707, Loss: 0.00000006495138, Improvement: -0.00000000086001, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1708
Epoch 1708, Loss: 0.00000007065561, Improvement: 0.00000000570423, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1709
Epoch 1709, Loss: 0.00000006517640, Improvement: -0.00000000547921, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1710
Epoch 1710, Loss: 0.00000006210178, Improvement: -0.00000000307462, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1711
Epoch 1711, Loss: 0.00000007420016, Improvement: 0.00000001209838, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1712
Epoch 1712, Loss: 0.00000006584280, Improvement: -0.00000000835736, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1713
Epoch 1713, Loss: 0.00000012193037, Improvement: 0.00000005608757, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1714
Epoch 1714, Loss: 0.00000024276523, Improvement: 0.00000012083486, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1715
Epoch 1715, Loss: 0.00000014740131, Improvement: -0.00000009536392, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1716
Epoch 1716, Loss: 0.00000010829563, Improvement: -0.00000003910568, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1717
Epoch 1717, Loss: 0.00000009877219, Improvement: -0.00000000952345, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1718
Epoch 1718, Loss: 0.00000009878622, Improvement: 0.00000000001403, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1719
Epoch 1719, Loss: 0.00000007474021, Improvement: -0.00000002404602, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1720
Epoch 1720, Loss: 0.00000005679484, Improvement: -0.00000001794536, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1721
Epoch 1721, Loss: 0.00000006026981, Improvement: 0.00000000347496, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1722
Epoch 1722, Loss: 0.00000005045666, Improvement: -0.00000000981314, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1723
Epoch 1723, Loss: 0.00000004889384, Improvement: -0.00000000156282, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1724
Epoch 1724, Loss: 0.00000004940854, Improvement: 0.00000000051469, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1725
Epoch 1725, Loss: 0.00000006060842, Improvement: 0.00000001119988, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1726
Epoch 1726, Loss: 0.00000008203987, Improvement: 0.00000002143145, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1727
Epoch 1727, Loss: 0.00000012017662, Improvement: 0.00000003813675, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1728
Epoch 1728, Loss: 0.00000009216334, Improvement: -0.00000002801328, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1729
Epoch 1729, Loss: 0.00000008429037, Improvement: -0.00000000787298, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1730
Epoch 1730, Loss: 0.00000014881857, Improvement: 0.00000006452820, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1731
Epoch 1731, Loss: 0.00000026736391, Improvement: 0.00000011854534, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1732
Epoch 1732, Loss: 0.00000015363330, Improvement: -0.00000011373060, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1733
Epoch 1733, Loss: 0.00000008295964, Improvement: -0.00000007067367, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1734
Epoch 1734, Loss: 0.00000006047224, Improvement: -0.00000002248739, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1735
Epoch 1735, Loss: 0.00000006181800, Improvement: 0.00000000134575, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1736
Epoch 1736, Loss: 0.00000004736470, Improvement: -0.00000001445329, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1737
Epoch 1737, Loss: 0.00000004738616, Improvement: 0.00000000002146, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1738
Epoch 1738, Loss: 0.00000005462946, Improvement: 0.00000000724329, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1739
Epoch 1739, Loss: 0.00000006055804, Improvement: 0.00000000592858, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1740
Epoch 1740, Loss: 0.00000005099983, Improvement: -0.00000000955820, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1741
Epoch 1741, Loss: 0.00000004456622, Improvement: -0.00000000643362, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1742
Epoch 1742, Loss: 0.00000004139540, Improvement: -0.00000000317082, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1743
Epoch 1743, Loss: 0.00000004953959, Improvement: 0.00000000814419, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1744
Epoch 1744, Loss: 0.00000007356245, Improvement: 0.00000002402286, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1745
Epoch 1745, Loss: 0.00000011910290, Improvement: 0.00000004554046, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1746
Epoch 1746, Loss: 0.00000014289049, Improvement: 0.00000002378759, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1747
Epoch 1747, Loss: 0.00000013298084, Improvement: -0.00000000990965, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1748
Epoch 1748, Loss: 0.00000011495126, Improvement: -0.00000001802958, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1749
Epoch 1749, Loss: 0.00000019829376, Improvement: 0.00000008334250, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000019098363, Improvement: -0.00000000731013, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1751
Epoch 1751, Loss: 0.00000025550183, Improvement: 0.00000006451819, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1752
Epoch 1752, Loss: 0.00000017719364, Improvement: -0.00000007830819, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1753
Epoch 1753, Loss: 0.00000018248439, Improvement: 0.00000000529075, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1754
Epoch 1754, Loss: 0.00000013577090, Improvement: -0.00000004671349, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1755
Epoch 1755, Loss: 0.00000007409047, Improvement: -0.00000006168043, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1756
Epoch 1756, Loss: 0.00000006655928, Improvement: -0.00000000753120, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1757
Epoch 1757, Loss: 0.00000006629938, Improvement: -0.00000000025989, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1758
Epoch 1758, Loss: 0.00000009865859, Improvement: 0.00000003235921, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1759
Epoch 1759, Loss: 0.00000008032488, Improvement: -0.00000001833371, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1760
Epoch 1760, Loss: 0.00000008710810, Improvement: 0.00000000678322, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1761
Epoch 1761, Loss: 0.00000005457861, Improvement: -0.00000003252949, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1762
Epoch 1762, Loss: 0.00000005739294, Improvement: 0.00000000281432, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1763
Epoch 1763, Loss: 0.00000007065340, Improvement: 0.00000001326046, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1764
Epoch 1764, Loss: 0.00000005938695, Improvement: -0.00000001126645, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1765
Epoch 1765, Loss: 0.00000006773151, Improvement: 0.00000000834457, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1766
Epoch 1766, Loss: 0.00000005620026, Improvement: -0.00000001153125, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1767
Epoch 1767, Loss: 0.00000005416483, Improvement: -0.00000000203544, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1768
Epoch 1768, Loss: 0.00000005655621, Improvement: 0.00000000239139, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1769
Epoch 1769, Loss: 0.00000006053386, Improvement: 0.00000000397765, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1770
Epoch 1770, Loss: 0.00000005863270, Improvement: -0.00000000190116, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1771
Epoch 1771, Loss: 0.00000004120562, Improvement: -0.00000001742708, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1772
Epoch 1772, Loss: 0.00000004601448, Improvement: 0.00000000480886, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1773
Epoch 1773, Loss: 0.00000006524826, Improvement: 0.00000001923378, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1774
Epoch 1774, Loss: 0.00000010733622, Improvement: 0.00000004208796, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1775
Epoch 1775, Loss: 0.00000020178075, Improvement: 0.00000009444453, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1776
Epoch 1776, Loss: 0.00000033109723, Improvement: 0.00000012931648, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1777
Epoch 1777, Loss: 0.00000016115432, Improvement: -0.00000016994291, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1778
Epoch 1778, Loss: 0.00000010388013, Improvement: -0.00000005727418, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1779
Epoch 1779, Loss: 0.00000011577370, Improvement: 0.00000001189357, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1780
Epoch 1780, Loss: 0.00000007828100, Improvement: -0.00000003749270, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1781
Epoch 1781, Loss: 0.00000005207659, Improvement: -0.00000002620440, Best Loss: 0.00000002559268 in Epoch 1673
Epoch 1782
A best model at epoch 1782 has been saved with training error 0.00000002450211.
Epoch 1782, Loss: 0.00000004352482, Improvement: -0.00000000855178, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1783
Epoch 1783, Loss: 0.00000005511065, Improvement: 0.00000001158584, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1784
Epoch 1784, Loss: 0.00000009544956, Improvement: 0.00000004033891, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1785
Epoch 1785, Loss: 0.00000006498301, Improvement: -0.00000003046655, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1786
Epoch 1786, Loss: 0.00000003924573, Improvement: -0.00000002573727, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1787
Epoch 1787, Loss: 0.00000003709228, Improvement: -0.00000000215345, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1788
Epoch 1788, Loss: 0.00000005557658, Improvement: 0.00000001848430, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1789
Epoch 1789, Loss: 0.00000006336164, Improvement: 0.00000000778506, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1790
Epoch 1790, Loss: 0.00000007189606, Improvement: 0.00000000853442, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1791
Epoch 1791, Loss: 0.00000009746941, Improvement: 0.00000002557334, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1792
Epoch 1792, Loss: 0.00000008553444, Improvement: -0.00000001193497, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1793
Epoch 1793, Loss: 0.00000008522513, Improvement: -0.00000000030930, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1794
Epoch 1794, Loss: 0.00000017685786, Improvement: 0.00000009163273, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1795
Epoch 1795, Loss: 0.00000026211993, Improvement: 0.00000008526207, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1796
Epoch 1796, Loss: 0.00000016076248, Improvement: -0.00000010135745, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1797
Epoch 1797, Loss: 0.00000012981798, Improvement: -0.00000003094450, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1798
Epoch 1798, Loss: 0.00000006874411, Improvement: -0.00000006107387, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1799
Epoch 1799, Loss: 0.00000003975791, Improvement: -0.00000002898620, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000003821958, Improvement: -0.00000000153833, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1801
Epoch 1801, Loss: 0.00000005967351, Improvement: 0.00000002145393, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1802
Epoch 1802, Loss: 0.00000005429121, Improvement: -0.00000000538230, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1803
Epoch 1803, Loss: 0.00000004389566, Improvement: -0.00000001039555, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1804
Epoch 1804, Loss: 0.00000003811905, Improvement: -0.00000000577661, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1805
Epoch 1805, Loss: 0.00000004011992, Improvement: 0.00000000200086, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1806
Epoch 1806, Loss: 0.00000004290623, Improvement: 0.00000000278632, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1807
Epoch 1807, Loss: 0.00000004837650, Improvement: 0.00000000547026, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1808
Epoch 1808, Loss: 0.00000004214829, Improvement: -0.00000000622821, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1809
Epoch 1809, Loss: 0.00000004059460, Improvement: -0.00000000155369, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1810
Epoch 1810, Loss: 0.00000004588973, Improvement: 0.00000000529513, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1811
Epoch 1811, Loss: 0.00000004808093, Improvement: 0.00000000219120, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1812
Epoch 1812, Loss: 0.00000005661699, Improvement: 0.00000000853607, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1813
Epoch 1813, Loss: 0.00000009958648, Improvement: 0.00000004296949, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1814
Epoch 1814, Loss: 0.00000010730597, Improvement: 0.00000000771949, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1815
Epoch 1815, Loss: 0.00000006868337, Improvement: -0.00000003862260, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1816
Epoch 1816, Loss: 0.00000005867385, Improvement: -0.00000001000952, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1817
Epoch 1817, Loss: 0.00000012094427, Improvement: 0.00000006227042, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1818
Epoch 1818, Loss: 0.00000015643404, Improvement: 0.00000003548977, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1819
Epoch 1819, Loss: 0.00000019328249, Improvement: 0.00000003684846, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1820
Epoch 1820, Loss: 0.00000023118061, Improvement: 0.00000003789811, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1821
Epoch 1821, Loss: 0.00000016206551, Improvement: -0.00000006911509, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1822
Epoch 1822, Loss: 0.00000012532173, Improvement: -0.00000003674379, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1823
Epoch 1823, Loss: 0.00000009369716, Improvement: -0.00000003162457, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1824
Epoch 1824, Loss: 0.00000010432968, Improvement: 0.00000001063252, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1825
Epoch 1825, Loss: 0.00000007195822, Improvement: -0.00000003237146, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1826
Epoch 1826, Loss: 0.00000006123198, Improvement: -0.00000001072624, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1827
Epoch 1827, Loss: 0.00000005085549, Improvement: -0.00000001037649, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1828
Epoch 1828, Loss: 0.00000005795449, Improvement: 0.00000000709899, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1829
Epoch 1829, Loss: 0.00000003923486, Improvement: -0.00000001871962, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1830
Epoch 1830, Loss: 0.00000003970823, Improvement: 0.00000000047337, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1831
Epoch 1831, Loss: 0.00000003842762, Improvement: -0.00000000128060, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1832
Epoch 1832, Loss: 0.00000004761162, Improvement: 0.00000000918400, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1833
Epoch 1833, Loss: 0.00000006453060, Improvement: 0.00000001691898, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1834
Epoch 1834, Loss: 0.00000004500035, Improvement: -0.00000001953025, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1835
Epoch 1835, Loss: 0.00000008360817, Improvement: 0.00000003860782, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1836
Epoch 1836, Loss: 0.00000010719588, Improvement: 0.00000002358771, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1837
Epoch 1837, Loss: 0.00000011898329, Improvement: 0.00000001178741, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1838
Epoch 1838, Loss: 0.00000013357444, Improvement: 0.00000001459114, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1839
Epoch 1839, Loss: 0.00000009421931, Improvement: -0.00000003935512, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1840
Epoch 1840, Loss: 0.00000020244313, Improvement: 0.00000010822382, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1841
Epoch 1841, Loss: 0.00000017866354, Improvement: -0.00000002377960, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1842
Epoch 1842, Loss: 0.00000010394312, Improvement: -0.00000007472042, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1843
Epoch 1843, Loss: 0.00000015744883, Improvement: 0.00000005350571, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1844
Epoch 1844, Loss: 0.00000011646366, Improvement: -0.00000004098516, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1845
Epoch 1845, Loss: 0.00000014884984, Improvement: 0.00000003238617, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1846
Epoch 1846, Loss: 0.00000014306231, Improvement: -0.00000000578753, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1847
Epoch 1847, Loss: 0.00000010885418, Improvement: -0.00000003420813, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1848
Epoch 1848, Loss: 0.00000006702461, Improvement: -0.00000004182957, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1849
Epoch 1849, Loss: 0.00000004951434, Improvement: -0.00000001751027, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000005080215, Improvement: 0.00000000128780, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1851
Epoch 1851, Loss: 0.00000003819850, Improvement: -0.00000001260364, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1852
Epoch 1852, Loss: 0.00000004162027, Improvement: 0.00000000342177, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1853
Epoch 1853, Loss: 0.00000004363859, Improvement: 0.00000000201832, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1854
Epoch 1854, Loss: 0.00000005610089, Improvement: 0.00000001246231, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1855
Epoch 1855, Loss: 0.00000008584273, Improvement: 0.00000002974184, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1856
Epoch 1856, Loss: 0.00000015517655, Improvement: 0.00000006933382, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1857
Epoch 1857, Loss: 0.00000016839709, Improvement: 0.00000001322054, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1858
Epoch 1858, Loss: 0.00000011077255, Improvement: -0.00000005762453, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1859
Epoch 1859, Loss: 0.00000006700309, Improvement: -0.00000004376946, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1860
Epoch 1860, Loss: 0.00000006697474, Improvement: -0.00000000002836, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1861
Epoch 1861, Loss: 0.00000004487145, Improvement: -0.00000002210329, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1862
Epoch 1862, Loss: 0.00000004592329, Improvement: 0.00000000105185, Best Loss: 0.00000002450211 in Epoch 1782
Epoch 1863
A best model at epoch 1863 has been saved with training error 0.00000002303238.
Epoch 1863, Loss: 0.00000003771296, Improvement: -0.00000000821033, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1864
Epoch 1864, Loss: 0.00000004828124, Improvement: 0.00000001056827, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1865
Epoch 1865, Loss: 0.00000004664072, Improvement: -0.00000000164051, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1866
Epoch 1866, Loss: 0.00000006415433, Improvement: 0.00000001751361, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1867
Epoch 1867, Loss: 0.00000011625666, Improvement: 0.00000005210233, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1868
Epoch 1868, Loss: 0.00000010270068, Improvement: -0.00000001355598, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1869
Epoch 1869, Loss: 0.00000007230706, Improvement: -0.00000003039362, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1870
Epoch 1870, Loss: 0.00000006758466, Improvement: -0.00000000472240, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1871
Epoch 1871, Loss: 0.00000005371029, Improvement: -0.00000001387437, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1872
Epoch 1872, Loss: 0.00000006685875, Improvement: 0.00000001314846, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1873
Epoch 1873, Loss: 0.00000007066627, Improvement: 0.00000000380752, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1874
Epoch 1874, Loss: 0.00000004793042, Improvement: -0.00000002273585, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1875
Epoch 1875, Loss: 0.00000004454898, Improvement: -0.00000000338144, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1876
Epoch 1876, Loss: 0.00000005257889, Improvement: 0.00000000802991, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1877
Epoch 1877, Loss: 0.00000007354604, Improvement: 0.00000002096715, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1878
Epoch 1878, Loss: 0.00000017278513, Improvement: 0.00000009923909, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1879
Epoch 1879, Loss: 0.00000019216959, Improvement: 0.00000001938446, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1880
Epoch 1880, Loss: 0.00000008719637, Improvement: -0.00000010497322, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1881
Epoch 1881, Loss: 0.00000008253328, Improvement: -0.00000000466310, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1882
Epoch 1882, Loss: 0.00000007484249, Improvement: -0.00000000769079, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1883
Epoch 1883, Loss: 0.00000008047580, Improvement: 0.00000000563331, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1884
Epoch 1884, Loss: 0.00000010733526, Improvement: 0.00000002685946, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1885
Epoch 1885, Loss: 0.00000007536507, Improvement: -0.00000003197020, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1886
Epoch 1886, Loss: 0.00000009386135, Improvement: 0.00000001849628, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1887
Epoch 1887, Loss: 0.00000010698125, Improvement: 0.00000001311991, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1888
Epoch 1888, Loss: 0.00000008158458, Improvement: -0.00000002539668, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1889
Epoch 1889, Loss: 0.00000006333297, Improvement: -0.00000001825161, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1890
Epoch 1890, Loss: 0.00000005600781, Improvement: -0.00000000732516, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1891
Epoch 1891, Loss: 0.00000005956830, Improvement: 0.00000000356049, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1892
Epoch 1892, Loss: 0.00000009018310, Improvement: 0.00000003061481, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1893
Epoch 1893, Loss: 0.00000007976290, Improvement: -0.00000001042020, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1894
Epoch 1894, Loss: 0.00000007165319, Improvement: -0.00000000810971, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1895
Epoch 1895, Loss: 0.00000006258110, Improvement: -0.00000000907209, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1896
Epoch 1896, Loss: 0.00000009785113, Improvement: 0.00000003527003, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1897
Epoch 1897, Loss: 0.00000011509018, Improvement: 0.00000001723905, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1898
Epoch 1898, Loss: 0.00000013939020, Improvement: 0.00000002430002, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1899
Epoch 1899, Loss: 0.00000016480410, Improvement: 0.00000002541389, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000016351021, Improvement: -0.00000000129389, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1901
Epoch 1901, Loss: 0.00000010353450, Improvement: -0.00000005997571, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1902
Epoch 1902, Loss: 0.00000011816653, Improvement: 0.00000001463203, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1903
Epoch 1903, Loss: 0.00000011064298, Improvement: -0.00000000752355, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1904
Epoch 1904, Loss: 0.00000010403414, Improvement: -0.00000000660884, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1905
Epoch 1905, Loss: 0.00000007717355, Improvement: -0.00000002686060, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1906
Epoch 1906, Loss: 0.00000005835549, Improvement: -0.00000001881806, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1907
Epoch 1907, Loss: 0.00000004626707, Improvement: -0.00000001208842, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1908
Epoch 1908, Loss: 0.00000004015339, Improvement: -0.00000000611368, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1909
Epoch 1909, Loss: 0.00000004772049, Improvement: 0.00000000756710, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1910
Epoch 1910, Loss: 0.00000015583346, Improvement: 0.00000010811297, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1911
Epoch 1911, Loss: 0.00000009392542, Improvement: -0.00000006190804, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1912
Epoch 1912, Loss: 0.00000006735501, Improvement: -0.00000002657040, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1913
Epoch 1913, Loss: 0.00000005682837, Improvement: -0.00000001052664, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1914
Epoch 1914, Loss: 0.00000006241557, Improvement: 0.00000000558720, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1915
Epoch 1915, Loss: 0.00000006487976, Improvement: 0.00000000246419, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1916
Epoch 1916, Loss: 0.00000008208918, Improvement: 0.00000001720942, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1917
Epoch 1917, Loss: 0.00000007074198, Improvement: -0.00000001134720, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1918
Epoch 1918, Loss: 0.00000005739516, Improvement: -0.00000001334682, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1919
Epoch 1919, Loss: 0.00000006663331, Improvement: 0.00000000923815, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1920
Epoch 1920, Loss: 0.00000005543748, Improvement: -0.00000001119583, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1921
Epoch 1921, Loss: 0.00000006388293, Improvement: 0.00000000844545, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1922
Epoch 1922, Loss: 0.00000015721164, Improvement: 0.00000009332871, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1923
Epoch 1923, Loss: 0.00000012816875, Improvement: -0.00000002904289, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1924
Epoch 1924, Loss: 0.00000011400162, Improvement: -0.00000001416713, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1925
Epoch 1925, Loss: 0.00000017368163, Improvement: 0.00000005968000, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1926
Epoch 1926, Loss: 0.00000014725592, Improvement: -0.00000002642571, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1927
Epoch 1927, Loss: 0.00000012871285, Improvement: -0.00000001854307, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1928
Epoch 1928, Loss: 0.00000010198616, Improvement: -0.00000002672669, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1929
Epoch 1929, Loss: 0.00000009384695, Improvement: -0.00000000813921, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1930
Epoch 1930, Loss: 0.00000006306670, Improvement: -0.00000003078024, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1931
Epoch 1931, Loss: 0.00000005926918, Improvement: -0.00000000379752, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1932
Epoch 1932, Loss: 0.00000006561646, Improvement: 0.00000000634728, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1933
Epoch 1933, Loss: 0.00000011672469, Improvement: 0.00000005110823, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1934
Epoch 1934, Loss: 0.00000016578526, Improvement: 0.00000004906057, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1935
Epoch 1935, Loss: 0.00000012244105, Improvement: -0.00000004334421, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1936
Epoch 1936, Loss: 0.00000009546157, Improvement: -0.00000002697949, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1937
Epoch 1937, Loss: 0.00000010848774, Improvement: 0.00000001302617, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1938
Epoch 1938, Loss: 0.00000011258404, Improvement: 0.00000000409631, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1939
Epoch 1939, Loss: 0.00000007516938, Improvement: -0.00000003741467, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1940
Epoch 1940, Loss: 0.00000006344708, Improvement: -0.00000001172230, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1941
Epoch 1941, Loss: 0.00000006125663, Improvement: -0.00000000219045, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1942
Epoch 1942, Loss: 0.00000004557840, Improvement: -0.00000001567823, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1943
Epoch 1943, Loss: 0.00000007942358, Improvement: 0.00000003384518, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1944
Epoch 1944, Loss: 0.00000011232304, Improvement: 0.00000003289946, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1945
Epoch 1945, Loss: 0.00000011332110, Improvement: 0.00000000099805, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1946
Epoch 1946, Loss: 0.00000009636232, Improvement: -0.00000001695877, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1947
Epoch 1947, Loss: 0.00000012342259, Improvement: 0.00000002706027, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1948
Epoch 1948, Loss: 0.00000013659661, Improvement: 0.00000001317402, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1949
Epoch 1949, Loss: 0.00000023602934, Improvement: 0.00000009943273, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000014331754, Improvement: -0.00000009271179, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1951
Epoch 1951, Loss: 0.00000012186824, Improvement: -0.00000002144930, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1952
Epoch 1952, Loss: 0.00000014888455, Improvement: 0.00000002701631, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1953
Epoch 1953, Loss: 0.00000007758582, Improvement: -0.00000007129873, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1954
Epoch 1954, Loss: 0.00000004955565, Improvement: -0.00000002803017, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1955
Epoch 1955, Loss: 0.00000003961955, Improvement: -0.00000000993609, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1956
Epoch 1956, Loss: 0.00000007470315, Improvement: 0.00000003508360, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1957
Epoch 1957, Loss: 0.00000005793390, Improvement: -0.00000001676925, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1958
Epoch 1958, Loss: 0.00000005980809, Improvement: 0.00000000187419, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1959
Epoch 1959, Loss: 0.00000006077017, Improvement: 0.00000000096208, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1960
Epoch 1960, Loss: 0.00000005113655, Improvement: -0.00000000963362, Best Loss: 0.00000002303238 in Epoch 1863
Epoch 1961
A best model at epoch 1961 has been saved with training error 0.00000002275905.
Epoch 1961, Loss: 0.00000004337663, Improvement: -0.00000000775992, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1962
Epoch 1962, Loss: 0.00000004821815, Improvement: 0.00000000484152, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1963
Epoch 1963, Loss: 0.00000004519233, Improvement: -0.00000000302582, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1964
Epoch 1964, Loss: 0.00000004083373, Improvement: -0.00000000435860, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1965
Epoch 1965, Loss: 0.00000005481649, Improvement: 0.00000001398276, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1966
Epoch 1966, Loss: 0.00000006779638, Improvement: 0.00000001297989, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1967
Epoch 1967, Loss: 0.00000016108679, Improvement: 0.00000009329041, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1968
Epoch 1968, Loss: 0.00000024603596, Improvement: 0.00000008494917, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1969
Epoch 1969, Loss: 0.00000019220972, Improvement: -0.00000005382625, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1970
Epoch 1970, Loss: 0.00000008450817, Improvement: -0.00000010770155, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1971
Epoch 1971, Loss: 0.00000004280676, Improvement: -0.00000004170141, Best Loss: 0.00000002275905 in Epoch 1961
Epoch 1972
A best model at epoch 1972 has been saved with training error 0.00000002123856.
Epoch 1972, Loss: 0.00000003490872, Improvement: -0.00000000789804, Best Loss: 0.00000002123856 in Epoch 1972
Epoch 1973
Epoch 1973, Loss: 0.00000005013798, Improvement: 0.00000001522926, Best Loss: 0.00000002123856 in Epoch 1972
Epoch 1974
Epoch 1974, Loss: 0.00000004719836, Improvement: -0.00000000293962, Best Loss: 0.00000002123856 in Epoch 1972
Epoch 1975
Epoch 1975, Loss: 0.00000003398676, Improvement: -0.00000001321160, Best Loss: 0.00000002123856 in Epoch 1972
Epoch 1976
Epoch 1976, Loss: 0.00000003084415, Improvement: -0.00000000314261, Best Loss: 0.00000002123856 in Epoch 1972
Epoch 1977
A best model at epoch 1977 has been saved with training error 0.00000002107140.
A best model at epoch 1977 has been saved with training error 0.00000002043977.
Epoch 1977, Loss: 0.00000002953071, Improvement: -0.00000000131345, Best Loss: 0.00000002043977 in Epoch 1977
Epoch 1978
A best model at epoch 1978 has been saved with training error 0.00000001962715.
Epoch 1978, Loss: 0.00000002888503, Improvement: -0.00000000064568, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1979
Epoch 1979, Loss: 0.00000003078346, Improvement: 0.00000000189843, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1980
Epoch 1980, Loss: 0.00000003651178, Improvement: 0.00000000572832, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1981
Epoch 1981, Loss: 0.00000004040429, Improvement: 0.00000000389251, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1982
Epoch 1982, Loss: 0.00000004882817, Improvement: 0.00000000842387, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1983
Epoch 1983, Loss: 0.00000011125357, Improvement: 0.00000006242540, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1984
Epoch 1984, Loss: 0.00000008345937, Improvement: -0.00000002779420, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1985
Epoch 1985, Loss: 0.00000005648158, Improvement: -0.00000002697779, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1986
Epoch 1986, Loss: 0.00000005093389, Improvement: -0.00000000554770, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1987
Epoch 1987, Loss: 0.00000006607635, Improvement: 0.00000001514246, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1988
Epoch 1988, Loss: 0.00000010051195, Improvement: 0.00000003443560, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1989
Epoch 1989, Loss: 0.00000008094176, Improvement: -0.00000001957019, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1990
Epoch 1990, Loss: 0.00000009998097, Improvement: 0.00000001903921, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1991
Epoch 1991, Loss: 0.00000015902292, Improvement: 0.00000005904195, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1992
Epoch 1992, Loss: 0.00000011363248, Improvement: -0.00000004539044, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1993
Epoch 1993, Loss: 0.00000008456087, Improvement: -0.00000002907161, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1994
Epoch 1994, Loss: 0.00000005151940, Improvement: -0.00000003304146, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1995
Epoch 1995, Loss: 0.00000004544811, Improvement: -0.00000000607129, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1996
Epoch 1996, Loss: 0.00000004225771, Improvement: -0.00000000319040, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1997
Epoch 1997, Loss: 0.00000004041476, Improvement: -0.00000000184295, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1998
Epoch 1998, Loss: 0.00000004597251, Improvement: 0.00000000555775, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 1999
Epoch 1999, Loss: 0.00000004700406, Improvement: 0.00000000103155, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000003365506, Improvement: -0.00000001334900, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2001
Epoch 2001, Loss: 0.00000004283391, Improvement: 0.00000000917886, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2002
Epoch 2002, Loss: 0.00000015503558, Improvement: 0.00000011220166, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2003
Epoch 2003, Loss: 0.00000021574033, Improvement: 0.00000006070476, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2004
Epoch 2004, Loss: 0.00000010813565, Improvement: -0.00000010760469, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2005
Epoch 2005, Loss: 0.00000006879151, Improvement: -0.00000003934414, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2006
Epoch 2006, Loss: 0.00000004815112, Improvement: -0.00000002064039, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2007
Epoch 2007, Loss: 0.00000003480920, Improvement: -0.00000001334191, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2008
Epoch 2008, Loss: 0.00000003638171, Improvement: 0.00000000157251, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2009
Epoch 2009, Loss: 0.00000004135911, Improvement: 0.00000000497739, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2010
Epoch 2010, Loss: 0.00000003590921, Improvement: -0.00000000544990, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2011
Epoch 2011, Loss: 0.00000003604615, Improvement: 0.00000000013694, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2012
Epoch 2012, Loss: 0.00000003799972, Improvement: 0.00000000195357, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2013
Epoch 2013, Loss: 0.00000003887694, Improvement: 0.00000000087722, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2014
Epoch 2014, Loss: 0.00000003479922, Improvement: -0.00000000407772, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2015
Epoch 2015, Loss: 0.00000004077033, Improvement: 0.00000000597111, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2016
Epoch 2016, Loss: 0.00000005478312, Improvement: 0.00000001401279, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2017
Epoch 2017, Loss: 0.00000007462948, Improvement: 0.00000001984636, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2018
Epoch 2018, Loss: 0.00000021622244, Improvement: 0.00000014159296, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2019
Epoch 2019, Loss: 0.00000025728719, Improvement: 0.00000004106475, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2020
Epoch 2020, Loss: 0.00000015620700, Improvement: -0.00000010108018, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2021
Epoch 2021, Loss: 0.00000010605214, Improvement: -0.00000005015486, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2022
Epoch 2022, Loss: 0.00000007525429, Improvement: -0.00000003079785, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2023
Epoch 2023, Loss: 0.00000004361385, Improvement: -0.00000003164044, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2024
Epoch 2024, Loss: 0.00000003449053, Improvement: -0.00000000912332, Best Loss: 0.00000001962715 in Epoch 1978
Epoch 2025
A best model at epoch 2025 has been saved with training error 0.00000001858277.
Epoch 2025, Loss: 0.00000002949985, Improvement: -0.00000000499068, Best Loss: 0.00000001858277 in Epoch 2025
Epoch 2026
Epoch 2026, Loss: 0.00000002909394, Improvement: -0.00000000040590, Best Loss: 0.00000001858277 in Epoch 2025
Epoch 2027
Epoch 2027, Loss: 0.00000002771144, Improvement: -0.00000000138251, Best Loss: 0.00000001858277 in Epoch 2025
Epoch 2028
Epoch 2028, Loss: 0.00000002898032, Improvement: 0.00000000126889, Best Loss: 0.00000001858277 in Epoch 2025
Epoch 2029
Epoch 2029, Loss: 0.00000002852481, Improvement: -0.00000000045552, Best Loss: 0.00000001858277 in Epoch 2025
Epoch 2030
A best model at epoch 2030 has been saved with training error 0.00000001613392.
Epoch 2030, Loss: 0.00000002847408, Improvement: -0.00000000005072, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2031
Epoch 2031, Loss: 0.00000003117136, Improvement: 0.00000000269728, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2032
Epoch 2032, Loss: 0.00000003406680, Improvement: 0.00000000289543, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2033
Epoch 2033, Loss: 0.00000003021792, Improvement: -0.00000000384888, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2034
Epoch 2034, Loss: 0.00000003029863, Improvement: 0.00000000008071, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2035
Epoch 2035, Loss: 0.00000003294587, Improvement: 0.00000000264724, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2036
Epoch 2036, Loss: 0.00000003113059, Improvement: -0.00000000181528, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2037
Epoch 2037, Loss: 0.00000003196996, Improvement: 0.00000000083937, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2038
Epoch 2038, Loss: 0.00000002888646, Improvement: -0.00000000308350, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2039
Epoch 2039, Loss: 0.00000003333057, Improvement: 0.00000000444412, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2040
Epoch 2040, Loss: 0.00000004711813, Improvement: 0.00000001378756, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2041
Epoch 2041, Loss: 0.00000005262852, Improvement: 0.00000000551039, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2042
Epoch 2042, Loss: 0.00000005003906, Improvement: -0.00000000258946, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2043
Epoch 2043, Loss: 0.00000005828841, Improvement: 0.00000000824935, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2044
Epoch 2044, Loss: 0.00000005246177, Improvement: -0.00000000582664, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2045
Epoch 2045, Loss: 0.00000006779038, Improvement: 0.00000001532861, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2046
Epoch 2046, Loss: 0.00000011931343, Improvement: 0.00000005152305, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2047
Epoch 2047, Loss: 0.00000010858541, Improvement: -0.00000001072802, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2048
Epoch 2048, Loss: 0.00000009382067, Improvement: -0.00000001476474, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2049
Epoch 2049, Loss: 0.00000004991775, Improvement: -0.00000004390292, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000005849968, Improvement: 0.00000000858193, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2051
Epoch 2051, Loss: 0.00000006063905, Improvement: 0.00000000213937, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2052
Epoch 2052, Loss: 0.00000010629455, Improvement: 0.00000004565550, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2053
Epoch 2053, Loss: 0.00000011912023, Improvement: 0.00000001282568, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2054
Epoch 2054, Loss: 0.00000008004610, Improvement: -0.00000003907413, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2055
Epoch 2055, Loss: 0.00000008090380, Improvement: 0.00000000085770, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2056
Epoch 2056, Loss: 0.00000010096321, Improvement: 0.00000002005940, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2057
Epoch 2057, Loss: 0.00000010892779, Improvement: 0.00000000796459, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2058
Epoch 2058, Loss: 0.00000011592709, Improvement: 0.00000000699930, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2059
Epoch 2059, Loss: 0.00000010898873, Improvement: -0.00000000693836, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2060
Epoch 2060, Loss: 0.00000015355853, Improvement: 0.00000004456980, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2061
Epoch 2061, Loss: 0.00000024702978, Improvement: 0.00000009347125, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2062
Epoch 2062, Loss: 0.00000010448893, Improvement: -0.00000014254085, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2063
Epoch 2063, Loss: 0.00000005663799, Improvement: -0.00000004785094, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2064
Epoch 2064, Loss: 0.00000005919782, Improvement: 0.00000000255983, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2065
Epoch 2065, Loss: 0.00000003567709, Improvement: -0.00000002352073, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2066
Epoch 2066, Loss: 0.00000003227811, Improvement: -0.00000000339898, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2067
Epoch 2067, Loss: 0.00000002916675, Improvement: -0.00000000311136, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2068
Epoch 2068, Loss: 0.00000003065788, Improvement: 0.00000000149113, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2069
Epoch 2069, Loss: 0.00000002844169, Improvement: -0.00000000221618, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2070
Epoch 2070, Loss: 0.00000002875480, Improvement: 0.00000000031310, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2071
Epoch 2071, Loss: 0.00000002830753, Improvement: -0.00000000044726, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2072
Epoch 2072, Loss: 0.00000002789124, Improvement: -0.00000000041629, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2073
Epoch 2073, Loss: 0.00000002881082, Improvement: 0.00000000091958, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2074
Epoch 2074, Loss: 0.00000002744761, Improvement: -0.00000000136321, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2075
Epoch 2075, Loss: 0.00000003132552, Improvement: 0.00000000387791, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2076
Epoch 2076, Loss: 0.00000003318353, Improvement: 0.00000000185801, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2077
Epoch 2077, Loss: 0.00000004791906, Improvement: 0.00000001473553, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2078
Epoch 2078, Loss: 0.00000005360414, Improvement: 0.00000000568508, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2079
Epoch 2079, Loss: 0.00000006736924, Improvement: 0.00000001376510, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2080
Epoch 2080, Loss: 0.00000010214564, Improvement: 0.00000003477640, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2081
Epoch 2081, Loss: 0.00000022807444, Improvement: 0.00000012592880, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2082
Epoch 2082, Loss: 0.00000018795520, Improvement: -0.00000004011923, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2083
Epoch 2083, Loss: 0.00000009763358, Improvement: -0.00000009032163, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2084
Epoch 2084, Loss: 0.00000008295232, Improvement: -0.00000001468126, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2085
Epoch 2085, Loss: 0.00000004978973, Improvement: -0.00000003316259, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2086
Epoch 2086, Loss: 0.00000003971365, Improvement: -0.00000001007608, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2087
Epoch 2087, Loss: 0.00000003489536, Improvement: -0.00000000481829, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2088
Epoch 2088, Loss: 0.00000003678723, Improvement: 0.00000000189187, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2089
Epoch 2089, Loss: 0.00000004494579, Improvement: 0.00000000815856, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2090
Epoch 2090, Loss: 0.00000005497427, Improvement: 0.00000001002848, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2091
Epoch 2091, Loss: 0.00000008699784, Improvement: 0.00000003202357, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2092
Epoch 2092, Loss: 0.00000008965202, Improvement: 0.00000000265417, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2093
Epoch 2093, Loss: 0.00000009951155, Improvement: 0.00000000985953, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2094
Epoch 2094, Loss: 0.00000010538640, Improvement: 0.00000000587485, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2095
Epoch 2095, Loss: 0.00000009883835, Improvement: -0.00000000654805, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2096
Epoch 2096, Loss: 0.00000007174319, Improvement: -0.00000002709516, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2097
Epoch 2097, Loss: 0.00000004412753, Improvement: -0.00000002761566, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2098
Epoch 2098, Loss: 0.00000003541752, Improvement: -0.00000000871001, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2099
Epoch 2099, Loss: 0.00000005222975, Improvement: 0.00000001681223, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000006309848, Improvement: 0.00000001086873, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2101
Epoch 2101, Loss: 0.00000003843421, Improvement: -0.00000002466427, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2102
Epoch 2102, Loss: 0.00000003930054, Improvement: 0.00000000086633, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2103
Epoch 2103, Loss: 0.00000003621912, Improvement: -0.00000000308142, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2104
Epoch 2104, Loss: 0.00000003985212, Improvement: 0.00000000363300, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2105
Epoch 2105, Loss: 0.00000007375687, Improvement: 0.00000003390475, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2106
Epoch 2106, Loss: 0.00000004826551, Improvement: -0.00000002549136, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2107
Epoch 2107, Loss: 0.00000004109715, Improvement: -0.00000000716836, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2108
Epoch 2108, Loss: 0.00000004801625, Improvement: 0.00000000691910, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2109
Epoch 2109, Loss: 0.00000012251556, Improvement: 0.00000007449931, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2110
Epoch 2110, Loss: 0.00000019899389, Improvement: 0.00000007647833, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2111
Epoch 2111, Loss: 0.00000012000339, Improvement: -0.00000007899050, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2112
Epoch 2112, Loss: 0.00000008666159, Improvement: -0.00000003334180, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2113
Epoch 2113, Loss: 0.00000006608185, Improvement: -0.00000002057974, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2114
Epoch 2114, Loss: 0.00000005034580, Improvement: -0.00000001573605, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2115
Epoch 2115, Loss: 0.00000003661158, Improvement: -0.00000001373422, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2116
Epoch 2116, Loss: 0.00000003325305, Improvement: -0.00000000335852, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2117
Epoch 2117, Loss: 0.00000003998223, Improvement: 0.00000000672917, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2118
Epoch 2118, Loss: 0.00000004156148, Improvement: 0.00000000157925, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2119
Epoch 2119, Loss: 0.00000003903594, Improvement: -0.00000000252554, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2120
Epoch 2120, Loss: 0.00000003785499, Improvement: -0.00000000118095, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2121
Epoch 2121, Loss: 0.00000004244995, Improvement: 0.00000000459496, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2122
Epoch 2122, Loss: 0.00000003956432, Improvement: -0.00000000288564, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2123
Epoch 2123, Loss: 0.00000003440994, Improvement: -0.00000000515438, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2124
Epoch 2124, Loss: 0.00000004204428, Improvement: 0.00000000763434, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2125
Epoch 2125, Loss: 0.00000007211071, Improvement: 0.00000003006643, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2126
Epoch 2126, Loss: 0.00000010202463, Improvement: 0.00000002991391, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2127
Epoch 2127, Loss: 0.00000007657188, Improvement: -0.00000002545274, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2128
Epoch 2128, Loss: 0.00000007291524, Improvement: -0.00000000365664, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2129
Epoch 2129, Loss: 0.00000007425269, Improvement: 0.00000000133744, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2130
Epoch 2130, Loss: 0.00000006713821, Improvement: -0.00000000711448, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2131
Epoch 2131, Loss: 0.00000007004081, Improvement: 0.00000000290260, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2132
Epoch 2132, Loss: 0.00000013317540, Improvement: 0.00000006313459, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2133
Epoch 2133, Loss: 0.00000011943967, Improvement: -0.00000001373573, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2134
Epoch 2134, Loss: 0.00000007273511, Improvement: -0.00000004670456, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2135
Epoch 2135, Loss: 0.00000005367989, Improvement: -0.00000001905522, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2136
Epoch 2136, Loss: 0.00000004848662, Improvement: -0.00000000519327, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2137
Epoch 2137, Loss: 0.00000004024890, Improvement: -0.00000000823772, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2138
Epoch 2138, Loss: 0.00000003716666, Improvement: -0.00000000308224, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2139
Epoch 2139, Loss: 0.00000004252309, Improvement: 0.00000000535643, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2140
Epoch 2140, Loss: 0.00000006176798, Improvement: 0.00000001924489, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2141
Epoch 2141, Loss: 0.00000009468321, Improvement: 0.00000003291524, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2142
Epoch 2142, Loss: 0.00000021170288, Improvement: 0.00000011701966, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2143
Epoch 2143, Loss: 0.00000027455924, Improvement: 0.00000006285636, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2144
Epoch 2144, Loss: 0.00000016268759, Improvement: -0.00000011187165, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2145
Epoch 2145, Loss: 0.00000008555075, Improvement: -0.00000007713684, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2146
Epoch 2146, Loss: 0.00000007235153, Improvement: -0.00000001319922, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2147
Epoch 2147, Loss: 0.00000005060165, Improvement: -0.00000002174988, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2148
Epoch 2148, Loss: 0.00000003440957, Improvement: -0.00000001619208, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2149
Epoch 2149, Loss: 0.00000003117696, Improvement: -0.00000000323261, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000002739123, Improvement: -0.00000000378573, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2151
Epoch 2151, Loss: 0.00000002996047, Improvement: 0.00000000256924, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2152
Epoch 2152, Loss: 0.00000002965741, Improvement: -0.00000000030306, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2153
Epoch 2153, Loss: 0.00000002785643, Improvement: -0.00000000180098, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2154
Epoch 2154, Loss: 0.00000002795097, Improvement: 0.00000000009454, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2155
Epoch 2155, Loss: 0.00000003218902, Improvement: 0.00000000423805, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2156
Epoch 2156, Loss: 0.00000003398322, Improvement: 0.00000000179421, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2157
Epoch 2157, Loss: 0.00000003004257, Improvement: -0.00000000394065, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2158
Epoch 2158, Loss: 0.00000002858239, Improvement: -0.00000000146018, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2159
Epoch 2159, Loss: 0.00000002623802, Improvement: -0.00000000234438, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2160
Epoch 2160, Loss: 0.00000002627657, Improvement: 0.00000000003856, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2161
Epoch 2161, Loss: 0.00000002706730, Improvement: 0.00000000079072, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2162
Epoch 2162, Loss: 0.00000002739160, Improvement: 0.00000000032431, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2163
Epoch 2163, Loss: 0.00000003568054, Improvement: 0.00000000828894, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2164
Epoch 2164, Loss: 0.00000012149367, Improvement: 0.00000008581313, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2165
Epoch 2165, Loss: 0.00000029010407, Improvement: 0.00000016861040, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2166
Epoch 2166, Loss: 0.00000014661371, Improvement: -0.00000014349036, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2167
Epoch 2167, Loss: 0.00000006456080, Improvement: -0.00000008205290, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2168
Epoch 2168, Loss: 0.00000003197994, Improvement: -0.00000003258086, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2169
Epoch 2169, Loss: 0.00000002754555, Improvement: -0.00000000443439, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2170
Epoch 2170, Loss: 0.00000002685851, Improvement: -0.00000000068704, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2171
Epoch 2171, Loss: 0.00000002803704, Improvement: 0.00000000117853, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2172
Epoch 2172, Loss: 0.00000002826205, Improvement: 0.00000000022501, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2173
Epoch 2173, Loss: 0.00000002749543, Improvement: -0.00000000076662, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2174
Epoch 2174, Loss: 0.00000002780409, Improvement: 0.00000000030866, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2175
Epoch 2175, Loss: 0.00000002651582, Improvement: -0.00000000128827, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2176
Epoch 2176, Loss: 0.00000002717497, Improvement: 0.00000000065916, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2177
Epoch 2177, Loss: 0.00000002787978, Improvement: 0.00000000070481, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2178
Epoch 2178, Loss: 0.00000002637104, Improvement: -0.00000000150874, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2179
Epoch 2179, Loss: 0.00000002675929, Improvement: 0.00000000038824, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2180
Epoch 2180, Loss: 0.00000002732244, Improvement: 0.00000000056316, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2181
Epoch 2181, Loss: 0.00000002492088, Improvement: -0.00000000240156, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2182
Epoch 2182, Loss: 0.00000002587096, Improvement: 0.00000000095008, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2183
Epoch 2183, Loss: 0.00000002627286, Improvement: 0.00000000040190, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2184
Epoch 2184, Loss: 0.00000002577885, Improvement: -0.00000000049401, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2185
Epoch 2185, Loss: 0.00000002650405, Improvement: 0.00000000072520, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2186
Epoch 2186, Loss: 0.00000003515519, Improvement: 0.00000000865114, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2187
Epoch 2187, Loss: 0.00000004269205, Improvement: 0.00000000753686, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2188
Epoch 2188, Loss: 0.00000005292397, Improvement: 0.00000001023192, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2189
Epoch 2189, Loss: 0.00000005465099, Improvement: 0.00000000172701, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2190
Epoch 2190, Loss: 0.00000007621242, Improvement: 0.00000002156144, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2191
Epoch 2191, Loss: 0.00000008545809, Improvement: 0.00000000924567, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2192
Epoch 2192, Loss: 0.00000007928378, Improvement: -0.00000000617431, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2193
Epoch 2193, Loss: 0.00000013785972, Improvement: 0.00000005857593, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2194
Epoch 2194, Loss: 0.00000008116890, Improvement: -0.00000005669082, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2195
Epoch 2195, Loss: 0.00000009880052, Improvement: 0.00000001763162, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2196
Epoch 2196, Loss: 0.00000010054363, Improvement: 0.00000000174312, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2197
Epoch 2197, Loss: 0.00000009193374, Improvement: -0.00000000860990, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2198
Epoch 2198, Loss: 0.00000006538962, Improvement: -0.00000002654412, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2199
Epoch 2199, Loss: 0.00000005327160, Improvement: -0.00000001211802, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000006307591, Improvement: 0.00000000980431, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2201
Epoch 2201, Loss: 0.00000006763357, Improvement: 0.00000000455766, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2202
Epoch 2202, Loss: 0.00000017988011, Improvement: 0.00000011224654, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2203
Epoch 2203, Loss: 0.00000019522649, Improvement: 0.00000001534638, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2204
Epoch 2204, Loss: 0.00000011905802, Improvement: -0.00000007616848, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2205
Epoch 2205, Loss: 0.00000008771366, Improvement: -0.00000003134436, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2206
Epoch 2206, Loss: 0.00000003625149, Improvement: -0.00000005146217, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2207
Epoch 2207, Loss: 0.00000002764356, Improvement: -0.00000000860793, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2208
Epoch 2208, Loss: 0.00000002624832, Improvement: -0.00000000139524, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2209
Epoch 2209, Loss: 0.00000002787645, Improvement: 0.00000000162813, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2210
Epoch 2210, Loss: 0.00000002673376, Improvement: -0.00000000114269, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2211
Epoch 2211, Loss: 0.00000003318193, Improvement: 0.00000000644816, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2212
Epoch 2212, Loss: 0.00000003847549, Improvement: 0.00000000529356, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2213
Epoch 2213, Loss: 0.00000003558319, Improvement: -0.00000000289230, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2214
Epoch 2214, Loss: 0.00000004112179, Improvement: 0.00000000553861, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2215
Epoch 2215, Loss: 0.00000004809217, Improvement: 0.00000000697038, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2216
Epoch 2216, Loss: 0.00000005846638, Improvement: 0.00000001037420, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2217
Epoch 2217, Loss: 0.00000005703494, Improvement: -0.00000000143144, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2218
Epoch 2218, Loss: 0.00000003705212, Improvement: -0.00000001998282, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2219
Epoch 2219, Loss: 0.00000004366027, Improvement: 0.00000000660815, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2220
Epoch 2220, Loss: 0.00000004884299, Improvement: 0.00000000518272, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2221
Epoch 2221, Loss: 0.00000003780927, Improvement: -0.00000001103372, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2222
Epoch 2222, Loss: 0.00000004172068, Improvement: 0.00000000391141, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2223
Epoch 2223, Loss: 0.00000004939525, Improvement: 0.00000000767458, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2224
Epoch 2224, Loss: 0.00000007328690, Improvement: 0.00000002389165, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2225
Epoch 2225, Loss: 0.00000017683121, Improvement: 0.00000010354430, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2226
Epoch 2226, Loss: 0.00000024309549, Improvement: 0.00000006626428, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2227
Epoch 2227, Loss: 0.00000015044701, Improvement: -0.00000009264847, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2228
Epoch 2228, Loss: 0.00000008253978, Improvement: -0.00000006790723, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2229
Epoch 2229, Loss: 0.00000005831922, Improvement: -0.00000002422056, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2230
Epoch 2230, Loss: 0.00000003909231, Improvement: -0.00000001922691, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2231
Epoch 2231, Loss: 0.00000003455226, Improvement: -0.00000000454005, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2232
Epoch 2232, Loss: 0.00000002886095, Improvement: -0.00000000569131, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2233
Epoch 2233, Loss: 0.00000002578393, Improvement: -0.00000000307702, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2234
Epoch 2234, Loss: 0.00000002674880, Improvement: 0.00000000096487, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2235
Epoch 2235, Loss: 0.00000002844411, Improvement: 0.00000000169531, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2236
Epoch 2236, Loss: 0.00000002971798, Improvement: 0.00000000127387, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2237
Epoch 2237, Loss: 0.00000002893180, Improvement: -0.00000000078618, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2238
Epoch 2238, Loss: 0.00000002916527, Improvement: 0.00000000023347, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2239
Epoch 2239, Loss: 0.00000003533769, Improvement: 0.00000000617242, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2240
Epoch 2240, Loss: 0.00000004133913, Improvement: 0.00000000600144, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2241
Epoch 2241, Loss: 0.00000003398372, Improvement: -0.00000000735541, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2242
Epoch 2242, Loss: 0.00000002732995, Improvement: -0.00000000665377, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2243
Epoch 2243, Loss: 0.00000002971324, Improvement: 0.00000000238328, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2244
Epoch 2244, Loss: 0.00000003355511, Improvement: 0.00000000384188, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2245
Epoch 2245, Loss: 0.00000003354648, Improvement: -0.00000000000863, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2246
Epoch 2246, Loss: 0.00000003501989, Improvement: 0.00000000147342, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2247
Epoch 2247, Loss: 0.00000006279243, Improvement: 0.00000002777254, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2248
Epoch 2248, Loss: 0.00000005611343, Improvement: -0.00000000667901, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2249
Epoch 2249, Loss: 0.00000006138525, Improvement: 0.00000000527182, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000011598883, Improvement: 0.00000005460358, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2251
Epoch 2251, Loss: 0.00000008724281, Improvement: -0.00000002874602, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2252
Epoch 2252, Loss: 0.00000006607707, Improvement: -0.00000002116574, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2253
Epoch 2253, Loss: 0.00000005395540, Improvement: -0.00000001212167, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2254
Epoch 2254, Loss: 0.00000004833418, Improvement: -0.00000000562121, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2255
Epoch 2255, Loss: 0.00000004962790, Improvement: 0.00000000129371, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2256
Epoch 2256, Loss: 0.00000007899284, Improvement: 0.00000002936494, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2257
Epoch 2257, Loss: 0.00000017291197, Improvement: 0.00000009391913, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2258
Epoch 2258, Loss: 0.00000011532544, Improvement: -0.00000005758654, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2259
Epoch 2259, Loss: 0.00000007231391, Improvement: -0.00000004301152, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2260
Epoch 2260, Loss: 0.00000004206843, Improvement: -0.00000003024549, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2261
Epoch 2261, Loss: 0.00000004415574, Improvement: 0.00000000208732, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2262
Epoch 2262, Loss: 0.00000003826934, Improvement: -0.00000000588640, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2263
Epoch 2263, Loss: 0.00000004168754, Improvement: 0.00000000341820, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2264
Epoch 2264, Loss: 0.00000003463788, Improvement: -0.00000000704966, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2265
Epoch 2265, Loss: 0.00000004125123, Improvement: 0.00000000661335, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2266
Epoch 2266, Loss: 0.00000004191321, Improvement: 0.00000000066198, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2267
Epoch 2267, Loss: 0.00000003684946, Improvement: -0.00000000506375, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2268
Epoch 2268, Loss: 0.00000005651859, Improvement: 0.00000001966913, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2269
Epoch 2269, Loss: 0.00000008628280, Improvement: 0.00000002976421, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2270
Epoch 2270, Loss: 0.00000005941425, Improvement: -0.00000002686855, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2271
Epoch 2271, Loss: 0.00000005330103, Improvement: -0.00000000611322, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2272
Epoch 2272, Loss: 0.00000004157470, Improvement: -0.00000001172634, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2273
Epoch 2273, Loss: 0.00000003928502, Improvement: -0.00000000228967, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2274
Epoch 2274, Loss: 0.00000003618298, Improvement: -0.00000000310204, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2275
Epoch 2275, Loss: 0.00000005955761, Improvement: 0.00000002337463, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2276
Epoch 2276, Loss: 0.00000007799444, Improvement: 0.00000001843683, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2277
Epoch 2277, Loss: 0.00000005937936, Improvement: -0.00000001861508, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2278
Epoch 2278, Loss: 0.00000005839842, Improvement: -0.00000000098094, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2279
Epoch 2279, Loss: 0.00000009268667, Improvement: 0.00000003428824, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2280
Epoch 2280, Loss: 0.00000005509805, Improvement: -0.00000003758861, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2281
Epoch 2281, Loss: 0.00000003847770, Improvement: -0.00000001662035, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2282
Epoch 2282, Loss: 0.00000004889095, Improvement: 0.00000001041325, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2283
Epoch 2283, Loss: 0.00000004540106, Improvement: -0.00000000348989, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2284
Epoch 2284, Loss: 0.00000004342969, Improvement: -0.00000000197136, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2285
Epoch 2285, Loss: 0.00000006503031, Improvement: 0.00000002160061, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2286
Epoch 2286, Loss: 0.00000008288796, Improvement: 0.00000001785765, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2287
Epoch 2287, Loss: 0.00000012227772, Improvement: 0.00000003938975, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2288
Epoch 2288, Loss: 0.00000015501810, Improvement: 0.00000003274038, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2289
Epoch 2289, Loss: 0.00000010894308, Improvement: -0.00000004607502, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2290
Epoch 2290, Loss: 0.00000015522110, Improvement: 0.00000004627802, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2291
Epoch 2291, Loss: 0.00000016104877, Improvement: 0.00000000582767, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2292
Epoch 2292, Loss: 0.00000009759708, Improvement: -0.00000006345169, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2293
Epoch 2293, Loss: 0.00000005317371, Improvement: -0.00000004442337, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2294
Epoch 2294, Loss: 0.00000003538055, Improvement: -0.00000001779317, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2295
Epoch 2295, Loss: 0.00000003280259, Improvement: -0.00000000257795, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2296
Epoch 2296, Loss: 0.00000002569392, Improvement: -0.00000000710867, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2297
Epoch 2297, Loss: 0.00000002471515, Improvement: -0.00000000097877, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2298
Epoch 2298, Loss: 0.00000002426200, Improvement: -0.00000000045316, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2299
Epoch 2299, Loss: 0.00000002556473, Improvement: 0.00000000130274, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000002678150, Improvement: 0.00000000121677, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2301
Epoch 2301, Loss: 0.00000002760157, Improvement: 0.00000000082007, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2302
Epoch 2302, Loss: 0.00000002412528, Improvement: -0.00000000347629, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2303
Epoch 2303, Loss: 0.00000002418965, Improvement: 0.00000000006437, Best Loss: 0.00000001613392 in Epoch 2030
Epoch 2304
A best model at epoch 2304 has been saved with training error 0.00000001578042.
Epoch 2304, Loss: 0.00000002465634, Improvement: 0.00000000046670, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2305
Epoch 2305, Loss: 0.00000003334927, Improvement: 0.00000000869293, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2306
Epoch 2306, Loss: 0.00000002672895, Improvement: -0.00000000662033, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2307
Epoch 2307, Loss: 0.00000002739256, Improvement: 0.00000000066361, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2308
Epoch 2308, Loss: 0.00000002673373, Improvement: -0.00000000065883, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2309
Epoch 2309, Loss: 0.00000002441649, Improvement: -0.00000000231724, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2310
Epoch 2310, Loss: 0.00000002663406, Improvement: 0.00000000221757, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2311
Epoch 2311, Loss: 0.00000002801943, Improvement: 0.00000000138537, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2312
Epoch 2312, Loss: 0.00000003343997, Improvement: 0.00000000542054, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2313
Epoch 2313, Loss: 0.00000006077074, Improvement: 0.00000002733077, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2314
Epoch 2314, Loss: 0.00000007113979, Improvement: 0.00000001036904, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2315
Epoch 2315, Loss: 0.00000007876120, Improvement: 0.00000000762141, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2316
Epoch 2316, Loss: 0.00000006180344, Improvement: -0.00000001695775, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2317
Epoch 2317, Loss: 0.00000008679910, Improvement: 0.00000002499565, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2318
Epoch 2318, Loss: 0.00000007619842, Improvement: -0.00000001060067, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2319
Epoch 2319, Loss: 0.00000006533577, Improvement: -0.00000001086265, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2320
Epoch 2320, Loss: 0.00000007662610, Improvement: 0.00000001129033, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2321
Epoch 2321, Loss: 0.00000008659481, Improvement: 0.00000000996871, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2322
Epoch 2322, Loss: 0.00000010533707, Improvement: 0.00000001874226, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2323
Epoch 2323, Loss: 0.00000005117652, Improvement: -0.00000005416055, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2324
Epoch 2324, Loss: 0.00000006074508, Improvement: 0.00000000956856, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2325
Epoch 2325, Loss: 0.00000006583332, Improvement: 0.00000000508824, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2326
Epoch 2326, Loss: 0.00000006779561, Improvement: 0.00000000196229, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2327
Epoch 2327, Loss: 0.00000004386218, Improvement: -0.00000002393343, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2328
Epoch 2328, Loss: 0.00000005689360, Improvement: 0.00000001303142, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2329
Epoch 2329, Loss: 0.00000004303446, Improvement: -0.00000001385914, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2330
Epoch 2330, Loss: 0.00000005053016, Improvement: 0.00000000749570, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2331
Epoch 2331, Loss: 0.00000008940754, Improvement: 0.00000003887738, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2332
Epoch 2332, Loss: 0.00000009642166, Improvement: 0.00000000701412, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2333
Epoch 2333, Loss: 0.00000007756546, Improvement: -0.00000001885620, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2334
Epoch 2334, Loss: 0.00000008883930, Improvement: 0.00000001127384, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2335
Epoch 2335, Loss: 0.00000006449951, Improvement: -0.00000002433980, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2336
Epoch 2336, Loss: 0.00000005410098, Improvement: -0.00000001039853, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2337
Epoch 2337, Loss: 0.00000004882358, Improvement: -0.00000000527740, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2338
Epoch 2338, Loss: 0.00000009656532, Improvement: 0.00000004774174, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2339
Epoch 2339, Loss: 0.00000012835046, Improvement: 0.00000003178514, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2340
Epoch 2340, Loss: 0.00000013135969, Improvement: 0.00000000300924, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2341
Epoch 2341, Loss: 0.00000010695292, Improvement: -0.00000002440677, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2342
Epoch 2342, Loss: 0.00000006217540, Improvement: -0.00000004477753, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2343
Epoch 2343, Loss: 0.00000005599858, Improvement: -0.00000000617681, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2344
Epoch 2344, Loss: 0.00000004336028, Improvement: -0.00000001263830, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2345
Epoch 2345, Loss: 0.00000003353432, Improvement: -0.00000000982597, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2346
Epoch 2346, Loss: 0.00000002691545, Improvement: -0.00000000661886, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2347
Epoch 2347, Loss: 0.00000002605841, Improvement: -0.00000000085705, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2348
Epoch 2348, Loss: 0.00000002620833, Improvement: 0.00000000014992, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2349
Epoch 2349, Loss: 0.00000003140379, Improvement: 0.00000000519546, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000002511070, Improvement: -0.00000000629309, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2351
Epoch 2351, Loss: 0.00000002424016, Improvement: -0.00000000087054, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2352
Epoch 2352, Loss: 0.00000003014403, Improvement: 0.00000000590387, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2353
Epoch 2353, Loss: 0.00000005289851, Improvement: 0.00000002275447, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2354
Epoch 2354, Loss: 0.00000012540120, Improvement: 0.00000007250269, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2355
Epoch 2355, Loss: 0.00000012373439, Improvement: -0.00000000166682, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2356
Epoch 2356, Loss: 0.00000007411928, Improvement: -0.00000004961510, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2357
Epoch 2357, Loss: 0.00000007915071, Improvement: 0.00000000503143, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2358
Epoch 2358, Loss: 0.00000008415339, Improvement: 0.00000000500268, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2359
Epoch 2359, Loss: 0.00000011747239, Improvement: 0.00000003331900, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2360
Epoch 2360, Loss: 0.00000010326491, Improvement: -0.00000001420748, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2361
Epoch 2361, Loss: 0.00000009548885, Improvement: -0.00000000777606, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2362
Epoch 2362, Loss: 0.00000004706667, Improvement: -0.00000004842218, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2363
Epoch 2363, Loss: 0.00000003747296, Improvement: -0.00000000959371, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2364
Epoch 2364, Loss: 0.00000003531329, Improvement: -0.00000000215967, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2365
Epoch 2365, Loss: 0.00000002723174, Improvement: -0.00000000808155, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2366
Epoch 2366, Loss: 0.00000003653362, Improvement: 0.00000000930187, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2367
Epoch 2367, Loss: 0.00000003802456, Improvement: 0.00000000149095, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2368
Epoch 2368, Loss: 0.00000003293782, Improvement: -0.00000000508674, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2369
Epoch 2369, Loss: 0.00000004385988, Improvement: 0.00000001092206, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2370
Epoch 2370, Loss: 0.00000007688720, Improvement: 0.00000003302732, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2371
Epoch 2371, Loss: 0.00000011057745, Improvement: 0.00000003369025, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2372
Epoch 2372, Loss: 0.00000008654325, Improvement: -0.00000002403420, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2373
Epoch 2373, Loss: 0.00000005760733, Improvement: -0.00000002893593, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2374
Epoch 2374, Loss: 0.00000004389179, Improvement: -0.00000001371553, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2375
Epoch 2375, Loss: 0.00000006089127, Improvement: 0.00000001699947, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2376
Epoch 2376, Loss: 0.00000006968488, Improvement: 0.00000000879362, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2377
Epoch 2377, Loss: 0.00000006033112, Improvement: -0.00000000935376, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2378
Epoch 2378, Loss: 0.00000006191205, Improvement: 0.00000000158093, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2379
Epoch 2379, Loss: 0.00000004802593, Improvement: -0.00000001388612, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2380
Epoch 2380, Loss: 0.00000006188765, Improvement: 0.00000001386172, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2381
Epoch 2381, Loss: 0.00000010162383, Improvement: 0.00000003973618, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2382
Epoch 2382, Loss: 0.00000012813672, Improvement: 0.00000002651289, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2383
Epoch 2383, Loss: 0.00000016797305, Improvement: 0.00000003983633, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2384
Epoch 2384, Loss: 0.00000008874072, Improvement: -0.00000007923234, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2385
Epoch 2385, Loss: 0.00000004834600, Improvement: -0.00000004039471, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2386
Epoch 2386, Loss: 0.00000003330449, Improvement: -0.00000001504151, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2387
Epoch 2387, Loss: 0.00000003123784, Improvement: -0.00000000206665, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2388
Epoch 2388, Loss: 0.00000003135144, Improvement: 0.00000000011359, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2389
Epoch 2389, Loss: 0.00000003159010, Improvement: 0.00000000023866, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2390
Epoch 2390, Loss: 0.00000004655580, Improvement: 0.00000001496571, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2391
Epoch 2391, Loss: 0.00000006110270, Improvement: 0.00000001454690, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2392
Epoch 2392, Loss: 0.00000005750727, Improvement: -0.00000000359543, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2393
Epoch 2393, Loss: 0.00000005982248, Improvement: 0.00000000231521, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2394
Epoch 2394, Loss: 0.00000006117659, Improvement: 0.00000000135411, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2395
Epoch 2395, Loss: 0.00000009892744, Improvement: 0.00000003775085, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2396
Epoch 2396, Loss: 0.00000028078584, Improvement: 0.00000018185840, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2397
Epoch 2397, Loss: 0.00000011283351, Improvement: -0.00000016795233, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2398
Epoch 2398, Loss: 0.00000006339693, Improvement: -0.00000004943658, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2399
Epoch 2399, Loss: 0.00000004114676, Improvement: -0.00000002225017, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000003933424, Improvement: -0.00000000181251, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2401
Epoch 2401, Loss: 0.00000003639082, Improvement: -0.00000000294342, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2402
Epoch 2402, Loss: 0.00000003606076, Improvement: -0.00000000033007, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2403
Epoch 2403, Loss: 0.00000003592532, Improvement: -0.00000000013544, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2404
Epoch 2404, Loss: 0.00000003964094, Improvement: 0.00000000371562, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2405
Epoch 2405, Loss: 0.00000004033683, Improvement: 0.00000000069589, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2406
Epoch 2406, Loss: 0.00000003919728, Improvement: -0.00000000113954, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2407
Epoch 2407, Loss: 0.00000002749806, Improvement: -0.00000001169922, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2408
Epoch 2408, Loss: 0.00000004479702, Improvement: 0.00000001729896, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2409
Epoch 2409, Loss: 0.00000002761923, Improvement: -0.00000001717779, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2410
Epoch 2410, Loss: 0.00000002424600, Improvement: -0.00000000337322, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2411
Epoch 2411, Loss: 0.00000002351527, Improvement: -0.00000000073074, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2412
Epoch 2412, Loss: 0.00000002466978, Improvement: 0.00000000115451, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2413
Epoch 2413, Loss: 0.00000002666170, Improvement: 0.00000000199192, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2414
Epoch 2414, Loss: 0.00000002508068, Improvement: -0.00000000158102, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2415
Epoch 2415, Loss: 0.00000002317023, Improvement: -0.00000000191045, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2416
Epoch 2416, Loss: 0.00000002391797, Improvement: 0.00000000074774, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2417
Epoch 2417, Loss: 0.00000002733617, Improvement: 0.00000000341820, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2418
Epoch 2418, Loss: 0.00000003088446, Improvement: 0.00000000354829, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2419
Epoch 2419, Loss: 0.00000004447942, Improvement: 0.00000001359496, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2420
Epoch 2420, Loss: 0.00000004701423, Improvement: 0.00000000253481, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2421
Epoch 2421, Loss: 0.00000004025107, Improvement: -0.00000000676317, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2422
Epoch 2422, Loss: 0.00000006017262, Improvement: 0.00000001992156, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2423
Epoch 2423, Loss: 0.00000011522333, Improvement: 0.00000005505071, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2424
Epoch 2424, Loss: 0.00000015520932, Improvement: 0.00000003998599, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2425
Epoch 2425, Loss: 0.00000012986793, Improvement: -0.00000002534139, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2426
Epoch 2426, Loss: 0.00000007061480, Improvement: -0.00000005925313, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2427
Epoch 2427, Loss: 0.00000004534052, Improvement: -0.00000002527427, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2428
Epoch 2428, Loss: 0.00000003593583, Improvement: -0.00000000940469, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2429
Epoch 2429, Loss: 0.00000003263321, Improvement: -0.00000000330262, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2430
Epoch 2430, Loss: 0.00000003131057, Improvement: -0.00000000132264, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2431
Epoch 2431, Loss: 0.00000003615586, Improvement: 0.00000000484529, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2432
Epoch 2432, Loss: 0.00000003169044, Improvement: -0.00000000446542, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2433
Epoch 2433, Loss: 0.00000004104280, Improvement: 0.00000000935236, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2434
Epoch 2434, Loss: 0.00000004818130, Improvement: 0.00000000713850, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2435
Epoch 2435, Loss: 0.00000003380624, Improvement: -0.00000001437505, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2436
Epoch 2436, Loss: 0.00000003477656, Improvement: 0.00000000097032, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2437
Epoch 2437, Loss: 0.00000002721503, Improvement: -0.00000000756153, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2438
Epoch 2438, Loss: 0.00000003021738, Improvement: 0.00000000300235, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2439
Epoch 2439, Loss: 0.00000003270049, Improvement: 0.00000000248311, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2440
Epoch 2440, Loss: 0.00000003040053, Improvement: -0.00000000229996, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2441
Epoch 2441, Loss: 0.00000004812497, Improvement: 0.00000001772444, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2442
Epoch 2442, Loss: 0.00000006657188, Improvement: 0.00000001844691, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2443
Epoch 2443, Loss: 0.00000005645427, Improvement: -0.00000001011761, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2444
Epoch 2444, Loss: 0.00000005496816, Improvement: -0.00000000148611, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2445
Epoch 2445, Loss: 0.00000004596346, Improvement: -0.00000000900471, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2446
Epoch 2446, Loss: 0.00000006110102, Improvement: 0.00000001513756, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2447
Epoch 2447, Loss: 0.00000004807050, Improvement: -0.00000001303052, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2448
Epoch 2448, Loss: 0.00000003716740, Improvement: -0.00000001090310, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2449
Epoch 2449, Loss: 0.00000003228912, Improvement: -0.00000000487828, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000002737271, Improvement: -0.00000000491641, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2451
Epoch 2451, Loss: 0.00000002854796, Improvement: 0.00000000117525, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2452
Epoch 2452, Loss: 0.00000003964088, Improvement: 0.00000001109293, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2453
Epoch 2453, Loss: 0.00000004539689, Improvement: 0.00000000575601, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2454
Epoch 2454, Loss: 0.00000005961563, Improvement: 0.00000001421874, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2455
Epoch 2455, Loss: 0.00000003815953, Improvement: -0.00000002145610, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2456
Epoch 2456, Loss: 0.00000006208598, Improvement: 0.00000002392645, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2457
Epoch 2457, Loss: 0.00000004550750, Improvement: -0.00000001657848, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2458
Epoch 2458, Loss: 0.00000003651694, Improvement: -0.00000000899056, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2459
Epoch 2459, Loss: 0.00000007454731, Improvement: 0.00000003803037, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2460
Epoch 2460, Loss: 0.00000007104707, Improvement: -0.00000000350024, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2461
Epoch 2461, Loss: 0.00000005791662, Improvement: -0.00000001313045, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2462
Epoch 2462, Loss: 0.00000003929020, Improvement: -0.00000001862642, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2463
Epoch 2463, Loss: 0.00000006128846, Improvement: 0.00000002199825, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2464
Epoch 2464, Loss: 0.00000007403652, Improvement: 0.00000001274806, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2465
Epoch 2465, Loss: 0.00000004490130, Improvement: -0.00000002913522, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2466
Epoch 2466, Loss: 0.00000003322409, Improvement: -0.00000001167720, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2467
Epoch 2467, Loss: 0.00000005718144, Improvement: 0.00000002395735, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2468
Epoch 2468, Loss: 0.00000005962589, Improvement: 0.00000000244445, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2469
Epoch 2469, Loss: 0.00000008623357, Improvement: 0.00000002660767, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2470
Epoch 2470, Loss: 0.00000014467614, Improvement: 0.00000005844257, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2471
Epoch 2471, Loss: 0.00000011761693, Improvement: -0.00000002705921, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2472
Epoch 2472, Loss: 0.00000005451140, Improvement: -0.00000006310553, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2473
Epoch 2473, Loss: 0.00000003534584, Improvement: -0.00000001916555, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2474
Epoch 2474, Loss: 0.00000003349758, Improvement: -0.00000000184827, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2475
Epoch 2475, Loss: 0.00000003805095, Improvement: 0.00000000455337, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2476
Epoch 2476, Loss: 0.00000003275243, Improvement: -0.00000000529852, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2477
Epoch 2477, Loss: 0.00000003370767, Improvement: 0.00000000095524, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2478
Epoch 2478, Loss: 0.00000003019039, Improvement: -0.00000000351728, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2479
Epoch 2479, Loss: 0.00000002948281, Improvement: -0.00000000070758, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2480
Epoch 2480, Loss: 0.00000005254569, Improvement: 0.00000002306287, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2481
Epoch 2481, Loss: 0.00000006281182, Improvement: 0.00000001026613, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2482
Epoch 2482, Loss: 0.00000004641658, Improvement: -0.00000001639523, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2483
Epoch 2483, Loss: 0.00000008411277, Improvement: 0.00000003769619, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2484
Epoch 2484, Loss: 0.00000007061647, Improvement: -0.00000001349631, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2485
Epoch 2485, Loss: 0.00000007089203, Improvement: 0.00000000027556, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2486
Epoch 2486, Loss: 0.00000005746948, Improvement: -0.00000001342255, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2487
Epoch 2487, Loss: 0.00000005091044, Improvement: -0.00000000655905, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2488
Epoch 2488, Loss: 0.00000003470048, Improvement: -0.00000001620996, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2489
Epoch 2489, Loss: 0.00000003586594, Improvement: 0.00000000116546, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2490
Epoch 2490, Loss: 0.00000003437847, Improvement: -0.00000000148747, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2491
Epoch 2491, Loss: 0.00000003133058, Improvement: -0.00000000304789, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2492
Epoch 2492, Loss: 0.00000003088918, Improvement: -0.00000000044140, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2493
Epoch 2493, Loss: 0.00000005312419, Improvement: 0.00000002223501, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2494
Epoch 2494, Loss: 0.00000014994750, Improvement: 0.00000009682331, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2495
Epoch 2495, Loss: 0.00000011372537, Improvement: -0.00000003622213, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2496
Epoch 2496, Loss: 0.00000007469059, Improvement: -0.00000003903478, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2497
Epoch 2497, Loss: 0.00000009035000, Improvement: 0.00000001565941, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2498
Epoch 2498, Loss: 0.00000009026143, Improvement: -0.00000000008857, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2499
Epoch 2499, Loss: 0.00000005220484, Improvement: -0.00000003805660, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000004646259, Improvement: -0.00000000574225, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2501
Epoch 2501, Loss: 0.00000003970339, Improvement: -0.00000000675920, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2502
Epoch 2502, Loss: 0.00000002820136, Improvement: -0.00000001150203, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2503
Epoch 2503, Loss: 0.00000002727272, Improvement: -0.00000000092864, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2504
Epoch 2504, Loss: 0.00000003619951, Improvement: 0.00000000892678, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2505
Epoch 2505, Loss: 0.00000003555289, Improvement: -0.00000000064662, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2506
Epoch 2506, Loss: 0.00000003430948, Improvement: -0.00000000124341, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2507
Epoch 2507, Loss: 0.00000007577643, Improvement: 0.00000004146695, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2508
Epoch 2508, Loss: 0.00000006562155, Improvement: -0.00000001015489, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2509
Epoch 2509, Loss: 0.00000004922666, Improvement: -0.00000001639489, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2510
Epoch 2510, Loss: 0.00000008231616, Improvement: 0.00000003308951, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2511
Epoch 2511, Loss: 0.00000006961509, Improvement: -0.00000001270107, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2512
Epoch 2512, Loss: 0.00000006842294, Improvement: -0.00000000119215, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2513
Epoch 2513, Loss: 0.00000004147776, Improvement: -0.00000002694518, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2514
Epoch 2514, Loss: 0.00000003957851, Improvement: -0.00000000189925, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2515
Epoch 2515, Loss: 0.00000004207492, Improvement: 0.00000000249641, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2516
Epoch 2516, Loss: 0.00000007933726, Improvement: 0.00000003726234, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2517
Epoch 2517, Loss: 0.00000008042638, Improvement: 0.00000000108912, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2518
Epoch 2518, Loss: 0.00000006595442, Improvement: -0.00000001447197, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2519
Epoch 2519, Loss: 0.00000003684605, Improvement: -0.00000002910837, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2520
Epoch 2520, Loss: 0.00000004158405, Improvement: 0.00000000473799, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2521
Epoch 2521, Loss: 0.00000003889801, Improvement: -0.00000000268603, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2522
Epoch 2522, Loss: 0.00000003016064, Improvement: -0.00000000873738, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2523
Epoch 2523, Loss: 0.00000002862443, Improvement: -0.00000000153621, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2524
Epoch 2524, Loss: 0.00000002994165, Improvement: 0.00000000131723, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2525
Epoch 2525, Loss: 0.00000002968809, Improvement: -0.00000000025357, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2526
Epoch 2526, Loss: 0.00000002677812, Improvement: -0.00000000290997, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2527
Epoch 2527, Loss: 0.00000003761804, Improvement: 0.00000001083993, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2528
Epoch 2528, Loss: 0.00000005543484, Improvement: 0.00000001781680, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2529
Epoch 2529, Loss: 0.00000014027485, Improvement: 0.00000008484001, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2530
Epoch 2530, Loss: 0.00000013672005, Improvement: -0.00000000355480, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2531
Epoch 2531, Loss: 0.00000008467587, Improvement: -0.00000005204417, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2532
Epoch 2532, Loss: 0.00000006409548, Improvement: -0.00000002058039, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2533
Epoch 2533, Loss: 0.00000007142281, Improvement: 0.00000000732732, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2534
Epoch 2534, Loss: 0.00000009960747, Improvement: 0.00000002818466, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2535
Epoch 2535, Loss: 0.00000004181996, Improvement: -0.00000005778751, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2536
Epoch 2536, Loss: 0.00000002817183, Improvement: -0.00000001364813, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2537
Epoch 2537, Loss: 0.00000002465317, Improvement: -0.00000000351866, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2538
Epoch 2538, Loss: 0.00000002582956, Improvement: 0.00000000117639, Best Loss: 0.00000001578042 in Epoch 2304
Epoch 2539
A best model at epoch 2539 has been saved with training error 0.00000001533447.
Epoch 2539, Loss: 0.00000002614017, Improvement: 0.00000000031061, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2540
Epoch 2540, Loss: 0.00000002203759, Improvement: -0.00000000410258, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2541
Epoch 2541, Loss: 0.00000002413535, Improvement: 0.00000000209776, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2542
Epoch 2542, Loss: 0.00000002751219, Improvement: 0.00000000337684, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2543
Epoch 2543, Loss: 0.00000002726251, Improvement: -0.00000000024967, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2544
Epoch 2544, Loss: 0.00000002734941, Improvement: 0.00000000008689, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2545
Epoch 2545, Loss: 0.00000002596993, Improvement: -0.00000000137948, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2546
Epoch 2546, Loss: 0.00000002627032, Improvement: 0.00000000030040, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2547
Epoch 2547, Loss: 0.00000003012044, Improvement: 0.00000000385011, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2548
Epoch 2548, Loss: 0.00000003639951, Improvement: 0.00000000627907, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2549
Epoch 2549, Loss: 0.00000006403474, Improvement: 0.00000002763524, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000005655067, Improvement: -0.00000000748407, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2551
Epoch 2551, Loss: 0.00000006153700, Improvement: 0.00000000498633, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2552
Epoch 2552, Loss: 0.00000006284173, Improvement: 0.00000000130473, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2553
Epoch 2553, Loss: 0.00000009822604, Improvement: 0.00000003538431, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2554
Epoch 2554, Loss: 0.00000007428732, Improvement: -0.00000002393872, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2555
Epoch 2555, Loss: 0.00000007535944, Improvement: 0.00000000107212, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2556
Epoch 2556, Loss: 0.00000010423999, Improvement: 0.00000002888055, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2557
Epoch 2557, Loss: 0.00000012217033, Improvement: 0.00000001793034, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2558
Epoch 2558, Loss: 0.00000006158805, Improvement: -0.00000006058228, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2559
Epoch 2559, Loss: 0.00000003702901, Improvement: -0.00000002455903, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2560
Epoch 2560, Loss: 0.00000003447958, Improvement: -0.00000000254944, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2561
Epoch 2561, Loss: 0.00000004727096, Improvement: 0.00000001279138, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2562
Epoch 2562, Loss: 0.00000005315210, Improvement: 0.00000000588114, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2563
Epoch 2563, Loss: 0.00000004959954, Improvement: -0.00000000355255, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2564
Epoch 2564, Loss: 0.00000004016442, Improvement: -0.00000000943512, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2565
Epoch 2565, Loss: 0.00000004431920, Improvement: 0.00000000415478, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2566
Epoch 2566, Loss: 0.00000004004232, Improvement: -0.00000000427688, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2567
Epoch 2567, Loss: 0.00000002902108, Improvement: -0.00000001102124, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2568
Epoch 2568, Loss: 0.00000002996053, Improvement: 0.00000000093944, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2569
Epoch 2569, Loss: 0.00000003820959, Improvement: 0.00000000824906, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2570
Epoch 2570, Loss: 0.00000005854860, Improvement: 0.00000002033901, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2571
Epoch 2571, Loss: 0.00000005494348, Improvement: -0.00000000360512, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2572
Epoch 2572, Loss: 0.00000003682410, Improvement: -0.00000001811938, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2573
Epoch 2573, Loss: 0.00000002868152, Improvement: -0.00000000814257, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2574
Epoch 2574, Loss: 0.00000002936340, Improvement: 0.00000000068188, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2575
Epoch 2575, Loss: 0.00000006379159, Improvement: 0.00000003442818, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2576
Epoch 2576, Loss: 0.00000006766628, Improvement: 0.00000000387469, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2577
Epoch 2577, Loss: 0.00000016216183, Improvement: 0.00000009449555, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2578
Epoch 2578, Loss: 0.00000010450319, Improvement: -0.00000005765864, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2579
Epoch 2579, Loss: 0.00000005399480, Improvement: -0.00000005050839, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2580
Epoch 2580, Loss: 0.00000003680218, Improvement: -0.00000001719263, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2581
Epoch 2581, Loss: 0.00000004262278, Improvement: 0.00000000582060, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2582
Epoch 2582, Loss: 0.00000004340918, Improvement: 0.00000000078640, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2583
Epoch 2583, Loss: 0.00000003220271, Improvement: -0.00000001120647, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2584
Epoch 2584, Loss: 0.00000002985052, Improvement: -0.00000000235219, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2585
Epoch 2585, Loss: 0.00000003118958, Improvement: 0.00000000133907, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2586
Epoch 2586, Loss: 0.00000002994103, Improvement: -0.00000000124856, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2587
Epoch 2587, Loss: 0.00000003033558, Improvement: 0.00000000039456, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2588
Epoch 2588, Loss: 0.00000003326308, Improvement: 0.00000000292749, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2589
Epoch 2589, Loss: 0.00000002500969, Improvement: -0.00000000825339, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2590
Epoch 2590, Loss: 0.00000002690286, Improvement: 0.00000000189317, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2591
Epoch 2591, Loss: 0.00000002329955, Improvement: -0.00000000360331, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2592
Epoch 2592, Loss: 0.00000002307060, Improvement: -0.00000000022895, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2593
Epoch 2593, Loss: 0.00000002438093, Improvement: 0.00000000131033, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2594
Epoch 2594, Loss: 0.00000002942761, Improvement: 0.00000000504668, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2595
Epoch 2595, Loss: 0.00000003274820, Improvement: 0.00000000332058, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2596
Epoch 2596, Loss: 0.00000007431587, Improvement: 0.00000004156768, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2597
Epoch 2597, Loss: 0.00000006903584, Improvement: -0.00000000528003, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2598
Epoch 2598, Loss: 0.00000005933100, Improvement: -0.00000000970485, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2599
Epoch 2599, Loss: 0.00000005831322, Improvement: -0.00000000101778, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000006007346, Improvement: 0.00000000176025, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2601
Epoch 2601, Loss: 0.00000005150601, Improvement: -0.00000000856746, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2602
Epoch 2602, Loss: 0.00000009042463, Improvement: 0.00000003891862, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2603
Epoch 2603, Loss: 0.00000013508640, Improvement: 0.00000004466177, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2604
Epoch 2604, Loss: 0.00000008636797, Improvement: -0.00000004871844, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2605
Epoch 2605, Loss: 0.00000006649058, Improvement: -0.00000001987739, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2606
Epoch 2606, Loss: 0.00000005488766, Improvement: -0.00000001160292, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2607
Epoch 2607, Loss: 0.00000003432059, Improvement: -0.00000002056707, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2608
Epoch 2608, Loss: 0.00000003024438, Improvement: -0.00000000407621, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2609
Epoch 2609, Loss: 0.00000003845309, Improvement: 0.00000000820871, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2610
Epoch 2610, Loss: 0.00000006030395, Improvement: 0.00000002185086, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2611
Epoch 2611, Loss: 0.00000006793822, Improvement: 0.00000000763427, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2612
Epoch 2612, Loss: 0.00000007211772, Improvement: 0.00000000417950, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2613
Epoch 2613, Loss: 0.00000005587976, Improvement: -0.00000001623796, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2614
Epoch 2614, Loss: 0.00000003819336, Improvement: -0.00000001768640, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2615
Epoch 2615, Loss: 0.00000004467928, Improvement: 0.00000000648592, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2616
Epoch 2616, Loss: 0.00000004766257, Improvement: 0.00000000298329, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2617
Epoch 2617, Loss: 0.00000006260934, Improvement: 0.00000001494677, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2618
Epoch 2618, Loss: 0.00000005921441, Improvement: -0.00000000339493, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2619
Epoch 2619, Loss: 0.00000008036843, Improvement: 0.00000002115403, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2620
Epoch 2620, Loss: 0.00000012309953, Improvement: 0.00000004273110, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2621
Epoch 2621, Loss: 0.00000011390419, Improvement: -0.00000000919534, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2622
Epoch 2622, Loss: 0.00000008536970, Improvement: -0.00000002853449, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2623
Epoch 2623, Loss: 0.00000008254355, Improvement: -0.00000000282615, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2624
Epoch 2624, Loss: 0.00000007549673, Improvement: -0.00000000704682, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2625
Epoch 2625, Loss: 0.00000008133800, Improvement: 0.00000000584127, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2626
Epoch 2626, Loss: 0.00000006714951, Improvement: -0.00000001418849, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2627
Epoch 2627, Loss: 0.00000005557766, Improvement: -0.00000001157185, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2628
Epoch 2628, Loss: 0.00000005279040, Improvement: -0.00000000278726, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2629
Epoch 2629, Loss: 0.00000004630176, Improvement: -0.00000000648863, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2630
Epoch 2630, Loss: 0.00000003015086, Improvement: -0.00000001615090, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2631
Epoch 2631, Loss: 0.00000003767915, Improvement: 0.00000000752829, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2632
Epoch 2632, Loss: 0.00000005241427, Improvement: 0.00000001473512, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2633
Epoch 2633, Loss: 0.00000003732463, Improvement: -0.00000001508964, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2634
Epoch 2634, Loss: 0.00000004350213, Improvement: 0.00000000617750, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2635
Epoch 2635, Loss: 0.00000007592156, Improvement: 0.00000003241943, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2636
Epoch 2636, Loss: 0.00000007172144, Improvement: -0.00000000420012, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2637
Epoch 2637, Loss: 0.00000004988790, Improvement: -0.00000002183353, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2638
Epoch 2638, Loss: 0.00000003332523, Improvement: -0.00000001656267, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2639
Epoch 2639, Loss: 0.00000002695513, Improvement: -0.00000000637010, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2640
Epoch 2640, Loss: 0.00000002839037, Improvement: 0.00000000143524, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2641
Epoch 2641, Loss: 0.00000003288775, Improvement: 0.00000000449738, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2642
Epoch 2642, Loss: 0.00000003102574, Improvement: -0.00000000186201, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2643
Epoch 2643, Loss: 0.00000002998602, Improvement: -0.00000000103972, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2644
Epoch 2644, Loss: 0.00000002636823, Improvement: -0.00000000361780, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2645
Epoch 2645, Loss: 0.00000003576271, Improvement: 0.00000000939449, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2646
Epoch 2646, Loss: 0.00000003632424, Improvement: 0.00000000056153, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2647
Epoch 2647, Loss: 0.00000005190305, Improvement: 0.00000001557880, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2648
Epoch 2648, Loss: 0.00000010384947, Improvement: 0.00000005194642, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2649
Epoch 2649, Loss: 0.00000007091162, Improvement: -0.00000003293785, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000004774800, Improvement: -0.00000002316361, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2651
Epoch 2651, Loss: 0.00000006218156, Improvement: 0.00000001443356, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2652
Epoch 2652, Loss: 0.00000003479486, Improvement: -0.00000002738670, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2653
Epoch 2653, Loss: 0.00000003343549, Improvement: -0.00000000135938, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2654
Epoch 2654, Loss: 0.00000003314735, Improvement: -0.00000000028813, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2655
Epoch 2655, Loss: 0.00000003178769, Improvement: -0.00000000135966, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2656
Epoch 2656, Loss: 0.00000002614399, Improvement: -0.00000000564370, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2657
Epoch 2657, Loss: 0.00000004202875, Improvement: 0.00000001588476, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2658
Epoch 2658, Loss: 0.00000004167313, Improvement: -0.00000000035561, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2659
Epoch 2659, Loss: 0.00000005521682, Improvement: 0.00000001354369, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2660
Epoch 2660, Loss: 0.00000003796765, Improvement: -0.00000001724917, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2661
Epoch 2661, Loss: 0.00000003102171, Improvement: -0.00000000694594, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2662
Epoch 2662, Loss: 0.00000003410547, Improvement: 0.00000000308376, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2663
Epoch 2663, Loss: 0.00000002672515, Improvement: -0.00000000738032, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2664
Epoch 2664, Loss: 0.00000002919762, Improvement: 0.00000000247247, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2665
Epoch 2665, Loss: 0.00000005102074, Improvement: 0.00000002182313, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2666
Epoch 2666, Loss: 0.00000007098111, Improvement: 0.00000001996036, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2667
Epoch 2667, Loss: 0.00000007771417, Improvement: 0.00000000673306, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2668
Epoch 2668, Loss: 0.00000017428165, Improvement: 0.00000009656748, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2669
Epoch 2669, Loss: 0.00000013573646, Improvement: -0.00000003854519, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2670
Epoch 2670, Loss: 0.00000008223030, Improvement: -0.00000005350616, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2671
Epoch 2671, Loss: 0.00000004634974, Improvement: -0.00000003588056, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2672
Epoch 2672, Loss: 0.00000002530601, Improvement: -0.00000002104373, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2673
Epoch 2673, Loss: 0.00000002431604, Improvement: -0.00000000098997, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2674
Epoch 2674, Loss: 0.00000002227049, Improvement: -0.00000000204555, Best Loss: 0.00000001533447 in Epoch 2539
Epoch 2675
A best model at epoch 2675 has been saved with training error 0.00000001529842.
Epoch 2675, Loss: 0.00000002124392, Improvement: -0.00000000102657, Best Loss: 0.00000001529842 in Epoch 2675
Epoch 2676
Epoch 2676, Loss: 0.00000002219191, Improvement: 0.00000000094800, Best Loss: 0.00000001529842 in Epoch 2675
Epoch 2677
A best model at epoch 2677 has been saved with training error 0.00000001226322.
Epoch 2677, Loss: 0.00000002125535, Improvement: -0.00000000093656, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2678
Epoch 2678, Loss: 0.00000002009471, Improvement: -0.00000000116065, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2679
Epoch 2679, Loss: 0.00000001964438, Improvement: -0.00000000045032, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2680
Epoch 2680, Loss: 0.00000001957651, Improvement: -0.00000000006787, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2681
Epoch 2681, Loss: 0.00000003405139, Improvement: 0.00000001447488, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2682
Epoch 2682, Loss: 0.00000004767131, Improvement: 0.00000001361992, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2683
Epoch 2683, Loss: 0.00000006127907, Improvement: 0.00000001360776, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2684
Epoch 2684, Loss: 0.00000004096864, Improvement: -0.00000002031044, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2685
Epoch 2685, Loss: 0.00000002680242, Improvement: -0.00000001416622, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2686
Epoch 2686, Loss: 0.00000002782730, Improvement: 0.00000000102488, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2687
Epoch 2687, Loss: 0.00000002253685, Improvement: -0.00000000529045, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2688
Epoch 2688, Loss: 0.00000002557123, Improvement: 0.00000000303438, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2689
Epoch 2689, Loss: 0.00000002715182, Improvement: 0.00000000158059, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2690
Epoch 2690, Loss: 0.00000003150515, Improvement: 0.00000000435333, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2691
Epoch 2691, Loss: 0.00000007114984, Improvement: 0.00000003964469, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2692
Epoch 2692, Loss: 0.00000005449553, Improvement: -0.00000001665430, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2693
Epoch 2693, Loss: 0.00000005341296, Improvement: -0.00000000108257, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2694
Epoch 2694, Loss: 0.00000009759840, Improvement: 0.00000004418544, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2695
Epoch 2695, Loss: 0.00000010910998, Improvement: 0.00000001151158, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2696
Epoch 2696, Loss: 0.00000006987044, Improvement: -0.00000003923954, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2697
Epoch 2697, Loss: 0.00000003485412, Improvement: -0.00000003501632, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2698
Epoch 2698, Loss: 0.00000003018463, Improvement: -0.00000000466948, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2699
Epoch 2699, Loss: 0.00000003402345, Improvement: 0.00000000383881, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000004820162, Improvement: 0.00000001417817, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2701
Epoch 2701, Loss: 0.00000005028791, Improvement: 0.00000000208629, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2702
Epoch 2702, Loss: 0.00000003825250, Improvement: -0.00000001203541, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2703
Epoch 2703, Loss: 0.00000004690911, Improvement: 0.00000000865661, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2704
Epoch 2704, Loss: 0.00000003840137, Improvement: -0.00000000850775, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2705
Epoch 2705, Loss: 0.00000005885275, Improvement: 0.00000002045138, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2706
Epoch 2706, Loss: 0.00000008687146, Improvement: 0.00000002801872, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2707
Epoch 2707, Loss: 0.00000004940614, Improvement: -0.00000003746532, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2708
Epoch 2708, Loss: 0.00000003161967, Improvement: -0.00000001778647, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2709
Epoch 2709, Loss: 0.00000004174593, Improvement: 0.00000001012626, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2710
Epoch 2710, Loss: 0.00000004722710, Improvement: 0.00000000548117, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2711
Epoch 2711, Loss: 0.00000011265967, Improvement: 0.00000006543257, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2712
Epoch 2712, Loss: 0.00000007423123, Improvement: -0.00000003842844, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2713
Epoch 2713, Loss: 0.00000005420946, Improvement: -0.00000002002177, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2714
Epoch 2714, Loss: 0.00000004263734, Improvement: -0.00000001157212, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2715
Epoch 2715, Loss: 0.00000003392806, Improvement: -0.00000000870927, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2716
Epoch 2716, Loss: 0.00000005129077, Improvement: 0.00000001736271, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2717
Epoch 2717, Loss: 0.00000003724336, Improvement: -0.00000001404741, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2718
Epoch 2718, Loss: 0.00000002482778, Improvement: -0.00000001241558, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2719
Epoch 2719, Loss: 0.00000002498320, Improvement: 0.00000000015543, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2720
Epoch 2720, Loss: 0.00000002475095, Improvement: -0.00000000023225, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2721
Epoch 2721, Loss: 0.00000002599164, Improvement: 0.00000000124068, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2722
Epoch 2722, Loss: 0.00000002612784, Improvement: 0.00000000013620, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2723
Epoch 2723, Loss: 0.00000003340292, Improvement: 0.00000000727508, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2724
Epoch 2724, Loss: 0.00000002674013, Improvement: -0.00000000666279, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2725
Epoch 2725, Loss: 0.00000002459500, Improvement: -0.00000000214513, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2726
Epoch 2726, Loss: 0.00000004128492, Improvement: 0.00000001668992, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2727
Epoch 2727, Loss: 0.00000004258699, Improvement: 0.00000000130207, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2728
Epoch 2728, Loss: 0.00000003855550, Improvement: -0.00000000403149, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2729
Epoch 2729, Loss: 0.00000011719710, Improvement: 0.00000007864161, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2730
Epoch 2730, Loss: 0.00000013745336, Improvement: 0.00000002025626, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2731
Epoch 2731, Loss: 0.00000010365508, Improvement: -0.00000003379828, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2732
Epoch 2732, Loss: 0.00000009026666, Improvement: -0.00000001338842, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2733
Epoch 2733, Loss: 0.00000007601485, Improvement: -0.00000001425181, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2734
Epoch 2734, Loss: 0.00000006005299, Improvement: -0.00000001596186, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2735
Epoch 2735, Loss: 0.00000005064653, Improvement: -0.00000000940646, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2736
Epoch 2736, Loss: 0.00000003226580, Improvement: -0.00000001838073, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2737
Epoch 2737, Loss: 0.00000002715804, Improvement: -0.00000000510776, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2738
Epoch 2738, Loss: 0.00000003053382, Improvement: 0.00000000337578, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2739
Epoch 2739, Loss: 0.00000007189911, Improvement: 0.00000004136529, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2740
Epoch 2740, Loss: 0.00000010078975, Improvement: 0.00000002889064, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2741
Epoch 2741, Loss: 0.00000006028973, Improvement: -0.00000004050002, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2742
Epoch 2742, Loss: 0.00000003602953, Improvement: -0.00000002426019, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2743
Epoch 2743, Loss: 0.00000002957149, Improvement: -0.00000000645804, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2744
Epoch 2744, Loss: 0.00000002185119, Improvement: -0.00000000772030, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2745
Epoch 2745, Loss: 0.00000001890618, Improvement: -0.00000000294501, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2746
Epoch 2746, Loss: 0.00000001872632, Improvement: -0.00000000017986, Best Loss: 0.00000001226322 in Epoch 2677
Epoch 2747
A best model at epoch 2747 has been saved with training error 0.00000001137494.
Epoch 2747, Loss: 0.00000002096732, Improvement: 0.00000000224100, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2748
Epoch 2748, Loss: 0.00000002429771, Improvement: 0.00000000333040, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2749
Epoch 2749, Loss: 0.00000002498645, Improvement: 0.00000000068873, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000002589099, Improvement: 0.00000000090454, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2751
Epoch 2751, Loss: 0.00000003273108, Improvement: 0.00000000684009, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2752
Epoch 2752, Loss: 0.00000003032200, Improvement: -0.00000000240908, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2753
Epoch 2753, Loss: 0.00000003506264, Improvement: 0.00000000474065, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2754
Epoch 2754, Loss: 0.00000003938107, Improvement: 0.00000000431843, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2755
Epoch 2755, Loss: 0.00000010958540, Improvement: 0.00000007020433, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2756
Epoch 2756, Loss: 0.00000010193896, Improvement: -0.00000000764644, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2757
Epoch 2757, Loss: 0.00000011586473, Improvement: 0.00000001392577, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2758
Epoch 2758, Loss: 0.00000008109531, Improvement: -0.00000003476943, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2759
Epoch 2759, Loss: 0.00000005047565, Improvement: -0.00000003061965, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2760
Epoch 2760, Loss: 0.00000002834346, Improvement: -0.00000002213219, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2761
Epoch 2761, Loss: 0.00000002208510, Improvement: -0.00000000625836, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2762
Epoch 2762, Loss: 0.00000002296581, Improvement: 0.00000000088071, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2763
Epoch 2763, Loss: 0.00000002900592, Improvement: 0.00000000604011, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2764
Epoch 2764, Loss: 0.00000002918930, Improvement: 0.00000000018338, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2765
Epoch 2765, Loss: 0.00000002736624, Improvement: -0.00000000182306, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2766
Epoch 2766, Loss: 0.00000002861164, Improvement: 0.00000000124540, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2767
Epoch 2767, Loss: 0.00000003472761, Improvement: 0.00000000611597, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2768
Epoch 2768, Loss: 0.00000003325602, Improvement: -0.00000000147159, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2769
Epoch 2769, Loss: 0.00000002594800, Improvement: -0.00000000730802, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2770
Epoch 2770, Loss: 0.00000002258950, Improvement: -0.00000000335850, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2771
Epoch 2771, Loss: 0.00000003790526, Improvement: 0.00000001531576, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2772
Epoch 2772, Loss: 0.00000003998828, Improvement: 0.00000000208302, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2773
Epoch 2773, Loss: 0.00000003740060, Improvement: -0.00000000258769, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2774
Epoch 2774, Loss: 0.00000006345908, Improvement: 0.00000002605849, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2775
Epoch 2775, Loss: 0.00000012274010, Improvement: 0.00000005928102, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2776
Epoch 2776, Loss: 0.00000008468767, Improvement: -0.00000003805243, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2777
Epoch 2777, Loss: 0.00000007071259, Improvement: -0.00000001397509, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2778
Epoch 2778, Loss: 0.00000008121968, Improvement: 0.00000001050709, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2779
Epoch 2779, Loss: 0.00000005040806, Improvement: -0.00000003081161, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2780
Epoch 2780, Loss: 0.00000005136867, Improvement: 0.00000000096061, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2781
Epoch 2781, Loss: 0.00000005124044, Improvement: -0.00000000012823, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2782
Epoch 2782, Loss: 0.00000004549874, Improvement: -0.00000000574170, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2783
Epoch 2783, Loss: 0.00000003052413, Improvement: -0.00000001497461, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2784
Epoch 2784, Loss: 0.00000002597431, Improvement: -0.00000000454983, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2785
Epoch 2785, Loss: 0.00000002512115, Improvement: -0.00000000085315, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2786
Epoch 2786, Loss: 0.00000002716310, Improvement: 0.00000000204195, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2787
Epoch 2787, Loss: 0.00000002233813, Improvement: -0.00000000482497, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2788
Epoch 2788, Loss: 0.00000001932119, Improvement: -0.00000000301694, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2789
Epoch 2789, Loss: 0.00000001829769, Improvement: -0.00000000102349, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2790
Epoch 2790, Loss: 0.00000001892388, Improvement: 0.00000000062619, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2791
Epoch 2791, Loss: 0.00000001833696, Improvement: -0.00000000058692, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2792
Epoch 2792, Loss: 0.00000001948038, Improvement: 0.00000000114342, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2793
Epoch 2793, Loss: 0.00000001976154, Improvement: 0.00000000028116, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2794
Epoch 2794, Loss: 0.00000002783120, Improvement: 0.00000000806966, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2795
Epoch 2795, Loss: 0.00000004588373, Improvement: 0.00000001805253, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2796
Epoch 2796, Loss: 0.00000006183316, Improvement: 0.00000001594942, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2797
Epoch 2797, Loss: 0.00000007508170, Improvement: 0.00000001324855, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2798
Epoch 2798, Loss: 0.00000007224848, Improvement: -0.00000000283322, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2799
Epoch 2799, Loss: 0.00000006568323, Improvement: -0.00000000656526, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000005785362, Improvement: -0.00000000782961, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2801
Epoch 2801, Loss: 0.00000005217294, Improvement: -0.00000000568068, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2802
Epoch 2802, Loss: 0.00000005014822, Improvement: -0.00000000202472, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2803
Epoch 2803, Loss: 0.00000003365530, Improvement: -0.00000001649291, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2804
Epoch 2804, Loss: 0.00000003724009, Improvement: 0.00000000358479, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2805
Epoch 2805, Loss: 0.00000003780151, Improvement: 0.00000000056143, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2806
Epoch 2806, Loss: 0.00000007035529, Improvement: 0.00000003255378, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2807
Epoch 2807, Loss: 0.00000009220827, Improvement: 0.00000002185297, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2808
Epoch 2808, Loss: 0.00000005426274, Improvement: -0.00000003794553, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2809
Epoch 2809, Loss: 0.00000003766569, Improvement: -0.00000001659705, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2810
Epoch 2810, Loss: 0.00000002697987, Improvement: -0.00000001068582, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2811
Epoch 2811, Loss: 0.00000003691262, Improvement: 0.00000000993276, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2812
Epoch 2812, Loss: 0.00000004929952, Improvement: 0.00000001238690, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2813
Epoch 2813, Loss: 0.00000003814288, Improvement: -0.00000001115664, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2814
Epoch 2814, Loss: 0.00000003591604, Improvement: -0.00000000222684, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2815
Epoch 2815, Loss: 0.00000004063085, Improvement: 0.00000000471481, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2816
Epoch 2816, Loss: 0.00000003413945, Improvement: -0.00000000649141, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2817
Epoch 2817, Loss: 0.00000003228966, Improvement: -0.00000000184978, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2818
Epoch 2818, Loss: 0.00000004496912, Improvement: 0.00000001267946, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2819
Epoch 2819, Loss: 0.00000007911342, Improvement: 0.00000003414430, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2820
Epoch 2820, Loss: 0.00000005197286, Improvement: -0.00000002714056, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2821
Epoch 2821, Loss: 0.00000004560158, Improvement: -0.00000000637128, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2822
Epoch 2822, Loss: 0.00000003051190, Improvement: -0.00000001508968, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2823
Epoch 2823, Loss: 0.00000003762726, Improvement: 0.00000000711536, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2824
Epoch 2824, Loss: 0.00000005921188, Improvement: 0.00000002158462, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2825
Epoch 2825, Loss: 0.00000006302337, Improvement: 0.00000000381149, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2826
Epoch 2826, Loss: 0.00000006057847, Improvement: -0.00000000244490, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2827
Epoch 2827, Loss: 0.00000006031120, Improvement: -0.00000000026727, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2828
Epoch 2828, Loss: 0.00000010749649, Improvement: 0.00000004718529, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2829
Epoch 2829, Loss: 0.00000009871766, Improvement: -0.00000000877883, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2830
Epoch 2830, Loss: 0.00000007590328, Improvement: -0.00000002281439, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2831
Epoch 2831, Loss: 0.00000005012015, Improvement: -0.00000002578313, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2832
Epoch 2832, Loss: 0.00000002867763, Improvement: -0.00000002144252, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2833
Epoch 2833, Loss: 0.00000002523419, Improvement: -0.00000000344344, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2834
Epoch 2834, Loss: 0.00000002507912, Improvement: -0.00000000015507, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2835
Epoch 2835, Loss: 0.00000002986435, Improvement: 0.00000000478523, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2836
Epoch 2836, Loss: 0.00000004101306, Improvement: 0.00000001114871, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2837
Epoch 2837, Loss: 0.00000004061457, Improvement: -0.00000000039849, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2838
Epoch 2838, Loss: 0.00000004003562, Improvement: -0.00000000057895, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2839
Epoch 2839, Loss: 0.00000003874819, Improvement: -0.00000000128743, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2840
Epoch 2840, Loss: 0.00000002871208, Improvement: -0.00000001003611, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2841
Epoch 2841, Loss: 0.00000002431088, Improvement: -0.00000000440120, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2842
Epoch 2842, Loss: 0.00000002827931, Improvement: 0.00000000396843, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2843
Epoch 2843, Loss: 0.00000004474433, Improvement: 0.00000001646502, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2844
Epoch 2844, Loss: 0.00000003410390, Improvement: -0.00000001064043, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2845
Epoch 2845, Loss: 0.00000004543675, Improvement: 0.00000001133285, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2846
Epoch 2846, Loss: 0.00000007001617, Improvement: 0.00000002457942, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2847
Epoch 2847, Loss: 0.00000007416540, Improvement: 0.00000000414923, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2848
Epoch 2848, Loss: 0.00000013188419, Improvement: 0.00000005771879, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2849
Epoch 2849, Loss: 0.00000011704891, Improvement: -0.00000001483528, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000005711167, Improvement: -0.00000005993724, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2851
Epoch 2851, Loss: 0.00000002927254, Improvement: -0.00000002783913, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2852
Epoch 2852, Loss: 0.00000003269171, Improvement: 0.00000000341917, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2853
Epoch 2853, Loss: 0.00000003913623, Improvement: 0.00000000644453, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2854
Epoch 2854, Loss: 0.00000002894868, Improvement: -0.00000001018756, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2855
Epoch 2855, Loss: 0.00000001901403, Improvement: -0.00000000993464, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2856
Epoch 2856, Loss: 0.00000001841237, Improvement: -0.00000000060166, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2857
Epoch 2857, Loss: 0.00000001990868, Improvement: 0.00000000149631, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2858
Epoch 2858, Loss: 0.00000001891348, Improvement: -0.00000000099520, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2859
Epoch 2859, Loss: 0.00000002144781, Improvement: 0.00000000253433, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2860
Epoch 2860, Loss: 0.00000002321835, Improvement: 0.00000000177054, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2861
Epoch 2861, Loss: 0.00000002349038, Improvement: 0.00000000027203, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2862
Epoch 2862, Loss: 0.00000002492540, Improvement: 0.00000000143502, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2863
Epoch 2863, Loss: 0.00000002634867, Improvement: 0.00000000142327, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2864
Epoch 2864, Loss: 0.00000003548535, Improvement: 0.00000000913668, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2865
Epoch 2865, Loss: 0.00000002690338, Improvement: -0.00000000858197, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2866
Epoch 2866, Loss: 0.00000002384677, Improvement: -0.00000000305660, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2867
Epoch 2867, Loss: 0.00000002478939, Improvement: 0.00000000094262, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2868
Epoch 2868, Loss: 0.00000004278005, Improvement: 0.00000001799066, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2869
Epoch 2869, Loss: 0.00000004114194, Improvement: -0.00000000163811, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2870
Epoch 2870, Loss: 0.00000003189374, Improvement: -0.00000000924820, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2871
Epoch 2871, Loss: 0.00000005469497, Improvement: 0.00000002280123, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2872
Epoch 2872, Loss: 0.00000003355510, Improvement: -0.00000002113987, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2873
Epoch 2873, Loss: 0.00000004893143, Improvement: 0.00000001537633, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2874
Epoch 2874, Loss: 0.00000003296911, Improvement: -0.00000001596232, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2875
Epoch 2875, Loss: 0.00000003192443, Improvement: -0.00000000104468, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2876
Epoch 2876, Loss: 0.00000003975455, Improvement: 0.00000000783012, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2877
Epoch 2877, Loss: 0.00000004553577, Improvement: 0.00000000578122, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2878
Epoch 2878, Loss: 0.00000006092661, Improvement: 0.00000001539084, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2879
Epoch 2879, Loss: 0.00000008252720, Improvement: 0.00000002160059, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2880
Epoch 2880, Loss: 0.00000009023028, Improvement: 0.00000000770309, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2881
Epoch 2881, Loss: 0.00000004576101, Improvement: -0.00000004446928, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2882
Epoch 2882, Loss: 0.00000004888021, Improvement: 0.00000000311921, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2883
Epoch 2883, Loss: 0.00000004345083, Improvement: -0.00000000542939, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2884
Epoch 2884, Loss: 0.00000003383783, Improvement: -0.00000000961299, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2885
Epoch 2885, Loss: 0.00000004569004, Improvement: 0.00000001185220, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2886
Epoch 2886, Loss: 0.00000003403434, Improvement: -0.00000001165570, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2887
Epoch 2887, Loss: 0.00000004100750, Improvement: 0.00000000697315, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2888
Epoch 2888, Loss: 0.00000006272918, Improvement: 0.00000002172168, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2889
Epoch 2889, Loss: 0.00000005045343, Improvement: -0.00000001227576, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2890
Epoch 2890, Loss: 0.00000003751701, Improvement: -0.00000001293642, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2891
Epoch 2891, Loss: 0.00000003653770, Improvement: -0.00000000097931, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2892
Epoch 2892, Loss: 0.00000007388177, Improvement: 0.00000003734407, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2893
Epoch 2893, Loss: 0.00000005509810, Improvement: -0.00000001878367, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2894
Epoch 2894, Loss: 0.00000003066253, Improvement: -0.00000002443556, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2895
Epoch 2895, Loss: 0.00000002504410, Improvement: -0.00000000561844, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2896
Epoch 2896, Loss: 0.00000003317831, Improvement: 0.00000000813421, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2897
Epoch 2897, Loss: 0.00000002905221, Improvement: -0.00000000412610, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2898
Epoch 2898, Loss: 0.00000002507762, Improvement: -0.00000000397459, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2899
Epoch 2899, Loss: 0.00000002939229, Improvement: 0.00000000431467, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000002373511, Improvement: -0.00000000565718, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2901
Epoch 2901, Loss: 0.00000002147460, Improvement: -0.00000000226051, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2902
Epoch 2902, Loss: 0.00000002312260, Improvement: 0.00000000164800, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2903
Epoch 2903, Loss: 0.00000003611525, Improvement: 0.00000001299265, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2904
Epoch 2904, Loss: 0.00000005333781, Improvement: 0.00000001722256, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2905
Epoch 2905, Loss: 0.00000004009141, Improvement: -0.00000001324640, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2906
Epoch 2906, Loss: 0.00000005729428, Improvement: 0.00000001720287, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2907
Epoch 2907, Loss: 0.00000006179159, Improvement: 0.00000000449732, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2908
Epoch 2908, Loss: 0.00000005066414, Improvement: -0.00000001112746, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2909
Epoch 2909, Loss: 0.00000014110205, Improvement: 0.00000009043791, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2910
Epoch 2910, Loss: 0.00000016491481, Improvement: 0.00000002381276, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2911
Epoch 2911, Loss: 0.00000005179513, Improvement: -0.00000011311968, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2912
Epoch 2912, Loss: 0.00000003297500, Improvement: -0.00000001882013, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2913
Epoch 2913, Loss: 0.00000002423672, Improvement: -0.00000000873828, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2914
Epoch 2914, Loss: 0.00000001967406, Improvement: -0.00000000456266, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2915
Epoch 2915, Loss: 0.00000001830768, Improvement: -0.00000000136637, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2916
Epoch 2916, Loss: 0.00000001893542, Improvement: 0.00000000062774, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2917
Epoch 2917, Loss: 0.00000001911661, Improvement: 0.00000000018119, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2918
Epoch 2918, Loss: 0.00000001785797, Improvement: -0.00000000125864, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2919
Epoch 2919, Loss: 0.00000001875849, Improvement: 0.00000000090052, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2920
Epoch 2920, Loss: 0.00000002138778, Improvement: 0.00000000262929, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2921
Epoch 2921, Loss: 0.00000002176430, Improvement: 0.00000000037652, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2922
Epoch 2922, Loss: 0.00000002064784, Improvement: -0.00000000111646, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2923
Epoch 2923, Loss: 0.00000001805261, Improvement: -0.00000000259523, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2924
Epoch 2924, Loss: 0.00000001870960, Improvement: 0.00000000065699, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2925
Epoch 2925, Loss: 0.00000001766975, Improvement: -0.00000000103985, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2926
Epoch 2926, Loss: 0.00000001852568, Improvement: 0.00000000085593, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2927
Epoch 2927, Loss: 0.00000002277965, Improvement: 0.00000000425396, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2928
Epoch 2928, Loss: 0.00000002580184, Improvement: 0.00000000302220, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2929
Epoch 2929, Loss: 0.00000002947579, Improvement: 0.00000000367395, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2930
Epoch 2930, Loss: 0.00000002217422, Improvement: -0.00000000730157, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2931
Epoch 2931, Loss: 0.00000002756960, Improvement: 0.00000000539538, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2932
Epoch 2932, Loss: 0.00000004527494, Improvement: 0.00000001770534, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2933
Epoch 2933, Loss: 0.00000004454287, Improvement: -0.00000000073207, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2934
Epoch 2934, Loss: 0.00000003234003, Improvement: -0.00000001220284, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2935
Epoch 2935, Loss: 0.00000004075411, Improvement: 0.00000000841408, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2936
Epoch 2936, Loss: 0.00000004638754, Improvement: 0.00000000563343, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2937
Epoch 2937, Loss: 0.00000004542691, Improvement: -0.00000000096063, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2938
Epoch 2938, Loss: 0.00000004735276, Improvement: 0.00000000192585, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2939
Epoch 2939, Loss: 0.00000002715353, Improvement: -0.00000002019923, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2940
Epoch 2940, Loss: 0.00000002651604, Improvement: -0.00000000063748, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2941
Epoch 2941, Loss: 0.00000006842668, Improvement: 0.00000004191064, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2942
Epoch 2942, Loss: 0.00000005640866, Improvement: -0.00000001201802, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2943
Epoch 2943, Loss: 0.00000007730422, Improvement: 0.00000002089557, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2944
Epoch 2944, Loss: 0.00000005544017, Improvement: -0.00000002186406, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2945
Epoch 2945, Loss: 0.00000005263057, Improvement: -0.00000000280960, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2946
Epoch 2946, Loss: 0.00000005350395, Improvement: 0.00000000087339, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2947
Epoch 2947, Loss: 0.00000004186431, Improvement: -0.00000001163964, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2948
Epoch 2948, Loss: 0.00000005563203, Improvement: 0.00000001376772, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2949
Epoch 2949, Loss: 0.00000005188199, Improvement: -0.00000000375004, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000005727681, Improvement: 0.00000000539481, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2951
Epoch 2951, Loss: 0.00000004814602, Improvement: -0.00000000913079, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2952
Epoch 2952, Loss: 0.00000007117555, Improvement: 0.00000002302953, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2953
Epoch 2953, Loss: 0.00000005036503, Improvement: -0.00000002081052, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2954
Epoch 2954, Loss: 0.00000006300047, Improvement: 0.00000001263544, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2955
Epoch 2955, Loss: 0.00000005519426, Improvement: -0.00000000780621, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2956
Epoch 2956, Loss: 0.00000004104862, Improvement: -0.00000001414564, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2957
Epoch 2957, Loss: 0.00000004016808, Improvement: -0.00000000088054, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2958
Epoch 2958, Loss: 0.00000002598341, Improvement: -0.00000001418468, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2959
Epoch 2959, Loss: 0.00000002119373, Improvement: -0.00000000478968, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2960
Epoch 2960, Loss: 0.00000002191726, Improvement: 0.00000000072354, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2961
Epoch 2961, Loss: 0.00000002312085, Improvement: 0.00000000120359, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2962
Epoch 2962, Loss: 0.00000004004678, Improvement: 0.00000001692593, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2963
Epoch 2963, Loss: 0.00000003525002, Improvement: -0.00000000479676, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2964
Epoch 2964, Loss: 0.00000004019178, Improvement: 0.00000000494176, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2965
Epoch 2965, Loss: 0.00000007024858, Improvement: 0.00000003005679, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2966
Epoch 2966, Loss: 0.00000006779562, Improvement: -0.00000000245296, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2967
Epoch 2967, Loss: 0.00000005406738, Improvement: -0.00000001372824, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2968
Epoch 2968, Loss: 0.00000008554323, Improvement: 0.00000003147586, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2969
Epoch 2969, Loss: 0.00000008183617, Improvement: -0.00000000370706, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2970
Epoch 2970, Loss: 0.00000007850816, Improvement: -0.00000000332801, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2971
Epoch 2971, Loss: 0.00000006970955, Improvement: -0.00000000879861, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2972
Epoch 2972, Loss: 0.00000009244401, Improvement: 0.00000002273446, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2973
Epoch 2973, Loss: 0.00000003943460, Improvement: -0.00000005300941, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2974
Epoch 2974, Loss: 0.00000003477050, Improvement: -0.00000000466410, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2975
Epoch 2975, Loss: 0.00000003000732, Improvement: -0.00000000476318, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2976
Epoch 2976, Loss: 0.00000002317072, Improvement: -0.00000000683661, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2977
Epoch 2977, Loss: 0.00000002776315, Improvement: 0.00000000459244, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2978
Epoch 2978, Loss: 0.00000003590793, Improvement: 0.00000000814477, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2979
Epoch 2979, Loss: 0.00000004300052, Improvement: 0.00000000709259, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2980
Epoch 2980, Loss: 0.00000003030581, Improvement: -0.00000001269471, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2981
Epoch 2981, Loss: 0.00000002506783, Improvement: -0.00000000523798, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2982
Epoch 2982, Loss: 0.00000003139600, Improvement: 0.00000000632817, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2983
Epoch 2983, Loss: 0.00000004700586, Improvement: 0.00000001560986, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2984
Epoch 2984, Loss: 0.00000003430286, Improvement: -0.00000001270301, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2985
Epoch 2985, Loss: 0.00000004412316, Improvement: 0.00000000982030, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2986
Epoch 2986, Loss: 0.00000004926150, Improvement: 0.00000000513834, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2987
Epoch 2987, Loss: 0.00000003500273, Improvement: -0.00000001425878, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2988
Epoch 2988, Loss: 0.00000003161133, Improvement: -0.00000000339140, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2989
Epoch 2989, Loss: 0.00000005309367, Improvement: 0.00000002148235, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2990
Epoch 2990, Loss: 0.00000007433506, Improvement: 0.00000002124139, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2991
Epoch 2991, Loss: 0.00000012684914, Improvement: 0.00000005251408, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2992
Epoch 2992, Loss: 0.00000008521991, Improvement: -0.00000004162923, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2993
Epoch 2993, Loss: 0.00000006609720, Improvement: -0.00000001912271, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2994
Epoch 2994, Loss: 0.00000006768985, Improvement: 0.00000000159265, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2995
Epoch 2995, Loss: 0.00000005449941, Improvement: -0.00000001319044, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2996
Epoch 2996, Loss: 0.00000004690847, Improvement: -0.00000000759094, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2997
Epoch 2997, Loss: 0.00000004999698, Improvement: 0.00000000308851, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2998
Epoch 2998, Loss: 0.00000004408622, Improvement: -0.00000000591076, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 2999
Epoch 2999, Loss: 0.00000003466345, Improvement: -0.00000000942276, Best Loss: 0.00000001137494 in Epoch 2747
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000002082054, Improvement: -0.00000001384291, Best Loss: 0.00000001137494 in Epoch 2747
