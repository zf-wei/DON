The dimension of y_tensor is torch.Size([5000, 2]).
The dimension of y_expanded is torch.Size([500, 5000, 2]) after expanding.
The dimensions of the initial conditions are: (500, 50)
The dimensions of the solutions are: (500, 100, 50)
The dimension of u_tensor is torch.Size([500, 50]).
The dimension of u_expanded is torch.Size([500, 5000, 50]) after expanding.
The loaded solution dataset has dimension (500, 100, 50),
	 while the arranged linearized dataset has dimension (500, 5000).
The dimension of s_tensor is torch.Size([500, 5000]).
The dimension of s_expanded is torch.Size([500, 5000, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.060961723.
A best model at epoch 1 has been saved with training error 0.036594443.
Epoch 1, Loss: 0.260718262, Improvement: 0.260718262, Best Loss: 0.036594443 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.029512985.
A best model at epoch 2 has been saved with training error 0.022966027.
Epoch 2, Loss: 0.084224300, Improvement: -0.176493962, Best Loss: 0.022966027 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.017262381.
A best model at epoch 3 has been saved with training error 0.015132034.
A best model at epoch 3 has been saved with training error 0.013202629.
A best model at epoch 3 has been saved with training error 0.011924230.
Epoch 3, Loss: 0.026022903, Improvement: -0.058201397, Best Loss: 0.011924230 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.011799620.
A best model at epoch 4 has been saved with training error 0.010139774.
A best model at epoch 4 has been saved with training error 0.008244103.
Epoch 4, Loss: 0.016040648, Improvement: -0.009982255, Best Loss: 0.008244103 in Epoch 4
Epoch 5
Epoch 5, Loss: 0.013672038, Improvement: -0.002368610, Best Loss: 0.008244103 in Epoch 4
Epoch 6
A best model at epoch 6 has been saved with training error 0.007593758.
A best model at epoch 6 has been saved with training error 0.007045695.
Epoch 6, Loss: 0.012703300, Improvement: -0.000968738, Best Loss: 0.007045695 in Epoch 6
Epoch 7
Epoch 7, Loss: 0.012240664, Improvement: -0.000462636, Best Loss: 0.007045695 in Epoch 6
Epoch 8
Epoch 8, Loss: 0.011890399, Improvement: -0.000350265, Best Loss: 0.007045695 in Epoch 6
Epoch 9
Epoch 9, Loss: 0.011662555, Improvement: -0.000227844, Best Loss: 0.007045695 in Epoch 6
Epoch 10
Epoch 10, Loss: 0.011507070, Improvement: -0.000155485, Best Loss: 0.007045695 in Epoch 6
Epoch 11
Epoch 11, Loss: 0.011367056, Improvement: -0.000140014, Best Loss: 0.007045695 in Epoch 6
Epoch 12
A best model at epoch 12 has been saved with training error 0.006712854.
A best model at epoch 12 has been saved with training error 0.005474281.
Epoch 12, Loss: 0.011239976, Improvement: -0.000127080, Best Loss: 0.005474281 in Epoch 12
Epoch 13
Epoch 13, Loss: 0.011145636, Improvement: -0.000094340, Best Loss: 0.005474281 in Epoch 12
Epoch 14
Epoch 14, Loss: 0.011057252, Improvement: -0.000088384, Best Loss: 0.005474281 in Epoch 12
Epoch 15
Epoch 15, Loss: 0.010973509, Improvement: -0.000083743, Best Loss: 0.005474281 in Epoch 12
Epoch 16
Epoch 16, Loss: 0.010903425, Improvement: -0.000070084, Best Loss: 0.005474281 in Epoch 12
Epoch 17
Epoch 17, Loss: 0.010812795, Improvement: -0.000090630, Best Loss: 0.005474281 in Epoch 12
Epoch 18
Epoch 18, Loss: 0.010745658, Improvement: -0.000067137, Best Loss: 0.005474281 in Epoch 12
Epoch 19
Epoch 19, Loss: 0.010674289, Improvement: -0.000071369, Best Loss: 0.005474281 in Epoch 12
Epoch 20
Epoch 20, Loss: 0.010602534, Improvement: -0.000071755, Best Loss: 0.005474281 in Epoch 12
Epoch 21
A best model at epoch 21 has been saved with training error 0.005309480.
Epoch 21, Loss: 0.010540853, Improvement: -0.000061681, Best Loss: 0.005309480 in Epoch 21
Epoch 22
Epoch 22, Loss: 0.010466330, Improvement: -0.000074524, Best Loss: 0.005309480 in Epoch 21
Epoch 23
A best model at epoch 23 has been saved with training error 0.004094624.
Epoch 23, Loss: 0.010400518, Improvement: -0.000065811, Best Loss: 0.004094624 in Epoch 23
Epoch 24
Epoch 24, Loss: 0.010335338, Improvement: -0.000065180, Best Loss: 0.004094624 in Epoch 23
Epoch 25
Epoch 25, Loss: 0.010261686, Improvement: -0.000073653, Best Loss: 0.004094624 in Epoch 23
Epoch 26
Epoch 26, Loss: 0.010194860, Improvement: -0.000066826, Best Loss: 0.004094624 in Epoch 23
Epoch 27
Epoch 27, Loss: 0.010128985, Improvement: -0.000065875, Best Loss: 0.004094624 in Epoch 23
Epoch 28
Epoch 28, Loss: 0.010060013, Improvement: -0.000068972, Best Loss: 0.004094624 in Epoch 23
Epoch 29
Epoch 29, Loss: 0.009986726, Improvement: -0.000073286, Best Loss: 0.004094624 in Epoch 23
Epoch 30
Epoch 30, Loss: 0.009906498, Improvement: -0.000080229, Best Loss: 0.004094624 in Epoch 23
Epoch 31
Epoch 31, Loss: 0.009838061, Improvement: -0.000068436, Best Loss: 0.004094624 in Epoch 23
Epoch 32
Epoch 32, Loss: 0.009777438, Improvement: -0.000060624, Best Loss: 0.004094624 in Epoch 23
Epoch 33
A best model at epoch 33 has been saved with training error 0.003980352.
Epoch 33, Loss: 0.009684797, Improvement: -0.000092641, Best Loss: 0.003980352 in Epoch 33
Epoch 34
Epoch 34, Loss: 0.009606398, Improvement: -0.000078399, Best Loss: 0.003980352 in Epoch 33
Epoch 35
Epoch 35, Loss: 0.009530431, Improvement: -0.000075967, Best Loss: 0.003980352 in Epoch 33
Epoch 36
Epoch 36, Loss: 0.009455301, Improvement: -0.000075130, Best Loss: 0.003980352 in Epoch 33
Epoch 37
Epoch 37, Loss: 0.009372712, Improvement: -0.000082589, Best Loss: 0.003980352 in Epoch 33
Epoch 38
Epoch 38, Loss: 0.009313173, Improvement: -0.000059539, Best Loss: 0.003980352 in Epoch 33
Epoch 39
Epoch 39, Loss: 0.009218239, Improvement: -0.000094935, Best Loss: 0.003980352 in Epoch 33
Epoch 40
Epoch 40, Loss: 0.009127679, Improvement: -0.000090560, Best Loss: 0.003980352 in Epoch 33
Epoch 41
Epoch 41, Loss: 0.009033520, Improvement: -0.000094159, Best Loss: 0.003980352 in Epoch 33
Epoch 42
Epoch 42, Loss: 0.008946161, Improvement: -0.000087359, Best Loss: 0.003980352 in Epoch 33
Epoch 43
Epoch 43, Loss: 0.008856821, Improvement: -0.000089340, Best Loss: 0.003980352 in Epoch 33
Epoch 44
Epoch 44, Loss: 0.008764650, Improvement: -0.000092172, Best Loss: 0.003980352 in Epoch 33
Epoch 45
Epoch 45, Loss: 0.008667262, Improvement: -0.000097388, Best Loss: 0.003980352 in Epoch 33
Epoch 46
Epoch 46, Loss: 0.008603538, Improvement: -0.000063724, Best Loss: 0.003980352 in Epoch 33
Epoch 47
Epoch 47, Loss: 0.008486072, Improvement: -0.000117466, Best Loss: 0.003980352 in Epoch 33
Epoch 48
Epoch 48, Loss: 0.008388671, Improvement: -0.000097400, Best Loss: 0.003980352 in Epoch 33
Epoch 49
Epoch 49, Loss: 0.008292082, Improvement: -0.000096589, Best Loss: 0.003980352 in Epoch 33
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.008183539, Improvement: -0.000108543, Best Loss: 0.003980352 in Epoch 33
Epoch 51
Epoch 51, Loss: 0.008083113, Improvement: -0.000100426, Best Loss: 0.003980352 in Epoch 33
Epoch 52
Epoch 52, Loss: 0.007987167, Improvement: -0.000095946, Best Loss: 0.003980352 in Epoch 33
Epoch 53
Epoch 53, Loss: 0.007887310, Improvement: -0.000099857, Best Loss: 0.003980352 in Epoch 33
Epoch 54
Epoch 54, Loss: 0.007791540, Improvement: -0.000095770, Best Loss: 0.003980352 in Epoch 33
Epoch 55
Epoch 55, Loss: 0.007708517, Improvement: -0.000083023, Best Loss: 0.003980352 in Epoch 33
Epoch 56
Epoch 56, Loss: 0.007617309, Improvement: -0.000091208, Best Loss: 0.003980352 in Epoch 33
Epoch 57
Epoch 57, Loss: 0.007540724, Improvement: -0.000076585, Best Loss: 0.003980352 in Epoch 33
Epoch 58
A best model at epoch 58 has been saved with training error 0.003830474.
Epoch 58, Loss: 0.007492501, Improvement: -0.000048223, Best Loss: 0.003830474 in Epoch 58
Epoch 59
Epoch 59, Loss: 0.007507080, Improvement: 0.000014579, Best Loss: 0.003830474 in Epoch 58
Epoch 60
Epoch 60, Loss: 0.007379914, Improvement: -0.000127166, Best Loss: 0.003830474 in Epoch 58
Epoch 61
Epoch 61, Loss: 0.007336964, Improvement: -0.000042950, Best Loss: 0.003830474 in Epoch 58
Epoch 62
Epoch 62, Loss: 0.007421004, Improvement: 0.000084040, Best Loss: 0.003830474 in Epoch 58
Epoch 63
Epoch 63, Loss: 0.007414748, Improvement: -0.000006256, Best Loss: 0.003830474 in Epoch 58
Epoch 64
Epoch 64, Loss: 0.007487583, Improvement: 0.000072835, Best Loss: 0.003830474 in Epoch 58
Epoch 65
Epoch 65, Loss: 0.007829734, Improvement: 0.000342152, Best Loss: 0.003830474 in Epoch 58
Epoch 66
Epoch 66, Loss: 0.009122784, Improvement: 0.001293050, Best Loss: 0.003830474 in Epoch 58
Epoch 67
Epoch 67, Loss: 0.009869920, Improvement: 0.000747135, Best Loss: 0.003830474 in Epoch 58
Epoch 68
Epoch 68, Loss: 0.011592901, Improvement: 0.001722981, Best Loss: 0.003830474 in Epoch 58
Epoch 69
A best model at epoch 69 has been saved with training error 0.003642547.
Epoch 69, Loss: 0.008839987, Improvement: -0.002752914, Best Loss: 0.003642547 in Epoch 69
Epoch 70
Epoch 70, Loss: 0.007580919, Improvement: -0.001259068, Best Loss: 0.003642547 in Epoch 69
Epoch 71
Epoch 71, Loss: 0.007077029, Improvement: -0.000503890, Best Loss: 0.003642547 in Epoch 69
Epoch 72
Epoch 72, Loss: 0.006892363, Improvement: -0.000184666, Best Loss: 0.003642547 in Epoch 69
Epoch 73
Epoch 73, Loss: 0.006773679, Improvement: -0.000118684, Best Loss: 0.003642547 in Epoch 69
Epoch 74
Epoch 74, Loss: 0.006639685, Improvement: -0.000133995, Best Loss: 0.003642547 in Epoch 69
Epoch 75
Epoch 75, Loss: 0.006535929, Improvement: -0.000103755, Best Loss: 0.003642547 in Epoch 69
Epoch 76
A best model at epoch 76 has been saved with training error 0.003129643.
Epoch 76, Loss: 0.006477546, Improvement: -0.000058383, Best Loss: 0.003129643 in Epoch 76
Epoch 77
Epoch 77, Loss: 0.006421983, Improvement: -0.000055563, Best Loss: 0.003129643 in Epoch 76
Epoch 78
Epoch 78, Loss: 0.006355771, Improvement: -0.000066213, Best Loss: 0.003129643 in Epoch 76
Epoch 79
Epoch 79, Loss: 0.006314940, Improvement: -0.000040831, Best Loss: 0.003129643 in Epoch 76
Epoch 80
Epoch 80, Loss: 0.006259291, Improvement: -0.000055649, Best Loss: 0.003129643 in Epoch 76
Epoch 81
Epoch 81, Loss: 0.006218584, Improvement: -0.000040707, Best Loss: 0.003129643 in Epoch 76
Epoch 82
Epoch 82, Loss: 0.006159360, Improvement: -0.000059223, Best Loss: 0.003129643 in Epoch 76
Epoch 83
Epoch 83, Loss: 0.006115271, Improvement: -0.000044090, Best Loss: 0.003129643 in Epoch 76
Epoch 84
Epoch 84, Loss: 0.006089106, Improvement: -0.000026165, Best Loss: 0.003129643 in Epoch 76
Epoch 85
Epoch 85, Loss: 0.006067020, Improvement: -0.000022087, Best Loss: 0.003129643 in Epoch 76
Epoch 86
Epoch 86, Loss: 0.006058490, Improvement: -0.000008529, Best Loss: 0.003129643 in Epoch 76
Epoch 87
Epoch 87, Loss: 0.006063198, Improvement: 0.000004708, Best Loss: 0.003129643 in Epoch 76
Epoch 88
Epoch 88, Loss: 0.006147691, Improvement: 0.000084493, Best Loss: 0.003129643 in Epoch 76
Epoch 89
Epoch 89, Loss: 0.006233904, Improvement: 0.000086213, Best Loss: 0.003129643 in Epoch 76
Epoch 90
Epoch 90, Loss: 0.006427362, Improvement: 0.000193457, Best Loss: 0.003129643 in Epoch 76
Epoch 91
Epoch 91, Loss: 0.006753857, Improvement: 0.000326495, Best Loss: 0.003129643 in Epoch 76
Epoch 92
Epoch 92, Loss: 0.006607775, Improvement: -0.000146082, Best Loss: 0.003129643 in Epoch 76
Epoch 93
Epoch 93, Loss: 0.006821263, Improvement: 0.000213488, Best Loss: 0.003129643 in Epoch 76
Epoch 94
Epoch 94, Loss: 0.006795686, Improvement: -0.000025577, Best Loss: 0.003129643 in Epoch 76
Epoch 95
Epoch 95, Loss: 0.006983204, Improvement: 0.000187519, Best Loss: 0.003129643 in Epoch 76
Epoch 96
Epoch 96, Loss: 0.007761946, Improvement: 0.000778742, Best Loss: 0.003129643 in Epoch 76
Epoch 97
Epoch 97, Loss: 0.006610902, Improvement: -0.001151044, Best Loss: 0.003129643 in Epoch 76
Epoch 98
Epoch 98, Loss: 0.006068370, Improvement: -0.000542532, Best Loss: 0.003129643 in Epoch 76
Epoch 99
A best model at epoch 99 has been saved with training error 0.003086734.
Epoch 99, Loss: 0.005614891, Improvement: -0.000453479, Best Loss: 0.003086734 in Epoch 99
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.005389541, Improvement: -0.000225350, Best Loss: 0.003086734 in Epoch 99
Epoch 101
Epoch 101, Loss: 0.005247420, Improvement: -0.000142121, Best Loss: 0.003086734 in Epoch 99
Epoch 102
Epoch 102, Loss: 0.005178321, Improvement: -0.000069099, Best Loss: 0.003086734 in Epoch 99
Epoch 103
Epoch 103, Loss: 0.005102995, Improvement: -0.000075326, Best Loss: 0.003086734 in Epoch 99
Epoch 104
Epoch 104, Loss: 0.005024813, Improvement: -0.000078182, Best Loss: 0.003086734 in Epoch 99
Epoch 105
Epoch 105, Loss: 0.005000950, Improvement: -0.000023864, Best Loss: 0.003086734 in Epoch 99
Epoch 106
Epoch 106, Loss: 0.004982289, Improvement: -0.000018661, Best Loss: 0.003086734 in Epoch 99
Epoch 107
Epoch 107, Loss: 0.005067494, Improvement: 0.000085205, Best Loss: 0.003086734 in Epoch 99
Epoch 108
Epoch 108, Loss: 0.005388616, Improvement: 0.000321121, Best Loss: 0.003086734 in Epoch 99
Epoch 109
Epoch 109, Loss: 0.005341806, Improvement: -0.000046810, Best Loss: 0.003086734 in Epoch 99
Epoch 110
Epoch 110, Loss: 0.005250795, Improvement: -0.000091010, Best Loss: 0.003086734 in Epoch 99
Epoch 111
Epoch 111, Loss: 0.005028772, Improvement: -0.000222023, Best Loss: 0.003086734 in Epoch 99
Epoch 112
Epoch 112, Loss: 0.004823454, Improvement: -0.000205317, Best Loss: 0.003086734 in Epoch 99
Epoch 113
A best model at epoch 113 has been saved with training error 0.003021556.
A best model at epoch 113 has been saved with training error 0.002857047.
Epoch 113, Loss: 0.004670107, Improvement: -0.000153348, Best Loss: 0.002857047 in Epoch 113
Epoch 114
Epoch 114, Loss: 0.004611421, Improvement: -0.000058686, Best Loss: 0.002857047 in Epoch 113
Epoch 115
Epoch 115, Loss: 0.004295858, Improvement: -0.000315563, Best Loss: 0.002857047 in Epoch 113
Epoch 116
Epoch 116, Loss: 0.004139930, Improvement: -0.000155928, Best Loss: 0.002857047 in Epoch 113
Epoch 117
A best model at epoch 117 has been saved with training error 0.002710388.
Epoch 117, Loss: 0.004025455, Improvement: -0.000114476, Best Loss: 0.002710388 in Epoch 117
Epoch 118
Epoch 118, Loss: 0.004005652, Improvement: -0.000019803, Best Loss: 0.002710388 in Epoch 117
Epoch 119
A best model at epoch 119 has been saved with training error 0.002530172.
Epoch 119, Loss: 0.003961360, Improvement: -0.000044292, Best Loss: 0.002530172 in Epoch 119
Epoch 120
A best model at epoch 120 has been saved with training error 0.002410148.
A best model at epoch 120 has been saved with training error 0.002285537.
A best model at epoch 120 has been saved with training error 0.002235325.
Epoch 120, Loss: 0.003877987, Improvement: -0.000083373, Best Loss: 0.002235325 in Epoch 120
Epoch 121
Epoch 121, Loss: 0.003940134, Improvement: 0.000062147, Best Loss: 0.002235325 in Epoch 120
Epoch 122
Epoch 122, Loss: 0.003930438, Improvement: -0.000009696, Best Loss: 0.002235325 in Epoch 120
Epoch 123
Epoch 123, Loss: 0.003762716, Improvement: -0.000167722, Best Loss: 0.002235325 in Epoch 120
Epoch 124
Epoch 124, Loss: 0.003892895, Improvement: 0.000130179, Best Loss: 0.002235325 in Epoch 120
Epoch 125
Epoch 125, Loss: 0.004200426, Improvement: 0.000307530, Best Loss: 0.002235325 in Epoch 120
Epoch 126
Epoch 126, Loss: 0.004304959, Improvement: 0.000104534, Best Loss: 0.002235325 in Epoch 120
Epoch 127
Epoch 127, Loss: 0.004118333, Improvement: -0.000186626, Best Loss: 0.002235325 in Epoch 120
Epoch 128
Epoch 128, Loss: 0.004144020, Improvement: 0.000025687, Best Loss: 0.002235325 in Epoch 120
Epoch 129
A best model at epoch 129 has been saved with training error 0.002143860.
Epoch 129, Loss: 0.003841164, Improvement: -0.000302856, Best Loss: 0.002143860 in Epoch 129
Epoch 130
Epoch 130, Loss: 0.003679519, Improvement: -0.000161645, Best Loss: 0.002143860 in Epoch 129
Epoch 131
Epoch 131, Loss: 0.003502331, Improvement: -0.000177188, Best Loss: 0.002143860 in Epoch 129
Epoch 132
Epoch 132, Loss: 0.003418329, Improvement: -0.000084001, Best Loss: 0.002143860 in Epoch 129
Epoch 133
Epoch 133, Loss: 0.003488829, Improvement: 0.000070499, Best Loss: 0.002143860 in Epoch 129
Epoch 134
A best model at epoch 134 has been saved with training error 0.001857177.
Epoch 134, Loss: 0.003332889, Improvement: -0.000155940, Best Loss: 0.001857177 in Epoch 134
Epoch 135
Epoch 135, Loss: 0.003236040, Improvement: -0.000096849, Best Loss: 0.001857177 in Epoch 134
Epoch 136
Epoch 136, Loss: 0.003056345, Improvement: -0.000179694, Best Loss: 0.001857177 in Epoch 134
Epoch 137
Epoch 137, Loss: 0.003023881, Improvement: -0.000032465, Best Loss: 0.001857177 in Epoch 134
Epoch 138
Epoch 138, Loss: 0.002970310, Improvement: -0.000053571, Best Loss: 0.001857177 in Epoch 134
Epoch 139
Epoch 139, Loss: 0.002912969, Improvement: -0.000057341, Best Loss: 0.001857177 in Epoch 134
Epoch 140
Epoch 140, Loss: 0.002866379, Improvement: -0.000046590, Best Loss: 0.001857177 in Epoch 134
Epoch 141
Epoch 141, Loss: 0.002835297, Improvement: -0.000031081, Best Loss: 0.001857177 in Epoch 134
Epoch 142
A best model at epoch 142 has been saved with training error 0.001769878.
Epoch 142, Loss: 0.002772892, Improvement: -0.000062405, Best Loss: 0.001769878 in Epoch 142
Epoch 143
Epoch 143, Loss: 0.002845666, Improvement: 0.000072774, Best Loss: 0.001769878 in Epoch 142
Epoch 144
A best model at epoch 144 has been saved with training error 0.001707603.
Epoch 144, Loss: 0.002934744, Improvement: 0.000089078, Best Loss: 0.001707603 in Epoch 144
Epoch 145
A best model at epoch 145 has been saved with training error 0.001525390.
Epoch 145, Loss: 0.002828275, Improvement: -0.000106469, Best Loss: 0.001525390 in Epoch 145
Epoch 146
Epoch 146, Loss: 0.002672068, Improvement: -0.000156207, Best Loss: 0.001525390 in Epoch 145
Epoch 147
Epoch 147, Loss: 0.002826863, Improvement: 0.000154796, Best Loss: 0.001525390 in Epoch 145
Epoch 148
Epoch 148, Loss: 0.003405343, Improvement: 0.000578480, Best Loss: 0.001525390 in Epoch 145
Epoch 149
Epoch 149, Loss: 0.003424225, Improvement: 0.000018881, Best Loss: 0.001525390 in Epoch 145
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.003104718, Improvement: -0.000319506, Best Loss: 0.001525390 in Epoch 145
Epoch 151
Epoch 151, Loss: 0.003553774, Improvement: 0.000449056, Best Loss: 0.001525390 in Epoch 145
Epoch 152
Epoch 152, Loss: 0.003883214, Improvement: 0.000329439, Best Loss: 0.001525390 in Epoch 145
Epoch 153
Epoch 153, Loss: 0.003811358, Improvement: -0.000071856, Best Loss: 0.001525390 in Epoch 145
Epoch 154
Epoch 154, Loss: 0.003298192, Improvement: -0.000513166, Best Loss: 0.001525390 in Epoch 145
Epoch 155
A best model at epoch 155 has been saved with training error 0.001411897.
Epoch 155, Loss: 0.002957699, Improvement: -0.000340493, Best Loss: 0.001411897 in Epoch 155
Epoch 156
Epoch 156, Loss: 0.002746177, Improvement: -0.000211522, Best Loss: 0.001411897 in Epoch 155
Epoch 157
A best model at epoch 157 has been saved with training error 0.001261423.
Epoch 157, Loss: 0.002569094, Improvement: -0.000177083, Best Loss: 0.001261423 in Epoch 157
Epoch 158
Epoch 158, Loss: 0.002393402, Improvement: -0.000175692, Best Loss: 0.001261423 in Epoch 157
Epoch 159
Epoch 159, Loss: 0.002282270, Improvement: -0.000111132, Best Loss: 0.001261423 in Epoch 157
Epoch 160
Epoch 160, Loss: 0.002196048, Improvement: -0.000086222, Best Loss: 0.001261423 in Epoch 157
Epoch 161
Epoch 161, Loss: 0.002142182, Improvement: -0.000053866, Best Loss: 0.001261423 in Epoch 157
Epoch 162
Epoch 162, Loss: 0.002063043, Improvement: -0.000079139, Best Loss: 0.001261423 in Epoch 157
Epoch 163
Epoch 163, Loss: 0.002086132, Improvement: 0.000023090, Best Loss: 0.001261423 in Epoch 157
Epoch 164
A best model at epoch 164 has been saved with training error 0.001141576.
Epoch 164, Loss: 0.001999967, Improvement: -0.000086166, Best Loss: 0.001141576 in Epoch 164
Epoch 165
A best model at epoch 165 has been saved with training error 0.001059416.
Epoch 165, Loss: 0.001969384, Improvement: -0.000030582, Best Loss: 0.001059416 in Epoch 165
Epoch 166
Epoch 166, Loss: 0.001961010, Improvement: -0.000008374, Best Loss: 0.001059416 in Epoch 165
Epoch 167
Epoch 167, Loss: 0.001955022, Improvement: -0.000005989, Best Loss: 0.001059416 in Epoch 165
Epoch 168
Epoch 168, Loss: 0.001934998, Improvement: -0.000020024, Best Loss: 0.001059416 in Epoch 165
Epoch 169
Epoch 169, Loss: 0.001915046, Improvement: -0.000019952, Best Loss: 0.001059416 in Epoch 165
Epoch 170
A best model at epoch 170 has been saved with training error 0.001047314.
A best model at epoch 170 has been saved with training error 0.001036219.
Epoch 170, Loss: 0.001893039, Improvement: -0.000022007, Best Loss: 0.001036219 in Epoch 170
Epoch 171
Epoch 171, Loss: 0.001839508, Improvement: -0.000053531, Best Loss: 0.001036219 in Epoch 170
Epoch 172
Epoch 172, Loss: 0.001836925, Improvement: -0.000002582, Best Loss: 0.001036219 in Epoch 170
Epoch 173
Epoch 173, Loss: 0.001800957, Improvement: -0.000035968, Best Loss: 0.001036219 in Epoch 170
Epoch 174
Epoch 174, Loss: 0.001861630, Improvement: 0.000060673, Best Loss: 0.001036219 in Epoch 170
Epoch 175
Epoch 175, Loss: 0.001860157, Improvement: -0.000001473, Best Loss: 0.001036219 in Epoch 170
Epoch 176
Epoch 176, Loss: 0.001852335, Improvement: -0.000007823, Best Loss: 0.001036219 in Epoch 170
Epoch 177
Epoch 177, Loss: 0.001995209, Improvement: 0.000142874, Best Loss: 0.001036219 in Epoch 170
Epoch 178
Epoch 178, Loss: 0.001939729, Improvement: -0.000055479, Best Loss: 0.001036219 in Epoch 170
Epoch 179
Epoch 179, Loss: 0.002057624, Improvement: 0.000117895, Best Loss: 0.001036219 in Epoch 170
Epoch 180
Epoch 180, Loss: 0.002069868, Improvement: 0.000012244, Best Loss: 0.001036219 in Epoch 170
Epoch 181
Epoch 181, Loss: 0.002252789, Improvement: 0.000182920, Best Loss: 0.001036219 in Epoch 170
Epoch 182
Epoch 182, Loss: 0.002296789, Improvement: 0.000044000, Best Loss: 0.001036219 in Epoch 170
Epoch 183
Epoch 183, Loss: 0.002349460, Improvement: 0.000052671, Best Loss: 0.001036219 in Epoch 170
Epoch 184
Epoch 184, Loss: 0.002210353, Improvement: -0.000139106, Best Loss: 0.001036219 in Epoch 170
Epoch 185
Epoch 185, Loss: 0.001897607, Improvement: -0.000312746, Best Loss: 0.001036219 in Epoch 170
Epoch 186
Epoch 186, Loss: 0.002415290, Improvement: 0.000517682, Best Loss: 0.001036219 in Epoch 170
Epoch 187
Epoch 187, Loss: 0.003095176, Improvement: 0.000679887, Best Loss: 0.001036219 in Epoch 170
Epoch 188
Epoch 188, Loss: 0.002219592, Improvement: -0.000875584, Best Loss: 0.001036219 in Epoch 170
Epoch 189
A best model at epoch 189 has been saved with training error 0.001004324.
Epoch 189, Loss: 0.002032296, Improvement: -0.000187296, Best Loss: 0.001004324 in Epoch 189
Epoch 190
Epoch 190, Loss: 0.001928826, Improvement: -0.000103470, Best Loss: 0.001004324 in Epoch 189
Epoch 191
Epoch 191, Loss: 0.001760966, Improvement: -0.000167860, Best Loss: 0.001004324 in Epoch 189
Epoch 192
Epoch 192, Loss: 0.001859714, Improvement: 0.000098748, Best Loss: 0.001004324 in Epoch 189
Epoch 193
A best model at epoch 193 has been saved with training error 0.001002977.
A best model at epoch 193 has been saved with training error 0.000896794.
Epoch 193, Loss: 0.001753211, Improvement: -0.000106503, Best Loss: 0.000896794 in Epoch 193
Epoch 194
A best model at epoch 194 has been saved with training error 0.000869249.
Epoch 194, Loss: 0.001631418, Improvement: -0.000121793, Best Loss: 0.000869249 in Epoch 194
Epoch 195
Epoch 195, Loss: 0.001549498, Improvement: -0.000081920, Best Loss: 0.000869249 in Epoch 194
Epoch 196
Epoch 196, Loss: 0.001498212, Improvement: -0.000051286, Best Loss: 0.000869249 in Epoch 194
Epoch 197
Epoch 197, Loss: 0.001470486, Improvement: -0.000027726, Best Loss: 0.000869249 in Epoch 194
Epoch 198
Epoch 198, Loss: 0.001445837, Improvement: -0.000024648, Best Loss: 0.000869249 in Epoch 194
Epoch 199
Epoch 199, Loss: 0.001440099, Improvement: -0.000005738, Best Loss: 0.000869249 in Epoch 194
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.001425706, Improvement: -0.000014394, Best Loss: 0.000869249 in Epoch 194
Epoch 201
A best model at epoch 201 has been saved with training error 0.000666218.
Epoch 201, Loss: 0.001410713, Improvement: -0.000014993, Best Loss: 0.000666218 in Epoch 201
Epoch 202
Epoch 202, Loss: 0.001397658, Improvement: -0.000013054, Best Loss: 0.000666218 in Epoch 201
Epoch 203
Epoch 203, Loss: 0.001380152, Improvement: -0.000017506, Best Loss: 0.000666218 in Epoch 201
Epoch 204
Epoch 204, Loss: 0.001393730, Improvement: 0.000013578, Best Loss: 0.000666218 in Epoch 201
Epoch 205
Epoch 205, Loss: 0.001384098, Improvement: -0.000009633, Best Loss: 0.000666218 in Epoch 201
Epoch 206
Epoch 206, Loss: 0.001366573, Improvement: -0.000017525, Best Loss: 0.000666218 in Epoch 201
Epoch 207
Epoch 207, Loss: 0.001355812, Improvement: -0.000010761, Best Loss: 0.000666218 in Epoch 201
Epoch 208
Epoch 208, Loss: 0.001406873, Improvement: 0.000051060, Best Loss: 0.000666218 in Epoch 201
Epoch 209
Epoch 209, Loss: 0.001418765, Improvement: 0.000011892, Best Loss: 0.000666218 in Epoch 201
Epoch 210
Epoch 210, Loss: 0.001385097, Improvement: -0.000033668, Best Loss: 0.000666218 in Epoch 201
Epoch 211
Epoch 211, Loss: 0.001343492, Improvement: -0.000041605, Best Loss: 0.000666218 in Epoch 201
Epoch 212
Epoch 212, Loss: 0.001341532, Improvement: -0.000001960, Best Loss: 0.000666218 in Epoch 201
Epoch 213
Epoch 213, Loss: 0.001340736, Improvement: -0.000000796, Best Loss: 0.000666218 in Epoch 201
Epoch 214
Epoch 214, Loss: 0.001360925, Improvement: 0.000020188, Best Loss: 0.000666218 in Epoch 201
Epoch 215
Epoch 215, Loss: 0.001375842, Improvement: 0.000014917, Best Loss: 0.000666218 in Epoch 201
Epoch 216
Epoch 216, Loss: 0.001569768, Improvement: 0.000193927, Best Loss: 0.000666218 in Epoch 201
Epoch 217
Epoch 217, Loss: 0.001452127, Improvement: -0.000117642, Best Loss: 0.000666218 in Epoch 201
Epoch 218
Epoch 218, Loss: 0.001470059, Improvement: 0.000017933, Best Loss: 0.000666218 in Epoch 201
Epoch 219
Epoch 219, Loss: 0.001425909, Improvement: -0.000044151, Best Loss: 0.000666218 in Epoch 201
Epoch 220
Epoch 220, Loss: 0.001448409, Improvement: 0.000022501, Best Loss: 0.000666218 in Epoch 201
Epoch 221
Epoch 221, Loss: 0.001503107, Improvement: 0.000054698, Best Loss: 0.000666218 in Epoch 201
Epoch 222
Epoch 222, Loss: 0.001372229, Improvement: -0.000130878, Best Loss: 0.000666218 in Epoch 201
Epoch 223
Epoch 223, Loss: 0.001301523, Improvement: -0.000070706, Best Loss: 0.000666218 in Epoch 201
Epoch 224
Epoch 224, Loss: 0.001359637, Improvement: 0.000058113, Best Loss: 0.000666218 in Epoch 201
Epoch 225
Epoch 225, Loss: 0.001364706, Improvement: 0.000005070, Best Loss: 0.000666218 in Epoch 201
Epoch 226
Epoch 226, Loss: 0.001340329, Improvement: -0.000024377, Best Loss: 0.000666218 in Epoch 201
Epoch 227
Epoch 227, Loss: 0.001266281, Improvement: -0.000074048, Best Loss: 0.000666218 in Epoch 201
Epoch 228
Epoch 228, Loss: 0.001219600, Improvement: -0.000046681, Best Loss: 0.000666218 in Epoch 201
Epoch 229
Epoch 229, Loss: 0.001195293, Improvement: -0.000024308, Best Loss: 0.000666218 in Epoch 201
Epoch 230
A best model at epoch 230 has been saved with training error 0.000579072.
Epoch 230, Loss: 0.001203074, Improvement: 0.000007782, Best Loss: 0.000579072 in Epoch 230
Epoch 231
Epoch 231, Loss: 0.001160421, Improvement: -0.000042653, Best Loss: 0.000579072 in Epoch 230
Epoch 232
Epoch 232, Loss: 0.001114579, Improvement: -0.000045842, Best Loss: 0.000579072 in Epoch 230
Epoch 233
Epoch 233, Loss: 0.001096315, Improvement: -0.000018264, Best Loss: 0.000579072 in Epoch 230
Epoch 234
A best model at epoch 234 has been saved with training error 0.000557648.
Epoch 234, Loss: 0.001105248, Improvement: 0.000008933, Best Loss: 0.000557648 in Epoch 234
Epoch 235
Epoch 235, Loss: 0.001200077, Improvement: 0.000094829, Best Loss: 0.000557648 in Epoch 234
Epoch 236
Epoch 236, Loss: 0.001300658, Improvement: 0.000100582, Best Loss: 0.000557648 in Epoch 234
Epoch 237
Epoch 237, Loss: 0.001403246, Improvement: 0.000102587, Best Loss: 0.000557648 in Epoch 234
Epoch 238
Epoch 238, Loss: 0.001398644, Improvement: -0.000004602, Best Loss: 0.000557648 in Epoch 234
Epoch 239
Epoch 239, Loss: 0.001228889, Improvement: -0.000169754, Best Loss: 0.000557648 in Epoch 234
Epoch 240
Epoch 240, Loss: 0.001135514, Improvement: -0.000093375, Best Loss: 0.000557648 in Epoch 234
Epoch 241
Epoch 241, Loss: 0.001108555, Improvement: -0.000026959, Best Loss: 0.000557648 in Epoch 234
Epoch 242
A best model at epoch 242 has been saved with training error 0.000475587.
Epoch 242, Loss: 0.001344015, Improvement: 0.000235460, Best Loss: 0.000475587 in Epoch 242
Epoch 243
Epoch 243, Loss: 0.001856992, Improvement: 0.000512977, Best Loss: 0.000475587 in Epoch 242
Epoch 244
Epoch 244, Loss: 0.001663709, Improvement: -0.000193284, Best Loss: 0.000475587 in Epoch 242
Epoch 245
Epoch 245, Loss: 0.001274215, Improvement: -0.000389494, Best Loss: 0.000475587 in Epoch 242
Epoch 246
Epoch 246, Loss: 0.001327537, Improvement: 0.000053322, Best Loss: 0.000475587 in Epoch 242
Epoch 247
Epoch 247, Loss: 0.001443811, Improvement: 0.000116274, Best Loss: 0.000475587 in Epoch 242
Epoch 248
Epoch 248, Loss: 0.001130803, Improvement: -0.000313007, Best Loss: 0.000475587 in Epoch 242
Epoch 249
Epoch 249, Loss: 0.001043948, Improvement: -0.000086855, Best Loss: 0.000475587 in Epoch 242
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.000982270, Improvement: -0.000061678, Best Loss: 0.000475587 in Epoch 242
Epoch 251
Epoch 251, Loss: 0.000964292, Improvement: -0.000017978, Best Loss: 0.000475587 in Epoch 242
Epoch 252
Epoch 252, Loss: 0.000927724, Improvement: -0.000036568, Best Loss: 0.000475587 in Epoch 242
Epoch 253
Epoch 253, Loss: 0.000905974, Improvement: -0.000021749, Best Loss: 0.000475587 in Epoch 242
Epoch 254
Epoch 254, Loss: 0.000898493, Improvement: -0.000007481, Best Loss: 0.000475587 in Epoch 242
Epoch 255
Epoch 255, Loss: 0.000892943, Improvement: -0.000005550, Best Loss: 0.000475587 in Epoch 242
Epoch 256
Epoch 256, Loss: 0.000887558, Improvement: -0.000005385, Best Loss: 0.000475587 in Epoch 242
Epoch 257
A best model at epoch 257 has been saved with training error 0.000428872.
Epoch 257, Loss: 0.000871323, Improvement: -0.000016235, Best Loss: 0.000428872 in Epoch 257
Epoch 258
Epoch 258, Loss: 0.000877066, Improvement: 0.000005742, Best Loss: 0.000428872 in Epoch 257
Epoch 259
Epoch 259, Loss: 0.000860117, Improvement: -0.000016949, Best Loss: 0.000428872 in Epoch 257
Epoch 260
Epoch 260, Loss: 0.000848820, Improvement: -0.000011297, Best Loss: 0.000428872 in Epoch 257
Epoch 261
Epoch 261, Loss: 0.000832767, Improvement: -0.000016053, Best Loss: 0.000428872 in Epoch 257
Epoch 262
Epoch 262, Loss: 0.000844489, Improvement: 0.000011722, Best Loss: 0.000428872 in Epoch 257
Epoch 263
Epoch 263, Loss: 0.000837824, Improvement: -0.000006665, Best Loss: 0.000428872 in Epoch 257
Epoch 264
Epoch 264, Loss: 0.000895441, Improvement: 0.000057617, Best Loss: 0.000428872 in Epoch 257
Epoch 265
Epoch 265, Loss: 0.001034490, Improvement: 0.000139049, Best Loss: 0.000428872 in Epoch 257
Epoch 266
Epoch 266, Loss: 0.001118699, Improvement: 0.000084209, Best Loss: 0.000428872 in Epoch 257
Epoch 267
Epoch 267, Loss: 0.001381322, Improvement: 0.000262624, Best Loss: 0.000428872 in Epoch 257
Epoch 268
Epoch 268, Loss: 0.001589719, Improvement: 0.000208397, Best Loss: 0.000428872 in Epoch 257
Epoch 269
Epoch 269, Loss: 0.001355451, Improvement: -0.000234269, Best Loss: 0.000428872 in Epoch 257
Epoch 270
Epoch 270, Loss: 0.001091712, Improvement: -0.000263739, Best Loss: 0.000428872 in Epoch 257
Epoch 271
Epoch 271, Loss: 0.000915986, Improvement: -0.000175726, Best Loss: 0.000428872 in Epoch 257
Epoch 272
Epoch 272, Loss: 0.000867798, Improvement: -0.000048189, Best Loss: 0.000428872 in Epoch 257
Epoch 273
Epoch 273, Loss: 0.001011364, Improvement: 0.000143567, Best Loss: 0.000428872 in Epoch 257
Epoch 274
Epoch 274, Loss: 0.000979917, Improvement: -0.000031447, Best Loss: 0.000428872 in Epoch 257
Epoch 275
Epoch 275, Loss: 0.000925770, Improvement: -0.000054147, Best Loss: 0.000428872 in Epoch 257
Epoch 276
Epoch 276, Loss: 0.001097025, Improvement: 0.000171256, Best Loss: 0.000428872 in Epoch 257
Epoch 277
Epoch 277, Loss: 0.001012133, Improvement: -0.000084893, Best Loss: 0.000428872 in Epoch 257
Epoch 278
A best model at epoch 278 has been saved with training error 0.000372869.
Epoch 278, Loss: 0.000966299, Improvement: -0.000045834, Best Loss: 0.000372869 in Epoch 278
Epoch 279
Epoch 279, Loss: 0.000865457, Improvement: -0.000100841, Best Loss: 0.000372869 in Epoch 278
Epoch 280
Epoch 280, Loss: 0.000777333, Improvement: -0.000088124, Best Loss: 0.000372869 in Epoch 278
Epoch 281
Epoch 281, Loss: 0.000761817, Improvement: -0.000015517, Best Loss: 0.000372869 in Epoch 278
Epoch 282
Epoch 282, Loss: 0.000783161, Improvement: 0.000021344, Best Loss: 0.000372869 in Epoch 278
Epoch 283
Epoch 283, Loss: 0.000769076, Improvement: -0.000014085, Best Loss: 0.000372869 in Epoch 278
Epoch 284
Epoch 284, Loss: 0.000725402, Improvement: -0.000043674, Best Loss: 0.000372869 in Epoch 278
Epoch 285
Epoch 285, Loss: 0.000705351, Improvement: -0.000020051, Best Loss: 0.000372869 in Epoch 278
Epoch 286
A best model at epoch 286 has been saved with training error 0.000370878.
Epoch 286, Loss: 0.000688272, Improvement: -0.000017079, Best Loss: 0.000370878 in Epoch 286
Epoch 287
Epoch 287, Loss: 0.000684440, Improvement: -0.000003831, Best Loss: 0.000370878 in Epoch 286
Epoch 288
Epoch 288, Loss: 0.000679005, Improvement: -0.000005436, Best Loss: 0.000370878 in Epoch 286
Epoch 289
Epoch 289, Loss: 0.000713657, Improvement: 0.000034653, Best Loss: 0.000370878 in Epoch 286
Epoch 290
Epoch 290, Loss: 0.000728512, Improvement: 0.000014855, Best Loss: 0.000370878 in Epoch 286
Epoch 291
Epoch 291, Loss: 0.000855142, Improvement: 0.000126630, Best Loss: 0.000370878 in Epoch 286
Epoch 292
Epoch 292, Loss: 0.000801056, Improvement: -0.000054086, Best Loss: 0.000370878 in Epoch 286
Epoch 293
Epoch 293, Loss: 0.000789279, Improvement: -0.000011777, Best Loss: 0.000370878 in Epoch 286
Epoch 294
Epoch 294, Loss: 0.000820716, Improvement: 0.000031437, Best Loss: 0.000370878 in Epoch 286
Epoch 295
Epoch 295, Loss: 0.000839619, Improvement: 0.000018903, Best Loss: 0.000370878 in Epoch 286
Epoch 296
Epoch 296, Loss: 0.000784435, Improvement: -0.000055184, Best Loss: 0.000370878 in Epoch 286
Epoch 297
Epoch 297, Loss: 0.000682088, Improvement: -0.000102347, Best Loss: 0.000370878 in Epoch 286
Epoch 298
Epoch 298, Loss: 0.000658476, Improvement: -0.000023612, Best Loss: 0.000370878 in Epoch 286
Epoch 299
Epoch 299, Loss: 0.000621648, Improvement: -0.000036828, Best Loss: 0.000370878 in Epoch 286
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.000609350, Improvement: -0.000012297, Best Loss: 0.000370878 in Epoch 286
Epoch 301
Epoch 301, Loss: 0.000755904, Improvement: 0.000146554, Best Loss: 0.000370878 in Epoch 286
Epoch 302
Epoch 302, Loss: 0.000696166, Improvement: -0.000059738, Best Loss: 0.000370878 in Epoch 286
Epoch 303
Epoch 303, Loss: 0.000689037, Improvement: -0.000007129, Best Loss: 0.000370878 in Epoch 286
Epoch 304
Epoch 304, Loss: 0.000650585, Improvement: -0.000038452, Best Loss: 0.000370878 in Epoch 286
Epoch 305
Epoch 305, Loss: 0.000622347, Improvement: -0.000028238, Best Loss: 0.000370878 in Epoch 286
Epoch 306
Epoch 306, Loss: 0.000623048, Improvement: 0.000000701, Best Loss: 0.000370878 in Epoch 286
Epoch 307
Epoch 307, Loss: 0.000607241, Improvement: -0.000015807, Best Loss: 0.000370878 in Epoch 286
Epoch 308
Epoch 308, Loss: 0.000574133, Improvement: -0.000033109, Best Loss: 0.000370878 in Epoch 286
Epoch 309
Epoch 309, Loss: 0.000613362, Improvement: 0.000039229, Best Loss: 0.000370878 in Epoch 286
Epoch 310
Epoch 310, Loss: 0.000841446, Improvement: 0.000228084, Best Loss: 0.000370878 in Epoch 286
Epoch 311
Epoch 311, Loss: 0.000847592, Improvement: 0.000006146, Best Loss: 0.000370878 in Epoch 286
Epoch 312
Epoch 312, Loss: 0.000751691, Improvement: -0.000095901, Best Loss: 0.000370878 in Epoch 286
Epoch 313
Epoch 313, Loss: 0.000720345, Improvement: -0.000031345, Best Loss: 0.000370878 in Epoch 286
Epoch 314
Epoch 314, Loss: 0.000799844, Improvement: 0.000079499, Best Loss: 0.000370878 in Epoch 286
Epoch 315
Epoch 315, Loss: 0.000697918, Improvement: -0.000101926, Best Loss: 0.000370878 in Epoch 286
Epoch 316
Epoch 316, Loss: 0.000732984, Improvement: 0.000035066, Best Loss: 0.000370878 in Epoch 286
Epoch 317
Epoch 317, Loss: 0.000769259, Improvement: 0.000036276, Best Loss: 0.000370878 in Epoch 286
Epoch 318
Epoch 318, Loss: 0.000743066, Improvement: -0.000026193, Best Loss: 0.000370878 in Epoch 286
Epoch 319
Epoch 319, Loss: 0.000649842, Improvement: -0.000093225, Best Loss: 0.000370878 in Epoch 286
Epoch 320
Epoch 320, Loss: 0.000677008, Improvement: 0.000027166, Best Loss: 0.000370878 in Epoch 286
Epoch 321
A best model at epoch 321 has been saved with training error 0.000320737.
Epoch 321, Loss: 0.000683438, Improvement: 0.000006430, Best Loss: 0.000320737 in Epoch 321
Epoch 322
Epoch 322, Loss: 0.000536004, Improvement: -0.000147434, Best Loss: 0.000320737 in Epoch 321
Epoch 323
Epoch 323, Loss: 0.000493520, Improvement: -0.000042484, Best Loss: 0.000320737 in Epoch 321
Epoch 324
A best model at epoch 324 has been saved with training error 0.000307680.
Epoch 324, Loss: 0.000481366, Improvement: -0.000012154, Best Loss: 0.000307680 in Epoch 324
Epoch 325
Epoch 325, Loss: 0.000481163, Improvement: -0.000000204, Best Loss: 0.000307680 in Epoch 324
Epoch 326
Epoch 326, Loss: 0.000478577, Improvement: -0.000002586, Best Loss: 0.000307680 in Epoch 324
Epoch 327
Epoch 327, Loss: 0.000487419, Improvement: 0.000008842, Best Loss: 0.000307680 in Epoch 324
Epoch 328
A best model at epoch 328 has been saved with training error 0.000273217.
Epoch 328, Loss: 0.000467493, Improvement: -0.000019926, Best Loss: 0.000273217 in Epoch 328
Epoch 329
Epoch 329, Loss: 0.000458178, Improvement: -0.000009315, Best Loss: 0.000273217 in Epoch 328
Epoch 330
Epoch 330, Loss: 0.000504079, Improvement: 0.000045901, Best Loss: 0.000273217 in Epoch 328
Epoch 331
Epoch 331, Loss: 0.000476446, Improvement: -0.000027633, Best Loss: 0.000273217 in Epoch 328
Epoch 332
Epoch 332, Loss: 0.000613857, Improvement: 0.000137411, Best Loss: 0.000273217 in Epoch 328
Epoch 333
Epoch 333, Loss: 0.000631417, Improvement: 0.000017560, Best Loss: 0.000273217 in Epoch 328
Epoch 334
Epoch 334, Loss: 0.000500930, Improvement: -0.000130487, Best Loss: 0.000273217 in Epoch 328
Epoch 335
Epoch 335, Loss: 0.000480760, Improvement: -0.000020170, Best Loss: 0.000273217 in Epoch 328
Epoch 336
A best model at epoch 336 has been saved with training error 0.000266599.
Epoch 336, Loss: 0.000477403, Improvement: -0.000003358, Best Loss: 0.000266599 in Epoch 336
Epoch 337
A best model at epoch 337 has been saved with training error 0.000253683.
Epoch 337, Loss: 0.000564606, Improvement: 0.000087203, Best Loss: 0.000253683 in Epoch 337
Epoch 338
Epoch 338, Loss: 0.000508062, Improvement: -0.000056544, Best Loss: 0.000253683 in Epoch 337
Epoch 339
Epoch 339, Loss: 0.000636838, Improvement: 0.000128776, Best Loss: 0.000253683 in Epoch 337
Epoch 340
Epoch 340, Loss: 0.000537155, Improvement: -0.000099682, Best Loss: 0.000253683 in Epoch 337
Epoch 341
Epoch 341, Loss: 0.000462152, Improvement: -0.000075004, Best Loss: 0.000253683 in Epoch 337
Epoch 342
Epoch 342, Loss: 0.000454068, Improvement: -0.000008084, Best Loss: 0.000253683 in Epoch 337
Epoch 343
Epoch 343, Loss: 0.000435887, Improvement: -0.000018181, Best Loss: 0.000253683 in Epoch 337
Epoch 344
Epoch 344, Loss: 0.000406877, Improvement: -0.000029009, Best Loss: 0.000253683 in Epoch 337
Epoch 345
Epoch 345, Loss: 0.000397959, Improvement: -0.000008918, Best Loss: 0.000253683 in Epoch 337
Epoch 346
A best model at epoch 346 has been saved with training error 0.000225291.
Epoch 346, Loss: 0.000389449, Improvement: -0.000008511, Best Loss: 0.000225291 in Epoch 346
Epoch 347
Epoch 347, Loss: 0.000368979, Improvement: -0.000020469, Best Loss: 0.000225291 in Epoch 346
Epoch 348
Epoch 348, Loss: 0.000409198, Improvement: 0.000040218, Best Loss: 0.000225291 in Epoch 346
Epoch 349
Epoch 349, Loss: 0.000414350, Improvement: 0.000005152, Best Loss: 0.000225291 in Epoch 346
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.000451090, Improvement: 0.000036741, Best Loss: 0.000225291 in Epoch 346
Epoch 351
Epoch 351, Loss: 0.000568313, Improvement: 0.000117222, Best Loss: 0.000225291 in Epoch 346
Epoch 352
Epoch 352, Loss: 0.000646306, Improvement: 0.000077994, Best Loss: 0.000225291 in Epoch 346
Epoch 353
Epoch 353, Loss: 0.000654519, Improvement: 0.000008212, Best Loss: 0.000225291 in Epoch 346
Epoch 354
Epoch 354, Loss: 0.000728923, Improvement: 0.000074404, Best Loss: 0.000225291 in Epoch 346
Epoch 355
Epoch 355, Loss: 0.000612987, Improvement: -0.000115936, Best Loss: 0.000225291 in Epoch 346
Epoch 356
Epoch 356, Loss: 0.000477000, Improvement: -0.000135987, Best Loss: 0.000225291 in Epoch 346
Epoch 357
Epoch 357, Loss: 0.000395309, Improvement: -0.000081691, Best Loss: 0.000225291 in Epoch 346
Epoch 358
Epoch 358, Loss: 0.000475410, Improvement: 0.000080100, Best Loss: 0.000225291 in Epoch 346
Epoch 359
Epoch 359, Loss: 0.000419241, Improvement: -0.000056169, Best Loss: 0.000225291 in Epoch 346
Epoch 360
Epoch 360, Loss: 0.000364954, Improvement: -0.000054287, Best Loss: 0.000225291 in Epoch 346
Epoch 361
A best model at epoch 361 has been saved with training error 0.000219962.
A best model at epoch 361 has been saved with training error 0.000215599.
A best model at epoch 361 has been saved with training error 0.000209059.
Epoch 361, Loss: 0.000324492, Improvement: -0.000040462, Best Loss: 0.000209059 in Epoch 361
Epoch 362
Epoch 362, Loss: 0.000315022, Improvement: -0.000009470, Best Loss: 0.000209059 in Epoch 361
Epoch 363
Epoch 363, Loss: 0.000303893, Improvement: -0.000011129, Best Loss: 0.000209059 in Epoch 361
Epoch 364
Epoch 364, Loss: 0.000295852, Improvement: -0.000008042, Best Loss: 0.000209059 in Epoch 361
Epoch 365
A best model at epoch 365 has been saved with training error 0.000202899.
A best model at epoch 365 has been saved with training error 0.000188456.
Epoch 365, Loss: 0.000289597, Improvement: -0.000006255, Best Loss: 0.000188456 in Epoch 365
Epoch 366
Epoch 366, Loss: 0.000288732, Improvement: -0.000000865, Best Loss: 0.000188456 in Epoch 365
Epoch 367
Epoch 367, Loss: 0.000286697, Improvement: -0.000002034, Best Loss: 0.000188456 in Epoch 365
Epoch 368
Epoch 368, Loss: 0.000292643, Improvement: 0.000005945, Best Loss: 0.000188456 in Epoch 365
Epoch 369
Epoch 369, Loss: 0.000329125, Improvement: 0.000036482, Best Loss: 0.000188456 in Epoch 365
Epoch 370
Epoch 370, Loss: 0.000420354, Improvement: 0.000091229, Best Loss: 0.000188456 in Epoch 365
Epoch 371
Epoch 371, Loss: 0.000396557, Improvement: -0.000023797, Best Loss: 0.000188456 in Epoch 365
Epoch 372
Epoch 372, Loss: 0.000404110, Improvement: 0.000007553, Best Loss: 0.000188456 in Epoch 365
Epoch 373
Epoch 373, Loss: 0.000397787, Improvement: -0.000006323, Best Loss: 0.000188456 in Epoch 365
Epoch 374
Epoch 374, Loss: 0.000359366, Improvement: -0.000038421, Best Loss: 0.000188456 in Epoch 365
Epoch 375
Epoch 375, Loss: 0.000368184, Improvement: 0.000008818, Best Loss: 0.000188456 in Epoch 365
Epoch 376
Epoch 376, Loss: 0.000377473, Improvement: 0.000009289, Best Loss: 0.000188456 in Epoch 365
Epoch 377
Epoch 377, Loss: 0.000401997, Improvement: 0.000024524, Best Loss: 0.000188456 in Epoch 365
Epoch 378
Epoch 378, Loss: 0.000713723, Improvement: 0.000311725, Best Loss: 0.000188456 in Epoch 365
Epoch 379
Epoch 379, Loss: 0.000870241, Improvement: 0.000156518, Best Loss: 0.000188456 in Epoch 365
Epoch 380
Epoch 380, Loss: 0.000683482, Improvement: -0.000186758, Best Loss: 0.000188456 in Epoch 365
Epoch 381
Epoch 381, Loss: 0.000452166, Improvement: -0.000231317, Best Loss: 0.000188456 in Epoch 365
Epoch 382
Epoch 382, Loss: 0.000340406, Improvement: -0.000111760, Best Loss: 0.000188456 in Epoch 365
Epoch 383
Epoch 383, Loss: 0.000299838, Improvement: -0.000040568, Best Loss: 0.000188456 in Epoch 365
Epoch 384
A best model at epoch 384 has been saved with training error 0.000186567.
Epoch 384, Loss: 0.000271829, Improvement: -0.000028009, Best Loss: 0.000186567 in Epoch 384
Epoch 385
A best model at epoch 385 has been saved with training error 0.000159850.
Epoch 385, Loss: 0.000263176, Improvement: -0.000008653, Best Loss: 0.000159850 in Epoch 385
Epoch 386
Epoch 386, Loss: 0.000241528, Improvement: -0.000021647, Best Loss: 0.000159850 in Epoch 385
Epoch 387
Epoch 387, Loss: 0.000234242, Improvement: -0.000007286, Best Loss: 0.000159850 in Epoch 385
Epoch 388
Epoch 388, Loss: 0.000229458, Improvement: -0.000004784, Best Loss: 0.000159850 in Epoch 385
Epoch 389
Epoch 389, Loss: 0.000226241, Improvement: -0.000003217, Best Loss: 0.000159850 in Epoch 385
Epoch 390
A best model at epoch 390 has been saved with training error 0.000135884.
Epoch 390, Loss: 0.000222718, Improvement: -0.000003523, Best Loss: 0.000135884 in Epoch 390
Epoch 391
Epoch 391, Loss: 0.000220545, Improvement: -0.000002173, Best Loss: 0.000135884 in Epoch 390
Epoch 392
Epoch 392, Loss: 0.000218467, Improvement: -0.000002078, Best Loss: 0.000135884 in Epoch 390
Epoch 393
A best model at epoch 393 has been saved with training error 0.000134025.
Epoch 393, Loss: 0.000214761, Improvement: -0.000003706, Best Loss: 0.000134025 in Epoch 393
Epoch 394
A best model at epoch 394 has been saved with training error 0.000129335.
Epoch 394, Loss: 0.000213000, Improvement: -0.000001761, Best Loss: 0.000129335 in Epoch 394
Epoch 395
Epoch 395, Loss: 0.000209240, Improvement: -0.000003760, Best Loss: 0.000129335 in Epoch 394
Epoch 396
A best model at epoch 396 has been saved with training error 0.000116106.
Epoch 396, Loss: 0.000207961, Improvement: -0.000001279, Best Loss: 0.000116106 in Epoch 396
Epoch 397
Epoch 397, Loss: 0.000210225, Improvement: 0.000002265, Best Loss: 0.000116106 in Epoch 396
Epoch 398
Epoch 398, Loss: 0.000205227, Improvement: -0.000004998, Best Loss: 0.000116106 in Epoch 396
Epoch 399
Epoch 399, Loss: 0.000207702, Improvement: 0.000002475, Best Loss: 0.000116106 in Epoch 396
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.000207553, Improvement: -0.000000150, Best Loss: 0.000116106 in Epoch 396
Epoch 401
Epoch 401, Loss: 0.000222041, Improvement: 0.000014488, Best Loss: 0.000116106 in Epoch 396
Epoch 402
Epoch 402, Loss: 0.000212261, Improvement: -0.000009780, Best Loss: 0.000116106 in Epoch 396
Epoch 403
Epoch 403, Loss: 0.000223442, Improvement: 0.000011181, Best Loss: 0.000116106 in Epoch 396
Epoch 404
Epoch 404, Loss: 0.000272911, Improvement: 0.000049470, Best Loss: 0.000116106 in Epoch 396
Epoch 405
Epoch 405, Loss: 0.000263602, Improvement: -0.000009309, Best Loss: 0.000116106 in Epoch 396
Epoch 406
Epoch 406, Loss: 0.000249722, Improvement: -0.000013880, Best Loss: 0.000116106 in Epoch 396
Epoch 407
Epoch 407, Loss: 0.000211883, Improvement: -0.000037839, Best Loss: 0.000116106 in Epoch 396
Epoch 408
Epoch 408, Loss: 0.000227420, Improvement: 0.000015537, Best Loss: 0.000116106 in Epoch 396
Epoch 409
Epoch 409, Loss: 0.000226773, Improvement: -0.000000648, Best Loss: 0.000116106 in Epoch 396
Epoch 410
Epoch 410, Loss: 0.000210094, Improvement: -0.000016679, Best Loss: 0.000116106 in Epoch 396
Epoch 411
Epoch 411, Loss: 0.000203257, Improvement: -0.000006837, Best Loss: 0.000116106 in Epoch 396
Epoch 412
A best model at epoch 412 has been saved with training error 0.000114996.
Epoch 412, Loss: 0.000213975, Improvement: 0.000010718, Best Loss: 0.000114996 in Epoch 412
Epoch 413
Epoch 413, Loss: 0.000333526, Improvement: 0.000119551, Best Loss: 0.000114996 in Epoch 412
Epoch 414
Epoch 414, Loss: 0.000260933, Improvement: -0.000072593, Best Loss: 0.000114996 in Epoch 412
Epoch 415
Epoch 415, Loss: 0.000296446, Improvement: 0.000035513, Best Loss: 0.000114996 in Epoch 412
Epoch 416
Epoch 416, Loss: 0.000244604, Improvement: -0.000051842, Best Loss: 0.000114996 in Epoch 412
Epoch 417
Epoch 417, Loss: 0.000236602, Improvement: -0.000008002, Best Loss: 0.000114996 in Epoch 412
Epoch 418
Epoch 418, Loss: 0.000202077, Improvement: -0.000034525, Best Loss: 0.000114996 in Epoch 412
Epoch 419
Epoch 419, Loss: 0.000202024, Improvement: -0.000000053, Best Loss: 0.000114996 in Epoch 412
Epoch 420
Epoch 420, Loss: 0.000217243, Improvement: 0.000015220, Best Loss: 0.000114996 in Epoch 412
Epoch 421
Epoch 421, Loss: 0.000236881, Improvement: 0.000019637, Best Loss: 0.000114996 in Epoch 412
Epoch 422
Epoch 422, Loss: 0.000251872, Improvement: 0.000014992, Best Loss: 0.000114996 in Epoch 412
Epoch 423
Epoch 423, Loss: 0.000265114, Improvement: 0.000013242, Best Loss: 0.000114996 in Epoch 412
Epoch 424
Epoch 424, Loss: 0.000218835, Improvement: -0.000046280, Best Loss: 0.000114996 in Epoch 412
Epoch 425
Epoch 425, Loss: 0.000219377, Improvement: 0.000000542, Best Loss: 0.000114996 in Epoch 412
Epoch 426
Epoch 426, Loss: 0.000249674, Improvement: 0.000030297, Best Loss: 0.000114996 in Epoch 412
Epoch 427
Epoch 427, Loss: 0.000234733, Improvement: -0.000014941, Best Loss: 0.000114996 in Epoch 412
Epoch 428
Epoch 428, Loss: 0.000230905, Improvement: -0.000003828, Best Loss: 0.000114996 in Epoch 412
Epoch 429
Epoch 429, Loss: 0.000302716, Improvement: 0.000071810, Best Loss: 0.000114996 in Epoch 412
Epoch 430
Epoch 430, Loss: 0.000389068, Improvement: 0.000086352, Best Loss: 0.000114996 in Epoch 412
Epoch 431
Epoch 431, Loss: 0.000420032, Improvement: 0.000030963, Best Loss: 0.000114996 in Epoch 412
Epoch 432
Epoch 432, Loss: 0.000284455, Improvement: -0.000135576, Best Loss: 0.000114996 in Epoch 412
Epoch 433
Epoch 433, Loss: 0.000205730, Improvement: -0.000078726, Best Loss: 0.000114996 in Epoch 412
Epoch 434
Epoch 434, Loss: 0.000177992, Improvement: -0.000027738, Best Loss: 0.000114996 in Epoch 412
Epoch 435
Epoch 435, Loss: 0.000168032, Improvement: -0.000009960, Best Loss: 0.000114996 in Epoch 412
Epoch 436
Epoch 436, Loss: 0.000155927, Improvement: -0.000012104, Best Loss: 0.000114996 in Epoch 412
Epoch 437
A best model at epoch 437 has been saved with training error 0.000101803.
Epoch 437, Loss: 0.000154530, Improvement: -0.000001397, Best Loss: 0.000101803 in Epoch 437
Epoch 438
Epoch 438, Loss: 0.000148091, Improvement: -0.000006439, Best Loss: 0.000101803 in Epoch 437
Epoch 439
Epoch 439, Loss: 0.000145517, Improvement: -0.000002573, Best Loss: 0.000101803 in Epoch 437
Epoch 440
Epoch 440, Loss: 0.000143336, Improvement: -0.000002182, Best Loss: 0.000101803 in Epoch 437
Epoch 441
Epoch 441, Loss: 0.000150059, Improvement: 0.000006724, Best Loss: 0.000101803 in Epoch 437
Epoch 442
A best model at epoch 442 has been saved with training error 0.000083157.
Epoch 442, Loss: 0.000146652, Improvement: -0.000003408, Best Loss: 0.000083157 in Epoch 442
Epoch 443
Epoch 443, Loss: 0.000143600, Improvement: -0.000003052, Best Loss: 0.000083157 in Epoch 442
Epoch 444
Epoch 444, Loss: 0.000134483, Improvement: -0.000009117, Best Loss: 0.000083157 in Epoch 442
Epoch 445
Epoch 445, Loss: 0.000132484, Improvement: -0.000001999, Best Loss: 0.000083157 in Epoch 442
Epoch 446
Epoch 446, Loss: 0.000129354, Improvement: -0.000003130, Best Loss: 0.000083157 in Epoch 442
Epoch 447
Epoch 447, Loss: 0.000126773, Improvement: -0.000002581, Best Loss: 0.000083157 in Epoch 442
Epoch 448
Epoch 448, Loss: 0.000126419, Improvement: -0.000000354, Best Loss: 0.000083157 in Epoch 442
Epoch 449
Epoch 449, Loss: 0.000126627, Improvement: 0.000000208, Best Loss: 0.000083157 in Epoch 442
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.000124863, Improvement: -0.000001764, Best Loss: 0.000083157 in Epoch 442
Epoch 451
A best model at epoch 451 has been saved with training error 0.000079755.
Epoch 451, Loss: 0.000121639, Improvement: -0.000003223, Best Loss: 0.000079755 in Epoch 451
Epoch 452
Epoch 452, Loss: 0.000121624, Improvement: -0.000000016, Best Loss: 0.000079755 in Epoch 451
Epoch 453
Epoch 453, Loss: 0.000119779, Improvement: -0.000001845, Best Loss: 0.000079755 in Epoch 451
Epoch 454
Epoch 454, Loss: 0.000119492, Improvement: -0.000000287, Best Loss: 0.000079755 in Epoch 451
Epoch 455
Epoch 455, Loss: 0.000123141, Improvement: 0.000003649, Best Loss: 0.000079755 in Epoch 451
Epoch 456
Epoch 456, Loss: 0.000129678, Improvement: 0.000006537, Best Loss: 0.000079755 in Epoch 451
Epoch 457
Epoch 457, Loss: 0.000174024, Improvement: 0.000044347, Best Loss: 0.000079755 in Epoch 451
Epoch 458
Epoch 458, Loss: 0.000216857, Improvement: 0.000042833, Best Loss: 0.000079755 in Epoch 451
Epoch 459
Epoch 459, Loss: 0.000252467, Improvement: 0.000035610, Best Loss: 0.000079755 in Epoch 451
Epoch 460
Epoch 460, Loss: 0.000215829, Improvement: -0.000036638, Best Loss: 0.000079755 in Epoch 451
Epoch 461
Epoch 461, Loss: 0.000283921, Improvement: 0.000068092, Best Loss: 0.000079755 in Epoch 451
Epoch 462
Epoch 462, Loss: 0.000319521, Improvement: 0.000035599, Best Loss: 0.000079755 in Epoch 451
Epoch 463
Epoch 463, Loss: 0.000220276, Improvement: -0.000099244, Best Loss: 0.000079755 in Epoch 451
Epoch 464
Epoch 464, Loss: 0.000253067, Improvement: 0.000032790, Best Loss: 0.000079755 in Epoch 451
Epoch 465
Epoch 465, Loss: 0.000230994, Improvement: -0.000022073, Best Loss: 0.000079755 in Epoch 451
Epoch 466
Epoch 466, Loss: 0.000186740, Improvement: -0.000044254, Best Loss: 0.000079755 in Epoch 451
Epoch 467
Epoch 467, Loss: 0.000166174, Improvement: -0.000020565, Best Loss: 0.000079755 in Epoch 451
Epoch 468
Epoch 468, Loss: 0.000133290, Improvement: -0.000032885, Best Loss: 0.000079755 in Epoch 451
Epoch 469
Epoch 469, Loss: 0.000139982, Improvement: 0.000006692, Best Loss: 0.000079755 in Epoch 451
Epoch 470
A best model at epoch 470 has been saved with training error 0.000075261.
Epoch 470, Loss: 0.000125252, Improvement: -0.000014730, Best Loss: 0.000075261 in Epoch 470
Epoch 471
A best model at epoch 471 has been saved with training error 0.000073723.
Epoch 471, Loss: 0.000110911, Improvement: -0.000014341, Best Loss: 0.000073723 in Epoch 471
Epoch 472
Epoch 472, Loss: 0.000111020, Improvement: 0.000000109, Best Loss: 0.000073723 in Epoch 471
Epoch 473
Epoch 473, Loss: 0.000113885, Improvement: 0.000002866, Best Loss: 0.000073723 in Epoch 471
Epoch 474
Epoch 474, Loss: 0.000111724, Improvement: -0.000002161, Best Loss: 0.000073723 in Epoch 471
Epoch 475
Epoch 475, Loss: 0.000108977, Improvement: -0.000002747, Best Loss: 0.000073723 in Epoch 471
Epoch 476
Epoch 476, Loss: 0.000121776, Improvement: 0.000012798, Best Loss: 0.000073723 in Epoch 471
Epoch 477
Epoch 477, Loss: 0.000122733, Improvement: 0.000000958, Best Loss: 0.000073723 in Epoch 471
Epoch 478
Epoch 478, Loss: 0.000135004, Improvement: 0.000012270, Best Loss: 0.000073723 in Epoch 471
Epoch 479
Epoch 479, Loss: 0.000119258, Improvement: -0.000015745, Best Loss: 0.000073723 in Epoch 471
Epoch 480
Epoch 480, Loss: 0.000122745, Improvement: 0.000003487, Best Loss: 0.000073723 in Epoch 471
Epoch 481
Epoch 481, Loss: 0.000150077, Improvement: 0.000027332, Best Loss: 0.000073723 in Epoch 471
Epoch 482
Epoch 482, Loss: 0.000151794, Improvement: 0.000001716, Best Loss: 0.000073723 in Epoch 471
Epoch 483
Epoch 483, Loss: 0.000189675, Improvement: 0.000037882, Best Loss: 0.000073723 in Epoch 471
Epoch 484
Epoch 484, Loss: 0.000263651, Improvement: 0.000073976, Best Loss: 0.000073723 in Epoch 471
Epoch 485
Epoch 485, Loss: 0.000191692, Improvement: -0.000071959, Best Loss: 0.000073723 in Epoch 471
Epoch 486
Epoch 486, Loss: 0.000154138, Improvement: -0.000037554, Best Loss: 0.000073723 in Epoch 471
Epoch 487
Epoch 487, Loss: 0.000149126, Improvement: -0.000005011, Best Loss: 0.000073723 in Epoch 471
Epoch 488
Epoch 488, Loss: 0.000149734, Improvement: 0.000000608, Best Loss: 0.000073723 in Epoch 471
Epoch 489
Epoch 489, Loss: 0.000135017, Improvement: -0.000014717, Best Loss: 0.000073723 in Epoch 471
Epoch 490
Epoch 490, Loss: 0.000137275, Improvement: 0.000002258, Best Loss: 0.000073723 in Epoch 471
Epoch 491
Epoch 491, Loss: 0.000150685, Improvement: 0.000013411, Best Loss: 0.000073723 in Epoch 471
Epoch 492
Epoch 492, Loss: 0.000309236, Improvement: 0.000158551, Best Loss: 0.000073723 in Epoch 471
Epoch 493
Epoch 493, Loss: 0.000233912, Improvement: -0.000075324, Best Loss: 0.000073723 in Epoch 471
Epoch 494
Epoch 494, Loss: 0.000176180, Improvement: -0.000057732, Best Loss: 0.000073723 in Epoch 471
Epoch 495
Epoch 495, Loss: 0.000125529, Improvement: -0.000050651, Best Loss: 0.000073723 in Epoch 471
Epoch 496
A best model at epoch 496 has been saved with training error 0.000069795.
Epoch 496, Loss: 0.000107863, Improvement: -0.000017666, Best Loss: 0.000069795 in Epoch 496
Epoch 497
A best model at epoch 497 has been saved with training error 0.000065633.
Epoch 497, Loss: 0.000120777, Improvement: 0.000012915, Best Loss: 0.000065633 in Epoch 497
Epoch 498
Epoch 498, Loss: 0.000139933, Improvement: 0.000019156, Best Loss: 0.000065633 in Epoch 497
Epoch 499
Epoch 499, Loss: 0.000144296, Improvement: 0.000004362, Best Loss: 0.000065633 in Epoch 497
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.000115355, Improvement: -0.000028941, Best Loss: 0.000065633 in Epoch 497
Epoch 501
Epoch 501, Loss: 0.000103005, Improvement: -0.000012350, Best Loss: 0.000065633 in Epoch 497
Epoch 502
Epoch 502, Loss: 0.000100403, Improvement: -0.000002602, Best Loss: 0.000065633 in Epoch 497
Epoch 503
Epoch 503, Loss: 0.000110400, Improvement: 0.000009997, Best Loss: 0.000065633 in Epoch 497
Epoch 504
Epoch 504, Loss: 0.000107760, Improvement: -0.000002641, Best Loss: 0.000065633 in Epoch 497
Epoch 505
Epoch 505, Loss: 0.000113148, Improvement: 0.000005389, Best Loss: 0.000065633 in Epoch 497
Epoch 506
A best model at epoch 506 has been saved with training error 0.000057147.
Epoch 506, Loss: 0.000102198, Improvement: -0.000010950, Best Loss: 0.000057147 in Epoch 506
Epoch 507
Epoch 507, Loss: 0.000097159, Improvement: -0.000005039, Best Loss: 0.000057147 in Epoch 506
Epoch 508
Epoch 508, Loss: 0.000103059, Improvement: 0.000005900, Best Loss: 0.000057147 in Epoch 506
Epoch 509
Epoch 509, Loss: 0.000091817, Improvement: -0.000011242, Best Loss: 0.000057147 in Epoch 506
Epoch 510
A best model at epoch 510 has been saved with training error 0.000051216.
Epoch 510, Loss: 0.000087867, Improvement: -0.000003950, Best Loss: 0.000051216 in Epoch 510
Epoch 511
Epoch 511, Loss: 0.000083669, Improvement: -0.000004199, Best Loss: 0.000051216 in Epoch 510
Epoch 512
Epoch 512, Loss: 0.000096540, Improvement: 0.000012872, Best Loss: 0.000051216 in Epoch 510
Epoch 513
Epoch 513, Loss: 0.000087561, Improvement: -0.000008979, Best Loss: 0.000051216 in Epoch 510
Epoch 514
Epoch 514, Loss: 0.000105669, Improvement: 0.000018108, Best Loss: 0.000051216 in Epoch 510
Epoch 515
Epoch 515, Loss: 0.000118742, Improvement: 0.000013073, Best Loss: 0.000051216 in Epoch 510
Epoch 516
Epoch 516, Loss: 0.000106665, Improvement: -0.000012078, Best Loss: 0.000051216 in Epoch 510
Epoch 517
Epoch 517, Loss: 0.000142139, Improvement: 0.000035474, Best Loss: 0.000051216 in Epoch 510
Epoch 518
Epoch 518, Loss: 0.000157203, Improvement: 0.000015065, Best Loss: 0.000051216 in Epoch 510
Epoch 519
Epoch 519, Loss: 0.000134722, Improvement: -0.000022481, Best Loss: 0.000051216 in Epoch 510
Epoch 520
Epoch 520, Loss: 0.000210152, Improvement: 0.000075430, Best Loss: 0.000051216 in Epoch 510
Epoch 521
Epoch 521, Loss: 0.000274952, Improvement: 0.000064799, Best Loss: 0.000051216 in Epoch 510
Epoch 522
Epoch 522, Loss: 0.000195368, Improvement: -0.000079584, Best Loss: 0.000051216 in Epoch 510
Epoch 523
Epoch 523, Loss: 0.000159927, Improvement: -0.000035441, Best Loss: 0.000051216 in Epoch 510
Epoch 524
Epoch 524, Loss: 0.000146237, Improvement: -0.000013690, Best Loss: 0.000051216 in Epoch 510
Epoch 525
Epoch 525, Loss: 0.000113490, Improvement: -0.000032746, Best Loss: 0.000051216 in Epoch 510
Epoch 526
Epoch 526, Loss: 0.000088276, Improvement: -0.000025215, Best Loss: 0.000051216 in Epoch 510
Epoch 527
Epoch 527, Loss: 0.000085478, Improvement: -0.000002798, Best Loss: 0.000051216 in Epoch 510
Epoch 528
Epoch 528, Loss: 0.000075998, Improvement: -0.000009480, Best Loss: 0.000051216 in Epoch 510
Epoch 529
Epoch 529, Loss: 0.000076184, Improvement: 0.000000186, Best Loss: 0.000051216 in Epoch 510
Epoch 530
Epoch 530, Loss: 0.000088304, Improvement: 0.000012120, Best Loss: 0.000051216 in Epoch 510
Epoch 531
Epoch 531, Loss: 0.000089282, Improvement: 0.000000978, Best Loss: 0.000051216 in Epoch 510
Epoch 532
Epoch 532, Loss: 0.000081366, Improvement: -0.000007915, Best Loss: 0.000051216 in Epoch 510
Epoch 533
A best model at epoch 533 has been saved with training error 0.000050955.
Epoch 533, Loss: 0.000076617, Improvement: -0.000004749, Best Loss: 0.000050955 in Epoch 533
Epoch 534
A best model at epoch 534 has been saved with training error 0.000042787.
Epoch 534, Loss: 0.000090725, Improvement: 0.000014108, Best Loss: 0.000042787 in Epoch 534
Epoch 535
Epoch 535, Loss: 0.000071469, Improvement: -0.000019256, Best Loss: 0.000042787 in Epoch 534
Epoch 536
Epoch 536, Loss: 0.000073956, Improvement: 0.000002487, Best Loss: 0.000042787 in Epoch 534
Epoch 537
Epoch 537, Loss: 0.000088958, Improvement: 0.000015003, Best Loss: 0.000042787 in Epoch 534
Epoch 538
Epoch 538, Loss: 0.000082192, Improvement: -0.000006766, Best Loss: 0.000042787 in Epoch 534
Epoch 539
Epoch 539, Loss: 0.000108526, Improvement: 0.000026334, Best Loss: 0.000042787 in Epoch 534
Epoch 540
Epoch 540, Loss: 0.000116778, Improvement: 0.000008251, Best Loss: 0.000042787 in Epoch 534
Epoch 541
Epoch 541, Loss: 0.000097754, Improvement: -0.000019024, Best Loss: 0.000042787 in Epoch 534
Epoch 542
Epoch 542, Loss: 0.000094170, Improvement: -0.000003584, Best Loss: 0.000042787 in Epoch 534
Epoch 543
Epoch 543, Loss: 0.000124153, Improvement: 0.000029984, Best Loss: 0.000042787 in Epoch 534
Epoch 544
Epoch 544, Loss: 0.000142115, Improvement: 0.000017962, Best Loss: 0.000042787 in Epoch 534
Epoch 545
Epoch 545, Loss: 0.000121296, Improvement: -0.000020819, Best Loss: 0.000042787 in Epoch 534
Epoch 546
Epoch 546, Loss: 0.000122030, Improvement: 0.000000734, Best Loss: 0.000042787 in Epoch 534
Epoch 547
Epoch 547, Loss: 0.000119896, Improvement: -0.000002134, Best Loss: 0.000042787 in Epoch 534
Epoch 548
Epoch 548, Loss: 0.000100619, Improvement: -0.000019277, Best Loss: 0.000042787 in Epoch 534
Epoch 549
Epoch 549, Loss: 0.000085872, Improvement: -0.000014746, Best Loss: 0.000042787 in Epoch 534
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.000072043, Improvement: -0.000013829, Best Loss: 0.000042787 in Epoch 534
Epoch 551
Epoch 551, Loss: 0.000084212, Improvement: 0.000012169, Best Loss: 0.000042787 in Epoch 534
Epoch 552
Epoch 552, Loss: 0.000102837, Improvement: 0.000018625, Best Loss: 0.000042787 in Epoch 534
Epoch 553
Epoch 553, Loss: 0.000097857, Improvement: -0.000004980, Best Loss: 0.000042787 in Epoch 534
Epoch 554
Epoch 554, Loss: 0.000074712, Improvement: -0.000023145, Best Loss: 0.000042787 in Epoch 534
Epoch 555
Epoch 555, Loss: 0.000068633, Improvement: -0.000006079, Best Loss: 0.000042787 in Epoch 534
Epoch 556
Epoch 556, Loss: 0.000073986, Improvement: 0.000005353, Best Loss: 0.000042787 in Epoch 534
Epoch 557
Epoch 557, Loss: 0.000124323, Improvement: 0.000050337, Best Loss: 0.000042787 in Epoch 534
Epoch 558
Epoch 558, Loss: 0.000169070, Improvement: 0.000044747, Best Loss: 0.000042787 in Epoch 534
Epoch 559
Epoch 559, Loss: 0.000176282, Improvement: 0.000007212, Best Loss: 0.000042787 in Epoch 534
Epoch 560
Epoch 560, Loss: 0.000227382, Improvement: 0.000051099, Best Loss: 0.000042787 in Epoch 534
Epoch 561
Epoch 561, Loss: 0.000192450, Improvement: -0.000034932, Best Loss: 0.000042787 in Epoch 534
Epoch 562
Epoch 562, Loss: 0.000146311, Improvement: -0.000046139, Best Loss: 0.000042787 in Epoch 534
Epoch 563
Epoch 563, Loss: 0.000157997, Improvement: 0.000011686, Best Loss: 0.000042787 in Epoch 534
Epoch 564
Epoch 564, Loss: 0.000115757, Improvement: -0.000042240, Best Loss: 0.000042787 in Epoch 534
Epoch 565
Epoch 565, Loss: 0.000092464, Improvement: -0.000023293, Best Loss: 0.000042787 in Epoch 534
Epoch 566
Epoch 566, Loss: 0.000074835, Improvement: -0.000017629, Best Loss: 0.000042787 in Epoch 534
Epoch 567
Epoch 567, Loss: 0.000078949, Improvement: 0.000004114, Best Loss: 0.000042787 in Epoch 534
Epoch 568
Epoch 568, Loss: 0.000086972, Improvement: 0.000008024, Best Loss: 0.000042787 in Epoch 534
Epoch 569
Epoch 569, Loss: 0.000074773, Improvement: -0.000012199, Best Loss: 0.000042787 in Epoch 534
Epoch 570
A best model at epoch 570 has been saved with training error 0.000040378.
A best model at epoch 570 has been saved with training error 0.000032206.
Epoch 570, Loss: 0.000055541, Improvement: -0.000019232, Best Loss: 0.000032206 in Epoch 570
Epoch 571
Epoch 571, Loss: 0.000050680, Improvement: -0.000004861, Best Loss: 0.000032206 in Epoch 570
Epoch 572
Epoch 572, Loss: 0.000048825, Improvement: -0.000001855, Best Loss: 0.000032206 in Epoch 570
Epoch 573
Epoch 573, Loss: 0.000048679, Improvement: -0.000000146, Best Loss: 0.000032206 in Epoch 570
Epoch 574
A best model at epoch 574 has been saved with training error 0.000031811.
A best model at epoch 574 has been saved with training error 0.000030489.
A best model at epoch 574 has been saved with training error 0.000023731.
Epoch 574, Loss: 0.000047580, Improvement: -0.000001099, Best Loss: 0.000023731 in Epoch 574
Epoch 575
Epoch 575, Loss: 0.000047212, Improvement: -0.000000367, Best Loss: 0.000023731 in Epoch 574
Epoch 576
Epoch 576, Loss: 0.000047085, Improvement: -0.000000127, Best Loss: 0.000023731 in Epoch 574
Epoch 577
Epoch 577, Loss: 0.000049611, Improvement: 0.000002526, Best Loss: 0.000023731 in Epoch 574
Epoch 578
Epoch 578, Loss: 0.000048234, Improvement: -0.000001377, Best Loss: 0.000023731 in Epoch 574
Epoch 579
Epoch 579, Loss: 0.000046666, Improvement: -0.000001568, Best Loss: 0.000023731 in Epoch 574
Epoch 580
Epoch 580, Loss: 0.000046734, Improvement: 0.000000068, Best Loss: 0.000023731 in Epoch 574
Epoch 581
Epoch 581, Loss: 0.000046852, Improvement: 0.000000118, Best Loss: 0.000023731 in Epoch 574
Epoch 582
Epoch 582, Loss: 0.000044551, Improvement: -0.000002301, Best Loss: 0.000023731 in Epoch 574
Epoch 583
Epoch 583, Loss: 0.000046137, Improvement: 0.000001586, Best Loss: 0.000023731 in Epoch 574
Epoch 584
Epoch 584, Loss: 0.000044745, Improvement: -0.000001393, Best Loss: 0.000023731 in Epoch 574
Epoch 585
Epoch 585, Loss: 0.000047448, Improvement: 0.000002703, Best Loss: 0.000023731 in Epoch 574
Epoch 586
Epoch 586, Loss: 0.000047877, Improvement: 0.000000429, Best Loss: 0.000023731 in Epoch 574
Epoch 587
Epoch 587, Loss: 0.000058949, Improvement: 0.000011072, Best Loss: 0.000023731 in Epoch 574
Epoch 588
Epoch 588, Loss: 0.000082002, Improvement: 0.000023053, Best Loss: 0.000023731 in Epoch 574
Epoch 589
Epoch 589, Loss: 0.000170418, Improvement: 0.000088415, Best Loss: 0.000023731 in Epoch 574
Epoch 590
Epoch 590, Loss: 0.000198549, Improvement: 0.000028131, Best Loss: 0.000023731 in Epoch 574
Epoch 591
Epoch 591, Loss: 0.000174669, Improvement: -0.000023880, Best Loss: 0.000023731 in Epoch 574
Epoch 592
Epoch 592, Loss: 0.000126333, Improvement: -0.000048335, Best Loss: 0.000023731 in Epoch 574
Epoch 593
Epoch 593, Loss: 0.000079523, Improvement: -0.000046811, Best Loss: 0.000023731 in Epoch 574
Epoch 594
Epoch 594, Loss: 0.000056897, Improvement: -0.000022625, Best Loss: 0.000023731 in Epoch 574
Epoch 595
Epoch 595, Loss: 0.000051324, Improvement: -0.000005573, Best Loss: 0.000023731 in Epoch 574
Epoch 596
Epoch 596, Loss: 0.000044368, Improvement: -0.000006956, Best Loss: 0.000023731 in Epoch 574
Epoch 597
Epoch 597, Loss: 0.000041444, Improvement: -0.000002924, Best Loss: 0.000023731 in Epoch 574
Epoch 598
Epoch 598, Loss: 0.000041199, Improvement: -0.000000245, Best Loss: 0.000023731 in Epoch 574
Epoch 599
Epoch 599, Loss: 0.000039048, Improvement: -0.000002151, Best Loss: 0.000023731 in Epoch 574
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.000038453, Improvement: -0.000000595, Best Loss: 0.000023731 in Epoch 574
Epoch 601
Epoch 601, Loss: 0.000038478, Improvement: 0.000000025, Best Loss: 0.000023731 in Epoch 574
Epoch 602
Epoch 602, Loss: 0.000037619, Improvement: -0.000000859, Best Loss: 0.000023731 in Epoch 574
Epoch 603
Epoch 603, Loss: 0.000037461, Improvement: -0.000000158, Best Loss: 0.000023731 in Epoch 574
Epoch 604
Epoch 604, Loss: 0.000037344, Improvement: -0.000000116, Best Loss: 0.000023731 in Epoch 574
Epoch 605
Epoch 605, Loss: 0.000036847, Improvement: -0.000000498, Best Loss: 0.000023731 in Epoch 574
Epoch 606
A best model at epoch 606 has been saved with training error 0.000022770.
A best model at epoch 606 has been saved with training error 0.000022473.
Epoch 606, Loss: 0.000036655, Improvement: -0.000000192, Best Loss: 0.000022473 in Epoch 606
Epoch 607
Epoch 607, Loss: 0.000036181, Improvement: -0.000000473, Best Loss: 0.000022473 in Epoch 606
Epoch 608
A best model at epoch 608 has been saved with training error 0.000020552.
Epoch 608, Loss: 0.000035384, Improvement: -0.000000797, Best Loss: 0.000020552 in Epoch 608
Epoch 609
Epoch 609, Loss: 0.000034803, Improvement: -0.000000581, Best Loss: 0.000020552 in Epoch 608
Epoch 610
Epoch 610, Loss: 0.000034735, Improvement: -0.000000068, Best Loss: 0.000020552 in Epoch 608
Epoch 611
Epoch 611, Loss: 0.000034371, Improvement: -0.000000364, Best Loss: 0.000020552 in Epoch 608
Epoch 612
Epoch 612, Loss: 0.000034270, Improvement: -0.000000100, Best Loss: 0.000020552 in Epoch 608
Epoch 613
Epoch 613, Loss: 0.000034437, Improvement: 0.000000167, Best Loss: 0.000020552 in Epoch 608
Epoch 614
Epoch 614, Loss: 0.000034217, Improvement: -0.000000220, Best Loss: 0.000020552 in Epoch 608
Epoch 615
Epoch 615, Loss: 0.000034409, Improvement: 0.000000192, Best Loss: 0.000020552 in Epoch 608
Epoch 616
Epoch 616, Loss: 0.000033542, Improvement: -0.000000866, Best Loss: 0.000020552 in Epoch 608
Epoch 617
Epoch 617, Loss: 0.000036286, Improvement: 0.000002743, Best Loss: 0.000020552 in Epoch 608
Epoch 618
Epoch 618, Loss: 0.000064864, Improvement: 0.000028578, Best Loss: 0.000020552 in Epoch 608
Epoch 619
Epoch 619, Loss: 0.000083961, Improvement: 0.000019097, Best Loss: 0.000020552 in Epoch 608
Epoch 620
Epoch 620, Loss: 0.000072655, Improvement: -0.000011306, Best Loss: 0.000020552 in Epoch 608
Epoch 621
Epoch 621, Loss: 0.000068302, Improvement: -0.000004353, Best Loss: 0.000020552 in Epoch 608
Epoch 622
Epoch 622, Loss: 0.000071752, Improvement: 0.000003450, Best Loss: 0.000020552 in Epoch 608
Epoch 623
Epoch 623, Loss: 0.000094372, Improvement: 0.000022620, Best Loss: 0.000020552 in Epoch 608
Epoch 624
Epoch 624, Loss: 0.000078887, Improvement: -0.000015485, Best Loss: 0.000020552 in Epoch 608
Epoch 625
Epoch 625, Loss: 0.000102003, Improvement: 0.000023116, Best Loss: 0.000020552 in Epoch 608
Epoch 626
Epoch 626, Loss: 0.000091132, Improvement: -0.000010871, Best Loss: 0.000020552 in Epoch 608
Epoch 627
Epoch 627, Loss: 0.000079273, Improvement: -0.000011859, Best Loss: 0.000020552 in Epoch 608
Epoch 628
Epoch 628, Loss: 0.000072258, Improvement: -0.000007015, Best Loss: 0.000020552 in Epoch 608
Epoch 629
Epoch 629, Loss: 0.000060322, Improvement: -0.000011935, Best Loss: 0.000020552 in Epoch 608
Epoch 630
Epoch 630, Loss: 0.000053405, Improvement: -0.000006917, Best Loss: 0.000020552 in Epoch 608
Epoch 631
Epoch 631, Loss: 0.000050171, Improvement: -0.000003234, Best Loss: 0.000020552 in Epoch 608
Epoch 632
Epoch 632, Loss: 0.000065107, Improvement: 0.000014936, Best Loss: 0.000020552 in Epoch 608
Epoch 633
Epoch 633, Loss: 0.000156682, Improvement: 0.000091575, Best Loss: 0.000020552 in Epoch 608
Epoch 634
Epoch 634, Loss: 0.000188494, Improvement: 0.000031811, Best Loss: 0.000020552 in Epoch 608
Epoch 635
Epoch 635, Loss: 0.000091286, Improvement: -0.000097208, Best Loss: 0.000020552 in Epoch 608
Epoch 636
Epoch 636, Loss: 0.000064938, Improvement: -0.000026348, Best Loss: 0.000020552 in Epoch 608
Epoch 637
Epoch 637, Loss: 0.000052592, Improvement: -0.000012345, Best Loss: 0.000020552 in Epoch 608
Epoch 638
Epoch 638, Loss: 0.000041299, Improvement: -0.000011293, Best Loss: 0.000020552 in Epoch 608
Epoch 639
Epoch 639, Loss: 0.000034735, Improvement: -0.000006564, Best Loss: 0.000020552 in Epoch 608
Epoch 640
Epoch 640, Loss: 0.000033029, Improvement: -0.000001706, Best Loss: 0.000020552 in Epoch 608
Epoch 641
Epoch 641, Loss: 0.000031110, Improvement: -0.000001918, Best Loss: 0.000020552 in Epoch 608
Epoch 642
A best model at epoch 642 has been saved with training error 0.000017941.
Epoch 642, Loss: 0.000030958, Improvement: -0.000000152, Best Loss: 0.000017941 in Epoch 642
Epoch 643
Epoch 643, Loss: 0.000030586, Improvement: -0.000000372, Best Loss: 0.000017941 in Epoch 642
Epoch 644
Epoch 644, Loss: 0.000030035, Improvement: -0.000000551, Best Loss: 0.000017941 in Epoch 642
Epoch 645
Epoch 645, Loss: 0.000028718, Improvement: -0.000001316, Best Loss: 0.000017941 in Epoch 642
Epoch 646
Epoch 646, Loss: 0.000027786, Improvement: -0.000000932, Best Loss: 0.000017941 in Epoch 642
Epoch 647
Epoch 647, Loss: 0.000027831, Improvement: 0.000000045, Best Loss: 0.000017941 in Epoch 642
Epoch 648
A best model at epoch 648 has been saved with training error 0.000016927.
Epoch 648, Loss: 0.000027327, Improvement: -0.000000504, Best Loss: 0.000016927 in Epoch 648
Epoch 649
Epoch 649, Loss: 0.000028360, Improvement: 0.000001033, Best Loss: 0.000016927 in Epoch 648
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.000027795, Improvement: -0.000000565, Best Loss: 0.000016927 in Epoch 648
Epoch 651
Epoch 651, Loss: 0.000027728, Improvement: -0.000000067, Best Loss: 0.000016927 in Epoch 648
Epoch 652
Epoch 652, Loss: 0.000027671, Improvement: -0.000000057, Best Loss: 0.000016927 in Epoch 648
Epoch 653
Epoch 653, Loss: 0.000028113, Improvement: 0.000000442, Best Loss: 0.000016927 in Epoch 648
Epoch 654
Epoch 654, Loss: 0.000029252, Improvement: 0.000001139, Best Loss: 0.000016927 in Epoch 648
Epoch 655
A best model at epoch 655 has been saved with training error 0.000015707.
Epoch 655, Loss: 0.000028341, Improvement: -0.000000910, Best Loss: 0.000015707 in Epoch 655
Epoch 656
Epoch 656, Loss: 0.000029984, Improvement: 0.000001643, Best Loss: 0.000015707 in Epoch 655
Epoch 657
Epoch 657, Loss: 0.000047552, Improvement: 0.000017568, Best Loss: 0.000015707 in Epoch 655
Epoch 658
Epoch 658, Loss: 0.000065022, Improvement: 0.000017470, Best Loss: 0.000015707 in Epoch 655
Epoch 659
Epoch 659, Loss: 0.000045594, Improvement: -0.000019428, Best Loss: 0.000015707 in Epoch 655
Epoch 660
Epoch 660, Loss: 0.000045389, Improvement: -0.000000205, Best Loss: 0.000015707 in Epoch 655
Epoch 661
Epoch 661, Loss: 0.000049402, Improvement: 0.000004013, Best Loss: 0.000015707 in Epoch 655
Epoch 662
Epoch 662, Loss: 0.000041229, Improvement: -0.000008173, Best Loss: 0.000015707 in Epoch 655
Epoch 663
Epoch 663, Loss: 0.000039218, Improvement: -0.000002011, Best Loss: 0.000015707 in Epoch 655
Epoch 664
Epoch 664, Loss: 0.000036337, Improvement: -0.000002881, Best Loss: 0.000015707 in Epoch 655
Epoch 665
Epoch 665, Loss: 0.000036378, Improvement: 0.000000041, Best Loss: 0.000015707 in Epoch 655
Epoch 666
Epoch 666, Loss: 0.000038165, Improvement: 0.000001787, Best Loss: 0.000015707 in Epoch 655
Epoch 667
Epoch 667, Loss: 0.000061666, Improvement: 0.000023501, Best Loss: 0.000015707 in Epoch 655
Epoch 668
Epoch 668, Loss: 0.000106302, Improvement: 0.000044636, Best Loss: 0.000015707 in Epoch 655
Epoch 669
Epoch 669, Loss: 0.000129930, Improvement: 0.000023629, Best Loss: 0.000015707 in Epoch 655
Epoch 670
Epoch 670, Loss: 0.000085613, Improvement: -0.000044317, Best Loss: 0.000015707 in Epoch 655
Epoch 671
Epoch 671, Loss: 0.000057593, Improvement: -0.000028020, Best Loss: 0.000015707 in Epoch 655
Epoch 672
Epoch 672, Loss: 0.000060188, Improvement: 0.000002595, Best Loss: 0.000015707 in Epoch 655
Epoch 673
Epoch 673, Loss: 0.000078433, Improvement: 0.000018245, Best Loss: 0.000015707 in Epoch 655
Epoch 674
Epoch 674, Loss: 0.000062846, Improvement: -0.000015587, Best Loss: 0.000015707 in Epoch 655
Epoch 675
Epoch 675, Loss: 0.000048623, Improvement: -0.000014224, Best Loss: 0.000015707 in Epoch 655
Epoch 676
Epoch 676, Loss: 0.000042212, Improvement: -0.000006410, Best Loss: 0.000015707 in Epoch 655
Epoch 677
Epoch 677, Loss: 0.000048461, Improvement: 0.000006249, Best Loss: 0.000015707 in Epoch 655
Epoch 678
Epoch 678, Loss: 0.000044110, Improvement: -0.000004351, Best Loss: 0.000015707 in Epoch 655
Epoch 679
Epoch 679, Loss: 0.000040280, Improvement: -0.000003830, Best Loss: 0.000015707 in Epoch 655
Epoch 680
Epoch 680, Loss: 0.000046876, Improvement: 0.000006596, Best Loss: 0.000015707 in Epoch 655
Epoch 681
Epoch 681, Loss: 0.000040479, Improvement: -0.000006397, Best Loss: 0.000015707 in Epoch 655
Epoch 682
Epoch 682, Loss: 0.000040059, Improvement: -0.000000420, Best Loss: 0.000015707 in Epoch 655
Epoch 683
Epoch 683, Loss: 0.000038925, Improvement: -0.000001134, Best Loss: 0.000015707 in Epoch 655
Epoch 684
Epoch 684, Loss: 0.000036565, Improvement: -0.000002360, Best Loss: 0.000015707 in Epoch 655
Epoch 685
Epoch 685, Loss: 0.000037581, Improvement: 0.000001016, Best Loss: 0.000015707 in Epoch 655
Epoch 686
Epoch 686, Loss: 0.000044901, Improvement: 0.000007320, Best Loss: 0.000015707 in Epoch 655
Epoch 687
Epoch 687, Loss: 0.000066111, Improvement: 0.000021210, Best Loss: 0.000015707 in Epoch 655
Epoch 688
Epoch 688, Loss: 0.000075702, Improvement: 0.000009591, Best Loss: 0.000015707 in Epoch 655
Epoch 689
Epoch 689, Loss: 0.000092658, Improvement: 0.000016956, Best Loss: 0.000015707 in Epoch 655
Epoch 690
Epoch 690, Loss: 0.000100083, Improvement: 0.000007425, Best Loss: 0.000015707 in Epoch 655
Epoch 691
Epoch 691, Loss: 0.000070443, Improvement: -0.000029640, Best Loss: 0.000015707 in Epoch 655
Epoch 692
Epoch 692, Loss: 0.000092652, Improvement: 0.000022209, Best Loss: 0.000015707 in Epoch 655
Epoch 693
Epoch 693, Loss: 0.000078714, Improvement: -0.000013938, Best Loss: 0.000015707 in Epoch 655
Epoch 694
Epoch 694, Loss: 0.000053132, Improvement: -0.000025582, Best Loss: 0.000015707 in Epoch 655
Epoch 695
Epoch 695, Loss: 0.000032889, Improvement: -0.000020243, Best Loss: 0.000015707 in Epoch 655
Epoch 696
Epoch 696, Loss: 0.000031755, Improvement: -0.000001134, Best Loss: 0.000015707 in Epoch 655
Epoch 697
Epoch 697, Loss: 0.000028649, Improvement: -0.000003106, Best Loss: 0.000015707 in Epoch 655
Epoch 698
Epoch 698, Loss: 0.000024162, Improvement: -0.000004487, Best Loss: 0.000015707 in Epoch 655
Epoch 699
Epoch 699, Loss: 0.000024328, Improvement: 0.000000166, Best Loss: 0.000015707 in Epoch 655
Epoch 700
A best model at epoch 700 has been saved with training error 0.000015443.
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.000022810, Improvement: -0.000001518, Best Loss: 0.000015443 in Epoch 700
Epoch 701
Epoch 701, Loss: 0.000023972, Improvement: 0.000001162, Best Loss: 0.000015443 in Epoch 700
Epoch 702
Epoch 702, Loss: 0.000023737, Improvement: -0.000000236, Best Loss: 0.000015443 in Epoch 700
Epoch 703
Epoch 703, Loss: 0.000023205, Improvement: -0.000000531, Best Loss: 0.000015443 in Epoch 700
Epoch 704
Epoch 704, Loss: 0.000030037, Improvement: 0.000006832, Best Loss: 0.000015443 in Epoch 700
Epoch 705
Epoch 705, Loss: 0.000029073, Improvement: -0.000000964, Best Loss: 0.000015443 in Epoch 700
Epoch 706
Epoch 706, Loss: 0.000040621, Improvement: 0.000011548, Best Loss: 0.000015443 in Epoch 700
Epoch 707
Epoch 707, Loss: 0.000041698, Improvement: 0.000001077, Best Loss: 0.000015443 in Epoch 700
Epoch 708
Epoch 708, Loss: 0.000039126, Improvement: -0.000002572, Best Loss: 0.000015443 in Epoch 700
Epoch 709
Epoch 709, Loss: 0.000049056, Improvement: 0.000009929, Best Loss: 0.000015443 in Epoch 700
Epoch 710
Epoch 710, Loss: 0.000040721, Improvement: -0.000008334, Best Loss: 0.000015443 in Epoch 700
Epoch 711
Epoch 711, Loss: 0.000047885, Improvement: 0.000007163, Best Loss: 0.000015443 in Epoch 700
Epoch 712
Epoch 712, Loss: 0.000059428, Improvement: 0.000011543, Best Loss: 0.000015443 in Epoch 700
Epoch 713
Epoch 713, Loss: 0.000066817, Improvement: 0.000007389, Best Loss: 0.000015443 in Epoch 700
Epoch 714
Epoch 714, Loss: 0.000051616, Improvement: -0.000015202, Best Loss: 0.000015443 in Epoch 700
Epoch 715
Epoch 715, Loss: 0.000082217, Improvement: 0.000030601, Best Loss: 0.000015443 in Epoch 700
Epoch 716
Epoch 716, Loss: 0.000072720, Improvement: -0.000009497, Best Loss: 0.000015443 in Epoch 700
Epoch 717
Epoch 717, Loss: 0.000048743, Improvement: -0.000023977, Best Loss: 0.000015443 in Epoch 700
Epoch 718
Epoch 718, Loss: 0.000055661, Improvement: 0.000006918, Best Loss: 0.000015443 in Epoch 700
Epoch 719
Epoch 719, Loss: 0.000057720, Improvement: 0.000002059, Best Loss: 0.000015443 in Epoch 700
Epoch 720
Epoch 720, Loss: 0.000079127, Improvement: 0.000021407, Best Loss: 0.000015443 in Epoch 700
Epoch 721
Epoch 721, Loss: 0.000075120, Improvement: -0.000004007, Best Loss: 0.000015443 in Epoch 700
Epoch 722
Epoch 722, Loss: 0.000066048, Improvement: -0.000009072, Best Loss: 0.000015443 in Epoch 700
Epoch 723
Epoch 723, Loss: 0.000043437, Improvement: -0.000022612, Best Loss: 0.000015443 in Epoch 700
Epoch 724
Epoch 724, Loss: 0.000047195, Improvement: 0.000003758, Best Loss: 0.000015443 in Epoch 700
Epoch 725
Epoch 725, Loss: 0.000052721, Improvement: 0.000005526, Best Loss: 0.000015443 in Epoch 700
Epoch 726
Epoch 726, Loss: 0.000048708, Improvement: -0.000004013, Best Loss: 0.000015443 in Epoch 700
Epoch 727
Epoch 727, Loss: 0.000058089, Improvement: 0.000009380, Best Loss: 0.000015443 in Epoch 700
Epoch 728
Epoch 728, Loss: 0.000078421, Improvement: 0.000020333, Best Loss: 0.000015443 in Epoch 700
Epoch 729
Epoch 729, Loss: 0.000073509, Improvement: -0.000004912, Best Loss: 0.000015443 in Epoch 700
Epoch 730
Epoch 730, Loss: 0.000038467, Improvement: -0.000035043, Best Loss: 0.000015443 in Epoch 700
Epoch 731
Epoch 731, Loss: 0.000027243, Improvement: -0.000011224, Best Loss: 0.000015443 in Epoch 700
Epoch 732
Epoch 732, Loss: 0.000026165, Improvement: -0.000001078, Best Loss: 0.000015443 in Epoch 700
Epoch 733
Epoch 733, Loss: 0.000031787, Improvement: 0.000005622, Best Loss: 0.000015443 in Epoch 700
Epoch 734
Epoch 734, Loss: 0.000027181, Improvement: -0.000004606, Best Loss: 0.000015443 in Epoch 700
Epoch 735
A best model at epoch 735 has been saved with training error 0.000015151.
Epoch 735, Loss: 0.000027383, Improvement: 0.000000201, Best Loss: 0.000015151 in Epoch 735
Epoch 736
Epoch 736, Loss: 0.000028975, Improvement: 0.000001592, Best Loss: 0.000015151 in Epoch 735
Epoch 737
Epoch 737, Loss: 0.000034519, Improvement: 0.000005544, Best Loss: 0.000015151 in Epoch 735
Epoch 738
Epoch 738, Loss: 0.000055937, Improvement: 0.000021419, Best Loss: 0.000015151 in Epoch 735
Epoch 739
Epoch 739, Loss: 0.000049462, Improvement: -0.000006475, Best Loss: 0.000015151 in Epoch 735
Epoch 740
Epoch 740, Loss: 0.000043655, Improvement: -0.000005807, Best Loss: 0.000015151 in Epoch 735
Epoch 741
Epoch 741, Loss: 0.000043630, Improvement: -0.000000025, Best Loss: 0.000015151 in Epoch 735
Epoch 742
Epoch 742, Loss: 0.000041275, Improvement: -0.000002354, Best Loss: 0.000015151 in Epoch 735
Epoch 743
Epoch 743, Loss: 0.000033404, Improvement: -0.000007871, Best Loss: 0.000015151 in Epoch 735
Epoch 744
Epoch 744, Loss: 0.000036150, Improvement: 0.000002746, Best Loss: 0.000015151 in Epoch 735
Epoch 745
Epoch 745, Loss: 0.000039039, Improvement: 0.000002890, Best Loss: 0.000015151 in Epoch 735
Epoch 746
Epoch 746, Loss: 0.000050803, Improvement: 0.000011764, Best Loss: 0.000015151 in Epoch 735
Epoch 747
Epoch 747, Loss: 0.000055723, Improvement: 0.000004920, Best Loss: 0.000015151 in Epoch 735
Epoch 748
Epoch 748, Loss: 0.000047710, Improvement: -0.000008013, Best Loss: 0.000015151 in Epoch 735
Epoch 749
Epoch 749, Loss: 0.000062453, Improvement: 0.000014743, Best Loss: 0.000015151 in Epoch 735
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.000061463, Improvement: -0.000000990, Best Loss: 0.000015151 in Epoch 735
Epoch 751
Epoch 751, Loss: 0.000075140, Improvement: 0.000013677, Best Loss: 0.000015151 in Epoch 735
Epoch 752
Epoch 752, Loss: 0.000064043, Improvement: -0.000011096, Best Loss: 0.000015151 in Epoch 735
Epoch 753
Epoch 753, Loss: 0.000052747, Improvement: -0.000011297, Best Loss: 0.000015151 in Epoch 735
Epoch 754
Epoch 754, Loss: 0.000044450, Improvement: -0.000008296, Best Loss: 0.000015151 in Epoch 735
Epoch 755
Epoch 755, Loss: 0.000028508, Improvement: -0.000015942, Best Loss: 0.000015151 in Epoch 735
Epoch 756
Epoch 756, Loss: 0.000022675, Improvement: -0.000005833, Best Loss: 0.000015151 in Epoch 735
Epoch 757
Epoch 757, Loss: 0.000031693, Improvement: 0.000009018, Best Loss: 0.000015151 in Epoch 735
Epoch 758
Epoch 758, Loss: 0.000025357, Improvement: -0.000006336, Best Loss: 0.000015151 in Epoch 735
Epoch 759
Epoch 759, Loss: 0.000027778, Improvement: 0.000002420, Best Loss: 0.000015151 in Epoch 735
Epoch 760
Epoch 760, Loss: 0.000031306, Improvement: 0.000003528, Best Loss: 0.000015151 in Epoch 735
Epoch 761
Epoch 761, Loss: 0.000026328, Improvement: -0.000004978, Best Loss: 0.000015151 in Epoch 735
Epoch 762
Epoch 762, Loss: 0.000030055, Improvement: 0.000003728, Best Loss: 0.000015151 in Epoch 735
Epoch 763
Epoch 763, Loss: 0.000023979, Improvement: -0.000006076, Best Loss: 0.000015151 in Epoch 735
Epoch 764
A best model at epoch 764 has been saved with training error 0.000014737.
Epoch 764, Loss: 0.000020816, Improvement: -0.000003164, Best Loss: 0.000014737 in Epoch 764
Epoch 765
A best model at epoch 765 has been saved with training error 0.000013536.
Epoch 765, Loss: 0.000020188, Improvement: -0.000000628, Best Loss: 0.000013536 in Epoch 765
Epoch 766
Epoch 766, Loss: 0.000019882, Improvement: -0.000000306, Best Loss: 0.000013536 in Epoch 765
Epoch 767
Epoch 767, Loss: 0.000020101, Improvement: 0.000000219, Best Loss: 0.000013536 in Epoch 765
Epoch 768
Epoch 768, Loss: 0.000018347, Improvement: -0.000001754, Best Loss: 0.000013536 in Epoch 765
Epoch 769
Epoch 769, Loss: 0.000027876, Improvement: 0.000009529, Best Loss: 0.000013536 in Epoch 765
Epoch 770
Epoch 770, Loss: 0.000045259, Improvement: 0.000017383, Best Loss: 0.000013536 in Epoch 765
Epoch 771
Epoch 771, Loss: 0.000070270, Improvement: 0.000025012, Best Loss: 0.000013536 in Epoch 765
Epoch 772
Epoch 772, Loss: 0.000036753, Improvement: -0.000033517, Best Loss: 0.000013536 in Epoch 765
Epoch 773
Epoch 773, Loss: 0.000025953, Improvement: -0.000010801, Best Loss: 0.000013536 in Epoch 765
Epoch 774
Epoch 774, Loss: 0.000027227, Improvement: 0.000001274, Best Loss: 0.000013536 in Epoch 765
Epoch 775
Epoch 775, Loss: 0.000023545, Improvement: -0.000003682, Best Loss: 0.000013536 in Epoch 765
Epoch 776
Epoch 776, Loss: 0.000021607, Improvement: -0.000001938, Best Loss: 0.000013536 in Epoch 765
Epoch 777
A best model at epoch 777 has been saved with training error 0.000010267.
Epoch 777, Loss: 0.000022178, Improvement: 0.000000571, Best Loss: 0.000010267 in Epoch 777
Epoch 778
Epoch 778, Loss: 0.000028305, Improvement: 0.000006127, Best Loss: 0.000010267 in Epoch 777
Epoch 779
Epoch 779, Loss: 0.000028353, Improvement: 0.000000048, Best Loss: 0.000010267 in Epoch 777
Epoch 780
Epoch 780, Loss: 0.000031415, Improvement: 0.000003062, Best Loss: 0.000010267 in Epoch 777
Epoch 781
Epoch 781, Loss: 0.000019213, Improvement: -0.000012201, Best Loss: 0.000010267 in Epoch 777
Epoch 782
Epoch 782, Loss: 0.000017055, Improvement: -0.000002158, Best Loss: 0.000010267 in Epoch 777
Epoch 783
Epoch 783, Loss: 0.000015153, Improvement: -0.000001902, Best Loss: 0.000010267 in Epoch 777
Epoch 784
Epoch 784, Loss: 0.000018595, Improvement: 0.000003442, Best Loss: 0.000010267 in Epoch 777
Epoch 785
Epoch 785, Loss: 0.000022535, Improvement: 0.000003940, Best Loss: 0.000010267 in Epoch 777
Epoch 786
Epoch 786, Loss: 0.000026641, Improvement: 0.000004106, Best Loss: 0.000010267 in Epoch 777
Epoch 787
Epoch 787, Loss: 0.000019909, Improvement: -0.000006732, Best Loss: 0.000010267 in Epoch 777
Epoch 788
Epoch 788, Loss: 0.000031085, Improvement: 0.000011176, Best Loss: 0.000010267 in Epoch 777
Epoch 789
Epoch 789, Loss: 0.000042159, Improvement: 0.000011074, Best Loss: 0.000010267 in Epoch 777
Epoch 790
Epoch 790, Loss: 0.000048388, Improvement: 0.000006229, Best Loss: 0.000010267 in Epoch 777
Epoch 791
Epoch 791, Loss: 0.000051857, Improvement: 0.000003469, Best Loss: 0.000010267 in Epoch 777
Epoch 792
Epoch 792, Loss: 0.000070922, Improvement: 0.000019065, Best Loss: 0.000010267 in Epoch 777
Epoch 793
Epoch 793, Loss: 0.000098145, Improvement: 0.000027223, Best Loss: 0.000010267 in Epoch 777
Epoch 794
Epoch 794, Loss: 0.000070643, Improvement: -0.000027502, Best Loss: 0.000010267 in Epoch 777
Epoch 795
Epoch 795, Loss: 0.000085446, Improvement: 0.000014803, Best Loss: 0.000010267 in Epoch 777
Epoch 796
Epoch 796, Loss: 0.000059092, Improvement: -0.000026353, Best Loss: 0.000010267 in Epoch 777
Epoch 797
Epoch 797, Loss: 0.000038024, Improvement: -0.000021068, Best Loss: 0.000010267 in Epoch 777
Epoch 798
Epoch 798, Loss: 0.000023870, Improvement: -0.000014154, Best Loss: 0.000010267 in Epoch 777
Epoch 799
Epoch 799, Loss: 0.000022526, Improvement: -0.000001345, Best Loss: 0.000010267 in Epoch 777
Epoch 800
A best model at epoch 800 has been saved with training error 0.000009832.
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.000015955, Improvement: -0.000006571, Best Loss: 0.000009832 in Epoch 800
Epoch 801
Epoch 801, Loss: 0.000013289, Improvement: -0.000002666, Best Loss: 0.000009832 in Epoch 800
Epoch 802
A best model at epoch 802 has been saved with training error 0.000008564.
Epoch 802, Loss: 0.000012695, Improvement: -0.000000594, Best Loss: 0.000008564 in Epoch 802
Epoch 803
Epoch 803, Loss: 0.000012639, Improvement: -0.000000057, Best Loss: 0.000008564 in Epoch 802
Epoch 804
Epoch 804, Loss: 0.000012910, Improvement: 0.000000271, Best Loss: 0.000008564 in Epoch 802
Epoch 805
Epoch 805, Loss: 0.000012425, Improvement: -0.000000485, Best Loss: 0.000008564 in Epoch 802
Epoch 806
A best model at epoch 806 has been saved with training error 0.000008480.
A best model at epoch 806 has been saved with training error 0.000008055.
Epoch 806, Loss: 0.000012202, Improvement: -0.000000222, Best Loss: 0.000008055 in Epoch 806
Epoch 807
Epoch 807, Loss: 0.000011465, Improvement: -0.000000738, Best Loss: 0.000008055 in Epoch 806
Epoch 808
Epoch 808, Loss: 0.000011402, Improvement: -0.000000063, Best Loss: 0.000008055 in Epoch 806
Epoch 809
Epoch 809, Loss: 0.000011448, Improvement: 0.000000046, Best Loss: 0.000008055 in Epoch 806
Epoch 810
Epoch 810, Loss: 0.000012146, Improvement: 0.000000698, Best Loss: 0.000008055 in Epoch 806
Epoch 811
A best model at epoch 811 has been saved with training error 0.000007740.
Epoch 811, Loss: 0.000011536, Improvement: -0.000000610, Best Loss: 0.000007740 in Epoch 811
Epoch 812
Epoch 812, Loss: 0.000011203, Improvement: -0.000000333, Best Loss: 0.000007740 in Epoch 811
Epoch 813
Epoch 813, Loss: 0.000011414, Improvement: 0.000000210, Best Loss: 0.000007740 in Epoch 811
Epoch 814
Epoch 814, Loss: 0.000011585, Improvement: 0.000000171, Best Loss: 0.000007740 in Epoch 811
Epoch 815
Epoch 815, Loss: 0.000012224, Improvement: 0.000000639, Best Loss: 0.000007740 in Epoch 811
Epoch 816
Epoch 816, Loss: 0.000014330, Improvement: 0.000002106, Best Loss: 0.000007740 in Epoch 811
Epoch 817
Epoch 817, Loss: 0.000013339, Improvement: -0.000000991, Best Loss: 0.000007740 in Epoch 811
Epoch 818
Epoch 818, Loss: 0.000013368, Improvement: 0.000000029, Best Loss: 0.000007740 in Epoch 811
Epoch 819
Epoch 819, Loss: 0.000013198, Improvement: -0.000000170, Best Loss: 0.000007740 in Epoch 811
Epoch 820
Epoch 820, Loss: 0.000017719, Improvement: 0.000004521, Best Loss: 0.000007740 in Epoch 811
Epoch 821
Epoch 821, Loss: 0.000019221, Improvement: 0.000001502, Best Loss: 0.000007740 in Epoch 811
Epoch 822
Epoch 822, Loss: 0.000023963, Improvement: 0.000004742, Best Loss: 0.000007740 in Epoch 811
Epoch 823
Epoch 823, Loss: 0.000028171, Improvement: 0.000004208, Best Loss: 0.000007740 in Epoch 811
Epoch 824
Epoch 824, Loss: 0.000047084, Improvement: 0.000018912, Best Loss: 0.000007740 in Epoch 811
Epoch 825
Epoch 825, Loss: 0.000068218, Improvement: 0.000021134, Best Loss: 0.000007740 in Epoch 811
Epoch 826
Epoch 826, Loss: 0.000053083, Improvement: -0.000015135, Best Loss: 0.000007740 in Epoch 811
Epoch 827
Epoch 827, Loss: 0.000037950, Improvement: -0.000015133, Best Loss: 0.000007740 in Epoch 811
Epoch 828
Epoch 828, Loss: 0.000027494, Improvement: -0.000010456, Best Loss: 0.000007740 in Epoch 811
Epoch 829
Epoch 829, Loss: 0.000032007, Improvement: 0.000004513, Best Loss: 0.000007740 in Epoch 811
Epoch 830
Epoch 830, Loss: 0.000043096, Improvement: 0.000011089, Best Loss: 0.000007740 in Epoch 811
Epoch 831
Epoch 831, Loss: 0.000030394, Improvement: -0.000012702, Best Loss: 0.000007740 in Epoch 811
Epoch 832
Epoch 832, Loss: 0.000031016, Improvement: 0.000000623, Best Loss: 0.000007740 in Epoch 811
Epoch 833
Epoch 833, Loss: 0.000033703, Improvement: 0.000002687, Best Loss: 0.000007740 in Epoch 811
Epoch 834
Epoch 834, Loss: 0.000039002, Improvement: 0.000005299, Best Loss: 0.000007740 in Epoch 811
Epoch 835
Epoch 835, Loss: 0.000025800, Improvement: -0.000013202, Best Loss: 0.000007740 in Epoch 811
Epoch 836
Epoch 836, Loss: 0.000018236, Improvement: -0.000007563, Best Loss: 0.000007740 in Epoch 811
Epoch 837
Epoch 837, Loss: 0.000019729, Improvement: 0.000001493, Best Loss: 0.000007740 in Epoch 811
Epoch 838
Epoch 838, Loss: 0.000023851, Improvement: 0.000004121, Best Loss: 0.000007740 in Epoch 811
Epoch 839
Epoch 839, Loss: 0.000028197, Improvement: 0.000004346, Best Loss: 0.000007740 in Epoch 811
Epoch 840
Epoch 840, Loss: 0.000039103, Improvement: 0.000010906, Best Loss: 0.000007740 in Epoch 811
Epoch 841
Epoch 841, Loss: 0.000058799, Improvement: 0.000019696, Best Loss: 0.000007740 in Epoch 811
Epoch 842
Epoch 842, Loss: 0.000071766, Improvement: 0.000012967, Best Loss: 0.000007740 in Epoch 811
Epoch 843
Epoch 843, Loss: 0.000061628, Improvement: -0.000010138, Best Loss: 0.000007740 in Epoch 811
Epoch 844
Epoch 844, Loss: 0.000043722, Improvement: -0.000017906, Best Loss: 0.000007740 in Epoch 811
Epoch 845
Epoch 845, Loss: 0.000043258, Improvement: -0.000000464, Best Loss: 0.000007740 in Epoch 811
Epoch 846
Epoch 846, Loss: 0.000078284, Improvement: 0.000035026, Best Loss: 0.000007740 in Epoch 811
Epoch 847
Epoch 847, Loss: 0.000052499, Improvement: -0.000025785, Best Loss: 0.000007740 in Epoch 811
Epoch 848
Epoch 848, Loss: 0.000032295, Improvement: -0.000020204, Best Loss: 0.000007740 in Epoch 811
Epoch 849
Epoch 849, Loss: 0.000020445, Improvement: -0.000011850, Best Loss: 0.000007740 in Epoch 811
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.000017699, Improvement: -0.000002746, Best Loss: 0.000007740 in Epoch 811
Epoch 851
Epoch 851, Loss: 0.000014350, Improvement: -0.000003348, Best Loss: 0.000007740 in Epoch 811
Epoch 852
Epoch 852, Loss: 0.000015776, Improvement: 0.000001425, Best Loss: 0.000007740 in Epoch 811
Epoch 853
Epoch 853, Loss: 0.000012002, Improvement: -0.000003774, Best Loss: 0.000007740 in Epoch 811
Epoch 854
A best model at epoch 854 has been saved with training error 0.000006912.
Epoch 854, Loss: 0.000010592, Improvement: -0.000001410, Best Loss: 0.000006912 in Epoch 854
Epoch 855
Epoch 855, Loss: 0.000010456, Improvement: -0.000000135, Best Loss: 0.000006912 in Epoch 854
Epoch 856
A best model at epoch 856 has been saved with training error 0.000006790.
Epoch 856, Loss: 0.000009756, Improvement: -0.000000700, Best Loss: 0.000006790 in Epoch 856
Epoch 857
A best model at epoch 857 has been saved with training error 0.000006001.
Epoch 857, Loss: 0.000009566, Improvement: -0.000000190, Best Loss: 0.000006001 in Epoch 857
Epoch 858
Epoch 858, Loss: 0.000009432, Improvement: -0.000000134, Best Loss: 0.000006001 in Epoch 857
Epoch 859
Epoch 859, Loss: 0.000008980, Improvement: -0.000000452, Best Loss: 0.000006001 in Epoch 857
Epoch 860
Epoch 860, Loss: 0.000009605, Improvement: 0.000000625, Best Loss: 0.000006001 in Epoch 857
Epoch 861
Epoch 861, Loss: 0.000010539, Improvement: 0.000000935, Best Loss: 0.000006001 in Epoch 857
Epoch 862
Epoch 862, Loss: 0.000010332, Improvement: -0.000000207, Best Loss: 0.000006001 in Epoch 857
Epoch 863
Epoch 863, Loss: 0.000010060, Improvement: -0.000000273, Best Loss: 0.000006001 in Epoch 857
Epoch 864
Epoch 864, Loss: 0.000011260, Improvement: 0.000001200, Best Loss: 0.000006001 in Epoch 857
Epoch 865
Epoch 865, Loss: 0.000014263, Improvement: 0.000003003, Best Loss: 0.000006001 in Epoch 857
Epoch 866
Epoch 866, Loss: 0.000017288, Improvement: 0.000003025, Best Loss: 0.000006001 in Epoch 857
Epoch 867
Epoch 867, Loss: 0.000019403, Improvement: 0.000002116, Best Loss: 0.000006001 in Epoch 857
Epoch 868
Epoch 868, Loss: 0.000070845, Improvement: 0.000051442, Best Loss: 0.000006001 in Epoch 857
Epoch 869
Epoch 869, Loss: 0.000053222, Improvement: -0.000017624, Best Loss: 0.000006001 in Epoch 857
Epoch 870
Epoch 870, Loss: 0.000032558, Improvement: -0.000020664, Best Loss: 0.000006001 in Epoch 857
Epoch 871
Epoch 871, Loss: 0.000030498, Improvement: -0.000002060, Best Loss: 0.000006001 in Epoch 857
Epoch 872
Epoch 872, Loss: 0.000027873, Improvement: -0.000002625, Best Loss: 0.000006001 in Epoch 857
Epoch 873
Epoch 873, Loss: 0.000016886, Improvement: -0.000010988, Best Loss: 0.000006001 in Epoch 857
Epoch 874
Epoch 874, Loss: 0.000012072, Improvement: -0.000004813, Best Loss: 0.000006001 in Epoch 857
Epoch 875
Epoch 875, Loss: 0.000009747, Improvement: -0.000002325, Best Loss: 0.000006001 in Epoch 857
Epoch 876
Epoch 876, Loss: 0.000010777, Improvement: 0.000001030, Best Loss: 0.000006001 in Epoch 857
Epoch 877
Epoch 877, Loss: 0.000011560, Improvement: 0.000000783, Best Loss: 0.000006001 in Epoch 857
Epoch 878
Epoch 878, Loss: 0.000012929, Improvement: 0.000001369, Best Loss: 0.000006001 in Epoch 857
Epoch 879
Epoch 879, Loss: 0.000012321, Improvement: -0.000000608, Best Loss: 0.000006001 in Epoch 857
Epoch 880
Epoch 880, Loss: 0.000010062, Improvement: -0.000002259, Best Loss: 0.000006001 in Epoch 857
Epoch 881
Epoch 881, Loss: 0.000008857, Improvement: -0.000001205, Best Loss: 0.000006001 in Epoch 857
Epoch 882
Epoch 882, Loss: 0.000011555, Improvement: 0.000002698, Best Loss: 0.000006001 in Epoch 857
Epoch 883
Epoch 883, Loss: 0.000011675, Improvement: 0.000000120, Best Loss: 0.000006001 in Epoch 857
Epoch 884
Epoch 884, Loss: 0.000011447, Improvement: -0.000000228, Best Loss: 0.000006001 in Epoch 857
Epoch 885
Epoch 885, Loss: 0.000017618, Improvement: 0.000006171, Best Loss: 0.000006001 in Epoch 857
Epoch 886
Epoch 886, Loss: 0.000020923, Improvement: 0.000003304, Best Loss: 0.000006001 in Epoch 857
Epoch 887
Epoch 887, Loss: 0.000021541, Improvement: 0.000000619, Best Loss: 0.000006001 in Epoch 857
Epoch 888
Epoch 888, Loss: 0.000045875, Improvement: 0.000024333, Best Loss: 0.000006001 in Epoch 857
Epoch 889
Epoch 889, Loss: 0.000096704, Improvement: 0.000050829, Best Loss: 0.000006001 in Epoch 857
Epoch 890
Epoch 890, Loss: 0.000053534, Improvement: -0.000043170, Best Loss: 0.000006001 in Epoch 857
Epoch 891
Epoch 891, Loss: 0.000036912, Improvement: -0.000016622, Best Loss: 0.000006001 in Epoch 857
Epoch 892
Epoch 892, Loss: 0.000022833, Improvement: -0.000014079, Best Loss: 0.000006001 in Epoch 857
Epoch 893
Epoch 893, Loss: 0.000015625, Improvement: -0.000007208, Best Loss: 0.000006001 in Epoch 857
Epoch 894
Epoch 894, Loss: 0.000013396, Improvement: -0.000002229, Best Loss: 0.000006001 in Epoch 857
Epoch 895
Epoch 895, Loss: 0.000013022, Improvement: -0.000000374, Best Loss: 0.000006001 in Epoch 857
Epoch 896
Epoch 896, Loss: 0.000010847, Improvement: -0.000002174, Best Loss: 0.000006001 in Epoch 857
Epoch 897
Epoch 897, Loss: 0.000019668, Improvement: 0.000008821, Best Loss: 0.000006001 in Epoch 857
Epoch 898
Epoch 898, Loss: 0.000013170, Improvement: -0.000006498, Best Loss: 0.000006001 in Epoch 857
Epoch 899
Epoch 899, Loss: 0.000010486, Improvement: -0.000002684, Best Loss: 0.000006001 in Epoch 857
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.000013401, Improvement: 0.000002916, Best Loss: 0.000006001 in Epoch 857
Epoch 901
Epoch 901, Loss: 0.000014354, Improvement: 0.000000953, Best Loss: 0.000006001 in Epoch 857
Epoch 902
Epoch 902, Loss: 0.000014014, Improvement: -0.000000341, Best Loss: 0.000006001 in Epoch 857
Epoch 903
Epoch 903, Loss: 0.000013937, Improvement: -0.000000076, Best Loss: 0.000006001 in Epoch 857
Epoch 904
Epoch 904, Loss: 0.000013198, Improvement: -0.000000739, Best Loss: 0.000006001 in Epoch 857
Epoch 905
Epoch 905, Loss: 0.000015384, Improvement: 0.000002186, Best Loss: 0.000006001 in Epoch 857
Epoch 906
Epoch 906, Loss: 0.000034637, Improvement: 0.000019253, Best Loss: 0.000006001 in Epoch 857
Epoch 907
Epoch 907, Loss: 0.000026976, Improvement: -0.000007660, Best Loss: 0.000006001 in Epoch 857
Epoch 908
Epoch 908, Loss: 0.000019698, Improvement: -0.000007279, Best Loss: 0.000006001 in Epoch 857
Epoch 909
Epoch 909, Loss: 0.000025285, Improvement: 0.000005588, Best Loss: 0.000006001 in Epoch 857
Epoch 910
Epoch 910, Loss: 0.000031864, Improvement: 0.000006579, Best Loss: 0.000006001 in Epoch 857
Epoch 911
Epoch 911, Loss: 0.000025701, Improvement: -0.000006163, Best Loss: 0.000006001 in Epoch 857
Epoch 912
Epoch 912, Loss: 0.000019479, Improvement: -0.000006222, Best Loss: 0.000006001 in Epoch 857
Epoch 913
Epoch 913, Loss: 0.000017728, Improvement: -0.000001752, Best Loss: 0.000006001 in Epoch 857
Epoch 914
Epoch 914, Loss: 0.000016339, Improvement: -0.000001388, Best Loss: 0.000006001 in Epoch 857
Epoch 915
Epoch 915, Loss: 0.000047084, Improvement: 0.000030744, Best Loss: 0.000006001 in Epoch 857
Epoch 916
Epoch 916, Loss: 0.000031823, Improvement: -0.000015261, Best Loss: 0.000006001 in Epoch 857
Epoch 917
Epoch 917, Loss: 0.000021388, Improvement: -0.000010435, Best Loss: 0.000006001 in Epoch 857
Epoch 918
Epoch 918, Loss: 0.000019945, Improvement: -0.000001443, Best Loss: 0.000006001 in Epoch 857
Epoch 919
Epoch 919, Loss: 0.000017890, Improvement: -0.000002055, Best Loss: 0.000006001 in Epoch 857
Epoch 920
Epoch 920, Loss: 0.000014299, Improvement: -0.000003592, Best Loss: 0.000006001 in Epoch 857
Epoch 921
Epoch 921, Loss: 0.000013086, Improvement: -0.000001212, Best Loss: 0.000006001 in Epoch 857
Epoch 922
Epoch 922, Loss: 0.000010004, Improvement: -0.000003082, Best Loss: 0.000006001 in Epoch 857
Epoch 923
Epoch 923, Loss: 0.000010840, Improvement: 0.000000836, Best Loss: 0.000006001 in Epoch 857
Epoch 924
Epoch 924, Loss: 0.000010835, Improvement: -0.000000005, Best Loss: 0.000006001 in Epoch 857
Epoch 925
Epoch 925, Loss: 0.000009862, Improvement: -0.000000972, Best Loss: 0.000006001 in Epoch 857
Epoch 926
Epoch 926, Loss: 0.000008911, Improvement: -0.000000951, Best Loss: 0.000006001 in Epoch 857
Epoch 927
Epoch 927, Loss: 0.000012192, Improvement: 0.000003281, Best Loss: 0.000006001 in Epoch 857
Epoch 928
Epoch 928, Loss: 0.000017828, Improvement: 0.000005636, Best Loss: 0.000006001 in Epoch 857
Epoch 929
Epoch 929, Loss: 0.000021409, Improvement: 0.000003581, Best Loss: 0.000006001 in Epoch 857
Epoch 930
Epoch 930, Loss: 0.000020692, Improvement: -0.000000717, Best Loss: 0.000006001 in Epoch 857
Epoch 931
Epoch 931, Loss: 0.000024966, Improvement: 0.000004274, Best Loss: 0.000006001 in Epoch 857
Epoch 932
Epoch 932, Loss: 0.000026568, Improvement: 0.000001602, Best Loss: 0.000006001 in Epoch 857
Epoch 933
Epoch 933, Loss: 0.000037703, Improvement: 0.000011135, Best Loss: 0.000006001 in Epoch 857
Epoch 934
Epoch 934, Loss: 0.000052369, Improvement: 0.000014667, Best Loss: 0.000006001 in Epoch 857
Epoch 935
Epoch 935, Loss: 0.000048117, Improvement: -0.000004253, Best Loss: 0.000006001 in Epoch 857
Epoch 936
Epoch 936, Loss: 0.000040887, Improvement: -0.000007229, Best Loss: 0.000006001 in Epoch 857
Epoch 937
Epoch 937, Loss: 0.000038112, Improvement: -0.000002775, Best Loss: 0.000006001 in Epoch 857
Epoch 938
Epoch 938, Loss: 0.000038166, Improvement: 0.000000054, Best Loss: 0.000006001 in Epoch 857
Epoch 939
Epoch 939, Loss: 0.000027125, Improvement: -0.000011041, Best Loss: 0.000006001 in Epoch 857
Epoch 940
Epoch 940, Loss: 0.000030664, Improvement: 0.000003538, Best Loss: 0.000006001 in Epoch 857
Epoch 941
Epoch 941, Loss: 0.000028504, Improvement: -0.000002160, Best Loss: 0.000006001 in Epoch 857
Epoch 942
Epoch 942, Loss: 0.000023515, Improvement: -0.000004988, Best Loss: 0.000006001 in Epoch 857
Epoch 943
Epoch 943, Loss: 0.000015881, Improvement: -0.000007635, Best Loss: 0.000006001 in Epoch 857
Epoch 944
Epoch 944, Loss: 0.000013983, Improvement: -0.000001898, Best Loss: 0.000006001 in Epoch 857
Epoch 945
Epoch 945, Loss: 0.000015555, Improvement: 0.000001572, Best Loss: 0.000006001 in Epoch 857
Epoch 946
Epoch 946, Loss: 0.000016878, Improvement: 0.000001323, Best Loss: 0.000006001 in Epoch 857
Epoch 947
Epoch 947, Loss: 0.000011820, Improvement: -0.000005058, Best Loss: 0.000006001 in Epoch 857
Epoch 948
Epoch 948, Loss: 0.000011460, Improvement: -0.000000360, Best Loss: 0.000006001 in Epoch 857
Epoch 949
Epoch 949, Loss: 0.000011584, Improvement: 0.000000124, Best Loss: 0.000006001 in Epoch 857
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.000009660, Improvement: -0.000001924, Best Loss: 0.000006001 in Epoch 857
Epoch 951
A best model at epoch 951 has been saved with training error 0.000004617.
Epoch 951, Loss: 0.000007386, Improvement: -0.000002274, Best Loss: 0.000004617 in Epoch 951
Epoch 952
Epoch 952, Loss: 0.000008743, Improvement: 0.000001357, Best Loss: 0.000004617 in Epoch 951
Epoch 953
Epoch 953, Loss: 0.000007820, Improvement: -0.000000923, Best Loss: 0.000004617 in Epoch 951
Epoch 954
Epoch 954, Loss: 0.000008224, Improvement: 0.000000404, Best Loss: 0.000004617 in Epoch 951
Epoch 955
Epoch 955, Loss: 0.000010412, Improvement: 0.000002188, Best Loss: 0.000004617 in Epoch 951
Epoch 956
Epoch 956, Loss: 0.000009485, Improvement: -0.000000927, Best Loss: 0.000004617 in Epoch 951
Epoch 957
Epoch 957, Loss: 0.000010237, Improvement: 0.000000752, Best Loss: 0.000004617 in Epoch 951
Epoch 958
Epoch 958, Loss: 0.000009162, Improvement: -0.000001076, Best Loss: 0.000004617 in Epoch 951
Epoch 959
Epoch 959, Loss: 0.000008391, Improvement: -0.000000770, Best Loss: 0.000004617 in Epoch 951
Epoch 960
Epoch 960, Loss: 0.000009841, Improvement: 0.000001450, Best Loss: 0.000004617 in Epoch 951
Epoch 961
Epoch 961, Loss: 0.000023978, Improvement: 0.000014137, Best Loss: 0.000004617 in Epoch 951
Epoch 962
Epoch 962, Loss: 0.000024170, Improvement: 0.000000192, Best Loss: 0.000004617 in Epoch 951
Epoch 963
Epoch 963, Loss: 0.000025717, Improvement: 0.000001546, Best Loss: 0.000004617 in Epoch 951
Epoch 964
Epoch 964, Loss: 0.000034214, Improvement: 0.000008498, Best Loss: 0.000004617 in Epoch 951
Epoch 965
Epoch 965, Loss: 0.000041993, Improvement: 0.000007779, Best Loss: 0.000004617 in Epoch 951
Epoch 966
Epoch 966, Loss: 0.000054021, Improvement: 0.000012027, Best Loss: 0.000004617 in Epoch 951
Epoch 967
Epoch 967, Loss: 0.000028071, Improvement: -0.000025950, Best Loss: 0.000004617 in Epoch 951
Epoch 968
Epoch 968, Loss: 0.000019683, Improvement: -0.000008388, Best Loss: 0.000004617 in Epoch 951
Epoch 969
Epoch 969, Loss: 0.000014692, Improvement: -0.000004991, Best Loss: 0.000004617 in Epoch 951
Epoch 970
Epoch 970, Loss: 0.000013680, Improvement: -0.000001012, Best Loss: 0.000004617 in Epoch 951
Epoch 971
Epoch 971, Loss: 0.000019015, Improvement: 0.000005335, Best Loss: 0.000004617 in Epoch 951
Epoch 972
Epoch 972, Loss: 0.000015939, Improvement: -0.000003076, Best Loss: 0.000004617 in Epoch 951
Epoch 973
Epoch 973, Loss: 0.000013573, Improvement: -0.000002366, Best Loss: 0.000004617 in Epoch 951
Epoch 974
Epoch 974, Loss: 0.000009917, Improvement: -0.000003656, Best Loss: 0.000004617 in Epoch 951
Epoch 975
Epoch 975, Loss: 0.000009259, Improvement: -0.000000657, Best Loss: 0.000004617 in Epoch 951
Epoch 976
Epoch 976, Loss: 0.000014412, Improvement: 0.000005153, Best Loss: 0.000004617 in Epoch 951
Epoch 977
Epoch 977, Loss: 0.000022452, Improvement: 0.000008040, Best Loss: 0.000004617 in Epoch 951
Epoch 978
Epoch 978, Loss: 0.000022399, Improvement: -0.000000053, Best Loss: 0.000004617 in Epoch 951
Epoch 979
Epoch 979, Loss: 0.000022475, Improvement: 0.000000076, Best Loss: 0.000004617 in Epoch 951
Epoch 980
Epoch 980, Loss: 0.000044504, Improvement: 0.000022029, Best Loss: 0.000004617 in Epoch 951
Epoch 981
Epoch 981, Loss: 0.000050190, Improvement: 0.000005686, Best Loss: 0.000004617 in Epoch 951
Epoch 982
Epoch 982, Loss: 0.000029375, Improvement: -0.000020816, Best Loss: 0.000004617 in Epoch 951
Epoch 983
Epoch 983, Loss: 0.000023169, Improvement: -0.000006206, Best Loss: 0.000004617 in Epoch 951
Epoch 984
Epoch 984, Loss: 0.000015011, Improvement: -0.000008157, Best Loss: 0.000004617 in Epoch 951
Epoch 985
Epoch 985, Loss: 0.000009728, Improvement: -0.000005283, Best Loss: 0.000004617 in Epoch 951
Epoch 986
Epoch 986, Loss: 0.000013537, Improvement: 0.000003809, Best Loss: 0.000004617 in Epoch 951
Epoch 987
Epoch 987, Loss: 0.000017866, Improvement: 0.000004329, Best Loss: 0.000004617 in Epoch 951
Epoch 988
Epoch 988, Loss: 0.000017459, Improvement: -0.000000407, Best Loss: 0.000004617 in Epoch 951
Epoch 989
Epoch 989, Loss: 0.000013948, Improvement: -0.000003511, Best Loss: 0.000004617 in Epoch 951
Epoch 990
Epoch 990, Loss: 0.000013396, Improvement: -0.000000552, Best Loss: 0.000004617 in Epoch 951
Epoch 991
Epoch 991, Loss: 0.000011717, Improvement: -0.000001679, Best Loss: 0.000004617 in Epoch 951
Epoch 992
Epoch 992, Loss: 0.000008814, Improvement: -0.000002903, Best Loss: 0.000004617 in Epoch 951
Epoch 993
Epoch 993, Loss: 0.000007986, Improvement: -0.000000828, Best Loss: 0.000004617 in Epoch 951
Epoch 994
Epoch 994, Loss: 0.000006841, Improvement: -0.000001145, Best Loss: 0.000004617 in Epoch 951
Epoch 995
Epoch 995, Loss: 0.000006428, Improvement: -0.000000413, Best Loss: 0.000004617 in Epoch 951
Epoch 996
A best model at epoch 996 has been saved with training error 0.000004578.
A best model at epoch 996 has been saved with training error 0.000004499.
Epoch 996, Loss: 0.000005850, Improvement: -0.000000579, Best Loss: 0.000004499 in Epoch 996
Epoch 997
Epoch 997, Loss: 0.000006308, Improvement: 0.000000459, Best Loss: 0.000004499 in Epoch 996
Epoch 998
Epoch 998, Loss: 0.000006583, Improvement: 0.000000275, Best Loss: 0.000004499 in Epoch 996
Epoch 999
Epoch 999, Loss: 0.000007405, Improvement: 0.000000822, Best Loss: 0.000004499 in Epoch 996
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.000008585, Improvement: 0.000001180, Best Loss: 0.000004499 in Epoch 996
Epoch 1001
Epoch 1001, Loss: 0.000014909, Improvement: 0.000006324, Best Loss: 0.000004499 in Epoch 996
Epoch 1002
Epoch 1002, Loss: 0.000022408, Improvement: 0.000007499, Best Loss: 0.000004499 in Epoch 996
Epoch 1003
Epoch 1003, Loss: 0.000020429, Improvement: -0.000001979, Best Loss: 0.000004499 in Epoch 996
Epoch 1004
Epoch 1004, Loss: 0.000020098, Improvement: -0.000000331, Best Loss: 0.000004499 in Epoch 996
Epoch 1005
Epoch 1005, Loss: 0.000023446, Improvement: 0.000003348, Best Loss: 0.000004499 in Epoch 996
Epoch 1006
Epoch 1006, Loss: 0.000024304, Improvement: 0.000000858, Best Loss: 0.000004499 in Epoch 996
Epoch 1007
Epoch 1007, Loss: 0.000024751, Improvement: 0.000000447, Best Loss: 0.000004499 in Epoch 996
Epoch 1008
Epoch 1008, Loss: 0.000021839, Improvement: -0.000002912, Best Loss: 0.000004499 in Epoch 996
Epoch 1009
Epoch 1009, Loss: 0.000017070, Improvement: -0.000004769, Best Loss: 0.000004499 in Epoch 996
Epoch 1010
Epoch 1010, Loss: 0.000013381, Improvement: -0.000003690, Best Loss: 0.000004499 in Epoch 996
Epoch 1011
Epoch 1011, Loss: 0.000017223, Improvement: 0.000003842, Best Loss: 0.000004499 in Epoch 996
Epoch 1012
Epoch 1012, Loss: 0.000018918, Improvement: 0.000001695, Best Loss: 0.000004499 in Epoch 996
Epoch 1013
Epoch 1013, Loss: 0.000020497, Improvement: 0.000001580, Best Loss: 0.000004499 in Epoch 996
Epoch 1014
Epoch 1014, Loss: 0.000015408, Improvement: -0.000005090, Best Loss: 0.000004499 in Epoch 996
Epoch 1015
Epoch 1015, Loss: 0.000019508, Improvement: 0.000004100, Best Loss: 0.000004499 in Epoch 996
Epoch 1016
Epoch 1016, Loss: 0.000018226, Improvement: -0.000001282, Best Loss: 0.000004499 in Epoch 996
Epoch 1017
Epoch 1017, Loss: 0.000012402, Improvement: -0.000005824, Best Loss: 0.000004499 in Epoch 996
Epoch 1018
Epoch 1018, Loss: 0.000015600, Improvement: 0.000003198, Best Loss: 0.000004499 in Epoch 996
Epoch 1019
Epoch 1019, Loss: 0.000045061, Improvement: 0.000029461, Best Loss: 0.000004499 in Epoch 996
Epoch 1020
Epoch 1020, Loss: 0.000052604, Improvement: 0.000007542, Best Loss: 0.000004499 in Epoch 996
Epoch 1021
Epoch 1021, Loss: 0.000030801, Improvement: -0.000021803, Best Loss: 0.000004499 in Epoch 996
Epoch 1022
Epoch 1022, Loss: 0.000020198, Improvement: -0.000010603, Best Loss: 0.000004499 in Epoch 996
Epoch 1023
Epoch 1023, Loss: 0.000035076, Improvement: 0.000014878, Best Loss: 0.000004499 in Epoch 996
Epoch 1024
Epoch 1024, Loss: 0.000031289, Improvement: -0.000003787, Best Loss: 0.000004499 in Epoch 996
Epoch 1025
Epoch 1025, Loss: 0.000028446, Improvement: -0.000002843, Best Loss: 0.000004499 in Epoch 996
Epoch 1026
Epoch 1026, Loss: 0.000017255, Improvement: -0.000011190, Best Loss: 0.000004499 in Epoch 996
Epoch 1027
Epoch 1027, Loss: 0.000012726, Improvement: -0.000004529, Best Loss: 0.000004499 in Epoch 996
Epoch 1028
Epoch 1028, Loss: 0.000010226, Improvement: -0.000002500, Best Loss: 0.000004499 in Epoch 996
Epoch 1029
Epoch 1029, Loss: 0.000009806, Improvement: -0.000000420, Best Loss: 0.000004499 in Epoch 996
Epoch 1030
Epoch 1030, Loss: 0.000009353, Improvement: -0.000000453, Best Loss: 0.000004499 in Epoch 996
Epoch 1031
Epoch 1031, Loss: 0.000008600, Improvement: -0.000000753, Best Loss: 0.000004499 in Epoch 996
Epoch 1032
Epoch 1032, Loss: 0.000012286, Improvement: 0.000003687, Best Loss: 0.000004499 in Epoch 996
Epoch 1033
Epoch 1033, Loss: 0.000021451, Improvement: 0.000009164, Best Loss: 0.000004499 in Epoch 996
Epoch 1034
Epoch 1034, Loss: 0.000024137, Improvement: 0.000002687, Best Loss: 0.000004499 in Epoch 996
Epoch 1035
Epoch 1035, Loss: 0.000021241, Improvement: -0.000002896, Best Loss: 0.000004499 in Epoch 996
Epoch 1036
Epoch 1036, Loss: 0.000016960, Improvement: -0.000004281, Best Loss: 0.000004499 in Epoch 996
Epoch 1037
Epoch 1037, Loss: 0.000022931, Improvement: 0.000005972, Best Loss: 0.000004499 in Epoch 996
Epoch 1038
Epoch 1038, Loss: 0.000038382, Improvement: 0.000015451, Best Loss: 0.000004499 in Epoch 996
Epoch 1039
Epoch 1039, Loss: 0.000047352, Improvement: 0.000008969, Best Loss: 0.000004499 in Epoch 996
Epoch 1040
Epoch 1040, Loss: 0.000026264, Improvement: -0.000021088, Best Loss: 0.000004499 in Epoch 996
Epoch 1041
Epoch 1041, Loss: 0.000014982, Improvement: -0.000011281, Best Loss: 0.000004499 in Epoch 996
Epoch 1042
Epoch 1042, Loss: 0.000011510, Improvement: -0.000003472, Best Loss: 0.000004499 in Epoch 996
Epoch 1043
Epoch 1043, Loss: 0.000019848, Improvement: 0.000008338, Best Loss: 0.000004499 in Epoch 996
Epoch 1044
Epoch 1044, Loss: 0.000018626, Improvement: -0.000001222, Best Loss: 0.000004499 in Epoch 996
Epoch 1045
Epoch 1045, Loss: 0.000014443, Improvement: -0.000004183, Best Loss: 0.000004499 in Epoch 996
Epoch 1046
Epoch 1046, Loss: 0.000011124, Improvement: -0.000003318, Best Loss: 0.000004499 in Epoch 996
Epoch 1047
Epoch 1047, Loss: 0.000009223, Improvement: -0.000001901, Best Loss: 0.000004499 in Epoch 996
Epoch 1048
Epoch 1048, Loss: 0.000008964, Improvement: -0.000000259, Best Loss: 0.000004499 in Epoch 996
Epoch 1049
Epoch 1049, Loss: 0.000008808, Improvement: -0.000000156, Best Loss: 0.000004499 in Epoch 996
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.000015895, Improvement: 0.000007086, Best Loss: 0.000004499 in Epoch 996
Epoch 1051
Epoch 1051, Loss: 0.000012022, Improvement: -0.000003873, Best Loss: 0.000004499 in Epoch 996
Epoch 1052
Epoch 1052, Loss: 0.000008410, Improvement: -0.000003612, Best Loss: 0.000004499 in Epoch 996
Epoch 1053
Epoch 1053, Loss: 0.000007079, Improvement: -0.000001331, Best Loss: 0.000004499 in Epoch 996
Epoch 1054
Epoch 1054, Loss: 0.000006104, Improvement: -0.000000975, Best Loss: 0.000004499 in Epoch 996
Epoch 1055
A best model at epoch 1055 has been saved with training error 0.000004340.
A best model at epoch 1055 has been saved with training error 0.000004249.
A best model at epoch 1055 has been saved with training error 0.000004108.
A best model at epoch 1055 has been saved with training error 0.000004080.
Epoch 1055, Loss: 0.000005396, Improvement: -0.000000708, Best Loss: 0.000004080 in Epoch 1055
Epoch 1056
A best model at epoch 1056 has been saved with training error 0.000004045.
Epoch 1056, Loss: 0.000005819, Improvement: 0.000000423, Best Loss: 0.000004045 in Epoch 1056
Epoch 1057
Epoch 1057, Loss: 0.000007889, Improvement: 0.000002070, Best Loss: 0.000004045 in Epoch 1056
Epoch 1058
Epoch 1058, Loss: 0.000016545, Improvement: 0.000008656, Best Loss: 0.000004045 in Epoch 1056
Epoch 1059
Epoch 1059, Loss: 0.000019294, Improvement: 0.000002749, Best Loss: 0.000004045 in Epoch 1056
Epoch 1060
Epoch 1060, Loss: 0.000025431, Improvement: 0.000006137, Best Loss: 0.000004045 in Epoch 1056
Epoch 1061
Epoch 1061, Loss: 0.000026262, Improvement: 0.000000831, Best Loss: 0.000004045 in Epoch 1056
Epoch 1062
Epoch 1062, Loss: 0.000035394, Improvement: 0.000009132, Best Loss: 0.000004045 in Epoch 1056
Epoch 1063
Epoch 1063, Loss: 0.000034825, Improvement: -0.000000569, Best Loss: 0.000004045 in Epoch 1056
Epoch 1064
Epoch 1064, Loss: 0.000037820, Improvement: 0.000002995, Best Loss: 0.000004045 in Epoch 1056
Epoch 1065
Epoch 1065, Loss: 0.000037016, Improvement: -0.000000804, Best Loss: 0.000004045 in Epoch 1056
Epoch 1066
Epoch 1066, Loss: 0.000018693, Improvement: -0.000018323, Best Loss: 0.000004045 in Epoch 1056
Epoch 1067
Epoch 1067, Loss: 0.000014270, Improvement: -0.000004423, Best Loss: 0.000004045 in Epoch 1056
Epoch 1068
Epoch 1068, Loss: 0.000008474, Improvement: -0.000005796, Best Loss: 0.000004045 in Epoch 1056
Epoch 1069
A best model at epoch 1069 has been saved with training error 0.000003765.
Epoch 1069, Loss: 0.000007093, Improvement: -0.000001381, Best Loss: 0.000003765 in Epoch 1069
Epoch 1070
Epoch 1070, Loss: 0.000005938, Improvement: -0.000001155, Best Loss: 0.000003765 in Epoch 1069
Epoch 1071
Epoch 1071, Loss: 0.000005683, Improvement: -0.000000256, Best Loss: 0.000003765 in Epoch 1069
Epoch 1072
Epoch 1072, Loss: 0.000006544, Improvement: 0.000000862, Best Loss: 0.000003765 in Epoch 1069
Epoch 1073
Epoch 1073, Loss: 0.000006879, Improvement: 0.000000335, Best Loss: 0.000003765 in Epoch 1069
Epoch 1074
Epoch 1074, Loss: 0.000008200, Improvement: 0.000001321, Best Loss: 0.000003765 in Epoch 1069
Epoch 1075
Epoch 1075, Loss: 0.000013471, Improvement: 0.000005271, Best Loss: 0.000003765 in Epoch 1069
Epoch 1076
Epoch 1076, Loss: 0.000040682, Improvement: 0.000027211, Best Loss: 0.000003765 in Epoch 1069
Epoch 1077
Epoch 1077, Loss: 0.000030237, Improvement: -0.000010445, Best Loss: 0.000003765 in Epoch 1069
Epoch 1078
Epoch 1078, Loss: 0.000023719, Improvement: -0.000006518, Best Loss: 0.000003765 in Epoch 1069
Epoch 1079
Epoch 1079, Loss: 0.000028302, Improvement: 0.000004583, Best Loss: 0.000003765 in Epoch 1069
Epoch 1080
Epoch 1080, Loss: 0.000011974, Improvement: -0.000016328, Best Loss: 0.000003765 in Epoch 1069
Epoch 1081
Epoch 1081, Loss: 0.000007904, Improvement: -0.000004069, Best Loss: 0.000003765 in Epoch 1069
Epoch 1082
Epoch 1082, Loss: 0.000005975, Improvement: -0.000001929, Best Loss: 0.000003765 in Epoch 1069
Epoch 1083
Epoch 1083, Loss: 0.000007441, Improvement: 0.000001466, Best Loss: 0.000003765 in Epoch 1069
Epoch 1084
Epoch 1084, Loss: 0.000010221, Improvement: 0.000002780, Best Loss: 0.000003765 in Epoch 1069
Epoch 1085
Epoch 1085, Loss: 0.000014155, Improvement: 0.000003934, Best Loss: 0.000003765 in Epoch 1069
Epoch 1086
Epoch 1086, Loss: 0.000031571, Improvement: 0.000017416, Best Loss: 0.000003765 in Epoch 1069
Epoch 1087
Epoch 1087, Loss: 0.000038912, Improvement: 0.000007340, Best Loss: 0.000003765 in Epoch 1069
Epoch 1088
Epoch 1088, Loss: 0.000018678, Improvement: -0.000020234, Best Loss: 0.000003765 in Epoch 1069
Epoch 1089
Epoch 1089, Loss: 0.000011180, Improvement: -0.000007498, Best Loss: 0.000003765 in Epoch 1069
Epoch 1090
Epoch 1090, Loss: 0.000007745, Improvement: -0.000003435, Best Loss: 0.000003765 in Epoch 1069
Epoch 1091
Epoch 1091, Loss: 0.000005924, Improvement: -0.000001821, Best Loss: 0.000003765 in Epoch 1069
Epoch 1092
Epoch 1092, Loss: 0.000006005, Improvement: 0.000000081, Best Loss: 0.000003765 in Epoch 1069
Epoch 1093
Epoch 1093, Loss: 0.000005933, Improvement: -0.000000072, Best Loss: 0.000003765 in Epoch 1069
Epoch 1094
Epoch 1094, Loss: 0.000009641, Improvement: 0.000003709, Best Loss: 0.000003765 in Epoch 1069
Epoch 1095
Epoch 1095, Loss: 0.000011801, Improvement: 0.000002159, Best Loss: 0.000003765 in Epoch 1069
Epoch 1096
Epoch 1096, Loss: 0.000009336, Improvement: -0.000002465, Best Loss: 0.000003765 in Epoch 1069
Epoch 1097
Epoch 1097, Loss: 0.000007284, Improvement: -0.000002051, Best Loss: 0.000003765 in Epoch 1069
Epoch 1098
Epoch 1098, Loss: 0.000011890, Improvement: 0.000004606, Best Loss: 0.000003765 in Epoch 1069
Epoch 1099
Epoch 1099, Loss: 0.000012231, Improvement: 0.000000341, Best Loss: 0.000003765 in Epoch 1069
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.000012450, Improvement: 0.000000220, Best Loss: 0.000003765 in Epoch 1069
Epoch 1101
Epoch 1101, Loss: 0.000010247, Improvement: -0.000002204, Best Loss: 0.000003765 in Epoch 1069
Epoch 1102
Epoch 1102, Loss: 0.000008449, Improvement: -0.000001798, Best Loss: 0.000003765 in Epoch 1069
Epoch 1103
Epoch 1103, Loss: 0.000010306, Improvement: 0.000001857, Best Loss: 0.000003765 in Epoch 1069
Epoch 1104
Epoch 1104, Loss: 0.000012742, Improvement: 0.000002436, Best Loss: 0.000003765 in Epoch 1069
Epoch 1105
Epoch 1105, Loss: 0.000013438, Improvement: 0.000000696, Best Loss: 0.000003765 in Epoch 1069
Epoch 1106
Epoch 1106, Loss: 0.000015177, Improvement: 0.000001739, Best Loss: 0.000003765 in Epoch 1069
Epoch 1107
Epoch 1107, Loss: 0.000032726, Improvement: 0.000017549, Best Loss: 0.000003765 in Epoch 1069
Epoch 1108
Epoch 1108, Loss: 0.000029516, Improvement: -0.000003210, Best Loss: 0.000003765 in Epoch 1069
Epoch 1109
Epoch 1109, Loss: 0.000017799, Improvement: -0.000011717, Best Loss: 0.000003765 in Epoch 1069
Epoch 1110
Epoch 1110, Loss: 0.000013872, Improvement: -0.000003928, Best Loss: 0.000003765 in Epoch 1069
Epoch 1111
Epoch 1111, Loss: 0.000009844, Improvement: -0.000004028, Best Loss: 0.000003765 in Epoch 1069
Epoch 1112
Epoch 1112, Loss: 0.000006632, Improvement: -0.000003212, Best Loss: 0.000003765 in Epoch 1069
Epoch 1113
Epoch 1113, Loss: 0.000007610, Improvement: 0.000000978, Best Loss: 0.000003765 in Epoch 1069
Epoch 1114
Epoch 1114, Loss: 0.000009881, Improvement: 0.000002271, Best Loss: 0.000003765 in Epoch 1069
Epoch 1115
Epoch 1115, Loss: 0.000013707, Improvement: 0.000003826, Best Loss: 0.000003765 in Epoch 1069
Epoch 1116
Epoch 1116, Loss: 0.000040498, Improvement: 0.000026791, Best Loss: 0.000003765 in Epoch 1069
Epoch 1117
Epoch 1117, Loss: 0.000042820, Improvement: 0.000002322, Best Loss: 0.000003765 in Epoch 1069
Epoch 1118
Epoch 1118, Loss: 0.000025263, Improvement: -0.000017558, Best Loss: 0.000003765 in Epoch 1069
Epoch 1119
Epoch 1119, Loss: 0.000020733, Improvement: -0.000004530, Best Loss: 0.000003765 in Epoch 1069
Epoch 1120
Epoch 1120, Loss: 0.000014418, Improvement: -0.000006314, Best Loss: 0.000003765 in Epoch 1069
Epoch 1121
Epoch 1121, Loss: 0.000011206, Improvement: -0.000003213, Best Loss: 0.000003765 in Epoch 1069
Epoch 1122
Epoch 1122, Loss: 0.000009088, Improvement: -0.000002118, Best Loss: 0.000003765 in Epoch 1069
Epoch 1123
Epoch 1123, Loss: 0.000006991, Improvement: -0.000002097, Best Loss: 0.000003765 in Epoch 1069
Epoch 1124
Epoch 1124, Loss: 0.000010332, Improvement: 0.000003341, Best Loss: 0.000003765 in Epoch 1069
Epoch 1125
Epoch 1125, Loss: 0.000010778, Improvement: 0.000000446, Best Loss: 0.000003765 in Epoch 1069
Epoch 1126
Epoch 1126, Loss: 0.000010720, Improvement: -0.000000057, Best Loss: 0.000003765 in Epoch 1069
Epoch 1127
Epoch 1127, Loss: 0.000010197, Improvement: -0.000000523, Best Loss: 0.000003765 in Epoch 1069
Epoch 1128
Epoch 1128, Loss: 0.000013280, Improvement: 0.000003083, Best Loss: 0.000003765 in Epoch 1069
Epoch 1129
Epoch 1129, Loss: 0.000008419, Improvement: -0.000004861, Best Loss: 0.000003765 in Epoch 1069
Epoch 1130
Epoch 1130, Loss: 0.000009145, Improvement: 0.000000727, Best Loss: 0.000003765 in Epoch 1069
Epoch 1131
Epoch 1131, Loss: 0.000008926, Improvement: -0.000000219, Best Loss: 0.000003765 in Epoch 1069
Epoch 1132
Epoch 1132, Loss: 0.000012417, Improvement: 0.000003490, Best Loss: 0.000003765 in Epoch 1069
Epoch 1133
Epoch 1133, Loss: 0.000010083, Improvement: -0.000002333, Best Loss: 0.000003765 in Epoch 1069
Epoch 1134
Epoch 1134, Loss: 0.000006988, Improvement: -0.000003096, Best Loss: 0.000003765 in Epoch 1069
Epoch 1135
Epoch 1135, Loss: 0.000006992, Improvement: 0.000000004, Best Loss: 0.000003765 in Epoch 1069
Epoch 1136
Epoch 1136, Loss: 0.000007566, Improvement: 0.000000575, Best Loss: 0.000003765 in Epoch 1069
Epoch 1137
Epoch 1137, Loss: 0.000007215, Improvement: -0.000000351, Best Loss: 0.000003765 in Epoch 1069
Epoch 1138
Epoch 1138, Loss: 0.000007440, Improvement: 0.000000225, Best Loss: 0.000003765 in Epoch 1069
Epoch 1139
Epoch 1139, Loss: 0.000017640, Improvement: 0.000010200, Best Loss: 0.000003765 in Epoch 1069
Epoch 1140
Epoch 1140, Loss: 0.000037688, Improvement: 0.000020048, Best Loss: 0.000003765 in Epoch 1069
Epoch 1141
Epoch 1141, Loss: 0.000037076, Improvement: -0.000000612, Best Loss: 0.000003765 in Epoch 1069
Epoch 1142
Epoch 1142, Loss: 0.000031854, Improvement: -0.000005222, Best Loss: 0.000003765 in Epoch 1069
Epoch 1143
Epoch 1143, Loss: 0.000023709, Improvement: -0.000008145, Best Loss: 0.000003765 in Epoch 1069
Epoch 1144
Epoch 1144, Loss: 0.000017083, Improvement: -0.000006625, Best Loss: 0.000003765 in Epoch 1069
Epoch 1145
Epoch 1145, Loss: 0.000017422, Improvement: 0.000000338, Best Loss: 0.000003765 in Epoch 1069
Epoch 1146
Epoch 1146, Loss: 0.000019644, Improvement: 0.000002222, Best Loss: 0.000003765 in Epoch 1069
Epoch 1147
Epoch 1147, Loss: 0.000021439, Improvement: 0.000001795, Best Loss: 0.000003765 in Epoch 1069
Epoch 1148
Epoch 1148, Loss: 0.000016633, Improvement: -0.000004806, Best Loss: 0.000003765 in Epoch 1069
Epoch 1149
Epoch 1149, Loss: 0.000014298, Improvement: -0.000002335, Best Loss: 0.000003765 in Epoch 1069
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.000008357, Improvement: -0.000005942, Best Loss: 0.000003765 in Epoch 1069
Epoch 1151
Epoch 1151, Loss: 0.000006567, Improvement: -0.000001790, Best Loss: 0.000003765 in Epoch 1069
Epoch 1152
Epoch 1152, Loss: 0.000009517, Improvement: 0.000002950, Best Loss: 0.000003765 in Epoch 1069
Epoch 1153
Epoch 1153, Loss: 0.000029276, Improvement: 0.000019759, Best Loss: 0.000003765 in Epoch 1069
Epoch 1154
Epoch 1154, Loss: 0.000022839, Improvement: -0.000006437, Best Loss: 0.000003765 in Epoch 1069
Epoch 1155
Epoch 1155, Loss: 0.000017981, Improvement: -0.000004857, Best Loss: 0.000003765 in Epoch 1069
Epoch 1156
Epoch 1156, Loss: 0.000010429, Improvement: -0.000007552, Best Loss: 0.000003765 in Epoch 1069
Epoch 1157
Epoch 1157, Loss: 0.000006057, Improvement: -0.000004372, Best Loss: 0.000003765 in Epoch 1069
Epoch 1158
A best model at epoch 1158 has been saved with training error 0.000003281.
Epoch 1158, Loss: 0.000004639, Improvement: -0.000001418, Best Loss: 0.000003281 in Epoch 1158
Epoch 1159
Epoch 1159, Loss: 0.000004770, Improvement: 0.000000131, Best Loss: 0.000003281 in Epoch 1158
Epoch 1160
A best model at epoch 1160 has been saved with training error 0.000002916.
Epoch 1160, Loss: 0.000004801, Improvement: 0.000000030, Best Loss: 0.000002916 in Epoch 1160
Epoch 1161
Epoch 1161, Loss: 0.000005180, Improvement: 0.000000379, Best Loss: 0.000002916 in Epoch 1160
Epoch 1162
Epoch 1162, Loss: 0.000005596, Improvement: 0.000000417, Best Loss: 0.000002916 in Epoch 1160
Epoch 1163
Epoch 1163, Loss: 0.000007214, Improvement: 0.000001618, Best Loss: 0.000002916 in Epoch 1160
Epoch 1164
Epoch 1164, Loss: 0.000009505, Improvement: 0.000002291, Best Loss: 0.000002916 in Epoch 1160
Epoch 1165
Epoch 1165, Loss: 0.000011841, Improvement: 0.000002336, Best Loss: 0.000002916 in Epoch 1160
Epoch 1166
Epoch 1166, Loss: 0.000020533, Improvement: 0.000008692, Best Loss: 0.000002916 in Epoch 1160
Epoch 1167
Epoch 1167, Loss: 0.000014773, Improvement: -0.000005760, Best Loss: 0.000002916 in Epoch 1160
Epoch 1168
Epoch 1168, Loss: 0.000021715, Improvement: 0.000006942, Best Loss: 0.000002916 in Epoch 1160
Epoch 1169
Epoch 1169, Loss: 0.000016924, Improvement: -0.000004791, Best Loss: 0.000002916 in Epoch 1160
Epoch 1170
Epoch 1170, Loss: 0.000013618, Improvement: -0.000003306, Best Loss: 0.000002916 in Epoch 1160
Epoch 1171
Epoch 1171, Loss: 0.000008618, Improvement: -0.000005000, Best Loss: 0.000002916 in Epoch 1160
Epoch 1172
Epoch 1172, Loss: 0.000007819, Improvement: -0.000000800, Best Loss: 0.000002916 in Epoch 1160
Epoch 1173
Epoch 1173, Loss: 0.000012654, Improvement: 0.000004835, Best Loss: 0.000002916 in Epoch 1160
Epoch 1174
Epoch 1174, Loss: 0.000018138, Improvement: 0.000005484, Best Loss: 0.000002916 in Epoch 1160
Epoch 1175
Epoch 1175, Loss: 0.000029796, Improvement: 0.000011657, Best Loss: 0.000002916 in Epoch 1160
Epoch 1176
Epoch 1176, Loss: 0.000020464, Improvement: -0.000009331, Best Loss: 0.000002916 in Epoch 1160
Epoch 1177
Epoch 1177, Loss: 0.000012319, Improvement: -0.000008145, Best Loss: 0.000002916 in Epoch 1160
Epoch 1178
Epoch 1178, Loss: 0.000010306, Improvement: -0.000002013, Best Loss: 0.000002916 in Epoch 1160
Epoch 1179
Epoch 1179, Loss: 0.000012177, Improvement: 0.000001871, Best Loss: 0.000002916 in Epoch 1160
Epoch 1180
Epoch 1180, Loss: 0.000014775, Improvement: 0.000002598, Best Loss: 0.000002916 in Epoch 1160
Epoch 1181
Epoch 1181, Loss: 0.000022422, Improvement: 0.000007647, Best Loss: 0.000002916 in Epoch 1160
Epoch 1182
Epoch 1182, Loss: 0.000034730, Improvement: 0.000012308, Best Loss: 0.000002916 in Epoch 1160
Epoch 1183
Epoch 1183, Loss: 0.000025676, Improvement: -0.000009055, Best Loss: 0.000002916 in Epoch 1160
Epoch 1184
Epoch 1184, Loss: 0.000016091, Improvement: -0.000009585, Best Loss: 0.000002916 in Epoch 1160
Epoch 1185
Epoch 1185, Loss: 0.000012272, Improvement: -0.000003818, Best Loss: 0.000002916 in Epoch 1160
Epoch 1186
Epoch 1186, Loss: 0.000011383, Improvement: -0.000000890, Best Loss: 0.000002916 in Epoch 1160
Epoch 1187
Epoch 1187, Loss: 0.000015316, Improvement: 0.000003933, Best Loss: 0.000002916 in Epoch 1160
Epoch 1188
Epoch 1188, Loss: 0.000020720, Improvement: 0.000005404, Best Loss: 0.000002916 in Epoch 1160
Epoch 1189
Epoch 1189, Loss: 0.000022132, Improvement: 0.000001413, Best Loss: 0.000002916 in Epoch 1160
Epoch 1190
Epoch 1190, Loss: 0.000018944, Improvement: -0.000003189, Best Loss: 0.000002916 in Epoch 1160
Epoch 1191
Epoch 1191, Loss: 0.000014393, Improvement: -0.000004551, Best Loss: 0.000002916 in Epoch 1160
Epoch 1192
Epoch 1192, Loss: 0.000010588, Improvement: -0.000003805, Best Loss: 0.000002916 in Epoch 1160
Epoch 1193
Epoch 1193, Loss: 0.000014270, Improvement: 0.000003682, Best Loss: 0.000002916 in Epoch 1160
Epoch 1194
Epoch 1194, Loss: 0.000012313, Improvement: -0.000001957, Best Loss: 0.000002916 in Epoch 1160
Epoch 1195
Epoch 1195, Loss: 0.000007367, Improvement: -0.000004946, Best Loss: 0.000002916 in Epoch 1160
Epoch 1196
Epoch 1196, Loss: 0.000005302, Improvement: -0.000002064, Best Loss: 0.000002916 in Epoch 1160
Epoch 1197
A best model at epoch 1197 has been saved with training error 0.000002818.
Epoch 1197, Loss: 0.000004306, Improvement: -0.000000997, Best Loss: 0.000002818 in Epoch 1197
Epoch 1198
Epoch 1198, Loss: 0.000003797, Improvement: -0.000000509, Best Loss: 0.000002818 in Epoch 1197
Epoch 1199
Epoch 1199, Loss: 0.000003890, Improvement: 0.000000094, Best Loss: 0.000002818 in Epoch 1197
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.000004017, Improvement: 0.000000127, Best Loss: 0.000002818 in Epoch 1197
Epoch 1201
A best model at epoch 1201 has been saved with training error 0.000002509.
Epoch 1201, Loss: 0.000004248, Improvement: 0.000000231, Best Loss: 0.000002509 in Epoch 1201
Epoch 1202
Epoch 1202, Loss: 0.000004113, Improvement: -0.000000134, Best Loss: 0.000002509 in Epoch 1201
Epoch 1203
Epoch 1203, Loss: 0.000004933, Improvement: 0.000000820, Best Loss: 0.000002509 in Epoch 1201
Epoch 1204
Epoch 1204, Loss: 0.000004906, Improvement: -0.000000026, Best Loss: 0.000002509 in Epoch 1201
Epoch 1205
Epoch 1205, Loss: 0.000004817, Improvement: -0.000000089, Best Loss: 0.000002509 in Epoch 1201
Epoch 1206
Epoch 1206, Loss: 0.000005532, Improvement: 0.000000715, Best Loss: 0.000002509 in Epoch 1201
Epoch 1207
Epoch 1207, Loss: 0.000005498, Improvement: -0.000000033, Best Loss: 0.000002509 in Epoch 1201
Epoch 1208
Epoch 1208, Loss: 0.000006887, Improvement: 0.000001389, Best Loss: 0.000002509 in Epoch 1201
Epoch 1209
Epoch 1209, Loss: 0.000009267, Improvement: 0.000002379, Best Loss: 0.000002509 in Epoch 1201
Epoch 1210
Epoch 1210, Loss: 0.000009189, Improvement: -0.000000077, Best Loss: 0.000002509 in Epoch 1201
Epoch 1211
Epoch 1211, Loss: 0.000013996, Improvement: 0.000004806, Best Loss: 0.000002509 in Epoch 1201
Epoch 1212
Epoch 1212, Loss: 0.000033407, Improvement: 0.000019411, Best Loss: 0.000002509 in Epoch 1201
Epoch 1213
Epoch 1213, Loss: 0.000022122, Improvement: -0.000011285, Best Loss: 0.000002509 in Epoch 1201
Epoch 1214
Epoch 1214, Loss: 0.000020036, Improvement: -0.000002086, Best Loss: 0.000002509 in Epoch 1201
Epoch 1215
Epoch 1215, Loss: 0.000018930, Improvement: -0.000001105, Best Loss: 0.000002509 in Epoch 1201
Epoch 1216
Epoch 1216, Loss: 0.000009992, Improvement: -0.000008939, Best Loss: 0.000002509 in Epoch 1201
Epoch 1217
Epoch 1217, Loss: 0.000008685, Improvement: -0.000001307, Best Loss: 0.000002509 in Epoch 1201
Epoch 1218
Epoch 1218, Loss: 0.000006984, Improvement: -0.000001701, Best Loss: 0.000002509 in Epoch 1201
Epoch 1219
Epoch 1219, Loss: 0.000004956, Improvement: -0.000002028, Best Loss: 0.000002509 in Epoch 1201
Epoch 1220
Epoch 1220, Loss: 0.000005924, Improvement: 0.000000968, Best Loss: 0.000002509 in Epoch 1201
Epoch 1221
Epoch 1221, Loss: 0.000006454, Improvement: 0.000000530, Best Loss: 0.000002509 in Epoch 1201
Epoch 1222
Epoch 1222, Loss: 0.000006909, Improvement: 0.000000455, Best Loss: 0.000002509 in Epoch 1201
Epoch 1223
Epoch 1223, Loss: 0.000010459, Improvement: 0.000003550, Best Loss: 0.000002509 in Epoch 1201
Epoch 1224
Epoch 1224, Loss: 0.000013400, Improvement: 0.000002941, Best Loss: 0.000002509 in Epoch 1201
Epoch 1225
Epoch 1225, Loss: 0.000023629, Improvement: 0.000010229, Best Loss: 0.000002509 in Epoch 1201
Epoch 1226
Epoch 1226, Loss: 0.000019373, Improvement: -0.000004256, Best Loss: 0.000002509 in Epoch 1201
Epoch 1227
Epoch 1227, Loss: 0.000018481, Improvement: -0.000000891, Best Loss: 0.000002509 in Epoch 1201
Epoch 1228
Epoch 1228, Loss: 0.000021996, Improvement: 0.000003515, Best Loss: 0.000002509 in Epoch 1201
Epoch 1229
Epoch 1229, Loss: 0.000027269, Improvement: 0.000005273, Best Loss: 0.000002509 in Epoch 1201
Epoch 1230
Epoch 1230, Loss: 0.000031937, Improvement: 0.000004668, Best Loss: 0.000002509 in Epoch 1201
Epoch 1231
Epoch 1231, Loss: 0.000026244, Improvement: -0.000005693, Best Loss: 0.000002509 in Epoch 1201
Epoch 1232
Epoch 1232, Loss: 0.000019349, Improvement: -0.000006895, Best Loss: 0.000002509 in Epoch 1201
Epoch 1233
Epoch 1233, Loss: 0.000010140, Improvement: -0.000009209, Best Loss: 0.000002509 in Epoch 1201
Epoch 1234
Epoch 1234, Loss: 0.000005680, Improvement: -0.000004460, Best Loss: 0.000002509 in Epoch 1201
Epoch 1235
Epoch 1235, Loss: 0.000004306, Improvement: -0.000001374, Best Loss: 0.000002509 in Epoch 1201
Epoch 1236
Epoch 1236, Loss: 0.000003900, Improvement: -0.000000406, Best Loss: 0.000002509 in Epoch 1201
Epoch 1237
A best model at epoch 1237 has been saved with training error 0.000002498.
Epoch 1237, Loss: 0.000003216, Improvement: -0.000000685, Best Loss: 0.000002498 in Epoch 1237
Epoch 1238
A best model at epoch 1238 has been saved with training error 0.000002474.
Epoch 1238, Loss: 0.000003476, Improvement: 0.000000260, Best Loss: 0.000002474 in Epoch 1238
Epoch 1239
A best model at epoch 1239 has been saved with training error 0.000002400.
Epoch 1239, Loss: 0.000003391, Improvement: -0.000000084, Best Loss: 0.000002400 in Epoch 1239
Epoch 1240
Epoch 1240, Loss: 0.000004325, Improvement: 0.000000934, Best Loss: 0.000002400 in Epoch 1239
Epoch 1241
Epoch 1241, Loss: 0.000003961, Improvement: -0.000000364, Best Loss: 0.000002400 in Epoch 1239
Epoch 1242
Epoch 1242, Loss: 0.000004095, Improvement: 0.000000134, Best Loss: 0.000002400 in Epoch 1239
Epoch 1243
Epoch 1243, Loss: 0.000003542, Improvement: -0.000000554, Best Loss: 0.000002400 in Epoch 1239
Epoch 1244
Epoch 1244, Loss: 0.000004646, Improvement: 0.000001105, Best Loss: 0.000002400 in Epoch 1239
Epoch 1245
Epoch 1245, Loss: 0.000004968, Improvement: 0.000000322, Best Loss: 0.000002400 in Epoch 1239
Epoch 1246
Epoch 1246, Loss: 0.000004740, Improvement: -0.000000228, Best Loss: 0.000002400 in Epoch 1239
Epoch 1247
Epoch 1247, Loss: 0.000006890, Improvement: 0.000002150, Best Loss: 0.000002400 in Epoch 1239
Epoch 1248
Epoch 1248, Loss: 0.000009093, Improvement: 0.000002203, Best Loss: 0.000002400 in Epoch 1239
Epoch 1249
Epoch 1249, Loss: 0.000018703, Improvement: 0.000009610, Best Loss: 0.000002400 in Epoch 1239
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.000016359, Improvement: -0.000002344, Best Loss: 0.000002400 in Epoch 1239
Epoch 1251
Epoch 1251, Loss: 0.000013249, Improvement: -0.000003110, Best Loss: 0.000002400 in Epoch 1239
Epoch 1252
Epoch 1252, Loss: 0.000012803, Improvement: -0.000000446, Best Loss: 0.000002400 in Epoch 1239
Epoch 1253
Epoch 1253, Loss: 0.000013382, Improvement: 0.000000579, Best Loss: 0.000002400 in Epoch 1239
Epoch 1254
Epoch 1254, Loss: 0.000011848, Improvement: -0.000001535, Best Loss: 0.000002400 in Epoch 1239
Epoch 1255
Epoch 1255, Loss: 0.000009324, Improvement: -0.000002524, Best Loss: 0.000002400 in Epoch 1239
Epoch 1256
Epoch 1256, Loss: 0.000013850, Improvement: 0.000004526, Best Loss: 0.000002400 in Epoch 1239
Epoch 1257
Epoch 1257, Loss: 0.000012006, Improvement: -0.000001844, Best Loss: 0.000002400 in Epoch 1239
Epoch 1258
Epoch 1258, Loss: 0.000020893, Improvement: 0.000008887, Best Loss: 0.000002400 in Epoch 1239
Epoch 1259
Epoch 1259, Loss: 0.000035454, Improvement: 0.000014561, Best Loss: 0.000002400 in Epoch 1239
Epoch 1260
Epoch 1260, Loss: 0.000028061, Improvement: -0.000007393, Best Loss: 0.000002400 in Epoch 1239
Epoch 1261
Epoch 1261, Loss: 0.000026344, Improvement: -0.000001717, Best Loss: 0.000002400 in Epoch 1239
Epoch 1262
Epoch 1262, Loss: 0.000013535, Improvement: -0.000012809, Best Loss: 0.000002400 in Epoch 1239
Epoch 1263
Epoch 1263, Loss: 0.000011342, Improvement: -0.000002194, Best Loss: 0.000002400 in Epoch 1239
Epoch 1264
Epoch 1264, Loss: 0.000007936, Improvement: -0.000003406, Best Loss: 0.000002400 in Epoch 1239
Epoch 1265
Epoch 1265, Loss: 0.000005394, Improvement: -0.000002542, Best Loss: 0.000002400 in Epoch 1239
Epoch 1266
Epoch 1266, Loss: 0.000004543, Improvement: -0.000000850, Best Loss: 0.000002400 in Epoch 1239
Epoch 1267
Epoch 1267, Loss: 0.000003771, Improvement: -0.000000772, Best Loss: 0.000002400 in Epoch 1239
Epoch 1268
Epoch 1268, Loss: 0.000003839, Improvement: 0.000000068, Best Loss: 0.000002400 in Epoch 1239
Epoch 1269
Epoch 1269, Loss: 0.000003613, Improvement: -0.000000226, Best Loss: 0.000002400 in Epoch 1239
Epoch 1270
A best model at epoch 1270 has been saved with training error 0.000002370.
A best model at epoch 1270 has been saved with training error 0.000002317.
Epoch 1270, Loss: 0.000003414, Improvement: -0.000000199, Best Loss: 0.000002317 in Epoch 1270
Epoch 1271
Epoch 1271, Loss: 0.000003337, Improvement: -0.000000077, Best Loss: 0.000002317 in Epoch 1270
Epoch 1272
Epoch 1272, Loss: 0.000003079, Improvement: -0.000000257, Best Loss: 0.000002317 in Epoch 1270
Epoch 1273
Epoch 1273, Loss: 0.000003134, Improvement: 0.000000055, Best Loss: 0.000002317 in Epoch 1270
Epoch 1274
Epoch 1274, Loss: 0.000004487, Improvement: 0.000001353, Best Loss: 0.000002317 in Epoch 1270
Epoch 1275
Epoch 1275, Loss: 0.000005199, Improvement: 0.000000713, Best Loss: 0.000002317 in Epoch 1270
Epoch 1276
Epoch 1276, Loss: 0.000006524, Improvement: 0.000001324, Best Loss: 0.000002317 in Epoch 1270
Epoch 1277
Epoch 1277, Loss: 0.000006922, Improvement: 0.000000398, Best Loss: 0.000002317 in Epoch 1270
Epoch 1278
Epoch 1278, Loss: 0.000009232, Improvement: 0.000002310, Best Loss: 0.000002317 in Epoch 1270
Epoch 1279
Epoch 1279, Loss: 0.000034717, Improvement: 0.000025485, Best Loss: 0.000002317 in Epoch 1270
Epoch 1280
Epoch 1280, Loss: 0.000025439, Improvement: -0.000009278, Best Loss: 0.000002317 in Epoch 1270
Epoch 1281
Epoch 1281, Loss: 0.000016543, Improvement: -0.000008896, Best Loss: 0.000002317 in Epoch 1270
Epoch 1282
Epoch 1282, Loss: 0.000017519, Improvement: 0.000000976, Best Loss: 0.000002317 in Epoch 1270
Epoch 1283
Epoch 1283, Loss: 0.000011503, Improvement: -0.000006016, Best Loss: 0.000002317 in Epoch 1270
Epoch 1284
Epoch 1284, Loss: 0.000005134, Improvement: -0.000006369, Best Loss: 0.000002317 in Epoch 1270
Epoch 1285
Epoch 1285, Loss: 0.000003091, Improvement: -0.000002043, Best Loss: 0.000002317 in Epoch 1270
Epoch 1286
A best model at epoch 1286 has been saved with training error 0.000002303.
Epoch 1286, Loss: 0.000002757, Improvement: -0.000000334, Best Loss: 0.000002303 in Epoch 1286
Epoch 1287
A best model at epoch 1287 has been saved with training error 0.000002099.
Epoch 1287, Loss: 0.000002828, Improvement: 0.000000072, Best Loss: 0.000002099 in Epoch 1287
Epoch 1288
A best model at epoch 1288 has been saved with training error 0.000002042.
Epoch 1288, Loss: 0.000002691, Improvement: -0.000000137, Best Loss: 0.000002042 in Epoch 1288
Epoch 1289
A best model at epoch 1289 has been saved with training error 0.000002019.
A best model at epoch 1289 has been saved with training error 0.000001886.
Epoch 1289, Loss: 0.000002497, Improvement: -0.000000194, Best Loss: 0.000001886 in Epoch 1289
Epoch 1290
Epoch 1290, Loss: 0.000002357, Improvement: -0.000000140, Best Loss: 0.000001886 in Epoch 1289
Epoch 1291
A best model at epoch 1291 has been saved with training error 0.000001860.
A best model at epoch 1291 has been saved with training error 0.000001827.
Epoch 1291, Loss: 0.000002345, Improvement: -0.000000012, Best Loss: 0.000001827 in Epoch 1291
Epoch 1292
Epoch 1292, Loss: 0.000002910, Improvement: 0.000000565, Best Loss: 0.000001827 in Epoch 1291
Epoch 1293
Epoch 1293, Loss: 0.000002750, Improvement: -0.000000161, Best Loss: 0.000001827 in Epoch 1291
Epoch 1294
Epoch 1294, Loss: 0.000002704, Improvement: -0.000000046, Best Loss: 0.000001827 in Epoch 1291
Epoch 1295
Epoch 1295, Loss: 0.000002941, Improvement: 0.000000237, Best Loss: 0.000001827 in Epoch 1291
Epoch 1296
Epoch 1296, Loss: 0.000004422, Improvement: 0.000001481, Best Loss: 0.000001827 in Epoch 1291
Epoch 1297
Epoch 1297, Loss: 0.000004539, Improvement: 0.000000117, Best Loss: 0.000001827 in Epoch 1291
Epoch 1298
Epoch 1298, Loss: 0.000004908, Improvement: 0.000000369, Best Loss: 0.000001827 in Epoch 1291
Epoch 1299
Epoch 1299, Loss: 0.000007626, Improvement: 0.000002718, Best Loss: 0.000001827 in Epoch 1291
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.000014001, Improvement: 0.000006375, Best Loss: 0.000001827 in Epoch 1291
Epoch 1301
Epoch 1301, Loss: 0.000010380, Improvement: -0.000003621, Best Loss: 0.000001827 in Epoch 1291
Epoch 1302
Epoch 1302, Loss: 0.000013693, Improvement: 0.000003313, Best Loss: 0.000001827 in Epoch 1291
Epoch 1303
Epoch 1303, Loss: 0.000031577, Improvement: 0.000017885, Best Loss: 0.000001827 in Epoch 1291
Epoch 1304
Epoch 1304, Loss: 0.000016493, Improvement: -0.000015084, Best Loss: 0.000001827 in Epoch 1291
Epoch 1305
Epoch 1305, Loss: 0.000016686, Improvement: 0.000000193, Best Loss: 0.000001827 in Epoch 1291
Epoch 1306
Epoch 1306, Loss: 0.000016324, Improvement: -0.000000362, Best Loss: 0.000001827 in Epoch 1291
Epoch 1307
Epoch 1307, Loss: 0.000009460, Improvement: -0.000006864, Best Loss: 0.000001827 in Epoch 1291
Epoch 1308
Epoch 1308, Loss: 0.000006839, Improvement: -0.000002621, Best Loss: 0.000001827 in Epoch 1291
Epoch 1309
Epoch 1309, Loss: 0.000006768, Improvement: -0.000000071, Best Loss: 0.000001827 in Epoch 1291
Epoch 1310
Epoch 1310, Loss: 0.000006353, Improvement: -0.000000415, Best Loss: 0.000001827 in Epoch 1291
Epoch 1311
Epoch 1311, Loss: 0.000009154, Improvement: 0.000002802, Best Loss: 0.000001827 in Epoch 1291
Epoch 1312
Epoch 1312, Loss: 0.000011229, Improvement: 0.000002075, Best Loss: 0.000001827 in Epoch 1291
Epoch 1313
Epoch 1313, Loss: 0.000007282, Improvement: -0.000003947, Best Loss: 0.000001827 in Epoch 1291
Epoch 1314
Epoch 1314, Loss: 0.000006938, Improvement: -0.000000344, Best Loss: 0.000001827 in Epoch 1291
Epoch 1315
Epoch 1315, Loss: 0.000006988, Improvement: 0.000000050, Best Loss: 0.000001827 in Epoch 1291
Epoch 1316
Epoch 1316, Loss: 0.000006089, Improvement: -0.000000898, Best Loss: 0.000001827 in Epoch 1291
Epoch 1317
Epoch 1317, Loss: 0.000005425, Improvement: -0.000000664, Best Loss: 0.000001827 in Epoch 1291
Epoch 1318
Epoch 1318, Loss: 0.000005318, Improvement: -0.000000107, Best Loss: 0.000001827 in Epoch 1291
Epoch 1319
Epoch 1319, Loss: 0.000004649, Improvement: -0.000000670, Best Loss: 0.000001827 in Epoch 1291
Epoch 1320
Epoch 1320, Loss: 0.000005985, Improvement: 0.000001336, Best Loss: 0.000001827 in Epoch 1291
Epoch 1321
Epoch 1321, Loss: 0.000010572, Improvement: 0.000004587, Best Loss: 0.000001827 in Epoch 1291
Epoch 1322
Epoch 1322, Loss: 0.000008952, Improvement: -0.000001619, Best Loss: 0.000001827 in Epoch 1291
Epoch 1323
Epoch 1323, Loss: 0.000017864, Improvement: 0.000008912, Best Loss: 0.000001827 in Epoch 1291
Epoch 1324
Epoch 1324, Loss: 0.000029497, Improvement: 0.000011633, Best Loss: 0.000001827 in Epoch 1291
Epoch 1325
Epoch 1325, Loss: 0.000013557, Improvement: -0.000015940, Best Loss: 0.000001827 in Epoch 1291
Epoch 1326
Epoch 1326, Loss: 0.000013936, Improvement: 0.000000379, Best Loss: 0.000001827 in Epoch 1291
Epoch 1327
Epoch 1327, Loss: 0.000009077, Improvement: -0.000004859, Best Loss: 0.000001827 in Epoch 1291
Epoch 1328
Epoch 1328, Loss: 0.000005320, Improvement: -0.000003757, Best Loss: 0.000001827 in Epoch 1291
Epoch 1329
Epoch 1329, Loss: 0.000004227, Improvement: -0.000001094, Best Loss: 0.000001827 in Epoch 1291
Epoch 1330
Epoch 1330, Loss: 0.000004474, Improvement: 0.000000248, Best Loss: 0.000001827 in Epoch 1291
Epoch 1331
Epoch 1331, Loss: 0.000003440, Improvement: -0.000001035, Best Loss: 0.000001827 in Epoch 1291
Epoch 1332
Epoch 1332, Loss: 0.000004567, Improvement: 0.000001128, Best Loss: 0.000001827 in Epoch 1291
Epoch 1333
Epoch 1333, Loss: 0.000009318, Improvement: 0.000004750, Best Loss: 0.000001827 in Epoch 1291
Epoch 1334
Epoch 1334, Loss: 0.000016655, Improvement: 0.000007337, Best Loss: 0.000001827 in Epoch 1291
Epoch 1335
Epoch 1335, Loss: 0.000017716, Improvement: 0.000001061, Best Loss: 0.000001827 in Epoch 1291
Epoch 1336
Epoch 1336, Loss: 0.000013897, Improvement: -0.000003819, Best Loss: 0.000001827 in Epoch 1291
Epoch 1337
Epoch 1337, Loss: 0.000015122, Improvement: 0.000001225, Best Loss: 0.000001827 in Epoch 1291
Epoch 1338
Epoch 1338, Loss: 0.000010479, Improvement: -0.000004643, Best Loss: 0.000001827 in Epoch 1291
Epoch 1339
Epoch 1339, Loss: 0.000006240, Improvement: -0.000004239, Best Loss: 0.000001827 in Epoch 1291
Epoch 1340
Epoch 1340, Loss: 0.000005324, Improvement: -0.000000916, Best Loss: 0.000001827 in Epoch 1291
Epoch 1341
Epoch 1341, Loss: 0.000007147, Improvement: 0.000001823, Best Loss: 0.000001827 in Epoch 1291
Epoch 1342
Epoch 1342, Loss: 0.000007385, Improvement: 0.000000238, Best Loss: 0.000001827 in Epoch 1291
Epoch 1343
Epoch 1343, Loss: 0.000008630, Improvement: 0.000001245, Best Loss: 0.000001827 in Epoch 1291
Epoch 1344
Epoch 1344, Loss: 0.000010328, Improvement: 0.000001698, Best Loss: 0.000001827 in Epoch 1291
Epoch 1345
Epoch 1345, Loss: 0.000011347, Improvement: 0.000001019, Best Loss: 0.000001827 in Epoch 1291
Epoch 1346
Epoch 1346, Loss: 0.000018611, Improvement: 0.000007264, Best Loss: 0.000001827 in Epoch 1291
Epoch 1347
Epoch 1347, Loss: 0.000027687, Improvement: 0.000009076, Best Loss: 0.000001827 in Epoch 1291
Epoch 1348
Epoch 1348, Loss: 0.000027005, Improvement: -0.000000682, Best Loss: 0.000001827 in Epoch 1291
Epoch 1349
Epoch 1349, Loss: 0.000011353, Improvement: -0.000015652, Best Loss: 0.000001827 in Epoch 1291
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.000007911, Improvement: -0.000003442, Best Loss: 0.000001827 in Epoch 1291
Epoch 1351
Epoch 1351, Loss: 0.000005591, Improvement: -0.000002320, Best Loss: 0.000001827 in Epoch 1291
Epoch 1352
Epoch 1352, Loss: 0.000007484, Improvement: 0.000001893, Best Loss: 0.000001827 in Epoch 1291
Epoch 1353
Epoch 1353, Loss: 0.000007365, Improvement: -0.000000118, Best Loss: 0.000001827 in Epoch 1291
Epoch 1354
Epoch 1354, Loss: 0.000007488, Improvement: 0.000000123, Best Loss: 0.000001827 in Epoch 1291
Epoch 1355
Epoch 1355, Loss: 0.000007039, Improvement: -0.000000449, Best Loss: 0.000001827 in Epoch 1291
Epoch 1356
Epoch 1356, Loss: 0.000004508, Improvement: -0.000002531, Best Loss: 0.000001827 in Epoch 1291
Epoch 1357
Epoch 1357, Loss: 0.000005106, Improvement: 0.000000598, Best Loss: 0.000001827 in Epoch 1291
Epoch 1358
Epoch 1358, Loss: 0.000011829, Improvement: 0.000006723, Best Loss: 0.000001827 in Epoch 1291
Epoch 1359
Epoch 1359, Loss: 0.000010730, Improvement: -0.000001099, Best Loss: 0.000001827 in Epoch 1291
Epoch 1360
Epoch 1360, Loss: 0.000011619, Improvement: 0.000000888, Best Loss: 0.000001827 in Epoch 1291
Epoch 1361
Epoch 1361, Loss: 0.000014750, Improvement: 0.000003132, Best Loss: 0.000001827 in Epoch 1291
Epoch 1362
Epoch 1362, Loss: 0.000009041, Improvement: -0.000005709, Best Loss: 0.000001827 in Epoch 1291
Epoch 1363
Epoch 1363, Loss: 0.000006593, Improvement: -0.000002448, Best Loss: 0.000001827 in Epoch 1291
Epoch 1364
Epoch 1364, Loss: 0.000006517, Improvement: -0.000000076, Best Loss: 0.000001827 in Epoch 1291
Epoch 1365
Epoch 1365, Loss: 0.000009415, Improvement: 0.000002898, Best Loss: 0.000001827 in Epoch 1291
Epoch 1366
Epoch 1366, Loss: 0.000007587, Improvement: -0.000001828, Best Loss: 0.000001827 in Epoch 1291
Epoch 1367
Epoch 1367, Loss: 0.000008282, Improvement: 0.000000695, Best Loss: 0.000001827 in Epoch 1291
Epoch 1368
Epoch 1368, Loss: 0.000011157, Improvement: 0.000002876, Best Loss: 0.000001827 in Epoch 1291
Epoch 1369
Epoch 1369, Loss: 0.000020981, Improvement: 0.000009823, Best Loss: 0.000001827 in Epoch 1291
Epoch 1370
Epoch 1370, Loss: 0.000018285, Improvement: -0.000002695, Best Loss: 0.000001827 in Epoch 1291
Epoch 1371
Epoch 1371, Loss: 0.000011083, Improvement: -0.000007203, Best Loss: 0.000001827 in Epoch 1291
Epoch 1372
Epoch 1372, Loss: 0.000011574, Improvement: 0.000000491, Best Loss: 0.000001827 in Epoch 1291
Epoch 1373
Epoch 1373, Loss: 0.000034354, Improvement: 0.000022780, Best Loss: 0.000001827 in Epoch 1291
Epoch 1374
Epoch 1374, Loss: 0.000036013, Improvement: 0.000001659, Best Loss: 0.000001827 in Epoch 1291
Epoch 1375
Epoch 1375, Loss: 0.000031458, Improvement: -0.000004555, Best Loss: 0.000001827 in Epoch 1291
Epoch 1376
Epoch 1376, Loss: 0.000021289, Improvement: -0.000010169, Best Loss: 0.000001827 in Epoch 1291
Epoch 1377
Epoch 1377, Loss: 0.000009521, Improvement: -0.000011768, Best Loss: 0.000001827 in Epoch 1291
Epoch 1378
Epoch 1378, Loss: 0.000005904, Improvement: -0.000003617, Best Loss: 0.000001827 in Epoch 1291
Epoch 1379
Epoch 1379, Loss: 0.000004284, Improvement: -0.000001620, Best Loss: 0.000001827 in Epoch 1291
Epoch 1380
Epoch 1380, Loss: 0.000004489, Improvement: 0.000000205, Best Loss: 0.000001827 in Epoch 1291
Epoch 1381
Epoch 1381, Loss: 0.000004453, Improvement: -0.000000037, Best Loss: 0.000001827 in Epoch 1291
Epoch 1382
Epoch 1382, Loss: 0.000003184, Improvement: -0.000001269, Best Loss: 0.000001827 in Epoch 1291
Epoch 1383
Epoch 1383, Loss: 0.000002689, Improvement: -0.000000494, Best Loss: 0.000001827 in Epoch 1291
Epoch 1384
Epoch 1384, Loss: 0.000003173, Improvement: 0.000000484, Best Loss: 0.000001827 in Epoch 1291
Epoch 1385
Epoch 1385, Loss: 0.000002853, Improvement: -0.000000320, Best Loss: 0.000001827 in Epoch 1291
Epoch 1386
Epoch 1386, Loss: 0.000003544, Improvement: 0.000000690, Best Loss: 0.000001827 in Epoch 1291
Epoch 1387
Epoch 1387, Loss: 0.000004345, Improvement: 0.000000801, Best Loss: 0.000001827 in Epoch 1291
Epoch 1388
Epoch 1388, Loss: 0.000003187, Improvement: -0.000001158, Best Loss: 0.000001827 in Epoch 1291
Epoch 1389
Epoch 1389, Loss: 0.000004123, Improvement: 0.000000936, Best Loss: 0.000001827 in Epoch 1291
Epoch 1390
Epoch 1390, Loss: 0.000008377, Improvement: 0.000004254, Best Loss: 0.000001827 in Epoch 1291
Epoch 1391
Epoch 1391, Loss: 0.000009186, Improvement: 0.000000809, Best Loss: 0.000001827 in Epoch 1291
Epoch 1392
Epoch 1392, Loss: 0.000017836, Improvement: 0.000008650, Best Loss: 0.000001827 in Epoch 1291
Epoch 1393
Epoch 1393, Loss: 0.000022088, Improvement: 0.000004252, Best Loss: 0.000001827 in Epoch 1291
Epoch 1394
Epoch 1394, Loss: 0.000018940, Improvement: -0.000003148, Best Loss: 0.000001827 in Epoch 1291
Epoch 1395
Epoch 1395, Loss: 0.000016725, Improvement: -0.000002215, Best Loss: 0.000001827 in Epoch 1291
Epoch 1396
Epoch 1396, Loss: 0.000011993, Improvement: -0.000004732, Best Loss: 0.000001827 in Epoch 1291
Epoch 1397
Epoch 1397, Loss: 0.000020046, Improvement: 0.000008054, Best Loss: 0.000001827 in Epoch 1291
Epoch 1398
Epoch 1398, Loss: 0.000018957, Improvement: -0.000001090, Best Loss: 0.000001827 in Epoch 1291
Epoch 1399
Epoch 1399, Loss: 0.000021686, Improvement: 0.000002729, Best Loss: 0.000001827 in Epoch 1291
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.000014263, Improvement: -0.000007423, Best Loss: 0.000001827 in Epoch 1291
Epoch 1401
Epoch 1401, Loss: 0.000015332, Improvement: 0.000001069, Best Loss: 0.000001827 in Epoch 1291
Epoch 1402
Epoch 1402, Loss: 0.000015714, Improvement: 0.000000381, Best Loss: 0.000001827 in Epoch 1291
Epoch 1403
Epoch 1403, Loss: 0.000016947, Improvement: 0.000001233, Best Loss: 0.000001827 in Epoch 1291
Epoch 1404
Epoch 1404, Loss: 0.000017622, Improvement: 0.000000675, Best Loss: 0.000001827 in Epoch 1291
Epoch 1405
Epoch 1405, Loss: 0.000022461, Improvement: 0.000004839, Best Loss: 0.000001827 in Epoch 1291
Epoch 1406
Epoch 1406, Loss: 0.000019865, Improvement: -0.000002596, Best Loss: 0.000001827 in Epoch 1291
Epoch 1407
Epoch 1407, Loss: 0.000017223, Improvement: -0.000002642, Best Loss: 0.000001827 in Epoch 1291
Epoch 1408
Epoch 1408, Loss: 0.000010243, Improvement: -0.000006980, Best Loss: 0.000001827 in Epoch 1291
Epoch 1409
Epoch 1409, Loss: 0.000005627, Improvement: -0.000004616, Best Loss: 0.000001827 in Epoch 1291
Epoch 1410
Epoch 1410, Loss: 0.000006276, Improvement: 0.000000649, Best Loss: 0.000001827 in Epoch 1291
Epoch 1411
Epoch 1411, Loss: 0.000007571, Improvement: 0.000001295, Best Loss: 0.000001827 in Epoch 1291
Epoch 1412
Epoch 1412, Loss: 0.000007667, Improvement: 0.000000096, Best Loss: 0.000001827 in Epoch 1291
Epoch 1413
Epoch 1413, Loss: 0.000005997, Improvement: -0.000001670, Best Loss: 0.000001827 in Epoch 1291
Epoch 1414
Epoch 1414, Loss: 0.000005006, Improvement: -0.000000990, Best Loss: 0.000001827 in Epoch 1291
Epoch 1415
Epoch 1415, Loss: 0.000003851, Improvement: -0.000001156, Best Loss: 0.000001827 in Epoch 1291
Epoch 1416
Epoch 1416, Loss: 0.000006493, Improvement: 0.000002642, Best Loss: 0.000001827 in Epoch 1291
Epoch 1417
Epoch 1417, Loss: 0.000010352, Improvement: 0.000003859, Best Loss: 0.000001827 in Epoch 1291
Epoch 1418
Epoch 1418, Loss: 0.000013440, Improvement: 0.000003088, Best Loss: 0.000001827 in Epoch 1291
Epoch 1419
Epoch 1419, Loss: 0.000008581, Improvement: -0.000004858, Best Loss: 0.000001827 in Epoch 1291
Epoch 1420
Epoch 1420, Loss: 0.000010892, Improvement: 0.000002311, Best Loss: 0.000001827 in Epoch 1291
Epoch 1421
Epoch 1421, Loss: 0.000013886, Improvement: 0.000002994, Best Loss: 0.000001827 in Epoch 1291
Epoch 1422
Epoch 1422, Loss: 0.000014063, Improvement: 0.000000177, Best Loss: 0.000001827 in Epoch 1291
Epoch 1423
Epoch 1423, Loss: 0.000009331, Improvement: -0.000004732, Best Loss: 0.000001827 in Epoch 1291
Epoch 1424
Epoch 1424, Loss: 0.000013969, Improvement: 0.000004637, Best Loss: 0.000001827 in Epoch 1291
Epoch 1425
Epoch 1425, Loss: 0.000011354, Improvement: -0.000002615, Best Loss: 0.000001827 in Epoch 1291
Epoch 1426
Epoch 1426, Loss: 0.000010062, Improvement: -0.000001292, Best Loss: 0.000001827 in Epoch 1291
Epoch 1427
Epoch 1427, Loss: 0.000009638, Improvement: -0.000000424, Best Loss: 0.000001827 in Epoch 1291
Epoch 1428
Epoch 1428, Loss: 0.000006961, Improvement: -0.000002677, Best Loss: 0.000001827 in Epoch 1291
Epoch 1429
Epoch 1429, Loss: 0.000005129, Improvement: -0.000001832, Best Loss: 0.000001827 in Epoch 1291
Epoch 1430
Epoch 1430, Loss: 0.000004143, Improvement: -0.000000986, Best Loss: 0.000001827 in Epoch 1291
Epoch 1431
Epoch 1431, Loss: 0.000003638, Improvement: -0.000000505, Best Loss: 0.000001827 in Epoch 1291
Epoch 1432
Epoch 1432, Loss: 0.000003306, Improvement: -0.000000332, Best Loss: 0.000001827 in Epoch 1291
Epoch 1433
Epoch 1433, Loss: 0.000004106, Improvement: 0.000000800, Best Loss: 0.000001827 in Epoch 1291
Epoch 1434
Epoch 1434, Loss: 0.000005737, Improvement: 0.000001631, Best Loss: 0.000001827 in Epoch 1291
Epoch 1435
Epoch 1435, Loss: 0.000012256, Improvement: 0.000006519, Best Loss: 0.000001827 in Epoch 1291
Epoch 1436
Epoch 1436, Loss: 0.000013253, Improvement: 0.000000996, Best Loss: 0.000001827 in Epoch 1291
Epoch 1437
Epoch 1437, Loss: 0.000009072, Improvement: -0.000004180, Best Loss: 0.000001827 in Epoch 1291
Epoch 1438
Epoch 1438, Loss: 0.000006718, Improvement: -0.000002354, Best Loss: 0.000001827 in Epoch 1291
Epoch 1439
Epoch 1439, Loss: 0.000008695, Improvement: 0.000001977, Best Loss: 0.000001827 in Epoch 1291
Epoch 1440
Epoch 1440, Loss: 0.000014037, Improvement: 0.000005343, Best Loss: 0.000001827 in Epoch 1291
Epoch 1441
Epoch 1441, Loss: 0.000027676, Improvement: 0.000013639, Best Loss: 0.000001827 in Epoch 1291
Epoch 1442
Epoch 1442, Loss: 0.000021995, Improvement: -0.000005681, Best Loss: 0.000001827 in Epoch 1291
Epoch 1443
Epoch 1443, Loss: 0.000037178, Improvement: 0.000015183, Best Loss: 0.000001827 in Epoch 1291
Epoch 1444
Epoch 1444, Loss: 0.000018177, Improvement: -0.000019001, Best Loss: 0.000001827 in Epoch 1291
Epoch 1445
Epoch 1445, Loss: 0.000016657, Improvement: -0.000001520, Best Loss: 0.000001827 in Epoch 1291
Epoch 1446
Epoch 1446, Loss: 0.000007312, Improvement: -0.000009346, Best Loss: 0.000001827 in Epoch 1291
Epoch 1447
Epoch 1447, Loss: 0.000004828, Improvement: -0.000002483, Best Loss: 0.000001827 in Epoch 1291
Epoch 1448
Epoch 1448, Loss: 0.000005949, Improvement: 0.000001121, Best Loss: 0.000001827 in Epoch 1291
Epoch 1449
Epoch 1449, Loss: 0.000004190, Improvement: -0.000001758, Best Loss: 0.000001827 in Epoch 1291
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.000004633, Improvement: 0.000000443, Best Loss: 0.000001827 in Epoch 1291
Epoch 1451
Epoch 1451, Loss: 0.000003441, Improvement: -0.000001191, Best Loss: 0.000001827 in Epoch 1291
Epoch 1452
Epoch 1452, Loss: 0.000002994, Improvement: -0.000000447, Best Loss: 0.000001827 in Epoch 1291
Epoch 1453
Epoch 1453, Loss: 0.000003970, Improvement: 0.000000976, Best Loss: 0.000001827 in Epoch 1291
Epoch 1454
Epoch 1454, Loss: 0.000003719, Improvement: -0.000000251, Best Loss: 0.000001827 in Epoch 1291
Epoch 1455
Epoch 1455, Loss: 0.000002533, Improvement: -0.000001186, Best Loss: 0.000001827 in Epoch 1291
Epoch 1456
A best model at epoch 1456 has been saved with training error 0.000001813.
A best model at epoch 1456 has been saved with training error 0.000001516.
Epoch 1456, Loss: 0.000002192, Improvement: -0.000000341, Best Loss: 0.000001516 in Epoch 1456
Epoch 1457
Epoch 1457, Loss: 0.000001987, Improvement: -0.000000205, Best Loss: 0.000001516 in Epoch 1456
Epoch 1458
A best model at epoch 1458 has been saved with training error 0.000001418.
Epoch 1458, Loss: 0.000001979, Improvement: -0.000000008, Best Loss: 0.000001418 in Epoch 1458
Epoch 1459
Epoch 1459, Loss: 0.000002257, Improvement: 0.000000279, Best Loss: 0.000001418 in Epoch 1458
Epoch 1460
Epoch 1460, Loss: 0.000002758, Improvement: 0.000000501, Best Loss: 0.000001418 in Epoch 1458
Epoch 1461
Epoch 1461, Loss: 0.000002459, Improvement: -0.000000299, Best Loss: 0.000001418 in Epoch 1458
Epoch 1462
Epoch 1462, Loss: 0.000002236, Improvement: -0.000000223, Best Loss: 0.000001418 in Epoch 1458
Epoch 1463
Epoch 1463, Loss: 0.000003032, Improvement: 0.000000797, Best Loss: 0.000001418 in Epoch 1458
Epoch 1464
Epoch 1464, Loss: 0.000002736, Improvement: -0.000000297, Best Loss: 0.000001418 in Epoch 1458
Epoch 1465
Epoch 1465, Loss: 0.000002266, Improvement: -0.000000470, Best Loss: 0.000001418 in Epoch 1458
Epoch 1466
Epoch 1466, Loss: 0.000002511, Improvement: 0.000000245, Best Loss: 0.000001418 in Epoch 1458
Epoch 1467
Epoch 1467, Loss: 0.000002174, Improvement: -0.000000337, Best Loss: 0.000001418 in Epoch 1458
Epoch 1468
Epoch 1468, Loss: 0.000002103, Improvement: -0.000000071, Best Loss: 0.000001418 in Epoch 1458
Epoch 1469
Epoch 1469, Loss: 0.000002231, Improvement: 0.000000128, Best Loss: 0.000001418 in Epoch 1458
Epoch 1470
Epoch 1470, Loss: 0.000003162, Improvement: 0.000000930, Best Loss: 0.000001418 in Epoch 1458
Epoch 1471
Epoch 1471, Loss: 0.000004133, Improvement: 0.000000971, Best Loss: 0.000001418 in Epoch 1458
Epoch 1472
Epoch 1472, Loss: 0.000006250, Improvement: 0.000002117, Best Loss: 0.000001418 in Epoch 1458
Epoch 1473
Epoch 1473, Loss: 0.000006379, Improvement: 0.000000129, Best Loss: 0.000001418 in Epoch 1458
Epoch 1474
Epoch 1474, Loss: 0.000008283, Improvement: 0.000001905, Best Loss: 0.000001418 in Epoch 1458
Epoch 1475
Epoch 1475, Loss: 0.000011263, Improvement: 0.000002980, Best Loss: 0.000001418 in Epoch 1458
Epoch 1476
Epoch 1476, Loss: 0.000017242, Improvement: 0.000005979, Best Loss: 0.000001418 in Epoch 1458
Epoch 1477
Epoch 1477, Loss: 0.000024073, Improvement: 0.000006831, Best Loss: 0.000001418 in Epoch 1458
Epoch 1478
Epoch 1478, Loss: 0.000013983, Improvement: -0.000010090, Best Loss: 0.000001418 in Epoch 1458
Epoch 1479
Epoch 1479, Loss: 0.000006469, Improvement: -0.000007514, Best Loss: 0.000001418 in Epoch 1458
Epoch 1480
Epoch 1480, Loss: 0.000005383, Improvement: -0.000001086, Best Loss: 0.000001418 in Epoch 1458
Epoch 1481
Epoch 1481, Loss: 0.000007341, Improvement: 0.000001958, Best Loss: 0.000001418 in Epoch 1458
Epoch 1482
Epoch 1482, Loss: 0.000006974, Improvement: -0.000000367, Best Loss: 0.000001418 in Epoch 1458
Epoch 1483
Epoch 1483, Loss: 0.000006435, Improvement: -0.000000539, Best Loss: 0.000001418 in Epoch 1458
Epoch 1484
Epoch 1484, Loss: 0.000005788, Improvement: -0.000000647, Best Loss: 0.000001418 in Epoch 1458
Epoch 1485
Epoch 1485, Loss: 0.000005069, Improvement: -0.000000720, Best Loss: 0.000001418 in Epoch 1458
Epoch 1486
Epoch 1486, Loss: 0.000009106, Improvement: 0.000004037, Best Loss: 0.000001418 in Epoch 1458
Epoch 1487
Epoch 1487, Loss: 0.000010681, Improvement: 0.000001575, Best Loss: 0.000001418 in Epoch 1458
Epoch 1488
Epoch 1488, Loss: 0.000015436, Improvement: 0.000004755, Best Loss: 0.000001418 in Epoch 1458
Epoch 1489
Epoch 1489, Loss: 0.000014322, Improvement: -0.000001113, Best Loss: 0.000001418 in Epoch 1458
Epoch 1490
Epoch 1490, Loss: 0.000016051, Improvement: 0.000001728, Best Loss: 0.000001418 in Epoch 1458
Epoch 1491
Epoch 1491, Loss: 0.000014382, Improvement: -0.000001669, Best Loss: 0.000001418 in Epoch 1458
Epoch 1492
Epoch 1492, Loss: 0.000008916, Improvement: -0.000005466, Best Loss: 0.000001418 in Epoch 1458
Epoch 1493
Epoch 1493, Loss: 0.000005668, Improvement: -0.000003248, Best Loss: 0.000001418 in Epoch 1458
Epoch 1494
Epoch 1494, Loss: 0.000006417, Improvement: 0.000000749, Best Loss: 0.000001418 in Epoch 1458
Epoch 1495
Epoch 1495, Loss: 0.000006462, Improvement: 0.000000044, Best Loss: 0.000001418 in Epoch 1458
Epoch 1496
Epoch 1496, Loss: 0.000012452, Improvement: 0.000005991, Best Loss: 0.000001418 in Epoch 1458
Epoch 1497
Epoch 1497, Loss: 0.000020664, Improvement: 0.000008212, Best Loss: 0.000001418 in Epoch 1458
Epoch 1498
Epoch 1498, Loss: 0.000011931, Improvement: -0.000008733, Best Loss: 0.000001418 in Epoch 1458
Epoch 1499
Epoch 1499, Loss: 0.000007765, Improvement: -0.000004166, Best Loss: 0.000001418 in Epoch 1458
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.000005320, Improvement: -0.000002445, Best Loss: 0.000001418 in Epoch 1458
Epoch 1501
Epoch 1501, Loss: 0.000004540, Improvement: -0.000000780, Best Loss: 0.000001418 in Epoch 1458
Epoch 1502
Epoch 1502, Loss: 0.000005320, Improvement: 0.000000780, Best Loss: 0.000001418 in Epoch 1458
Epoch 1503
Epoch 1503, Loss: 0.000003724, Improvement: -0.000001596, Best Loss: 0.000001418 in Epoch 1458
Epoch 1504
Epoch 1504, Loss: 0.000004341, Improvement: 0.000000617, Best Loss: 0.000001418 in Epoch 1458
Epoch 1505
Epoch 1505, Loss: 0.000006920, Improvement: 0.000002579, Best Loss: 0.000001418 in Epoch 1458
Epoch 1506
Epoch 1506, Loss: 0.000006899, Improvement: -0.000000021, Best Loss: 0.000001418 in Epoch 1458
Epoch 1507
Epoch 1507, Loss: 0.000006208, Improvement: -0.000000691, Best Loss: 0.000001418 in Epoch 1458
Epoch 1508
Epoch 1508, Loss: 0.000010791, Improvement: 0.000004582, Best Loss: 0.000001418 in Epoch 1458
Epoch 1509
Epoch 1509, Loss: 0.000012274, Improvement: 0.000001484, Best Loss: 0.000001418 in Epoch 1458
Epoch 1510
Epoch 1510, Loss: 0.000010530, Improvement: -0.000001744, Best Loss: 0.000001418 in Epoch 1458
Epoch 1511
Epoch 1511, Loss: 0.000017553, Improvement: 0.000007023, Best Loss: 0.000001418 in Epoch 1458
Epoch 1512
Epoch 1512, Loss: 0.000021774, Improvement: 0.000004222, Best Loss: 0.000001418 in Epoch 1458
Epoch 1513
Epoch 1513, Loss: 0.000017414, Improvement: -0.000004361, Best Loss: 0.000001418 in Epoch 1458
Epoch 1514
Epoch 1514, Loss: 0.000011107, Improvement: -0.000006306, Best Loss: 0.000001418 in Epoch 1458
Epoch 1515
Epoch 1515, Loss: 0.000007126, Improvement: -0.000003982, Best Loss: 0.000001418 in Epoch 1458
Epoch 1516
Epoch 1516, Loss: 0.000007323, Improvement: 0.000000198, Best Loss: 0.000001418 in Epoch 1458
Epoch 1517
Epoch 1517, Loss: 0.000005429, Improvement: -0.000001894, Best Loss: 0.000001418 in Epoch 1458
Epoch 1518
Epoch 1518, Loss: 0.000003498, Improvement: -0.000001931, Best Loss: 0.000001418 in Epoch 1458
Epoch 1519
Epoch 1519, Loss: 0.000002919, Improvement: -0.000000579, Best Loss: 0.000001418 in Epoch 1458
Epoch 1520
Epoch 1520, Loss: 0.000002841, Improvement: -0.000000078, Best Loss: 0.000001418 in Epoch 1458
Epoch 1521
Epoch 1521, Loss: 0.000002636, Improvement: -0.000000205, Best Loss: 0.000001418 in Epoch 1458
Epoch 1522
Epoch 1522, Loss: 0.000003439, Improvement: 0.000000803, Best Loss: 0.000001418 in Epoch 1458
Epoch 1523
Epoch 1523, Loss: 0.000002583, Improvement: -0.000000857, Best Loss: 0.000001418 in Epoch 1458
Epoch 1524
Epoch 1524, Loss: 0.000002339, Improvement: -0.000000243, Best Loss: 0.000001418 in Epoch 1458
Epoch 1525
Epoch 1525, Loss: 0.000002346, Improvement: 0.000000007, Best Loss: 0.000001418 in Epoch 1458
Epoch 1526
Epoch 1526, Loss: 0.000003291, Improvement: 0.000000945, Best Loss: 0.000001418 in Epoch 1458
Epoch 1527
Epoch 1527, Loss: 0.000004631, Improvement: 0.000001340, Best Loss: 0.000001418 in Epoch 1458
Epoch 1528
Epoch 1528, Loss: 0.000005421, Improvement: 0.000000789, Best Loss: 0.000001418 in Epoch 1458
Epoch 1529
Epoch 1529, Loss: 0.000010255, Improvement: 0.000004834, Best Loss: 0.000001418 in Epoch 1458
Epoch 1530
Epoch 1530, Loss: 0.000013219, Improvement: 0.000002964, Best Loss: 0.000001418 in Epoch 1458
Epoch 1531
Epoch 1531, Loss: 0.000009423, Improvement: -0.000003796, Best Loss: 0.000001418 in Epoch 1458
Epoch 1532
Epoch 1532, Loss: 0.000007837, Improvement: -0.000001586, Best Loss: 0.000001418 in Epoch 1458
Epoch 1533
Epoch 1533, Loss: 0.000007397, Improvement: -0.000000440, Best Loss: 0.000001418 in Epoch 1458
Epoch 1534
Epoch 1534, Loss: 0.000009058, Improvement: 0.000001661, Best Loss: 0.000001418 in Epoch 1458
Epoch 1535
Epoch 1535, Loss: 0.000007094, Improvement: -0.000001963, Best Loss: 0.000001418 in Epoch 1458
Epoch 1536
Epoch 1536, Loss: 0.000008315, Improvement: 0.000001221, Best Loss: 0.000001418 in Epoch 1458
Epoch 1537
Epoch 1537, Loss: 0.000008108, Improvement: -0.000000207, Best Loss: 0.000001418 in Epoch 1458
Epoch 1538
Epoch 1538, Loss: 0.000012575, Improvement: 0.000004467, Best Loss: 0.000001418 in Epoch 1458
Epoch 1539
Epoch 1539, Loss: 0.000025732, Improvement: 0.000013157, Best Loss: 0.000001418 in Epoch 1458
Epoch 1540
Epoch 1540, Loss: 0.000026012, Improvement: 0.000000280, Best Loss: 0.000001418 in Epoch 1458
Epoch 1541
Epoch 1541, Loss: 0.000014552, Improvement: -0.000011460, Best Loss: 0.000001418 in Epoch 1458
Epoch 1542
Epoch 1542, Loss: 0.000012274, Improvement: -0.000002278, Best Loss: 0.000001418 in Epoch 1458
Epoch 1543
Epoch 1543, Loss: 0.000005582, Improvement: -0.000006692, Best Loss: 0.000001418 in Epoch 1458
Epoch 1544
Epoch 1544, Loss: 0.000003337, Improvement: -0.000002245, Best Loss: 0.000001418 in Epoch 1458
Epoch 1545
Epoch 1545, Loss: 0.000002994, Improvement: -0.000000343, Best Loss: 0.000001418 in Epoch 1458
Epoch 1546
Epoch 1546, Loss: 0.000002678, Improvement: -0.000000316, Best Loss: 0.000001418 in Epoch 1458
Epoch 1547
Epoch 1547, Loss: 0.000002691, Improvement: 0.000000013, Best Loss: 0.000001418 in Epoch 1458
Epoch 1548
Epoch 1548, Loss: 0.000002353, Improvement: -0.000000338, Best Loss: 0.000001418 in Epoch 1458
Epoch 1549
Epoch 1549, Loss: 0.000002481, Improvement: 0.000000127, Best Loss: 0.000001418 in Epoch 1458
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.000001928, Improvement: -0.000000553, Best Loss: 0.000001418 in Epoch 1458
Epoch 1551
A best model at epoch 1551 has been saved with training error 0.000001220.
Epoch 1551, Loss: 0.000001674, Improvement: -0.000000254, Best Loss: 0.000001220 in Epoch 1551
Epoch 1552
Epoch 1552, Loss: 0.000002078, Improvement: 0.000000404, Best Loss: 0.000001220 in Epoch 1551
Epoch 1553
Epoch 1553, Loss: 0.000002125, Improvement: 0.000000047, Best Loss: 0.000001220 in Epoch 1551
Epoch 1554
Epoch 1554, Loss: 0.000002284, Improvement: 0.000000159, Best Loss: 0.000001220 in Epoch 1551
Epoch 1555
Epoch 1555, Loss: 0.000002304, Improvement: 0.000000020, Best Loss: 0.000001220 in Epoch 1551
Epoch 1556
Epoch 1556, Loss: 0.000002110, Improvement: -0.000000194, Best Loss: 0.000001220 in Epoch 1551
Epoch 1557
Epoch 1557, Loss: 0.000002106, Improvement: -0.000000004, Best Loss: 0.000001220 in Epoch 1551
Epoch 1558
Epoch 1558, Loss: 0.000002081, Improvement: -0.000000024, Best Loss: 0.000001220 in Epoch 1551
Epoch 1559
Epoch 1559, Loss: 0.000002462, Improvement: 0.000000380, Best Loss: 0.000001220 in Epoch 1551
Epoch 1560
Epoch 1560, Loss: 0.000003049, Improvement: 0.000000587, Best Loss: 0.000001220 in Epoch 1551
Epoch 1561
Epoch 1561, Loss: 0.000003530, Improvement: 0.000000481, Best Loss: 0.000001220 in Epoch 1551
Epoch 1562
Epoch 1562, Loss: 0.000003503, Improvement: -0.000000027, Best Loss: 0.000001220 in Epoch 1551
Epoch 1563
Epoch 1563, Loss: 0.000004627, Improvement: 0.000001124, Best Loss: 0.000001220 in Epoch 1551
Epoch 1564
Epoch 1564, Loss: 0.000012473, Improvement: 0.000007846, Best Loss: 0.000001220 in Epoch 1551
Epoch 1565
Epoch 1565, Loss: 0.000010168, Improvement: -0.000002305, Best Loss: 0.000001220 in Epoch 1551
Epoch 1566
Epoch 1566, Loss: 0.000008851, Improvement: -0.000001317, Best Loss: 0.000001220 in Epoch 1551
Epoch 1567
Epoch 1567, Loss: 0.000014003, Improvement: 0.000005152, Best Loss: 0.000001220 in Epoch 1551
Epoch 1568
Epoch 1568, Loss: 0.000013403, Improvement: -0.000000600, Best Loss: 0.000001220 in Epoch 1551
Epoch 1569
Epoch 1569, Loss: 0.000018161, Improvement: 0.000004759, Best Loss: 0.000001220 in Epoch 1551
Epoch 1570
Epoch 1570, Loss: 0.000033686, Improvement: 0.000015525, Best Loss: 0.000001220 in Epoch 1551
Epoch 1571
Epoch 1571, Loss: 0.000018216, Improvement: -0.000015470, Best Loss: 0.000001220 in Epoch 1551
Epoch 1572
Epoch 1572, Loss: 0.000008794, Improvement: -0.000009422, Best Loss: 0.000001220 in Epoch 1551
Epoch 1573
Epoch 1573, Loss: 0.000006021, Improvement: -0.000002773, Best Loss: 0.000001220 in Epoch 1551
Epoch 1574
Epoch 1574, Loss: 0.000004088, Improvement: -0.000001933, Best Loss: 0.000001220 in Epoch 1551
Epoch 1575
Epoch 1575, Loss: 0.000003598, Improvement: -0.000000490, Best Loss: 0.000001220 in Epoch 1551
Epoch 1576
Epoch 1576, Loss: 0.000002106, Improvement: -0.000001492, Best Loss: 0.000001220 in Epoch 1551
Epoch 1577
Epoch 1577, Loss: 0.000001795, Improvement: -0.000000311, Best Loss: 0.000001220 in Epoch 1551
Epoch 1578
Epoch 1578, Loss: 0.000001777, Improvement: -0.000000018, Best Loss: 0.000001220 in Epoch 1551
Epoch 1579
Epoch 1579, Loss: 0.000001828, Improvement: 0.000000052, Best Loss: 0.000001220 in Epoch 1551
Epoch 1580
A best model at epoch 1580 has been saved with training error 0.000001040.
Epoch 1580, Loss: 0.000001787, Improvement: -0.000000041, Best Loss: 0.000001040 in Epoch 1580
Epoch 1581
Epoch 1581, Loss: 0.000001786, Improvement: -0.000000001, Best Loss: 0.000001040 in Epoch 1580
Epoch 1582
Epoch 1582, Loss: 0.000001781, Improvement: -0.000000005, Best Loss: 0.000001040 in Epoch 1580
Epoch 1583
Epoch 1583, Loss: 0.000001757, Improvement: -0.000000025, Best Loss: 0.000001040 in Epoch 1580
Epoch 1584
Epoch 1584, Loss: 0.000001659, Improvement: -0.000000097, Best Loss: 0.000001040 in Epoch 1580
Epoch 1585
Epoch 1585, Loss: 0.000001416, Improvement: -0.000000244, Best Loss: 0.000001040 in Epoch 1580
Epoch 1586
Epoch 1586, Loss: 0.000001406, Improvement: -0.000000009, Best Loss: 0.000001040 in Epoch 1580
Epoch 1587
Epoch 1587, Loss: 0.000001414, Improvement: 0.000000008, Best Loss: 0.000001040 in Epoch 1580
Epoch 1588
Epoch 1588, Loss: 0.000001466, Improvement: 0.000000051, Best Loss: 0.000001040 in Epoch 1580
Epoch 1589
Epoch 1589, Loss: 0.000001380, Improvement: -0.000000086, Best Loss: 0.000001040 in Epoch 1580
Epoch 1590
Epoch 1590, Loss: 0.000001328, Improvement: -0.000000051, Best Loss: 0.000001040 in Epoch 1580
Epoch 1591
Epoch 1591, Loss: 0.000001492, Improvement: 0.000000164, Best Loss: 0.000001040 in Epoch 1580
Epoch 1592
A best model at epoch 1592 has been saved with training error 0.000000940.
Epoch 1592, Loss: 0.000001436, Improvement: -0.000000056, Best Loss: 0.000000940 in Epoch 1592
Epoch 1593
Epoch 1593, Loss: 0.000001511, Improvement: 0.000000076, Best Loss: 0.000000940 in Epoch 1592
Epoch 1594
Epoch 1594, Loss: 0.000001745, Improvement: 0.000000234, Best Loss: 0.000000940 in Epoch 1592
Epoch 1595
Epoch 1595, Loss: 0.000003951, Improvement: 0.000002206, Best Loss: 0.000000940 in Epoch 1592
Epoch 1596
Epoch 1596, Loss: 0.000003226, Improvement: -0.000000725, Best Loss: 0.000000940 in Epoch 1592
Epoch 1597
Epoch 1597, Loss: 0.000003368, Improvement: 0.000000142, Best Loss: 0.000000940 in Epoch 1592
Epoch 1598
Epoch 1598, Loss: 0.000007742, Improvement: 0.000004375, Best Loss: 0.000000940 in Epoch 1592
Epoch 1599
Epoch 1599, Loss: 0.000005358, Improvement: -0.000002384, Best Loss: 0.000000940 in Epoch 1592
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.000010015, Improvement: 0.000004657, Best Loss: 0.000000940 in Epoch 1592
Epoch 1601
Epoch 1601, Loss: 0.000024572, Improvement: 0.000014557, Best Loss: 0.000000940 in Epoch 1592
Epoch 1602
Epoch 1602, Loss: 0.000014428, Improvement: -0.000010144, Best Loss: 0.000000940 in Epoch 1592
Epoch 1603
Epoch 1603, Loss: 0.000007904, Improvement: -0.000006524, Best Loss: 0.000000940 in Epoch 1592
Epoch 1604
Epoch 1604, Loss: 0.000005597, Improvement: -0.000002307, Best Loss: 0.000000940 in Epoch 1592
Epoch 1605
Epoch 1605, Loss: 0.000004769, Improvement: -0.000000828, Best Loss: 0.000000940 in Epoch 1592
Epoch 1606
Epoch 1606, Loss: 0.000003528, Improvement: -0.000001241, Best Loss: 0.000000940 in Epoch 1592
Epoch 1607
Epoch 1607, Loss: 0.000005105, Improvement: 0.000001577, Best Loss: 0.000000940 in Epoch 1592
Epoch 1608
Epoch 1608, Loss: 0.000004557, Improvement: -0.000000548, Best Loss: 0.000000940 in Epoch 1592
Epoch 1609
Epoch 1609, Loss: 0.000005179, Improvement: 0.000000621, Best Loss: 0.000000940 in Epoch 1592
Epoch 1610
Epoch 1610, Loss: 0.000004538, Improvement: -0.000000641, Best Loss: 0.000000940 in Epoch 1592
Epoch 1611
Epoch 1611, Loss: 0.000006763, Improvement: 0.000002225, Best Loss: 0.000000940 in Epoch 1592
Epoch 1612
Epoch 1612, Loss: 0.000009800, Improvement: 0.000003037, Best Loss: 0.000000940 in Epoch 1592
Epoch 1613
Epoch 1613, Loss: 0.000009931, Improvement: 0.000000130, Best Loss: 0.000000940 in Epoch 1592
Epoch 1614
Epoch 1614, Loss: 0.000015143, Improvement: 0.000005212, Best Loss: 0.000000940 in Epoch 1592
Epoch 1615
Epoch 1615, Loss: 0.000011004, Improvement: -0.000004139, Best Loss: 0.000000940 in Epoch 1592
Epoch 1616
Epoch 1616, Loss: 0.000007068, Improvement: -0.000003936, Best Loss: 0.000000940 in Epoch 1592
Epoch 1617
Epoch 1617, Loss: 0.000005341, Improvement: -0.000001727, Best Loss: 0.000000940 in Epoch 1592
Epoch 1618
Epoch 1618, Loss: 0.000006822, Improvement: 0.000001481, Best Loss: 0.000000940 in Epoch 1592
Epoch 1619
Epoch 1619, Loss: 0.000006694, Improvement: -0.000000128, Best Loss: 0.000000940 in Epoch 1592
Epoch 1620
Epoch 1620, Loss: 0.000004425, Improvement: -0.000002270, Best Loss: 0.000000940 in Epoch 1592
Epoch 1621
Epoch 1621, Loss: 0.000006453, Improvement: 0.000002028, Best Loss: 0.000000940 in Epoch 1592
Epoch 1622
Epoch 1622, Loss: 0.000013191, Improvement: 0.000006738, Best Loss: 0.000000940 in Epoch 1592
Epoch 1623
Epoch 1623, Loss: 0.000013433, Improvement: 0.000000242, Best Loss: 0.000000940 in Epoch 1592
Epoch 1624
Epoch 1624, Loss: 0.000010257, Improvement: -0.000003176, Best Loss: 0.000000940 in Epoch 1592
Epoch 1625
Epoch 1625, Loss: 0.000009705, Improvement: -0.000000552, Best Loss: 0.000000940 in Epoch 1592
Epoch 1626
Epoch 1626, Loss: 0.000007306, Improvement: -0.000002398, Best Loss: 0.000000940 in Epoch 1592
Epoch 1627
Epoch 1627, Loss: 0.000005065, Improvement: -0.000002242, Best Loss: 0.000000940 in Epoch 1592
Epoch 1628
Epoch 1628, Loss: 0.000006849, Improvement: 0.000001784, Best Loss: 0.000000940 in Epoch 1592
Epoch 1629
Epoch 1629, Loss: 0.000011721, Improvement: 0.000004871, Best Loss: 0.000000940 in Epoch 1592
Epoch 1630
Epoch 1630, Loss: 0.000015879, Improvement: 0.000004158, Best Loss: 0.000000940 in Epoch 1592
Epoch 1631
Epoch 1631, Loss: 0.000008398, Improvement: -0.000007481, Best Loss: 0.000000940 in Epoch 1592
Epoch 1632
Epoch 1632, Loss: 0.000006300, Improvement: -0.000002098, Best Loss: 0.000000940 in Epoch 1592
Epoch 1633
Epoch 1633, Loss: 0.000004092, Improvement: -0.000002208, Best Loss: 0.000000940 in Epoch 1592
Epoch 1634
Epoch 1634, Loss: 0.000003957, Improvement: -0.000000135, Best Loss: 0.000000940 in Epoch 1592
Epoch 1635
Epoch 1635, Loss: 0.000003759, Improvement: -0.000000197, Best Loss: 0.000000940 in Epoch 1592
Epoch 1636
Epoch 1636, Loss: 0.000002278, Improvement: -0.000001481, Best Loss: 0.000000940 in Epoch 1592
Epoch 1637
Epoch 1637, Loss: 0.000001929, Improvement: -0.000000349, Best Loss: 0.000000940 in Epoch 1592
Epoch 1638
Epoch 1638, Loss: 0.000002792, Improvement: 0.000000863, Best Loss: 0.000000940 in Epoch 1592
Epoch 1639
Epoch 1639, Loss: 0.000003244, Improvement: 0.000000452, Best Loss: 0.000000940 in Epoch 1592
Epoch 1640
Epoch 1640, Loss: 0.000003090, Improvement: -0.000000154, Best Loss: 0.000000940 in Epoch 1592
Epoch 1641
Epoch 1641, Loss: 0.000004482, Improvement: 0.000001392, Best Loss: 0.000000940 in Epoch 1592
Epoch 1642
Epoch 1642, Loss: 0.000012353, Improvement: 0.000007871, Best Loss: 0.000000940 in Epoch 1592
Epoch 1643
Epoch 1643, Loss: 0.000014984, Improvement: 0.000002630, Best Loss: 0.000000940 in Epoch 1592
Epoch 1644
Epoch 1644, Loss: 0.000013427, Improvement: -0.000001557, Best Loss: 0.000000940 in Epoch 1592
Epoch 1645
Epoch 1645, Loss: 0.000013566, Improvement: 0.000000139, Best Loss: 0.000000940 in Epoch 1592
Epoch 1646
Epoch 1646, Loss: 0.000006947, Improvement: -0.000006619, Best Loss: 0.000000940 in Epoch 1592
Epoch 1647
Epoch 1647, Loss: 0.000004609, Improvement: -0.000002337, Best Loss: 0.000000940 in Epoch 1592
Epoch 1648
Epoch 1648, Loss: 0.000003228, Improvement: -0.000001381, Best Loss: 0.000000940 in Epoch 1592
Epoch 1649
Epoch 1649, Loss: 0.000002232, Improvement: -0.000000996, Best Loss: 0.000000940 in Epoch 1592
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.000001885, Improvement: -0.000000347, Best Loss: 0.000000940 in Epoch 1592
Epoch 1651
Epoch 1651, Loss: 0.000002295, Improvement: 0.000000410, Best Loss: 0.000000940 in Epoch 1592
Epoch 1652
Epoch 1652, Loss: 0.000004238, Improvement: 0.000001943, Best Loss: 0.000000940 in Epoch 1592
Epoch 1653
Epoch 1653, Loss: 0.000008617, Improvement: 0.000004379, Best Loss: 0.000000940 in Epoch 1592
Epoch 1654
Epoch 1654, Loss: 0.000003900, Improvement: -0.000004717, Best Loss: 0.000000940 in Epoch 1592
Epoch 1655
Epoch 1655, Loss: 0.000004510, Improvement: 0.000000610, Best Loss: 0.000000940 in Epoch 1592
Epoch 1656
Epoch 1656, Loss: 0.000004531, Improvement: 0.000000021, Best Loss: 0.000000940 in Epoch 1592
Epoch 1657
Epoch 1657, Loss: 0.000009055, Improvement: 0.000004524, Best Loss: 0.000000940 in Epoch 1592
Epoch 1658
Epoch 1658, Loss: 0.000008631, Improvement: -0.000000424, Best Loss: 0.000000940 in Epoch 1592
Epoch 1659
Epoch 1659, Loss: 0.000008944, Improvement: 0.000000313, Best Loss: 0.000000940 in Epoch 1592
Epoch 1660
Epoch 1660, Loss: 0.000011532, Improvement: 0.000002588, Best Loss: 0.000000940 in Epoch 1592
Epoch 1661
Epoch 1661, Loss: 0.000035065, Improvement: 0.000023533, Best Loss: 0.000000940 in Epoch 1592
Epoch 1662
Epoch 1662, Loss: 0.000020893, Improvement: -0.000014172, Best Loss: 0.000000940 in Epoch 1592
Epoch 1663
Epoch 1663, Loss: 0.000007500, Improvement: -0.000013392, Best Loss: 0.000000940 in Epoch 1592
Epoch 1664
Epoch 1664, Loss: 0.000004126, Improvement: -0.000003374, Best Loss: 0.000000940 in Epoch 1592
Epoch 1665
Epoch 1665, Loss: 0.000003188, Improvement: -0.000000938, Best Loss: 0.000000940 in Epoch 1592
Epoch 1666
Epoch 1666, Loss: 0.000002444, Improvement: -0.000000744, Best Loss: 0.000000940 in Epoch 1592
Epoch 1667
Epoch 1667, Loss: 0.000001777, Improvement: -0.000000667, Best Loss: 0.000000940 in Epoch 1592
Epoch 1668
Epoch 1668, Loss: 0.000001610, Improvement: -0.000000167, Best Loss: 0.000000940 in Epoch 1592
Epoch 1669
Epoch 1669, Loss: 0.000001376, Improvement: -0.000000235, Best Loss: 0.000000940 in Epoch 1592
Epoch 1670
Epoch 1670, Loss: 0.000001315, Improvement: -0.000000060, Best Loss: 0.000000940 in Epoch 1592
Epoch 1671
Epoch 1671, Loss: 0.000001345, Improvement: 0.000000029, Best Loss: 0.000000940 in Epoch 1592
Epoch 1672
Epoch 1672, Loss: 0.000001504, Improvement: 0.000000159, Best Loss: 0.000000940 in Epoch 1592
Epoch 1673
Epoch 1673, Loss: 0.000001489, Improvement: -0.000000015, Best Loss: 0.000000940 in Epoch 1592
Epoch 1674
Epoch 1674, Loss: 0.000001340, Improvement: -0.000000149, Best Loss: 0.000000940 in Epoch 1592
Epoch 1675
A best model at epoch 1675 has been saved with training error 0.000000920.
Epoch 1675, Loss: 0.000001238, Improvement: -0.000000102, Best Loss: 0.000000920 in Epoch 1675
Epoch 1676
A best model at epoch 1676 has been saved with training error 0.000000882.
Epoch 1676, Loss: 0.000001259, Improvement: 0.000000020, Best Loss: 0.000000882 in Epoch 1676
Epoch 1677
Epoch 1677, Loss: 0.000001288, Improvement: 0.000000029, Best Loss: 0.000000882 in Epoch 1676
Epoch 1678
A best model at epoch 1678 has been saved with training error 0.000000871.
Epoch 1678, Loss: 0.000001280, Improvement: -0.000000007, Best Loss: 0.000000871 in Epoch 1678
Epoch 1679
Epoch 1679, Loss: 0.000001238, Improvement: -0.000000043, Best Loss: 0.000000871 in Epoch 1678
Epoch 1680
Epoch 1680, Loss: 0.000001176, Improvement: -0.000000062, Best Loss: 0.000000871 in Epoch 1678
Epoch 1681
A best model at epoch 1681 has been saved with training error 0.000000845.
Epoch 1681, Loss: 0.000001173, Improvement: -0.000000003, Best Loss: 0.000000845 in Epoch 1681
Epoch 1682
Epoch 1682, Loss: 0.000001184, Improvement: 0.000000011, Best Loss: 0.000000845 in Epoch 1681
Epoch 1683
Epoch 1683, Loss: 0.000001169, Improvement: -0.000000016, Best Loss: 0.000000845 in Epoch 1681
Epoch 1684
Epoch 1684, Loss: 0.000001154, Improvement: -0.000000015, Best Loss: 0.000000845 in Epoch 1681
Epoch 1685
Epoch 1685, Loss: 0.000001416, Improvement: 0.000000262, Best Loss: 0.000000845 in Epoch 1681
Epoch 1686
Epoch 1686, Loss: 0.000001391, Improvement: -0.000000025, Best Loss: 0.000000845 in Epoch 1681
Epoch 1687
Epoch 1687, Loss: 0.000001471, Improvement: 0.000000079, Best Loss: 0.000000845 in Epoch 1681
Epoch 1688
Epoch 1688, Loss: 0.000002445, Improvement: 0.000000974, Best Loss: 0.000000845 in Epoch 1681
Epoch 1689
Epoch 1689, Loss: 0.000005702, Improvement: 0.000003258, Best Loss: 0.000000845 in Epoch 1681
Epoch 1690
Epoch 1690, Loss: 0.000009389, Improvement: 0.000003687, Best Loss: 0.000000845 in Epoch 1681
Epoch 1691
Epoch 1691, Loss: 0.000013074, Improvement: 0.000003685, Best Loss: 0.000000845 in Epoch 1681
Epoch 1692
Epoch 1692, Loss: 0.000017670, Improvement: 0.000004597, Best Loss: 0.000000845 in Epoch 1681
Epoch 1693
Epoch 1693, Loss: 0.000014535, Improvement: -0.000003135, Best Loss: 0.000000845 in Epoch 1681
Epoch 1694
Epoch 1694, Loss: 0.000008253, Improvement: -0.000006282, Best Loss: 0.000000845 in Epoch 1681
Epoch 1695
Epoch 1695, Loss: 0.000004367, Improvement: -0.000003886, Best Loss: 0.000000845 in Epoch 1681
Epoch 1696
Epoch 1696, Loss: 0.000003582, Improvement: -0.000000785, Best Loss: 0.000000845 in Epoch 1681
Epoch 1697
Epoch 1697, Loss: 0.000004328, Improvement: 0.000000746, Best Loss: 0.000000845 in Epoch 1681
Epoch 1698
Epoch 1698, Loss: 0.000003612, Improvement: -0.000000716, Best Loss: 0.000000845 in Epoch 1681
Epoch 1699
Epoch 1699, Loss: 0.000004364, Improvement: 0.000000753, Best Loss: 0.000000845 in Epoch 1681
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.000003307, Improvement: -0.000001057, Best Loss: 0.000000845 in Epoch 1681
Epoch 1701
Epoch 1701, Loss: 0.000003522, Improvement: 0.000000215, Best Loss: 0.000000845 in Epoch 1681
Epoch 1702
Epoch 1702, Loss: 0.000003154, Improvement: -0.000000368, Best Loss: 0.000000845 in Epoch 1681
Epoch 1703
Epoch 1703, Loss: 0.000004212, Improvement: 0.000001058, Best Loss: 0.000000845 in Epoch 1681
Epoch 1704
Epoch 1704, Loss: 0.000004651, Improvement: 0.000000438, Best Loss: 0.000000845 in Epoch 1681
Epoch 1705
Epoch 1705, Loss: 0.000003729, Improvement: -0.000000922, Best Loss: 0.000000845 in Epoch 1681
Epoch 1706
Epoch 1706, Loss: 0.000003815, Improvement: 0.000000086, Best Loss: 0.000000845 in Epoch 1681
Epoch 1707
Epoch 1707, Loss: 0.000009445, Improvement: 0.000005630, Best Loss: 0.000000845 in Epoch 1681
Epoch 1708
Epoch 1708, Loss: 0.000008279, Improvement: -0.000001166, Best Loss: 0.000000845 in Epoch 1681
Epoch 1709
Epoch 1709, Loss: 0.000006411, Improvement: -0.000001868, Best Loss: 0.000000845 in Epoch 1681
Epoch 1710
Epoch 1710, Loss: 0.000005600, Improvement: -0.000000811, Best Loss: 0.000000845 in Epoch 1681
Epoch 1711
Epoch 1711, Loss: 0.000005696, Improvement: 0.000000096, Best Loss: 0.000000845 in Epoch 1681
Epoch 1712
Epoch 1712, Loss: 0.000003688, Improvement: -0.000002008, Best Loss: 0.000000845 in Epoch 1681
Epoch 1713
Epoch 1713, Loss: 0.000006087, Improvement: 0.000002399, Best Loss: 0.000000845 in Epoch 1681
Epoch 1714
Epoch 1714, Loss: 0.000004997, Improvement: -0.000001090, Best Loss: 0.000000845 in Epoch 1681
Epoch 1715
Epoch 1715, Loss: 0.000008762, Improvement: 0.000003765, Best Loss: 0.000000845 in Epoch 1681
Epoch 1716
Epoch 1716, Loss: 0.000006928, Improvement: -0.000001834, Best Loss: 0.000000845 in Epoch 1681
Epoch 1717
Epoch 1717, Loss: 0.000012301, Improvement: 0.000005373, Best Loss: 0.000000845 in Epoch 1681
Epoch 1718
Epoch 1718, Loss: 0.000012019, Improvement: -0.000000282, Best Loss: 0.000000845 in Epoch 1681
Epoch 1719
Epoch 1719, Loss: 0.000018811, Improvement: 0.000006792, Best Loss: 0.000000845 in Epoch 1681
Epoch 1720
Epoch 1720, Loss: 0.000009069, Improvement: -0.000009741, Best Loss: 0.000000845 in Epoch 1681
Epoch 1721
Epoch 1721, Loss: 0.000006593, Improvement: -0.000002476, Best Loss: 0.000000845 in Epoch 1681
Epoch 1722
Epoch 1722, Loss: 0.000005362, Improvement: -0.000001231, Best Loss: 0.000000845 in Epoch 1681
Epoch 1723
Epoch 1723, Loss: 0.000003999, Improvement: -0.000001364, Best Loss: 0.000000845 in Epoch 1681
Epoch 1724
Epoch 1724, Loss: 0.000003809, Improvement: -0.000000190, Best Loss: 0.000000845 in Epoch 1681
Epoch 1725
Epoch 1725, Loss: 0.000004138, Improvement: 0.000000329, Best Loss: 0.000000845 in Epoch 1681
Epoch 1726
Epoch 1726, Loss: 0.000005185, Improvement: 0.000001046, Best Loss: 0.000000845 in Epoch 1681
Epoch 1727
Epoch 1727, Loss: 0.000004024, Improvement: -0.000001160, Best Loss: 0.000000845 in Epoch 1681
Epoch 1728
Epoch 1728, Loss: 0.000004783, Improvement: 0.000000759, Best Loss: 0.000000845 in Epoch 1681
Epoch 1729
Epoch 1729, Loss: 0.000004554, Improvement: -0.000000229, Best Loss: 0.000000845 in Epoch 1681
Epoch 1730
Epoch 1730, Loss: 0.000004770, Improvement: 0.000000216, Best Loss: 0.000000845 in Epoch 1681
Epoch 1731
Epoch 1731, Loss: 0.000004820, Improvement: 0.000000049, Best Loss: 0.000000845 in Epoch 1681
Epoch 1732
Epoch 1732, Loss: 0.000007704, Improvement: 0.000002885, Best Loss: 0.000000845 in Epoch 1681
Epoch 1733
Epoch 1733, Loss: 0.000010795, Improvement: 0.000003091, Best Loss: 0.000000845 in Epoch 1681
Epoch 1734
Epoch 1734, Loss: 0.000026438, Improvement: 0.000015643, Best Loss: 0.000000845 in Epoch 1681
Epoch 1735
Epoch 1735, Loss: 0.000030090, Improvement: 0.000003652, Best Loss: 0.000000845 in Epoch 1681
Epoch 1736
Epoch 1736, Loss: 0.000020690, Improvement: -0.000009400, Best Loss: 0.000000845 in Epoch 1681
Epoch 1737
Epoch 1737, Loss: 0.000011527, Improvement: -0.000009164, Best Loss: 0.000000845 in Epoch 1681
Epoch 1738
Epoch 1738, Loss: 0.000006294, Improvement: -0.000005232, Best Loss: 0.000000845 in Epoch 1681
Epoch 1739
Epoch 1739, Loss: 0.000003866, Improvement: -0.000002429, Best Loss: 0.000000845 in Epoch 1681
Epoch 1740
Epoch 1740, Loss: 0.000002841, Improvement: -0.000001025, Best Loss: 0.000000845 in Epoch 1681
Epoch 1741
Epoch 1741, Loss: 0.000002178, Improvement: -0.000000663, Best Loss: 0.000000845 in Epoch 1681
Epoch 1742
Epoch 1742, Loss: 0.000002047, Improvement: -0.000000131, Best Loss: 0.000000845 in Epoch 1681
Epoch 1743
Epoch 1743, Loss: 0.000001699, Improvement: -0.000000348, Best Loss: 0.000000845 in Epoch 1681
Epoch 1744
Epoch 1744, Loss: 0.000001395, Improvement: -0.000000304, Best Loss: 0.000000845 in Epoch 1681
Epoch 1745
Epoch 1745, Loss: 0.000001325, Improvement: -0.000000070, Best Loss: 0.000000845 in Epoch 1681
Epoch 1746
Epoch 1746, Loss: 0.000001339, Improvement: 0.000000014, Best Loss: 0.000000845 in Epoch 1681
Epoch 1747
Epoch 1747, Loss: 0.000001237, Improvement: -0.000000103, Best Loss: 0.000000845 in Epoch 1681
Epoch 1748
Epoch 1748, Loss: 0.000001177, Improvement: -0.000000059, Best Loss: 0.000000845 in Epoch 1681
Epoch 1749
A best model at epoch 1749 has been saved with training error 0.000000807.
Epoch 1749, Loss: 0.000001094, Improvement: -0.000000083, Best Loss: 0.000000807 in Epoch 1749
Epoch 1750
A best model at epoch 1750 has been saved with training error 0.000000790.
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.000001077, Improvement: -0.000000017, Best Loss: 0.000000790 in Epoch 1750
Epoch 1751
Epoch 1751, Loss: 0.000001192, Improvement: 0.000000115, Best Loss: 0.000000790 in Epoch 1750
Epoch 1752
Epoch 1752, Loss: 0.000001318, Improvement: 0.000000126, Best Loss: 0.000000790 in Epoch 1750
Epoch 1753
Epoch 1753, Loss: 0.000003244, Improvement: 0.000001926, Best Loss: 0.000000790 in Epoch 1750
Epoch 1754
Epoch 1754, Loss: 0.000003555, Improvement: 0.000000311, Best Loss: 0.000000790 in Epoch 1750
Epoch 1755
Epoch 1755, Loss: 0.000003081, Improvement: -0.000000475, Best Loss: 0.000000790 in Epoch 1750
Epoch 1756
Epoch 1756, Loss: 0.000002052, Improvement: -0.000001029, Best Loss: 0.000000790 in Epoch 1750
Epoch 1757
Epoch 1757, Loss: 0.000001764, Improvement: -0.000000287, Best Loss: 0.000000790 in Epoch 1750
Epoch 1758
Epoch 1758, Loss: 0.000001545, Improvement: -0.000000219, Best Loss: 0.000000790 in Epoch 1750
Epoch 1759
Epoch 1759, Loss: 0.000002596, Improvement: 0.000001051, Best Loss: 0.000000790 in Epoch 1750
Epoch 1760
Epoch 1760, Loss: 0.000002379, Improvement: -0.000000217, Best Loss: 0.000000790 in Epoch 1750
Epoch 1761
Epoch 1761, Loss: 0.000002026, Improvement: -0.000000353, Best Loss: 0.000000790 in Epoch 1750
Epoch 1762
Epoch 1762, Loss: 0.000002070, Improvement: 0.000000044, Best Loss: 0.000000790 in Epoch 1750
Epoch 1763
Epoch 1763, Loss: 0.000001841, Improvement: -0.000000230, Best Loss: 0.000000790 in Epoch 1750
Epoch 1764
Epoch 1764, Loss: 0.000002812, Improvement: 0.000000971, Best Loss: 0.000000790 in Epoch 1750
Epoch 1765
Epoch 1765, Loss: 0.000003856, Improvement: 0.000001044, Best Loss: 0.000000790 in Epoch 1750
Epoch 1766
Epoch 1766, Loss: 0.000005907, Improvement: 0.000002052, Best Loss: 0.000000790 in Epoch 1750
Epoch 1767
Epoch 1767, Loss: 0.000005910, Improvement: 0.000000003, Best Loss: 0.000000790 in Epoch 1750
Epoch 1768
Epoch 1768, Loss: 0.000007962, Improvement: 0.000002052, Best Loss: 0.000000790 in Epoch 1750
Epoch 1769
Epoch 1769, Loss: 0.000004354, Improvement: -0.000003608, Best Loss: 0.000000790 in Epoch 1750
Epoch 1770
Epoch 1770, Loss: 0.000003930, Improvement: -0.000000424, Best Loss: 0.000000790 in Epoch 1750
Epoch 1771
Epoch 1771, Loss: 0.000003136, Improvement: -0.000000794, Best Loss: 0.000000790 in Epoch 1750
Epoch 1772
Epoch 1772, Loss: 0.000003383, Improvement: 0.000000247, Best Loss: 0.000000790 in Epoch 1750
Epoch 1773
Epoch 1773, Loss: 0.000007362, Improvement: 0.000003979, Best Loss: 0.000000790 in Epoch 1750
Epoch 1774
Epoch 1774, Loss: 0.000009521, Improvement: 0.000002159, Best Loss: 0.000000790 in Epoch 1750
Epoch 1775
Epoch 1775, Loss: 0.000011000, Improvement: 0.000001479, Best Loss: 0.000000790 in Epoch 1750
Epoch 1776
Epoch 1776, Loss: 0.000012579, Improvement: 0.000001579, Best Loss: 0.000000790 in Epoch 1750
Epoch 1777
Epoch 1777, Loss: 0.000007667, Improvement: -0.000004911, Best Loss: 0.000000790 in Epoch 1750
Epoch 1778
Epoch 1778, Loss: 0.000006306, Improvement: -0.000001362, Best Loss: 0.000000790 in Epoch 1750
Epoch 1779
Epoch 1779, Loss: 0.000004185, Improvement: -0.000002121, Best Loss: 0.000000790 in Epoch 1750
Epoch 1780
Epoch 1780, Loss: 0.000004873, Improvement: 0.000000689, Best Loss: 0.000000790 in Epoch 1750
Epoch 1781
Epoch 1781, Loss: 0.000003369, Improvement: -0.000001504, Best Loss: 0.000000790 in Epoch 1750
Epoch 1782
Epoch 1782, Loss: 0.000004881, Improvement: 0.000001512, Best Loss: 0.000000790 in Epoch 1750
Epoch 1783
Epoch 1783, Loss: 0.000006705, Improvement: 0.000001823, Best Loss: 0.000000790 in Epoch 1750
Epoch 1784
Epoch 1784, Loss: 0.000005600, Improvement: -0.000001105, Best Loss: 0.000000790 in Epoch 1750
Epoch 1785
Epoch 1785, Loss: 0.000005244, Improvement: -0.000000356, Best Loss: 0.000000790 in Epoch 1750
Epoch 1786
Epoch 1786, Loss: 0.000006216, Improvement: 0.000000972, Best Loss: 0.000000790 in Epoch 1750
Epoch 1787
Epoch 1787, Loss: 0.000008132, Improvement: 0.000001916, Best Loss: 0.000000790 in Epoch 1750
Epoch 1788
Epoch 1788, Loss: 0.000007945, Improvement: -0.000000188, Best Loss: 0.000000790 in Epoch 1750
Epoch 1789
Epoch 1789, Loss: 0.000012167, Improvement: 0.000004222, Best Loss: 0.000000790 in Epoch 1750
Epoch 1790
Epoch 1790, Loss: 0.000011468, Improvement: -0.000000698, Best Loss: 0.000000790 in Epoch 1750
Epoch 1791
Epoch 1791, Loss: 0.000006868, Improvement: -0.000004601, Best Loss: 0.000000790 in Epoch 1750
Epoch 1792
Epoch 1792, Loss: 0.000008766, Improvement: 0.000001899, Best Loss: 0.000000790 in Epoch 1750
Epoch 1793
Epoch 1793, Loss: 0.000015583, Improvement: 0.000006816, Best Loss: 0.000000790 in Epoch 1750
Epoch 1794
Epoch 1794, Loss: 0.000010882, Improvement: -0.000004701, Best Loss: 0.000000790 in Epoch 1750
Epoch 1795
Epoch 1795, Loss: 0.000008100, Improvement: -0.000002782, Best Loss: 0.000000790 in Epoch 1750
Epoch 1796
Epoch 1796, Loss: 0.000009509, Improvement: 0.000001408, Best Loss: 0.000000790 in Epoch 1750
Epoch 1797
Epoch 1797, Loss: 0.000008163, Improvement: -0.000001346, Best Loss: 0.000000790 in Epoch 1750
Epoch 1798
Epoch 1798, Loss: 0.000005677, Improvement: -0.000002486, Best Loss: 0.000000790 in Epoch 1750
Epoch 1799
Epoch 1799, Loss: 0.000003232, Improvement: -0.000002446, Best Loss: 0.000000790 in Epoch 1750
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.000002612, Improvement: -0.000000620, Best Loss: 0.000000790 in Epoch 1750
Epoch 1801
Epoch 1801, Loss: 0.000002302, Improvement: -0.000000310, Best Loss: 0.000000790 in Epoch 1750
Epoch 1802
Epoch 1802, Loss: 0.000002035, Improvement: -0.000000267, Best Loss: 0.000000790 in Epoch 1750
Epoch 1803
Epoch 1803, Loss: 0.000001959, Improvement: -0.000000076, Best Loss: 0.000000790 in Epoch 1750
Epoch 1804
Epoch 1804, Loss: 0.000002778, Improvement: 0.000000819, Best Loss: 0.000000790 in Epoch 1750
Epoch 1805
Epoch 1805, Loss: 0.000004183, Improvement: 0.000001405, Best Loss: 0.000000790 in Epoch 1750
Epoch 1806
Epoch 1806, Loss: 0.000005027, Improvement: 0.000000844, Best Loss: 0.000000790 in Epoch 1750
Epoch 1807
Epoch 1807, Loss: 0.000006979, Improvement: 0.000001952, Best Loss: 0.000000790 in Epoch 1750
Epoch 1808
Epoch 1808, Loss: 0.000009180, Improvement: 0.000002201, Best Loss: 0.000000790 in Epoch 1750
Epoch 1809
Epoch 1809, Loss: 0.000008213, Improvement: -0.000000967, Best Loss: 0.000000790 in Epoch 1750
Epoch 1810
Epoch 1810, Loss: 0.000006500, Improvement: -0.000001713, Best Loss: 0.000000790 in Epoch 1750
Epoch 1811
Epoch 1811, Loss: 0.000003923, Improvement: -0.000002576, Best Loss: 0.000000790 in Epoch 1750
Epoch 1812
Epoch 1812, Loss: 0.000003226, Improvement: -0.000000697, Best Loss: 0.000000790 in Epoch 1750
Epoch 1813
Epoch 1813, Loss: 0.000003463, Improvement: 0.000000236, Best Loss: 0.000000790 in Epoch 1750
Epoch 1814
Epoch 1814, Loss: 0.000005987, Improvement: 0.000002525, Best Loss: 0.000000790 in Epoch 1750
Epoch 1815
Epoch 1815, Loss: 0.000014791, Improvement: 0.000008804, Best Loss: 0.000000790 in Epoch 1750
Epoch 1816
Epoch 1816, Loss: 0.000014638, Improvement: -0.000000153, Best Loss: 0.000000790 in Epoch 1750
Epoch 1817
Epoch 1817, Loss: 0.000020746, Improvement: 0.000006108, Best Loss: 0.000000790 in Epoch 1750
Epoch 1818
Epoch 1818, Loss: 0.000020636, Improvement: -0.000000110, Best Loss: 0.000000790 in Epoch 1750
Epoch 1819
Epoch 1819, Loss: 0.000014422, Improvement: -0.000006214, Best Loss: 0.000000790 in Epoch 1750
Epoch 1820
Epoch 1820, Loss: 0.000006638, Improvement: -0.000007784, Best Loss: 0.000000790 in Epoch 1750
Epoch 1821
Epoch 1821, Loss: 0.000005171, Improvement: -0.000001467, Best Loss: 0.000000790 in Epoch 1750
Epoch 1822
Epoch 1822, Loss: 0.000003076, Improvement: -0.000002096, Best Loss: 0.000000790 in Epoch 1750
Epoch 1823
Epoch 1823, Loss: 0.000003456, Improvement: 0.000000380, Best Loss: 0.000000790 in Epoch 1750
Epoch 1824
Epoch 1824, Loss: 0.000003341, Improvement: -0.000000115, Best Loss: 0.000000790 in Epoch 1750
Epoch 1825
Epoch 1825, Loss: 0.000005214, Improvement: 0.000001874, Best Loss: 0.000000790 in Epoch 1750
Epoch 1826
Epoch 1826, Loss: 0.000005003, Improvement: -0.000000212, Best Loss: 0.000000790 in Epoch 1750
Epoch 1827
Epoch 1827, Loss: 0.000004742, Improvement: -0.000000261, Best Loss: 0.000000790 in Epoch 1750
Epoch 1828
Epoch 1828, Loss: 0.000003698, Improvement: -0.000001044, Best Loss: 0.000000790 in Epoch 1750
Epoch 1829
Epoch 1829, Loss: 0.000002640, Improvement: -0.000001057, Best Loss: 0.000000790 in Epoch 1750
Epoch 1830
Epoch 1830, Loss: 0.000006470, Improvement: 0.000003830, Best Loss: 0.000000790 in Epoch 1750
Epoch 1831
Epoch 1831, Loss: 0.000007906, Improvement: 0.000001436, Best Loss: 0.000000790 in Epoch 1750
Epoch 1832
Epoch 1832, Loss: 0.000004664, Improvement: -0.000003242, Best Loss: 0.000000790 in Epoch 1750
Epoch 1833
Epoch 1833, Loss: 0.000003556, Improvement: -0.000001108, Best Loss: 0.000000790 in Epoch 1750
Epoch 1834
Epoch 1834, Loss: 0.000005003, Improvement: 0.000001447, Best Loss: 0.000000790 in Epoch 1750
Epoch 1835
Epoch 1835, Loss: 0.000009786, Improvement: 0.000004783, Best Loss: 0.000000790 in Epoch 1750
Epoch 1836
Epoch 1836, Loss: 0.000018298, Improvement: 0.000008512, Best Loss: 0.000000790 in Epoch 1750
Epoch 1837
Epoch 1837, Loss: 0.000012794, Improvement: -0.000005505, Best Loss: 0.000000790 in Epoch 1750
Epoch 1838
Epoch 1838, Loss: 0.000007986, Improvement: -0.000004808, Best Loss: 0.000000790 in Epoch 1750
Epoch 1839
Epoch 1839, Loss: 0.000005635, Improvement: -0.000002351, Best Loss: 0.000000790 in Epoch 1750
Epoch 1840
Epoch 1840, Loss: 0.000005382, Improvement: -0.000000253, Best Loss: 0.000000790 in Epoch 1750
Epoch 1841
Epoch 1841, Loss: 0.000003165, Improvement: -0.000002217, Best Loss: 0.000000790 in Epoch 1750
Epoch 1842
Epoch 1842, Loss: 0.000002555, Improvement: -0.000000610, Best Loss: 0.000000790 in Epoch 1750
Epoch 1843
Epoch 1843, Loss: 0.000006079, Improvement: 0.000003524, Best Loss: 0.000000790 in Epoch 1750
Epoch 1844
Epoch 1844, Loss: 0.000007686, Improvement: 0.000001607, Best Loss: 0.000000790 in Epoch 1750
Epoch 1845
Epoch 1845, Loss: 0.000005948, Improvement: -0.000001738, Best Loss: 0.000000790 in Epoch 1750
Epoch 1846
Epoch 1846, Loss: 0.000003392, Improvement: -0.000002555, Best Loss: 0.000000790 in Epoch 1750
Epoch 1847
Epoch 1847, Loss: 0.000002537, Improvement: -0.000000855, Best Loss: 0.000000790 in Epoch 1750
Epoch 1848
Epoch 1848, Loss: 0.000001713, Improvement: -0.000000823, Best Loss: 0.000000790 in Epoch 1750
Epoch 1849
Epoch 1849, Loss: 0.000002874, Improvement: 0.000001160, Best Loss: 0.000000790 in Epoch 1750
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.000003386, Improvement: 0.000000512, Best Loss: 0.000000790 in Epoch 1750
Epoch 1851
Epoch 1851, Loss: 0.000003786, Improvement: 0.000000400, Best Loss: 0.000000790 in Epoch 1750
Epoch 1852
Epoch 1852, Loss: 0.000002634, Improvement: -0.000001152, Best Loss: 0.000000790 in Epoch 1750
Epoch 1853
Epoch 1853, Loss: 0.000002121, Improvement: -0.000000513, Best Loss: 0.000000790 in Epoch 1750
Epoch 1854
Epoch 1854, Loss: 0.000002362, Improvement: 0.000000241, Best Loss: 0.000000790 in Epoch 1750
Epoch 1855
Epoch 1855, Loss: 0.000002594, Improvement: 0.000000231, Best Loss: 0.000000790 in Epoch 1750
Epoch 1856
Epoch 1856, Loss: 0.000002621, Improvement: 0.000000027, Best Loss: 0.000000790 in Epoch 1750
Epoch 1857
Epoch 1857, Loss: 0.000003188, Improvement: 0.000000567, Best Loss: 0.000000790 in Epoch 1750
Epoch 1858
Epoch 1858, Loss: 0.000004328, Improvement: 0.000001140, Best Loss: 0.000000790 in Epoch 1750
Epoch 1859
Epoch 1859, Loss: 0.000003415, Improvement: -0.000000913, Best Loss: 0.000000790 in Epoch 1750
Epoch 1860
Epoch 1860, Loss: 0.000003921, Improvement: 0.000000506, Best Loss: 0.000000790 in Epoch 1750
Epoch 1861
Epoch 1861, Loss: 0.000005663, Improvement: 0.000001742, Best Loss: 0.000000790 in Epoch 1750
Epoch 1862
Epoch 1862, Loss: 0.000006417, Improvement: 0.000000754, Best Loss: 0.000000790 in Epoch 1750
Epoch 1863
Epoch 1863, Loss: 0.000028444, Improvement: 0.000022027, Best Loss: 0.000000790 in Epoch 1750
Epoch 1864
Epoch 1864, Loss: 0.000025605, Improvement: -0.000002838, Best Loss: 0.000000790 in Epoch 1750
Epoch 1865
Epoch 1865, Loss: 0.000019337, Improvement: -0.000006268, Best Loss: 0.000000790 in Epoch 1750
Epoch 1866
Epoch 1866, Loss: 0.000009479, Improvement: -0.000009858, Best Loss: 0.000000790 in Epoch 1750
Epoch 1867
Epoch 1867, Loss: 0.000005999, Improvement: -0.000003480, Best Loss: 0.000000790 in Epoch 1750
Epoch 1868
Epoch 1868, Loss: 0.000004085, Improvement: -0.000001914, Best Loss: 0.000000790 in Epoch 1750
Epoch 1869
Epoch 1869, Loss: 0.000002743, Improvement: -0.000001343, Best Loss: 0.000000790 in Epoch 1750
Epoch 1870
Epoch 1870, Loss: 0.000001679, Improvement: -0.000001064, Best Loss: 0.000000790 in Epoch 1750
Epoch 1871
Epoch 1871, Loss: 0.000001233, Improvement: -0.000000446, Best Loss: 0.000000790 in Epoch 1750
Epoch 1872
Epoch 1872, Loss: 0.000001231, Improvement: -0.000000002, Best Loss: 0.000000790 in Epoch 1750
Epoch 1873
A best model at epoch 1873 has been saved with training error 0.000000775.
Epoch 1873, Loss: 0.000001085, Improvement: -0.000000146, Best Loss: 0.000000775 in Epoch 1873
Epoch 1874
A best model at epoch 1874 has been saved with training error 0.000000729.
Epoch 1874, Loss: 0.000000995, Improvement: -0.000000090, Best Loss: 0.000000729 in Epoch 1874
Epoch 1875
A best model at epoch 1875 has been saved with training error 0.000000638.
Epoch 1875, Loss: 0.000001097, Improvement: 0.000000102, Best Loss: 0.000000638 in Epoch 1875
Epoch 1876
Epoch 1876, Loss: 0.000000987, Improvement: -0.000000110, Best Loss: 0.000000638 in Epoch 1875
Epoch 1877
Epoch 1877, Loss: 0.000000933, Improvement: -0.000000054, Best Loss: 0.000000638 in Epoch 1875
Epoch 1878
Epoch 1878, Loss: 0.000000934, Improvement: 0.000000000, Best Loss: 0.000000638 in Epoch 1875
Epoch 1879
Epoch 1879, Loss: 0.000000917, Improvement: -0.000000017, Best Loss: 0.000000638 in Epoch 1875
Epoch 1880
Epoch 1880, Loss: 0.000000895, Improvement: -0.000000022, Best Loss: 0.000000638 in Epoch 1875
Epoch 1881
Epoch 1881, Loss: 0.000000900, Improvement: 0.000000005, Best Loss: 0.000000638 in Epoch 1875
Epoch 1882
Epoch 1882, Loss: 0.000000924, Improvement: 0.000000024, Best Loss: 0.000000638 in Epoch 1875
Epoch 1883
Epoch 1883, Loss: 0.000000900, Improvement: -0.000000024, Best Loss: 0.000000638 in Epoch 1875
Epoch 1884
A best model at epoch 1884 has been saved with training error 0.000000632.
Epoch 1884, Loss: 0.000000900, Improvement: 0.000000000, Best Loss: 0.000000632 in Epoch 1884
Epoch 1885
Epoch 1885, Loss: 0.000000869, Improvement: -0.000000032, Best Loss: 0.000000632 in Epoch 1884
Epoch 1886
Epoch 1886, Loss: 0.000000895, Improvement: 0.000000026, Best Loss: 0.000000632 in Epoch 1884
Epoch 1887
A best model at epoch 1887 has been saved with training error 0.000000624.
Epoch 1887, Loss: 0.000000877, Improvement: -0.000000018, Best Loss: 0.000000624 in Epoch 1887
Epoch 1888
Epoch 1888, Loss: 0.000000879, Improvement: 0.000000002, Best Loss: 0.000000624 in Epoch 1887
Epoch 1889
Epoch 1889, Loss: 0.000000914, Improvement: 0.000000035, Best Loss: 0.000000624 in Epoch 1887
Epoch 1890
Epoch 1890, Loss: 0.000001014, Improvement: 0.000000099, Best Loss: 0.000000624 in Epoch 1887
Epoch 1891
Epoch 1891, Loss: 0.000000987, Improvement: -0.000000026, Best Loss: 0.000000624 in Epoch 1887
Epoch 1892
Epoch 1892, Loss: 0.000001004, Improvement: 0.000000017, Best Loss: 0.000000624 in Epoch 1887
Epoch 1893
Epoch 1893, Loss: 0.000000985, Improvement: -0.000000019, Best Loss: 0.000000624 in Epoch 1887
Epoch 1894
Epoch 1894, Loss: 0.000001018, Improvement: 0.000000033, Best Loss: 0.000000624 in Epoch 1887
Epoch 1895
Epoch 1895, Loss: 0.000001277, Improvement: 0.000000259, Best Loss: 0.000000624 in Epoch 1887
Epoch 1896
Epoch 1896, Loss: 0.000001518, Improvement: 0.000000241, Best Loss: 0.000000624 in Epoch 1887
Epoch 1897
Epoch 1897, Loss: 0.000001985, Improvement: 0.000000467, Best Loss: 0.000000624 in Epoch 1887
Epoch 1898
Epoch 1898, Loss: 0.000003370, Improvement: 0.000001385, Best Loss: 0.000000624 in Epoch 1887
Epoch 1899
Epoch 1899, Loss: 0.000005742, Improvement: 0.000002372, Best Loss: 0.000000624 in Epoch 1887
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.000006335, Improvement: 0.000000594, Best Loss: 0.000000624 in Epoch 1887
Epoch 1901
Epoch 1901, Loss: 0.000005736, Improvement: -0.000000599, Best Loss: 0.000000624 in Epoch 1887
Epoch 1902
Epoch 1902, Loss: 0.000007832, Improvement: 0.000002096, Best Loss: 0.000000624 in Epoch 1887
Epoch 1903
Epoch 1903, Loss: 0.000008452, Improvement: 0.000000620, Best Loss: 0.000000624 in Epoch 1887
Epoch 1904
Epoch 1904, Loss: 0.000010403, Improvement: 0.000001952, Best Loss: 0.000000624 in Epoch 1887
Epoch 1905
Epoch 1905, Loss: 0.000010853, Improvement: 0.000000450, Best Loss: 0.000000624 in Epoch 1887
Epoch 1906
Epoch 1906, Loss: 0.000008873, Improvement: -0.000001981, Best Loss: 0.000000624 in Epoch 1887
Epoch 1907
Epoch 1907, Loss: 0.000005952, Improvement: -0.000002920, Best Loss: 0.000000624 in Epoch 1887
Epoch 1908
Epoch 1908, Loss: 0.000006672, Improvement: 0.000000720, Best Loss: 0.000000624 in Epoch 1887
Epoch 1909
Epoch 1909, Loss: 0.000007934, Improvement: 0.000001262, Best Loss: 0.000000624 in Epoch 1887
Epoch 1910
Epoch 1910, Loss: 0.000007376, Improvement: -0.000000558, Best Loss: 0.000000624 in Epoch 1887
Epoch 1911
Epoch 1911, Loss: 0.000007756, Improvement: 0.000000380, Best Loss: 0.000000624 in Epoch 1887
Epoch 1912
Epoch 1912, Loss: 0.000006108, Improvement: -0.000001649, Best Loss: 0.000000624 in Epoch 1887
Epoch 1913
Epoch 1913, Loss: 0.000009289, Improvement: 0.000003182, Best Loss: 0.000000624 in Epoch 1887
Epoch 1914
Epoch 1914, Loss: 0.000007855, Improvement: -0.000001434, Best Loss: 0.000000624 in Epoch 1887
Epoch 1915
Epoch 1915, Loss: 0.000005507, Improvement: -0.000002348, Best Loss: 0.000000624 in Epoch 1887
Epoch 1916
Epoch 1916, Loss: 0.000004428, Improvement: -0.000001079, Best Loss: 0.000000624 in Epoch 1887
Epoch 1917
Epoch 1917, Loss: 0.000003303, Improvement: -0.000001124, Best Loss: 0.000000624 in Epoch 1887
Epoch 1918
Epoch 1918, Loss: 0.000006192, Improvement: 0.000002888, Best Loss: 0.000000624 in Epoch 1887
Epoch 1919
Epoch 1919, Loss: 0.000010888, Improvement: 0.000004696, Best Loss: 0.000000624 in Epoch 1887
Epoch 1920
Epoch 1920, Loss: 0.000008596, Improvement: -0.000002292, Best Loss: 0.000000624 in Epoch 1887
Epoch 1921
Epoch 1921, Loss: 0.000010925, Improvement: 0.000002329, Best Loss: 0.000000624 in Epoch 1887
Epoch 1922
Epoch 1922, Loss: 0.000007018, Improvement: -0.000003906, Best Loss: 0.000000624 in Epoch 1887
Epoch 1923
Epoch 1923, Loss: 0.000005496, Improvement: -0.000001523, Best Loss: 0.000000624 in Epoch 1887
Epoch 1924
Epoch 1924, Loss: 0.000007643, Improvement: 0.000002147, Best Loss: 0.000000624 in Epoch 1887
Epoch 1925
Epoch 1925, Loss: 0.000009285, Improvement: 0.000001642, Best Loss: 0.000000624 in Epoch 1887
Epoch 1926
Epoch 1926, Loss: 0.000006120, Improvement: -0.000003165, Best Loss: 0.000000624 in Epoch 1887
Epoch 1927
Epoch 1927, Loss: 0.000010251, Improvement: 0.000004131, Best Loss: 0.000000624 in Epoch 1887
Epoch 1928
Epoch 1928, Loss: 0.000009096, Improvement: -0.000001155, Best Loss: 0.000000624 in Epoch 1887
Epoch 1929
Epoch 1929, Loss: 0.000006800, Improvement: -0.000002296, Best Loss: 0.000000624 in Epoch 1887
Epoch 1930
Epoch 1930, Loss: 0.000004090, Improvement: -0.000002710, Best Loss: 0.000000624 in Epoch 1887
Epoch 1931
Epoch 1931, Loss: 0.000002853, Improvement: -0.000001237, Best Loss: 0.000000624 in Epoch 1887
Epoch 1932
Epoch 1932, Loss: 0.000003806, Improvement: 0.000000953, Best Loss: 0.000000624 in Epoch 1887
Epoch 1933
Epoch 1933, Loss: 0.000005479, Improvement: 0.000001672, Best Loss: 0.000000624 in Epoch 1887
Epoch 1934
Epoch 1934, Loss: 0.000004364, Improvement: -0.000001115, Best Loss: 0.000000624 in Epoch 1887
Epoch 1935
Epoch 1935, Loss: 0.000004863, Improvement: 0.000000499, Best Loss: 0.000000624 in Epoch 1887
Epoch 1936
Epoch 1936, Loss: 0.000003100, Improvement: -0.000001764, Best Loss: 0.000000624 in Epoch 1887
Epoch 1937
Epoch 1937, Loss: 0.000003000, Improvement: -0.000000100, Best Loss: 0.000000624 in Epoch 1887
Epoch 1938
Epoch 1938, Loss: 0.000006107, Improvement: 0.000003107, Best Loss: 0.000000624 in Epoch 1887
Epoch 1939
Epoch 1939, Loss: 0.000010198, Improvement: 0.000004091, Best Loss: 0.000000624 in Epoch 1887
Epoch 1940
Epoch 1940, Loss: 0.000017110, Improvement: 0.000006912, Best Loss: 0.000000624 in Epoch 1887
Epoch 1941
Epoch 1941, Loss: 0.000015084, Improvement: -0.000002026, Best Loss: 0.000000624 in Epoch 1887
Epoch 1942
Epoch 1942, Loss: 0.000009928, Improvement: -0.000005156, Best Loss: 0.000000624 in Epoch 1887
Epoch 1943
Epoch 1943, Loss: 0.000005446, Improvement: -0.000004482, Best Loss: 0.000000624 in Epoch 1887
Epoch 1944
Epoch 1944, Loss: 0.000003607, Improvement: -0.000001839, Best Loss: 0.000000624 in Epoch 1887
Epoch 1945
Epoch 1945, Loss: 0.000002537, Improvement: -0.000001070, Best Loss: 0.000000624 in Epoch 1887
Epoch 1946
Epoch 1946, Loss: 0.000002260, Improvement: -0.000000278, Best Loss: 0.000000624 in Epoch 1887
Epoch 1947
Epoch 1947, Loss: 0.000002210, Improvement: -0.000000049, Best Loss: 0.000000624 in Epoch 1887
Epoch 1948
Epoch 1948, Loss: 0.000003831, Improvement: 0.000001621, Best Loss: 0.000000624 in Epoch 1887
Epoch 1949
Epoch 1949, Loss: 0.000003101, Improvement: -0.000000731, Best Loss: 0.000000624 in Epoch 1887
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.000002635, Improvement: -0.000000466, Best Loss: 0.000000624 in Epoch 1887
Epoch 1951
Epoch 1951, Loss: 0.000003307, Improvement: 0.000000672, Best Loss: 0.000000624 in Epoch 1887
Epoch 1952
Epoch 1952, Loss: 0.000002629, Improvement: -0.000000679, Best Loss: 0.000000624 in Epoch 1887
Epoch 1953
Epoch 1953, Loss: 0.000004179, Improvement: 0.000001550, Best Loss: 0.000000624 in Epoch 1887
Epoch 1954
Epoch 1954, Loss: 0.000007545, Improvement: 0.000003366, Best Loss: 0.000000624 in Epoch 1887
Epoch 1955
Epoch 1955, Loss: 0.000008036, Improvement: 0.000000491, Best Loss: 0.000000624 in Epoch 1887
Epoch 1956
Epoch 1956, Loss: 0.000005389, Improvement: -0.000002648, Best Loss: 0.000000624 in Epoch 1887
Epoch 1957
Epoch 1957, Loss: 0.000004633, Improvement: -0.000000755, Best Loss: 0.000000624 in Epoch 1887
Epoch 1958
Epoch 1958, Loss: 0.000006434, Improvement: 0.000001801, Best Loss: 0.000000624 in Epoch 1887
Epoch 1959
Epoch 1959, Loss: 0.000011299, Improvement: 0.000004865, Best Loss: 0.000000624 in Epoch 1887
Epoch 1960
Epoch 1960, Loss: 0.000007314, Improvement: -0.000003985, Best Loss: 0.000000624 in Epoch 1887
Epoch 1961
Epoch 1961, Loss: 0.000006213, Improvement: -0.000001101, Best Loss: 0.000000624 in Epoch 1887
Epoch 1962
Epoch 1962, Loss: 0.000005214, Improvement: -0.000000999, Best Loss: 0.000000624 in Epoch 1887
Epoch 1963
Epoch 1963, Loss: 0.000003821, Improvement: -0.000001393, Best Loss: 0.000000624 in Epoch 1887
Epoch 1964
Epoch 1964, Loss: 0.000002958, Improvement: -0.000000863, Best Loss: 0.000000624 in Epoch 1887
Epoch 1965
Epoch 1965, Loss: 0.000003267, Improvement: 0.000000309, Best Loss: 0.000000624 in Epoch 1887
Epoch 1966
Epoch 1966, Loss: 0.000003145, Improvement: -0.000000123, Best Loss: 0.000000624 in Epoch 1887
Epoch 1967
Epoch 1967, Loss: 0.000002820, Improvement: -0.000000325, Best Loss: 0.000000624 in Epoch 1887
Epoch 1968
Epoch 1968, Loss: 0.000002449, Improvement: -0.000000370, Best Loss: 0.000000624 in Epoch 1887
Epoch 1969
Epoch 1969, Loss: 0.000002606, Improvement: 0.000000157, Best Loss: 0.000000624 in Epoch 1887
Epoch 1970
Epoch 1970, Loss: 0.000006001, Improvement: 0.000003395, Best Loss: 0.000000624 in Epoch 1887
Epoch 1971
Epoch 1971, Loss: 0.000015287, Improvement: 0.000009286, Best Loss: 0.000000624 in Epoch 1887
Epoch 1972
Epoch 1972, Loss: 0.000015184, Improvement: -0.000000103, Best Loss: 0.000000624 in Epoch 1887
Epoch 1973
Epoch 1973, Loss: 0.000013751, Improvement: -0.000001433, Best Loss: 0.000000624 in Epoch 1887
Epoch 1974
Epoch 1974, Loss: 0.000011682, Improvement: -0.000002070, Best Loss: 0.000000624 in Epoch 1887
Epoch 1975
Epoch 1975, Loss: 0.000005496, Improvement: -0.000006186, Best Loss: 0.000000624 in Epoch 1887
Epoch 1976
Epoch 1976, Loss: 0.000003882, Improvement: -0.000001614, Best Loss: 0.000000624 in Epoch 1887
Epoch 1977
Epoch 1977, Loss: 0.000003237, Improvement: -0.000000644, Best Loss: 0.000000624 in Epoch 1887
Epoch 1978
Epoch 1978, Loss: 0.000004247, Improvement: 0.000001009, Best Loss: 0.000000624 in Epoch 1887
Epoch 1979
Epoch 1979, Loss: 0.000004057, Improvement: -0.000000190, Best Loss: 0.000000624 in Epoch 1887
Epoch 1980
Epoch 1980, Loss: 0.000003826, Improvement: -0.000000231, Best Loss: 0.000000624 in Epoch 1887
Epoch 1981
Epoch 1981, Loss: 0.000003246, Improvement: -0.000000580, Best Loss: 0.000000624 in Epoch 1887
Epoch 1982
Epoch 1982, Loss: 0.000003390, Improvement: 0.000000144, Best Loss: 0.000000624 in Epoch 1887
Epoch 1983
Epoch 1983, Loss: 0.000003335, Improvement: -0.000000054, Best Loss: 0.000000624 in Epoch 1887
Epoch 1984
Epoch 1984, Loss: 0.000002779, Improvement: -0.000000556, Best Loss: 0.000000624 in Epoch 1887
Epoch 1985
Epoch 1985, Loss: 0.000002969, Improvement: 0.000000190, Best Loss: 0.000000624 in Epoch 1887
Epoch 1986
Epoch 1986, Loss: 0.000004258, Improvement: 0.000001289, Best Loss: 0.000000624 in Epoch 1887
Epoch 1987
Epoch 1987, Loss: 0.000007716, Improvement: 0.000003458, Best Loss: 0.000000624 in Epoch 1887
Epoch 1988
Epoch 1988, Loss: 0.000009750, Improvement: 0.000002034, Best Loss: 0.000000624 in Epoch 1887
Epoch 1989
Epoch 1989, Loss: 0.000020919, Improvement: 0.000011169, Best Loss: 0.000000624 in Epoch 1887
Epoch 1990
Epoch 1990, Loss: 0.000013795, Improvement: -0.000007125, Best Loss: 0.000000624 in Epoch 1887
Epoch 1991
Epoch 1991, Loss: 0.000010335, Improvement: -0.000003460, Best Loss: 0.000000624 in Epoch 1887
Epoch 1992
Epoch 1992, Loss: 0.000005497, Improvement: -0.000004838, Best Loss: 0.000000624 in Epoch 1887
Epoch 1993
Epoch 1993, Loss: 0.000002315, Improvement: -0.000003181, Best Loss: 0.000000624 in Epoch 1887
Epoch 1994
Epoch 1994, Loss: 0.000001419, Improvement: -0.000000897, Best Loss: 0.000000624 in Epoch 1887
Epoch 1995
Epoch 1995, Loss: 0.000001283, Improvement: -0.000000135, Best Loss: 0.000000624 in Epoch 1887
Epoch 1996
Epoch 1996, Loss: 0.000001132, Improvement: -0.000000151, Best Loss: 0.000000624 in Epoch 1887
Epoch 1997
Epoch 1997, Loss: 0.000001002, Improvement: -0.000000130, Best Loss: 0.000000624 in Epoch 1887
Epoch 1998
Epoch 1998, Loss: 0.000001209, Improvement: 0.000000207, Best Loss: 0.000000624 in Epoch 1887
Epoch 1999
Epoch 1999, Loss: 0.000001048, Improvement: -0.000000161, Best Loss: 0.000000624 in Epoch 1887
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.000001022, Improvement: -0.000000026, Best Loss: 0.000000624 in Epoch 1887
Epoch 2001
Epoch 2001, Loss: 0.000000888, Improvement: -0.000000134, Best Loss: 0.000000624 in Epoch 1887
Epoch 2002
Epoch 2002, Loss: 0.000001005, Improvement: 0.000000117, Best Loss: 0.000000624 in Epoch 1887
Epoch 2003
Epoch 2003, Loss: 0.000001196, Improvement: 0.000000191, Best Loss: 0.000000624 in Epoch 1887
Epoch 2004
Epoch 2004, Loss: 0.000001576, Improvement: 0.000000380, Best Loss: 0.000000624 in Epoch 1887
Epoch 2005
Epoch 2005, Loss: 0.000002014, Improvement: 0.000000438, Best Loss: 0.000000624 in Epoch 1887
Epoch 2006
Epoch 2006, Loss: 0.000002098, Improvement: 0.000000084, Best Loss: 0.000000624 in Epoch 1887
Epoch 2007
Epoch 2007, Loss: 0.000001982, Improvement: -0.000000116, Best Loss: 0.000000624 in Epoch 1887
Epoch 2008
Epoch 2008, Loss: 0.000001809, Improvement: -0.000000173, Best Loss: 0.000000624 in Epoch 1887
Epoch 2009
Epoch 2009, Loss: 0.000002346, Improvement: 0.000000537, Best Loss: 0.000000624 in Epoch 1887
Epoch 2010
Epoch 2010, Loss: 0.000001728, Improvement: -0.000000618, Best Loss: 0.000000624 in Epoch 1887
Epoch 2011
Epoch 2011, Loss: 0.000001921, Improvement: 0.000000193, Best Loss: 0.000000624 in Epoch 1887
Epoch 2012
Epoch 2012, Loss: 0.000002338, Improvement: 0.000000417, Best Loss: 0.000000624 in Epoch 1887
Epoch 2013
Epoch 2013, Loss: 0.000002844, Improvement: 0.000000506, Best Loss: 0.000000624 in Epoch 1887
Epoch 2014
Epoch 2014, Loss: 0.000006917, Improvement: 0.000004073, Best Loss: 0.000000624 in Epoch 1887
Epoch 2015
Epoch 2015, Loss: 0.000016272, Improvement: 0.000009355, Best Loss: 0.000000624 in Epoch 1887
Epoch 2016
Epoch 2016, Loss: 0.000024729, Improvement: 0.000008457, Best Loss: 0.000000624 in Epoch 1887
Epoch 2017
Epoch 2017, Loss: 0.000018033, Improvement: -0.000006696, Best Loss: 0.000000624 in Epoch 1887
Epoch 2018
Epoch 2018, Loss: 0.000010462, Improvement: -0.000007570, Best Loss: 0.000000624 in Epoch 1887
Epoch 2019
Epoch 2019, Loss: 0.000006377, Improvement: -0.000004085, Best Loss: 0.000000624 in Epoch 1887
Epoch 2020
Epoch 2020, Loss: 0.000003129, Improvement: -0.000003249, Best Loss: 0.000000624 in Epoch 1887
Epoch 2021
Epoch 2021, Loss: 0.000002437, Improvement: -0.000000691, Best Loss: 0.000000624 in Epoch 1887
Epoch 2022
Epoch 2022, Loss: 0.000001590, Improvement: -0.000000847, Best Loss: 0.000000624 in Epoch 1887
Epoch 2023
Epoch 2023, Loss: 0.000001198, Improvement: -0.000000393, Best Loss: 0.000000624 in Epoch 1887
Epoch 2024
Epoch 2024, Loss: 0.000001453, Improvement: 0.000000256, Best Loss: 0.000000624 in Epoch 1887
Epoch 2025
Epoch 2025, Loss: 0.000001295, Improvement: -0.000000159, Best Loss: 0.000000624 in Epoch 1887
Epoch 2026
Epoch 2026, Loss: 0.000001211, Improvement: -0.000000084, Best Loss: 0.000000624 in Epoch 1887
Epoch 2027
Epoch 2027, Loss: 0.000001139, Improvement: -0.000000072, Best Loss: 0.000000624 in Epoch 1887
Epoch 2028
Epoch 2028, Loss: 0.000001196, Improvement: 0.000000057, Best Loss: 0.000000624 in Epoch 1887
Epoch 2029
Epoch 2029, Loss: 0.000001151, Improvement: -0.000000045, Best Loss: 0.000000624 in Epoch 1887
Epoch 2030
Epoch 2030, Loss: 0.000000969, Improvement: -0.000000182, Best Loss: 0.000000624 in Epoch 1887
Epoch 2031
A best model at epoch 2031 has been saved with training error 0.000000570.
Epoch 2031, Loss: 0.000000914, Improvement: -0.000000055, Best Loss: 0.000000570 in Epoch 2031
Epoch 2032
Epoch 2032, Loss: 0.000000879, Improvement: -0.000000035, Best Loss: 0.000000570 in Epoch 2031
Epoch 2033
Epoch 2033, Loss: 0.000000910, Improvement: 0.000000031, Best Loss: 0.000000570 in Epoch 2031
Epoch 2034
Epoch 2034, Loss: 0.000000972, Improvement: 0.000000062, Best Loss: 0.000000570 in Epoch 2031
Epoch 2035
Epoch 2035, Loss: 0.000001025, Improvement: 0.000000053, Best Loss: 0.000000570 in Epoch 2031
Epoch 2036
Epoch 2036, Loss: 0.000001148, Improvement: 0.000000123, Best Loss: 0.000000570 in Epoch 2031
Epoch 2037
Epoch 2037, Loss: 0.000001493, Improvement: 0.000000345, Best Loss: 0.000000570 in Epoch 2031
Epoch 2038
Epoch 2038, Loss: 0.000001635, Improvement: 0.000000142, Best Loss: 0.000000570 in Epoch 2031
Epoch 2039
Epoch 2039, Loss: 0.000001748, Improvement: 0.000000113, Best Loss: 0.000000570 in Epoch 2031
Epoch 2040
Epoch 2040, Loss: 0.000001383, Improvement: -0.000000364, Best Loss: 0.000000570 in Epoch 2031
Epoch 2041
Epoch 2041, Loss: 0.000001654, Improvement: 0.000000271, Best Loss: 0.000000570 in Epoch 2031
Epoch 2042
Epoch 2042, Loss: 0.000001715, Improvement: 0.000000061, Best Loss: 0.000000570 in Epoch 2031
Epoch 2043
Epoch 2043, Loss: 0.000002659, Improvement: 0.000000944, Best Loss: 0.000000570 in Epoch 2031
Epoch 2044
Epoch 2044, Loss: 0.000009143, Improvement: 0.000006484, Best Loss: 0.000000570 in Epoch 2031
Epoch 2045
Epoch 2045, Loss: 0.000008906, Improvement: -0.000000237, Best Loss: 0.000000570 in Epoch 2031
Epoch 2046
Epoch 2046, Loss: 0.000015589, Improvement: 0.000006683, Best Loss: 0.000000570 in Epoch 2031
Epoch 2047
Epoch 2047, Loss: 0.000011104, Improvement: -0.000004485, Best Loss: 0.000000570 in Epoch 2031
Epoch 2048
Epoch 2048, Loss: 0.000007320, Improvement: -0.000003784, Best Loss: 0.000000570 in Epoch 2031
Epoch 2049
Epoch 2049, Loss: 0.000003511, Improvement: -0.000003809, Best Loss: 0.000000570 in Epoch 2031
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.000002780, Improvement: -0.000000731, Best Loss: 0.000000570 in Epoch 2031
Epoch 2051
Epoch 2051, Loss: 0.000002181, Improvement: -0.000000599, Best Loss: 0.000000570 in Epoch 2031
Epoch 2052
Epoch 2052, Loss: 0.000001899, Improvement: -0.000000282, Best Loss: 0.000000570 in Epoch 2031
Epoch 2053
Epoch 2053, Loss: 0.000001351, Improvement: -0.000000548, Best Loss: 0.000000570 in Epoch 2031
Epoch 2054
Epoch 2054, Loss: 0.000001779, Improvement: 0.000000428, Best Loss: 0.000000570 in Epoch 2031
Epoch 2055
Epoch 2055, Loss: 0.000001783, Improvement: 0.000000004, Best Loss: 0.000000570 in Epoch 2031
Epoch 2056
Epoch 2056, Loss: 0.000001863, Improvement: 0.000000080, Best Loss: 0.000000570 in Epoch 2031
Epoch 2057
Epoch 2057, Loss: 0.000002205, Improvement: 0.000000342, Best Loss: 0.000000570 in Epoch 2031
Epoch 2058
Epoch 2058, Loss: 0.000002285, Improvement: 0.000000080, Best Loss: 0.000000570 in Epoch 2031
Epoch 2059
Epoch 2059, Loss: 0.000003346, Improvement: 0.000001061, Best Loss: 0.000000570 in Epoch 2031
Epoch 2060
Epoch 2060, Loss: 0.000004033, Improvement: 0.000000687, Best Loss: 0.000000570 in Epoch 2031
Epoch 2061
Epoch 2061, Loss: 0.000006962, Improvement: 0.000002929, Best Loss: 0.000000570 in Epoch 2031
Epoch 2062
Epoch 2062, Loss: 0.000011771, Improvement: 0.000004808, Best Loss: 0.000000570 in Epoch 2031
Epoch 2063
Epoch 2063, Loss: 0.000008135, Improvement: -0.000003635, Best Loss: 0.000000570 in Epoch 2031
Epoch 2064
Epoch 2064, Loss: 0.000009300, Improvement: 0.000001165, Best Loss: 0.000000570 in Epoch 2031
Epoch 2065
Epoch 2065, Loss: 0.000006325, Improvement: -0.000002976, Best Loss: 0.000000570 in Epoch 2031
Epoch 2066
Epoch 2066, Loss: 0.000004291, Improvement: -0.000002034, Best Loss: 0.000000570 in Epoch 2031
Epoch 2067
Epoch 2067, Loss: 0.000003700, Improvement: -0.000000591, Best Loss: 0.000000570 in Epoch 2031
Epoch 2068
Epoch 2068, Loss: 0.000003211, Improvement: -0.000000490, Best Loss: 0.000000570 in Epoch 2031
Epoch 2069
Epoch 2069, Loss: 0.000004645, Improvement: 0.000001434, Best Loss: 0.000000570 in Epoch 2031
Epoch 2070
Epoch 2070, Loss: 0.000006269, Improvement: 0.000001625, Best Loss: 0.000000570 in Epoch 2031
Epoch 2071
Epoch 2071, Loss: 0.000005778, Improvement: -0.000000491, Best Loss: 0.000000570 in Epoch 2031
Epoch 2072
Epoch 2072, Loss: 0.000005867, Improvement: 0.000000089, Best Loss: 0.000000570 in Epoch 2031
Epoch 2073
Epoch 2073, Loss: 0.000009882, Improvement: 0.000004015, Best Loss: 0.000000570 in Epoch 2031
Epoch 2074
Epoch 2074, Loss: 0.000006120, Improvement: -0.000003762, Best Loss: 0.000000570 in Epoch 2031
Epoch 2075
Epoch 2075, Loss: 0.000003441, Improvement: -0.000002678, Best Loss: 0.000000570 in Epoch 2031
Epoch 2076
Epoch 2076, Loss: 0.000003244, Improvement: -0.000000197, Best Loss: 0.000000570 in Epoch 2031
Epoch 2077
Epoch 2077, Loss: 0.000003383, Improvement: 0.000000139, Best Loss: 0.000000570 in Epoch 2031
Epoch 2078
Epoch 2078, Loss: 0.000003714, Improvement: 0.000000331, Best Loss: 0.000000570 in Epoch 2031
Epoch 2079
Epoch 2079, Loss: 0.000010368, Improvement: 0.000006654, Best Loss: 0.000000570 in Epoch 2031
Epoch 2080
Epoch 2080, Loss: 0.000011694, Improvement: 0.000001326, Best Loss: 0.000000570 in Epoch 2031
Epoch 2081
Epoch 2081, Loss: 0.000009168, Improvement: -0.000002526, Best Loss: 0.000000570 in Epoch 2031
Epoch 2082
Epoch 2082, Loss: 0.000005375, Improvement: -0.000003792, Best Loss: 0.000000570 in Epoch 2031
Epoch 2083
Epoch 2083, Loss: 0.000003073, Improvement: -0.000002303, Best Loss: 0.000000570 in Epoch 2031
Epoch 2084
Epoch 2084, Loss: 0.000007446, Improvement: 0.000004373, Best Loss: 0.000000570 in Epoch 2031
Epoch 2085
Epoch 2085, Loss: 0.000007836, Improvement: 0.000000390, Best Loss: 0.000000570 in Epoch 2031
Epoch 2086
Epoch 2086, Loss: 0.000009495, Improvement: 0.000001659, Best Loss: 0.000000570 in Epoch 2031
Epoch 2087
Epoch 2087, Loss: 0.000008380, Improvement: -0.000001115, Best Loss: 0.000000570 in Epoch 2031
Epoch 2088
Epoch 2088, Loss: 0.000005234, Improvement: -0.000003146, Best Loss: 0.000000570 in Epoch 2031
Epoch 2089
Epoch 2089, Loss: 0.000004517, Improvement: -0.000000716, Best Loss: 0.000000570 in Epoch 2031
Epoch 2090
Epoch 2090, Loss: 0.000006411, Improvement: 0.000001894, Best Loss: 0.000000570 in Epoch 2031
Epoch 2091
Epoch 2091, Loss: 0.000004938, Improvement: -0.000001473, Best Loss: 0.000000570 in Epoch 2031
Epoch 2092
Epoch 2092, Loss: 0.000003276, Improvement: -0.000001662, Best Loss: 0.000000570 in Epoch 2031
Epoch 2093
Epoch 2093, Loss: 0.000002356, Improvement: -0.000000919, Best Loss: 0.000000570 in Epoch 2031
Epoch 2094
Epoch 2094, Loss: 0.000001675, Improvement: -0.000000682, Best Loss: 0.000000570 in Epoch 2031
Epoch 2095
Epoch 2095, Loss: 0.000001689, Improvement: 0.000000015, Best Loss: 0.000000570 in Epoch 2031
Epoch 2096
Epoch 2096, Loss: 0.000001372, Improvement: -0.000000317, Best Loss: 0.000000570 in Epoch 2031
Epoch 2097
Epoch 2097, Loss: 0.000001407, Improvement: 0.000000035, Best Loss: 0.000000570 in Epoch 2031
Epoch 2098
Epoch 2098, Loss: 0.000001675, Improvement: 0.000000268, Best Loss: 0.000000570 in Epoch 2031
Epoch 2099
Epoch 2099, Loss: 0.000001600, Improvement: -0.000000075, Best Loss: 0.000000570 in Epoch 2031
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.000003245, Improvement: 0.000001644, Best Loss: 0.000000570 in Epoch 2031
Epoch 2101
Epoch 2101, Loss: 0.000003755, Improvement: 0.000000510, Best Loss: 0.000000570 in Epoch 2031
Epoch 2102
Epoch 2102, Loss: 0.000003728, Improvement: -0.000000027, Best Loss: 0.000000570 in Epoch 2031
Epoch 2103
Epoch 2103, Loss: 0.000005554, Improvement: 0.000001826, Best Loss: 0.000000570 in Epoch 2031
Epoch 2104
Epoch 2104, Loss: 0.000010943, Improvement: 0.000005389, Best Loss: 0.000000570 in Epoch 2031
Epoch 2105
Epoch 2105, Loss: 0.000006722, Improvement: -0.000004221, Best Loss: 0.000000570 in Epoch 2031
Epoch 2106
Epoch 2106, Loss: 0.000006138, Improvement: -0.000000585, Best Loss: 0.000000570 in Epoch 2031
Epoch 2107
Epoch 2107, Loss: 0.000006916, Improvement: 0.000000778, Best Loss: 0.000000570 in Epoch 2031
Epoch 2108
Epoch 2108, Loss: 0.000004272, Improvement: -0.000002644, Best Loss: 0.000000570 in Epoch 2031
Epoch 2109
Epoch 2109, Loss: 0.000004727, Improvement: 0.000000455, Best Loss: 0.000000570 in Epoch 2031
Epoch 2110
Epoch 2110, Loss: 0.000004346, Improvement: -0.000000382, Best Loss: 0.000000570 in Epoch 2031
Epoch 2111
Epoch 2111, Loss: 0.000003773, Improvement: -0.000000572, Best Loss: 0.000000570 in Epoch 2031
Epoch 2112
Epoch 2112, Loss: 0.000002787, Improvement: -0.000000987, Best Loss: 0.000000570 in Epoch 2031
Epoch 2113
Epoch 2113, Loss: 0.000002502, Improvement: -0.000000284, Best Loss: 0.000000570 in Epoch 2031
Epoch 2114
Epoch 2114, Loss: 0.000004684, Improvement: 0.000002181, Best Loss: 0.000000570 in Epoch 2031
Epoch 2115
Epoch 2115, Loss: 0.000012935, Improvement: 0.000008251, Best Loss: 0.000000570 in Epoch 2031
Epoch 2116
Epoch 2116, Loss: 0.000012561, Improvement: -0.000000374, Best Loss: 0.000000570 in Epoch 2031
Epoch 2117
Epoch 2117, Loss: 0.000012410, Improvement: -0.000000152, Best Loss: 0.000000570 in Epoch 2031
Epoch 2118
Epoch 2118, Loss: 0.000012179, Improvement: -0.000000230, Best Loss: 0.000000570 in Epoch 2031
Epoch 2119
Epoch 2119, Loss: 0.000005663, Improvement: -0.000006517, Best Loss: 0.000000570 in Epoch 2031
Epoch 2120
Epoch 2120, Loss: 0.000004108, Improvement: -0.000001555, Best Loss: 0.000000570 in Epoch 2031
Epoch 2121
Epoch 2121, Loss: 0.000002624, Improvement: -0.000001484, Best Loss: 0.000000570 in Epoch 2031
Epoch 2122
Epoch 2122, Loss: 0.000001783, Improvement: -0.000000841, Best Loss: 0.000000570 in Epoch 2031
Epoch 2123
Epoch 2123, Loss: 0.000001618, Improvement: -0.000000165, Best Loss: 0.000000570 in Epoch 2031
Epoch 2124
Epoch 2124, Loss: 0.000001452, Improvement: -0.000000165, Best Loss: 0.000000570 in Epoch 2031
Epoch 2125
Epoch 2125, Loss: 0.000001912, Improvement: 0.000000459, Best Loss: 0.000000570 in Epoch 2031
Epoch 2126
Epoch 2126, Loss: 0.000001559, Improvement: -0.000000352, Best Loss: 0.000000570 in Epoch 2031
Epoch 2127
Epoch 2127, Loss: 0.000001118, Improvement: -0.000000441, Best Loss: 0.000000570 in Epoch 2031
Epoch 2128
Epoch 2128, Loss: 0.000001031, Improvement: -0.000000087, Best Loss: 0.000000570 in Epoch 2031
Epoch 2129
Epoch 2129, Loss: 0.000000936, Improvement: -0.000000095, Best Loss: 0.000000570 in Epoch 2031
Epoch 2130
Epoch 2130, Loss: 0.000001178, Improvement: 0.000000242, Best Loss: 0.000000570 in Epoch 2031
Epoch 2131
Epoch 2131, Loss: 0.000001120, Improvement: -0.000000059, Best Loss: 0.000000570 in Epoch 2031
Epoch 2132
Epoch 2132, Loss: 0.000001625, Improvement: 0.000000505, Best Loss: 0.000000570 in Epoch 2031
Epoch 2133
Epoch 2133, Loss: 0.000001542, Improvement: -0.000000083, Best Loss: 0.000000570 in Epoch 2031
Epoch 2134
Epoch 2134, Loss: 0.000001589, Improvement: 0.000000047, Best Loss: 0.000000570 in Epoch 2031
Epoch 2135
Epoch 2135, Loss: 0.000001647, Improvement: 0.000000059, Best Loss: 0.000000570 in Epoch 2031
Epoch 2136
Epoch 2136, Loss: 0.000001651, Improvement: 0.000000004, Best Loss: 0.000000570 in Epoch 2031
Epoch 2137
Epoch 2137, Loss: 0.000004633, Improvement: 0.000002982, Best Loss: 0.000000570 in Epoch 2031
Epoch 2138
Epoch 2138, Loss: 0.000007110, Improvement: 0.000002477, Best Loss: 0.000000570 in Epoch 2031
Epoch 2139
Epoch 2139, Loss: 0.000014816, Improvement: 0.000007705, Best Loss: 0.000000570 in Epoch 2031
Epoch 2140
Epoch 2140, Loss: 0.000011261, Improvement: -0.000003555, Best Loss: 0.000000570 in Epoch 2031
Epoch 2141
Epoch 2141, Loss: 0.000008129, Improvement: -0.000003132, Best Loss: 0.000000570 in Epoch 2031
Epoch 2142
Epoch 2142, Loss: 0.000005798, Improvement: -0.000002331, Best Loss: 0.000000570 in Epoch 2031
Epoch 2143
Epoch 2143, Loss: 0.000006139, Improvement: 0.000000340, Best Loss: 0.000000570 in Epoch 2031
Epoch 2144
Epoch 2144, Loss: 0.000006621, Improvement: 0.000000482, Best Loss: 0.000000570 in Epoch 2031
Epoch 2145
Epoch 2145, Loss: 0.000011852, Improvement: 0.000005231, Best Loss: 0.000000570 in Epoch 2031
Epoch 2146
Epoch 2146, Loss: 0.000017755, Improvement: 0.000005903, Best Loss: 0.000000570 in Epoch 2031
Epoch 2147
Epoch 2147, Loss: 0.000010692, Improvement: -0.000007063, Best Loss: 0.000000570 in Epoch 2031
Epoch 2148
Epoch 2148, Loss: 0.000006873, Improvement: -0.000003819, Best Loss: 0.000000570 in Epoch 2031
Epoch 2149
Epoch 2149, Loss: 0.000005937, Improvement: -0.000000936, Best Loss: 0.000000570 in Epoch 2031
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.000004034, Improvement: -0.000001904, Best Loss: 0.000000570 in Epoch 2031
Epoch 2151
Epoch 2151, Loss: 0.000001919, Improvement: -0.000002115, Best Loss: 0.000000570 in Epoch 2031
Epoch 2152
Epoch 2152, Loss: 0.000001263, Improvement: -0.000000656, Best Loss: 0.000000570 in Epoch 2031
Epoch 2153
Epoch 2153, Loss: 0.000001023, Improvement: -0.000000239, Best Loss: 0.000000570 in Epoch 2031
Epoch 2154
Epoch 2154, Loss: 0.000000933, Improvement: -0.000000091, Best Loss: 0.000000570 in Epoch 2031
Epoch 2155
Epoch 2155, Loss: 0.000001293, Improvement: 0.000000360, Best Loss: 0.000000570 in Epoch 2031
Epoch 2156
Epoch 2156, Loss: 0.000002198, Improvement: 0.000000906, Best Loss: 0.000000570 in Epoch 2031
Epoch 2157
Epoch 2157, Loss: 0.000002167, Improvement: -0.000000031, Best Loss: 0.000000570 in Epoch 2031
Epoch 2158
Epoch 2158, Loss: 0.000002802, Improvement: 0.000000635, Best Loss: 0.000000570 in Epoch 2031
Epoch 2159
Epoch 2159, Loss: 0.000001869, Improvement: -0.000000933, Best Loss: 0.000000570 in Epoch 2031
Epoch 2160
Epoch 2160, Loss: 0.000002054, Improvement: 0.000000185, Best Loss: 0.000000570 in Epoch 2031
Epoch 2161
Epoch 2161, Loss: 0.000001732, Improvement: -0.000000322, Best Loss: 0.000000570 in Epoch 2031
Epoch 2162
Epoch 2162, Loss: 0.000002739, Improvement: 0.000001007, Best Loss: 0.000000570 in Epoch 2031
Epoch 2163
Epoch 2163, Loss: 0.000003255, Improvement: 0.000000516, Best Loss: 0.000000570 in Epoch 2031
Epoch 2164
Epoch 2164, Loss: 0.000004570, Improvement: 0.000001315, Best Loss: 0.000000570 in Epoch 2031
Epoch 2165
Epoch 2165, Loss: 0.000005204, Improvement: 0.000000634, Best Loss: 0.000000570 in Epoch 2031
Epoch 2166
Epoch 2166, Loss: 0.000004571, Improvement: -0.000000633, Best Loss: 0.000000570 in Epoch 2031
Epoch 2167
Epoch 2167, Loss: 0.000007023, Improvement: 0.000002452, Best Loss: 0.000000570 in Epoch 2031
Epoch 2168
Epoch 2168, Loss: 0.000017821, Improvement: 0.000010798, Best Loss: 0.000000570 in Epoch 2031
Epoch 2169
Epoch 2169, Loss: 0.000016332, Improvement: -0.000001489, Best Loss: 0.000000570 in Epoch 2031
Epoch 2170
Epoch 2170, Loss: 0.000023800, Improvement: 0.000007467, Best Loss: 0.000000570 in Epoch 2031
Epoch 2171
Epoch 2171, Loss: 0.000021628, Improvement: -0.000002172, Best Loss: 0.000000570 in Epoch 2031
Epoch 2172
Epoch 2172, Loss: 0.000010220, Improvement: -0.000011407, Best Loss: 0.000000570 in Epoch 2031
Epoch 2173
Epoch 2173, Loss: 0.000004733, Improvement: -0.000005487, Best Loss: 0.000000570 in Epoch 2031
Epoch 2174
Epoch 2174, Loss: 0.000004175, Improvement: -0.000000559, Best Loss: 0.000000570 in Epoch 2031
Epoch 2175
Epoch 2175, Loss: 0.000002080, Improvement: -0.000002094, Best Loss: 0.000000570 in Epoch 2031
Epoch 2176
Epoch 2176, Loss: 0.000001499, Improvement: -0.000000581, Best Loss: 0.000000570 in Epoch 2031
Epoch 2177
Epoch 2177, Loss: 0.000001279, Improvement: -0.000000220, Best Loss: 0.000000570 in Epoch 2031
Epoch 2178
Epoch 2178, Loss: 0.000001109, Improvement: -0.000000170, Best Loss: 0.000000570 in Epoch 2031
Epoch 2179
Epoch 2179, Loss: 0.000001039, Improvement: -0.000000070, Best Loss: 0.000000570 in Epoch 2031
Epoch 2180
Epoch 2180, Loss: 0.000000964, Improvement: -0.000000075, Best Loss: 0.000000570 in Epoch 2031
Epoch 2181
Epoch 2181, Loss: 0.000000909, Improvement: -0.000000055, Best Loss: 0.000000570 in Epoch 2031
Epoch 2182
Epoch 2182, Loss: 0.000000882, Improvement: -0.000000027, Best Loss: 0.000000570 in Epoch 2031
Epoch 2183
Epoch 2183, Loss: 0.000000821, Improvement: -0.000000061, Best Loss: 0.000000570 in Epoch 2031
Epoch 2184
Epoch 2184, Loss: 0.000000818, Improvement: -0.000000003, Best Loss: 0.000000570 in Epoch 2031
Epoch 2185
A best model at epoch 2185 has been saved with training error 0.000000515.
Epoch 2185, Loss: 0.000000752, Improvement: -0.000000066, Best Loss: 0.000000515 in Epoch 2185
Epoch 2186
Epoch 2186, Loss: 0.000000734, Improvement: -0.000000018, Best Loss: 0.000000515 in Epoch 2185
Epoch 2187
Epoch 2187, Loss: 0.000000757, Improvement: 0.000000023, Best Loss: 0.000000515 in Epoch 2185
Epoch 2188
Epoch 2188, Loss: 0.000000738, Improvement: -0.000000019, Best Loss: 0.000000515 in Epoch 2185
Epoch 2189
Epoch 2189, Loss: 0.000000715, Improvement: -0.000000023, Best Loss: 0.000000515 in Epoch 2185
Epoch 2190
Epoch 2190, Loss: 0.000000811, Improvement: 0.000000096, Best Loss: 0.000000515 in Epoch 2185
Epoch 2191
Epoch 2191, Loss: 0.000000754, Improvement: -0.000000057, Best Loss: 0.000000515 in Epoch 2185
Epoch 2192
Epoch 2192, Loss: 0.000000748, Improvement: -0.000000006, Best Loss: 0.000000515 in Epoch 2185
Epoch 2193
Epoch 2193, Loss: 0.000000832, Improvement: 0.000000084, Best Loss: 0.000000515 in Epoch 2185
Epoch 2194
Epoch 2194, Loss: 0.000000992, Improvement: 0.000000160, Best Loss: 0.000000515 in Epoch 2185
Epoch 2195
Epoch 2195, Loss: 0.000001072, Improvement: 0.000000080, Best Loss: 0.000000515 in Epoch 2185
Epoch 2196
Epoch 2196, Loss: 0.000000997, Improvement: -0.000000076, Best Loss: 0.000000515 in Epoch 2185
Epoch 2197
Epoch 2197, Loss: 0.000001055, Improvement: 0.000000059, Best Loss: 0.000000515 in Epoch 2185
Epoch 2198
Epoch 2198, Loss: 0.000001010, Improvement: -0.000000045, Best Loss: 0.000000515 in Epoch 2185
Epoch 2199
Epoch 2199, Loss: 0.000001305, Improvement: 0.000000295, Best Loss: 0.000000515 in Epoch 2185
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.000001237, Improvement: -0.000000068, Best Loss: 0.000000515 in Epoch 2185
Epoch 2201
Epoch 2201, Loss: 0.000001174, Improvement: -0.000000062, Best Loss: 0.000000515 in Epoch 2185
Epoch 2202
Epoch 2202, Loss: 0.000001468, Improvement: 0.000000294, Best Loss: 0.000000515 in Epoch 2185
Epoch 2203
Epoch 2203, Loss: 0.000003702, Improvement: 0.000002234, Best Loss: 0.000000515 in Epoch 2185
Epoch 2204
Epoch 2204, Loss: 0.000006123, Improvement: 0.000002421, Best Loss: 0.000000515 in Epoch 2185
Epoch 2205
Epoch 2205, Loss: 0.000010595, Improvement: 0.000004472, Best Loss: 0.000000515 in Epoch 2185
Epoch 2206
Epoch 2206, Loss: 0.000006872, Improvement: -0.000003723, Best Loss: 0.000000515 in Epoch 2185
Epoch 2207
Epoch 2207, Loss: 0.000006081, Improvement: -0.000000791, Best Loss: 0.000000515 in Epoch 2185
Epoch 2208
Epoch 2208, Loss: 0.000005958, Improvement: -0.000000123, Best Loss: 0.000000515 in Epoch 2185
Epoch 2209
Epoch 2209, Loss: 0.000006626, Improvement: 0.000000668, Best Loss: 0.000000515 in Epoch 2185
Epoch 2210
Epoch 2210, Loss: 0.000012346, Improvement: 0.000005720, Best Loss: 0.000000515 in Epoch 2185
Epoch 2211
Epoch 2211, Loss: 0.000009858, Improvement: -0.000002488, Best Loss: 0.000000515 in Epoch 2185
Epoch 2212
Epoch 2212, Loss: 0.000013985, Improvement: 0.000004127, Best Loss: 0.000000515 in Epoch 2185
Epoch 2213
Epoch 2213, Loss: 0.000010312, Improvement: -0.000003673, Best Loss: 0.000000515 in Epoch 2185
Epoch 2214
Epoch 2214, Loss: 0.000004661, Improvement: -0.000005651, Best Loss: 0.000000515 in Epoch 2185
Epoch 2215
Epoch 2215, Loss: 0.000003125, Improvement: -0.000001536, Best Loss: 0.000000515 in Epoch 2185
Epoch 2216
Epoch 2216, Loss: 0.000002746, Improvement: -0.000000379, Best Loss: 0.000000515 in Epoch 2185
Epoch 2217
Epoch 2217, Loss: 0.000001561, Improvement: -0.000001184, Best Loss: 0.000000515 in Epoch 2185
Epoch 2218
Epoch 2218, Loss: 0.000001365, Improvement: -0.000000197, Best Loss: 0.000000515 in Epoch 2185
Epoch 2219
Epoch 2219, Loss: 0.000001797, Improvement: 0.000000432, Best Loss: 0.000000515 in Epoch 2185
Epoch 2220
Epoch 2220, Loss: 0.000003731, Improvement: 0.000001934, Best Loss: 0.000000515 in Epoch 2185
Epoch 2221
Epoch 2221, Loss: 0.000003355, Improvement: -0.000000376, Best Loss: 0.000000515 in Epoch 2185
Epoch 2222
Epoch 2222, Loss: 0.000004520, Improvement: 0.000001165, Best Loss: 0.000000515 in Epoch 2185
Epoch 2223
Epoch 2223, Loss: 0.000003025, Improvement: -0.000001495, Best Loss: 0.000000515 in Epoch 2185
Epoch 2224
Epoch 2224, Loss: 0.000006520, Improvement: 0.000003495, Best Loss: 0.000000515 in Epoch 2185
Epoch 2225
Epoch 2225, Loss: 0.000007014, Improvement: 0.000000494, Best Loss: 0.000000515 in Epoch 2185
Epoch 2226
Epoch 2226, Loss: 0.000004654, Improvement: -0.000002360, Best Loss: 0.000000515 in Epoch 2185
Epoch 2227
Epoch 2227, Loss: 0.000004329, Improvement: -0.000000325, Best Loss: 0.000000515 in Epoch 2185
Epoch 2228
Epoch 2228, Loss: 0.000003268, Improvement: -0.000001061, Best Loss: 0.000000515 in Epoch 2185
Epoch 2229
Epoch 2229, Loss: 0.000003328, Improvement: 0.000000060, Best Loss: 0.000000515 in Epoch 2185
Epoch 2230
Epoch 2230, Loss: 0.000004693, Improvement: 0.000001365, Best Loss: 0.000000515 in Epoch 2185
Epoch 2231
Epoch 2231, Loss: 0.000006120, Improvement: 0.000001427, Best Loss: 0.000000515 in Epoch 2185
Epoch 2232
Epoch 2232, Loss: 0.000007409, Improvement: 0.000001289, Best Loss: 0.000000515 in Epoch 2185
Epoch 2233
Epoch 2233, Loss: 0.000005447, Improvement: -0.000001962, Best Loss: 0.000000515 in Epoch 2185
Epoch 2234
Epoch 2234, Loss: 0.000003601, Improvement: -0.000001845, Best Loss: 0.000000515 in Epoch 2185
Epoch 2235
Epoch 2235, Loss: 0.000005977, Improvement: 0.000002376, Best Loss: 0.000000515 in Epoch 2185
Epoch 2236
Epoch 2236, Loss: 0.000004955, Improvement: -0.000001022, Best Loss: 0.000000515 in Epoch 2185
Epoch 2237
Epoch 2237, Loss: 0.000008665, Improvement: 0.000003710, Best Loss: 0.000000515 in Epoch 2185
Epoch 2238
Epoch 2238, Loss: 0.000013754, Improvement: 0.000005089, Best Loss: 0.000000515 in Epoch 2185
Epoch 2239
Epoch 2239, Loss: 0.000009033, Improvement: -0.000004721, Best Loss: 0.000000515 in Epoch 2185
Epoch 2240
Epoch 2240, Loss: 0.000004095, Improvement: -0.000004938, Best Loss: 0.000000515 in Epoch 2185
Epoch 2241
Epoch 2241, Loss: 0.000003223, Improvement: -0.000000872, Best Loss: 0.000000515 in Epoch 2185
Epoch 2242
Epoch 2242, Loss: 0.000002218, Improvement: -0.000001005, Best Loss: 0.000000515 in Epoch 2185
Epoch 2243
Epoch 2243, Loss: 0.000002111, Improvement: -0.000000107, Best Loss: 0.000000515 in Epoch 2185
Epoch 2244
Epoch 2244, Loss: 0.000001602, Improvement: -0.000000509, Best Loss: 0.000000515 in Epoch 2185
Epoch 2245
Epoch 2245, Loss: 0.000002024, Improvement: 0.000000421, Best Loss: 0.000000515 in Epoch 2185
Epoch 2246
Epoch 2246, Loss: 0.000002591, Improvement: 0.000000568, Best Loss: 0.000000515 in Epoch 2185
Epoch 2247
Epoch 2247, Loss: 0.000011287, Improvement: 0.000008696, Best Loss: 0.000000515 in Epoch 2185
Epoch 2248
Epoch 2248, Loss: 0.000024007, Improvement: 0.000012720, Best Loss: 0.000000515 in Epoch 2185
Epoch 2249
Epoch 2249, Loss: 0.000015984, Improvement: -0.000008023, Best Loss: 0.000000515 in Epoch 2185
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.000009143, Improvement: -0.000006841, Best Loss: 0.000000515 in Epoch 2185
Epoch 2251
Epoch 2251, Loss: 0.000005320, Improvement: -0.000003823, Best Loss: 0.000000515 in Epoch 2185
Epoch 2252
Epoch 2252, Loss: 0.000003271, Improvement: -0.000002050, Best Loss: 0.000000515 in Epoch 2185
Epoch 2253
Epoch 2253, Loss: 0.000001992, Improvement: -0.000001279, Best Loss: 0.000000515 in Epoch 2185
Epoch 2254
Epoch 2254, Loss: 0.000001859, Improvement: -0.000000133, Best Loss: 0.000000515 in Epoch 2185
Epoch 2255
Epoch 2255, Loss: 0.000001421, Improvement: -0.000000438, Best Loss: 0.000000515 in Epoch 2185
Epoch 2256
Epoch 2256, Loss: 0.000001465, Improvement: 0.000000045, Best Loss: 0.000000515 in Epoch 2185
Epoch 2257
Epoch 2257, Loss: 0.000001045, Improvement: -0.000000420, Best Loss: 0.000000515 in Epoch 2185
Epoch 2258
Epoch 2258, Loss: 0.000001275, Improvement: 0.000000230, Best Loss: 0.000000515 in Epoch 2185
Epoch 2259
Epoch 2259, Loss: 0.000000905, Improvement: -0.000000370, Best Loss: 0.000000515 in Epoch 2185
Epoch 2260
Epoch 2260, Loss: 0.000000796, Improvement: -0.000000110, Best Loss: 0.000000515 in Epoch 2185
Epoch 2261
A best model at epoch 2261 has been saved with training error 0.000000510.
Epoch 2261, Loss: 0.000000760, Improvement: -0.000000035, Best Loss: 0.000000510 in Epoch 2261
Epoch 2262
Epoch 2262, Loss: 0.000000693, Improvement: -0.000000067, Best Loss: 0.000000510 in Epoch 2261
Epoch 2263
Epoch 2263, Loss: 0.000000672, Improvement: -0.000000021, Best Loss: 0.000000510 in Epoch 2261
Epoch 2264
Epoch 2264, Loss: 0.000000663, Improvement: -0.000000010, Best Loss: 0.000000510 in Epoch 2261
Epoch 2265
Epoch 2265, Loss: 0.000000672, Improvement: 0.000000010, Best Loss: 0.000000510 in Epoch 2261
Epoch 2266
A best model at epoch 2266 has been saved with training error 0.000000468.
Epoch 2266, Loss: 0.000000638, Improvement: -0.000000034, Best Loss: 0.000000468 in Epoch 2266
Epoch 2267
Epoch 2267, Loss: 0.000000617, Improvement: -0.000000022, Best Loss: 0.000000468 in Epoch 2266
Epoch 2268
Epoch 2268, Loss: 0.000000645, Improvement: 0.000000028, Best Loss: 0.000000468 in Epoch 2266
Epoch 2269
Epoch 2269, Loss: 0.000000652, Improvement: 0.000000007, Best Loss: 0.000000468 in Epoch 2266
Epoch 2270
Epoch 2270, Loss: 0.000000640, Improvement: -0.000000012, Best Loss: 0.000000468 in Epoch 2266
Epoch 2271
A best model at epoch 2271 has been saved with training error 0.000000459.
Epoch 2271, Loss: 0.000000655, Improvement: 0.000000014, Best Loss: 0.000000459 in Epoch 2271
Epoch 2272
Epoch 2272, Loss: 0.000000653, Improvement: -0.000000002, Best Loss: 0.000000459 in Epoch 2271
Epoch 2273
Epoch 2273, Loss: 0.000000762, Improvement: 0.000000109, Best Loss: 0.000000459 in Epoch 2271
Epoch 2274
Epoch 2274, Loss: 0.000000769, Improvement: 0.000000007, Best Loss: 0.000000459 in Epoch 2271
Epoch 2275
Epoch 2275, Loss: 0.000000793, Improvement: 0.000000025, Best Loss: 0.000000459 in Epoch 2271
Epoch 2276
Epoch 2276, Loss: 0.000001750, Improvement: 0.000000957, Best Loss: 0.000000459 in Epoch 2271
Epoch 2277
Epoch 2277, Loss: 0.000002671, Improvement: 0.000000921, Best Loss: 0.000000459 in Epoch 2271
Epoch 2278
Epoch 2278, Loss: 0.000002898, Improvement: 0.000000226, Best Loss: 0.000000459 in Epoch 2271
Epoch 2279
Epoch 2279, Loss: 0.000002513, Improvement: -0.000000384, Best Loss: 0.000000459 in Epoch 2271
Epoch 2280
Epoch 2280, Loss: 0.000004092, Improvement: 0.000001579, Best Loss: 0.000000459 in Epoch 2271
Epoch 2281
Epoch 2281, Loss: 0.000006975, Improvement: 0.000002882, Best Loss: 0.000000459 in Epoch 2271
Epoch 2282
Epoch 2282, Loss: 0.000004786, Improvement: -0.000002188, Best Loss: 0.000000459 in Epoch 2271
Epoch 2283
Epoch 2283, Loss: 0.000005101, Improvement: 0.000000315, Best Loss: 0.000000459 in Epoch 2271
Epoch 2284
Epoch 2284, Loss: 0.000004134, Improvement: -0.000000968, Best Loss: 0.000000459 in Epoch 2271
Epoch 2285
Epoch 2285, Loss: 0.000003754, Improvement: -0.000000380, Best Loss: 0.000000459 in Epoch 2271
Epoch 2286
Epoch 2286, Loss: 0.000004843, Improvement: 0.000001089, Best Loss: 0.000000459 in Epoch 2271
Epoch 2287
Epoch 2287, Loss: 0.000009140, Improvement: 0.000004296, Best Loss: 0.000000459 in Epoch 2271
Epoch 2288
Epoch 2288, Loss: 0.000006014, Improvement: -0.000003125, Best Loss: 0.000000459 in Epoch 2271
Epoch 2289
Epoch 2289, Loss: 0.000006037, Improvement: 0.000000022, Best Loss: 0.000000459 in Epoch 2271
Epoch 2290
Epoch 2290, Loss: 0.000006585, Improvement: 0.000000548, Best Loss: 0.000000459 in Epoch 2271
Epoch 2291
Epoch 2291, Loss: 0.000004040, Improvement: -0.000002545, Best Loss: 0.000000459 in Epoch 2271
Epoch 2292
Epoch 2292, Loss: 0.000003515, Improvement: -0.000000525, Best Loss: 0.000000459 in Epoch 2271
Epoch 2293
Epoch 2293, Loss: 0.000005781, Improvement: 0.000002266, Best Loss: 0.000000459 in Epoch 2271
Epoch 2294
Epoch 2294, Loss: 0.000004008, Improvement: -0.000001773, Best Loss: 0.000000459 in Epoch 2271
Epoch 2295
Epoch 2295, Loss: 0.000002919, Improvement: -0.000001089, Best Loss: 0.000000459 in Epoch 2271
Epoch 2296
Epoch 2296, Loss: 0.000003817, Improvement: 0.000000897, Best Loss: 0.000000459 in Epoch 2271
Epoch 2297
Epoch 2297, Loss: 0.000005499, Improvement: 0.000001682, Best Loss: 0.000000459 in Epoch 2271
Epoch 2298
Epoch 2298, Loss: 0.000004340, Improvement: -0.000001159, Best Loss: 0.000000459 in Epoch 2271
Epoch 2299
Epoch 2299, Loss: 0.000005407, Improvement: 0.000001067, Best Loss: 0.000000459 in Epoch 2271
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.000005200, Improvement: -0.000000207, Best Loss: 0.000000459 in Epoch 2271
Epoch 2301
Epoch 2301, Loss: 0.000003527, Improvement: -0.000001673, Best Loss: 0.000000459 in Epoch 2271
Epoch 2302
Epoch 2302, Loss: 0.000003404, Improvement: -0.000000123, Best Loss: 0.000000459 in Epoch 2271
Epoch 2303
Epoch 2303, Loss: 0.000004931, Improvement: 0.000001527, Best Loss: 0.000000459 in Epoch 2271
Epoch 2304
Epoch 2304, Loss: 0.000004361, Improvement: -0.000000570, Best Loss: 0.000000459 in Epoch 2271
Epoch 2305
Epoch 2305, Loss: 0.000004312, Improvement: -0.000000049, Best Loss: 0.000000459 in Epoch 2271
Epoch 2306
Epoch 2306, Loss: 0.000009790, Improvement: 0.000005478, Best Loss: 0.000000459 in Epoch 2271
Epoch 2307
Epoch 2307, Loss: 0.000010023, Improvement: 0.000000233, Best Loss: 0.000000459 in Epoch 2271
Epoch 2308
Epoch 2308, Loss: 0.000011925, Improvement: 0.000001901, Best Loss: 0.000000459 in Epoch 2271
Epoch 2309
Epoch 2309, Loss: 0.000010794, Improvement: -0.000001130, Best Loss: 0.000000459 in Epoch 2271
Epoch 2310
Epoch 2310, Loss: 0.000009179, Improvement: -0.000001615, Best Loss: 0.000000459 in Epoch 2271
Epoch 2311
Epoch 2311, Loss: 0.000005155, Improvement: -0.000004024, Best Loss: 0.000000459 in Epoch 2271
Epoch 2312
Epoch 2312, Loss: 0.000002866, Improvement: -0.000002289, Best Loss: 0.000000459 in Epoch 2271
Epoch 2313
Epoch 2313, Loss: 0.000002373, Improvement: -0.000000493, Best Loss: 0.000000459 in Epoch 2271
Epoch 2314
Epoch 2314, Loss: 0.000001771, Improvement: -0.000000602, Best Loss: 0.000000459 in Epoch 2271
Epoch 2315
Epoch 2315, Loss: 0.000002206, Improvement: 0.000000435, Best Loss: 0.000000459 in Epoch 2271
Epoch 2316
Epoch 2316, Loss: 0.000001728, Improvement: -0.000000478, Best Loss: 0.000000459 in Epoch 2271
Epoch 2317
Epoch 2317, Loss: 0.000001137, Improvement: -0.000000592, Best Loss: 0.000000459 in Epoch 2271
Epoch 2318
Epoch 2318, Loss: 0.000001644, Improvement: 0.000000507, Best Loss: 0.000000459 in Epoch 2271
Epoch 2319
Epoch 2319, Loss: 0.000002309, Improvement: 0.000000665, Best Loss: 0.000000459 in Epoch 2271
Epoch 2320
Epoch 2320, Loss: 0.000001672, Improvement: -0.000000637, Best Loss: 0.000000459 in Epoch 2271
Epoch 2321
Epoch 2321, Loss: 0.000001547, Improvement: -0.000000125, Best Loss: 0.000000459 in Epoch 2271
Epoch 2322
Epoch 2322, Loss: 0.000001279, Improvement: -0.000000268, Best Loss: 0.000000459 in Epoch 2271
Epoch 2323
Epoch 2323, Loss: 0.000001000, Improvement: -0.000000279, Best Loss: 0.000000459 in Epoch 2271
Epoch 2324
Epoch 2324, Loss: 0.000000977, Improvement: -0.000000024, Best Loss: 0.000000459 in Epoch 2271
Epoch 2325
Epoch 2325, Loss: 0.000001019, Improvement: 0.000000043, Best Loss: 0.000000459 in Epoch 2271
Epoch 2326
Epoch 2326, Loss: 0.000001056, Improvement: 0.000000037, Best Loss: 0.000000459 in Epoch 2271
Epoch 2327
Epoch 2327, Loss: 0.000001020, Improvement: -0.000000036, Best Loss: 0.000000459 in Epoch 2271
Epoch 2328
Epoch 2328, Loss: 0.000001641, Improvement: 0.000000621, Best Loss: 0.000000459 in Epoch 2271
Epoch 2329
Epoch 2329, Loss: 0.000004480, Improvement: 0.000002839, Best Loss: 0.000000459 in Epoch 2271
Epoch 2330
Epoch 2330, Loss: 0.000005581, Improvement: 0.000001101, Best Loss: 0.000000459 in Epoch 2271
Epoch 2331
Epoch 2331, Loss: 0.000007572, Improvement: 0.000001991, Best Loss: 0.000000459 in Epoch 2271
Epoch 2332
Epoch 2332, Loss: 0.000016466, Improvement: 0.000008894, Best Loss: 0.000000459 in Epoch 2271
Epoch 2333
Epoch 2333, Loss: 0.000009731, Improvement: -0.000006735, Best Loss: 0.000000459 in Epoch 2271
Epoch 2334
Epoch 2334, Loss: 0.000008593, Improvement: -0.000001138, Best Loss: 0.000000459 in Epoch 2271
Epoch 2335
Epoch 2335, Loss: 0.000005848, Improvement: -0.000002745, Best Loss: 0.000000459 in Epoch 2271
Epoch 2336
Epoch 2336, Loss: 0.000002767, Improvement: -0.000003081, Best Loss: 0.000000459 in Epoch 2271
Epoch 2337
Epoch 2337, Loss: 0.000001797, Improvement: -0.000000970, Best Loss: 0.000000459 in Epoch 2271
Epoch 2338
Epoch 2338, Loss: 0.000001567, Improvement: -0.000000230, Best Loss: 0.000000459 in Epoch 2271
Epoch 2339
Epoch 2339, Loss: 0.000001540, Improvement: -0.000000027, Best Loss: 0.000000459 in Epoch 2271
Epoch 2340
Epoch 2340, Loss: 0.000001494, Improvement: -0.000000046, Best Loss: 0.000000459 in Epoch 2271
Epoch 2341
Epoch 2341, Loss: 0.000001699, Improvement: 0.000000205, Best Loss: 0.000000459 in Epoch 2271
Epoch 2342
Epoch 2342, Loss: 0.000002007, Improvement: 0.000000308, Best Loss: 0.000000459 in Epoch 2271
Epoch 2343
Epoch 2343, Loss: 0.000001723, Improvement: -0.000000284, Best Loss: 0.000000459 in Epoch 2271
Epoch 2344
Epoch 2344, Loss: 0.000001050, Improvement: -0.000000673, Best Loss: 0.000000459 in Epoch 2271
Epoch 2345
Epoch 2345, Loss: 0.000001152, Improvement: 0.000000102, Best Loss: 0.000000459 in Epoch 2271
Epoch 2346
Epoch 2346, Loss: 0.000001616, Improvement: 0.000000464, Best Loss: 0.000000459 in Epoch 2271
Epoch 2347
Epoch 2347, Loss: 0.000003080, Improvement: 0.000001464, Best Loss: 0.000000459 in Epoch 2271
Epoch 2348
Epoch 2348, Loss: 0.000004961, Improvement: 0.000001880, Best Loss: 0.000000459 in Epoch 2271
Epoch 2349
Epoch 2349, Loss: 0.000003435, Improvement: -0.000001526, Best Loss: 0.000000459 in Epoch 2271
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.000006999, Improvement: 0.000003564, Best Loss: 0.000000459 in Epoch 2271
Epoch 2351
Epoch 2351, Loss: 0.000009901, Improvement: 0.000002902, Best Loss: 0.000000459 in Epoch 2271
Epoch 2352
Epoch 2352, Loss: 0.000009802, Improvement: -0.000000099, Best Loss: 0.000000459 in Epoch 2271
Epoch 2353
Epoch 2353, Loss: 0.000010557, Improvement: 0.000000755, Best Loss: 0.000000459 in Epoch 2271
Epoch 2354
Epoch 2354, Loss: 0.000006996, Improvement: -0.000003561, Best Loss: 0.000000459 in Epoch 2271
Epoch 2355
Epoch 2355, Loss: 0.000006946, Improvement: -0.000000050, Best Loss: 0.000000459 in Epoch 2271
Epoch 2356
Epoch 2356, Loss: 0.000006178, Improvement: -0.000000767, Best Loss: 0.000000459 in Epoch 2271
Epoch 2357
Epoch 2357, Loss: 0.000004444, Improvement: -0.000001734, Best Loss: 0.000000459 in Epoch 2271
Epoch 2358
Epoch 2358, Loss: 0.000006996, Improvement: 0.000002552, Best Loss: 0.000000459 in Epoch 2271
Epoch 2359
Epoch 2359, Loss: 0.000005171, Improvement: -0.000001825, Best Loss: 0.000000459 in Epoch 2271
Epoch 2360
Epoch 2360, Loss: 0.000002265, Improvement: -0.000002906, Best Loss: 0.000000459 in Epoch 2271
Epoch 2361
Epoch 2361, Loss: 0.000001968, Improvement: -0.000000297, Best Loss: 0.000000459 in Epoch 2271
Epoch 2362
Epoch 2362, Loss: 0.000001713, Improvement: -0.000000256, Best Loss: 0.000000459 in Epoch 2271
Epoch 2363
Epoch 2363, Loss: 0.000001441, Improvement: -0.000000272, Best Loss: 0.000000459 in Epoch 2271
Epoch 2364
Epoch 2364, Loss: 0.000001236, Improvement: -0.000000205, Best Loss: 0.000000459 in Epoch 2271
Epoch 2365
Epoch 2365, Loss: 0.000001632, Improvement: 0.000000397, Best Loss: 0.000000459 in Epoch 2271
Epoch 2366
Epoch 2366, Loss: 0.000002449, Improvement: 0.000000817, Best Loss: 0.000000459 in Epoch 2271
Epoch 2367
Epoch 2367, Loss: 0.000002157, Improvement: -0.000000293, Best Loss: 0.000000459 in Epoch 2271
Epoch 2368
Epoch 2368, Loss: 0.000003085, Improvement: 0.000000928, Best Loss: 0.000000459 in Epoch 2271
Epoch 2369
Epoch 2369, Loss: 0.000003949, Improvement: 0.000000865, Best Loss: 0.000000459 in Epoch 2271
Epoch 2370
Epoch 2370, Loss: 0.000003404, Improvement: -0.000000545, Best Loss: 0.000000459 in Epoch 2271
Epoch 2371
Epoch 2371, Loss: 0.000003552, Improvement: 0.000000147, Best Loss: 0.000000459 in Epoch 2271
Epoch 2372
Epoch 2372, Loss: 0.000003190, Improvement: -0.000000361, Best Loss: 0.000000459 in Epoch 2271
Epoch 2373
Epoch 2373, Loss: 0.000003928, Improvement: 0.000000738, Best Loss: 0.000000459 in Epoch 2271
Epoch 2374
Epoch 2374, Loss: 0.000007973, Improvement: 0.000004045, Best Loss: 0.000000459 in Epoch 2271
Epoch 2375
Epoch 2375, Loss: 0.000009615, Improvement: 0.000001642, Best Loss: 0.000000459 in Epoch 2271
Epoch 2376
Epoch 2376, Loss: 0.000008602, Improvement: -0.000001014, Best Loss: 0.000000459 in Epoch 2271
Epoch 2377
Epoch 2377, Loss: 0.000008306, Improvement: -0.000000296, Best Loss: 0.000000459 in Epoch 2271
Epoch 2378
Epoch 2378, Loss: 0.000011350, Improvement: 0.000003044, Best Loss: 0.000000459 in Epoch 2271
Epoch 2379
Epoch 2379, Loss: 0.000012273, Improvement: 0.000000923, Best Loss: 0.000000459 in Epoch 2271
Epoch 2380
Epoch 2380, Loss: 0.000007650, Improvement: -0.000004622, Best Loss: 0.000000459 in Epoch 2271
Epoch 2381
Epoch 2381, Loss: 0.000006766, Improvement: -0.000000884, Best Loss: 0.000000459 in Epoch 2271
Epoch 2382
Epoch 2382, Loss: 0.000004513, Improvement: -0.000002254, Best Loss: 0.000000459 in Epoch 2271
Epoch 2383
Epoch 2383, Loss: 0.000003066, Improvement: -0.000001446, Best Loss: 0.000000459 in Epoch 2271
Epoch 2384
Epoch 2384, Loss: 0.000001816, Improvement: -0.000001250, Best Loss: 0.000000459 in Epoch 2271
Epoch 2385
Epoch 2385, Loss: 0.000003478, Improvement: 0.000001662, Best Loss: 0.000000459 in Epoch 2271
Epoch 2386
Epoch 2386, Loss: 0.000002663, Improvement: -0.000000815, Best Loss: 0.000000459 in Epoch 2271
Epoch 2387
Epoch 2387, Loss: 0.000003268, Improvement: 0.000000604, Best Loss: 0.000000459 in Epoch 2271
Epoch 2388
Epoch 2388, Loss: 0.000002358, Improvement: -0.000000910, Best Loss: 0.000000459 in Epoch 2271
Epoch 2389
Epoch 2389, Loss: 0.000001518, Improvement: -0.000000839, Best Loss: 0.000000459 in Epoch 2271
Epoch 2390
Epoch 2390, Loss: 0.000001393, Improvement: -0.000000125, Best Loss: 0.000000459 in Epoch 2271
Epoch 2391
Epoch 2391, Loss: 0.000001269, Improvement: -0.000000124, Best Loss: 0.000000459 in Epoch 2271
Epoch 2392
Epoch 2392, Loss: 0.000001449, Improvement: 0.000000180, Best Loss: 0.000000459 in Epoch 2271
Epoch 2393
Epoch 2393, Loss: 0.000002465, Improvement: 0.000001016, Best Loss: 0.000000459 in Epoch 2271
Epoch 2394
Epoch 2394, Loss: 0.000005005, Improvement: 0.000002540, Best Loss: 0.000000459 in Epoch 2271
Epoch 2395
Epoch 2395, Loss: 0.000009024, Improvement: 0.000004019, Best Loss: 0.000000459 in Epoch 2271
Epoch 2396
Epoch 2396, Loss: 0.000012117, Improvement: 0.000003093, Best Loss: 0.000000459 in Epoch 2271
Epoch 2397
Epoch 2397, Loss: 0.000011078, Improvement: -0.000001039, Best Loss: 0.000000459 in Epoch 2271
Epoch 2398
Epoch 2398, Loss: 0.000010785, Improvement: -0.000000293, Best Loss: 0.000000459 in Epoch 2271
Epoch 2399
Epoch 2399, Loss: 0.000017185, Improvement: 0.000006400, Best Loss: 0.000000459 in Epoch 2271
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.000012135, Improvement: -0.000005050, Best Loss: 0.000000459 in Epoch 2271
Epoch 2401
Epoch 2401, Loss: 0.000007775, Improvement: -0.000004360, Best Loss: 0.000000459 in Epoch 2271
Epoch 2402
Epoch 2402, Loss: 0.000007547, Improvement: -0.000000228, Best Loss: 0.000000459 in Epoch 2271
Epoch 2403
Epoch 2403, Loss: 0.000003644, Improvement: -0.000003903, Best Loss: 0.000000459 in Epoch 2271
Epoch 2404
Epoch 2404, Loss: 0.000002419, Improvement: -0.000001225, Best Loss: 0.000000459 in Epoch 2271
Epoch 2405
Epoch 2405, Loss: 0.000001412, Improvement: -0.000001007, Best Loss: 0.000000459 in Epoch 2271
Epoch 2406
Epoch 2406, Loss: 0.000000875, Improvement: -0.000000537, Best Loss: 0.000000459 in Epoch 2271
Epoch 2407
Epoch 2407, Loss: 0.000000861, Improvement: -0.000000014, Best Loss: 0.000000459 in Epoch 2271
Epoch 2408
Epoch 2408, Loss: 0.000000767, Improvement: -0.000000093, Best Loss: 0.000000459 in Epoch 2271
Epoch 2409
Epoch 2409, Loss: 0.000000712, Improvement: -0.000000055, Best Loss: 0.000000459 in Epoch 2271
Epoch 2410
Epoch 2410, Loss: 0.000000716, Improvement: 0.000000003, Best Loss: 0.000000459 in Epoch 2271
Epoch 2411
Epoch 2411, Loss: 0.000000757, Improvement: 0.000000042, Best Loss: 0.000000459 in Epoch 2271
Epoch 2412
Epoch 2412, Loss: 0.000000728, Improvement: -0.000000029, Best Loss: 0.000000459 in Epoch 2271
Epoch 2413
A best model at epoch 2413 has been saved with training error 0.000000458.
Epoch 2413, Loss: 0.000000623, Improvement: -0.000000105, Best Loss: 0.000000458 in Epoch 2413
Epoch 2414
Epoch 2414, Loss: 0.000000638, Improvement: 0.000000014, Best Loss: 0.000000458 in Epoch 2413
Epoch 2415
Epoch 2415, Loss: 0.000000926, Improvement: 0.000000288, Best Loss: 0.000000458 in Epoch 2413
Epoch 2416
Epoch 2416, Loss: 0.000001461, Improvement: 0.000000535, Best Loss: 0.000000458 in Epoch 2413
Epoch 2417
Epoch 2417, Loss: 0.000001551, Improvement: 0.000000090, Best Loss: 0.000000458 in Epoch 2413
Epoch 2418
Epoch 2418, Loss: 0.000001376, Improvement: -0.000000175, Best Loss: 0.000000458 in Epoch 2413
Epoch 2419
Epoch 2419, Loss: 0.000001196, Improvement: -0.000000180, Best Loss: 0.000000458 in Epoch 2413
Epoch 2420
Epoch 2420, Loss: 0.000001337, Improvement: 0.000000141, Best Loss: 0.000000458 in Epoch 2413
Epoch 2421
Epoch 2421, Loss: 0.000001650, Improvement: 0.000000314, Best Loss: 0.000000458 in Epoch 2413
Epoch 2422
Epoch 2422, Loss: 0.000002163, Improvement: 0.000000512, Best Loss: 0.000000458 in Epoch 2413
Epoch 2423
Epoch 2423, Loss: 0.000004438, Improvement: 0.000002276, Best Loss: 0.000000458 in Epoch 2413
Epoch 2424
Epoch 2424, Loss: 0.000007340, Improvement: 0.000002902, Best Loss: 0.000000458 in Epoch 2413
Epoch 2425
Epoch 2425, Loss: 0.000009735, Improvement: 0.000002394, Best Loss: 0.000000458 in Epoch 2413
Epoch 2426
Epoch 2426, Loss: 0.000009740, Improvement: 0.000000005, Best Loss: 0.000000458 in Epoch 2413
Epoch 2427
Epoch 2427, Loss: 0.000015570, Improvement: 0.000005830, Best Loss: 0.000000458 in Epoch 2413
Epoch 2428
Epoch 2428, Loss: 0.000010835, Improvement: -0.000004735, Best Loss: 0.000000458 in Epoch 2413
Epoch 2429
Epoch 2429, Loss: 0.000003783, Improvement: -0.000007052, Best Loss: 0.000000458 in Epoch 2413
Epoch 2430
Epoch 2430, Loss: 0.000002146, Improvement: -0.000001637, Best Loss: 0.000000458 in Epoch 2413
Epoch 2431
Epoch 2431, Loss: 0.000003631, Improvement: 0.000001486, Best Loss: 0.000000458 in Epoch 2413
Epoch 2432
Epoch 2432, Loss: 0.000003067, Improvement: -0.000000564, Best Loss: 0.000000458 in Epoch 2413
Epoch 2433
Epoch 2433, Loss: 0.000001524, Improvement: -0.000001543, Best Loss: 0.000000458 in Epoch 2413
Epoch 2434
Epoch 2434, Loss: 0.000001564, Improvement: 0.000000040, Best Loss: 0.000000458 in Epoch 2413
Epoch 2435
Epoch 2435, Loss: 0.000001816, Improvement: 0.000000252, Best Loss: 0.000000458 in Epoch 2413
Epoch 2436
Epoch 2436, Loss: 0.000002419, Improvement: 0.000000603, Best Loss: 0.000000458 in Epoch 2413
Epoch 2437
Epoch 2437, Loss: 0.000002548, Improvement: 0.000000129, Best Loss: 0.000000458 in Epoch 2413
Epoch 2438
Epoch 2438, Loss: 0.000002808, Improvement: 0.000000261, Best Loss: 0.000000458 in Epoch 2413
Epoch 2439
Epoch 2439, Loss: 0.000003649, Improvement: 0.000000841, Best Loss: 0.000000458 in Epoch 2413
Epoch 2440
Epoch 2440, Loss: 0.000001972, Improvement: -0.000001677, Best Loss: 0.000000458 in Epoch 2413
Epoch 2441
Epoch 2441, Loss: 0.000001505, Improvement: -0.000000467, Best Loss: 0.000000458 in Epoch 2413
Epoch 2442
Epoch 2442, Loss: 0.000001474, Improvement: -0.000000031, Best Loss: 0.000000458 in Epoch 2413
Epoch 2443
Epoch 2443, Loss: 0.000001927, Improvement: 0.000000453, Best Loss: 0.000000458 in Epoch 2413
Epoch 2444
Epoch 2444, Loss: 0.000001816, Improvement: -0.000000112, Best Loss: 0.000000458 in Epoch 2413
Epoch 2445
Epoch 2445, Loss: 0.000001679, Improvement: -0.000000136, Best Loss: 0.000000458 in Epoch 2413
Epoch 2446
Epoch 2446, Loss: 0.000002701, Improvement: 0.000001022, Best Loss: 0.000000458 in Epoch 2413
Epoch 2447
Epoch 2447, Loss: 0.000004667, Improvement: 0.000001966, Best Loss: 0.000000458 in Epoch 2413
Epoch 2448
Epoch 2448, Loss: 0.000019000, Improvement: 0.000014333, Best Loss: 0.000000458 in Epoch 2413
Epoch 2449
Epoch 2449, Loss: 0.000009579, Improvement: -0.000009422, Best Loss: 0.000000458 in Epoch 2413
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.000006768, Improvement: -0.000002810, Best Loss: 0.000000458 in Epoch 2413
Epoch 2451
Epoch 2451, Loss: 0.000004859, Improvement: -0.000001909, Best Loss: 0.000000458 in Epoch 2413
Epoch 2452
Epoch 2452, Loss: 0.000005137, Improvement: 0.000000277, Best Loss: 0.000000458 in Epoch 2413
Epoch 2453
Epoch 2453, Loss: 0.000006226, Improvement: 0.000001089, Best Loss: 0.000000458 in Epoch 2413
Epoch 2454
Epoch 2454, Loss: 0.000011159, Improvement: 0.000004933, Best Loss: 0.000000458 in Epoch 2413
Epoch 2455
Epoch 2455, Loss: 0.000006257, Improvement: -0.000004902, Best Loss: 0.000000458 in Epoch 2413
Epoch 2456
Epoch 2456, Loss: 0.000007870, Improvement: 0.000001612, Best Loss: 0.000000458 in Epoch 2413
Epoch 2457
Epoch 2457, Loss: 0.000006167, Improvement: -0.000001703, Best Loss: 0.000000458 in Epoch 2413
Epoch 2458
Epoch 2458, Loss: 0.000005996, Improvement: -0.000000171, Best Loss: 0.000000458 in Epoch 2413
Epoch 2459
Epoch 2459, Loss: 0.000005867, Improvement: -0.000000129, Best Loss: 0.000000458 in Epoch 2413
Epoch 2460
Epoch 2460, Loss: 0.000004867, Improvement: -0.000001000, Best Loss: 0.000000458 in Epoch 2413
Epoch 2461
Epoch 2461, Loss: 0.000004106, Improvement: -0.000000761, Best Loss: 0.000000458 in Epoch 2413
Epoch 2462
Epoch 2462, Loss: 0.000002445, Improvement: -0.000001661, Best Loss: 0.000000458 in Epoch 2413
Epoch 2463
Epoch 2463, Loss: 0.000001544, Improvement: -0.000000901, Best Loss: 0.000000458 in Epoch 2413
Epoch 2464
Epoch 2464, Loss: 0.000001255, Improvement: -0.000000289, Best Loss: 0.000000458 in Epoch 2413
Epoch 2465
Epoch 2465, Loss: 0.000001149, Improvement: -0.000000105, Best Loss: 0.000000458 in Epoch 2413
Epoch 2466
Epoch 2466, Loss: 0.000000967, Improvement: -0.000000183, Best Loss: 0.000000458 in Epoch 2413
Epoch 2467
Epoch 2467, Loss: 0.000000995, Improvement: 0.000000029, Best Loss: 0.000000458 in Epoch 2413
Epoch 2468
Epoch 2468, Loss: 0.000000772, Improvement: -0.000000223, Best Loss: 0.000000458 in Epoch 2413
Epoch 2469
Epoch 2469, Loss: 0.000000743, Improvement: -0.000000030, Best Loss: 0.000000458 in Epoch 2413
Epoch 2470
Epoch 2470, Loss: 0.000000758, Improvement: 0.000000015, Best Loss: 0.000000458 in Epoch 2413
Epoch 2471
Epoch 2471, Loss: 0.000000832, Improvement: 0.000000074, Best Loss: 0.000000458 in Epoch 2413
Epoch 2472
A best model at epoch 2472 has been saved with training error 0.000000417.
Epoch 2472, Loss: 0.000000660, Improvement: -0.000000172, Best Loss: 0.000000417 in Epoch 2472
Epoch 2473
A best model at epoch 2473 has been saved with training error 0.000000412.
Epoch 2473, Loss: 0.000000650, Improvement: -0.000000010, Best Loss: 0.000000412 in Epoch 2473
Epoch 2474
Epoch 2474, Loss: 0.000000662, Improvement: 0.000000012, Best Loss: 0.000000412 in Epoch 2473
Epoch 2475
Epoch 2475, Loss: 0.000000776, Improvement: 0.000000115, Best Loss: 0.000000412 in Epoch 2473
Epoch 2476
Epoch 2476, Loss: 0.000000767, Improvement: -0.000000010, Best Loss: 0.000000412 in Epoch 2473
Epoch 2477
Epoch 2477, Loss: 0.000000732, Improvement: -0.000000034, Best Loss: 0.000000412 in Epoch 2473
Epoch 2478
Epoch 2478, Loss: 0.000000953, Improvement: 0.000000220, Best Loss: 0.000000412 in Epoch 2473
Epoch 2479
Epoch 2479, Loss: 0.000001189, Improvement: 0.000000236, Best Loss: 0.000000412 in Epoch 2473
Epoch 2480
Epoch 2480, Loss: 0.000002291, Improvement: 0.000001103, Best Loss: 0.000000412 in Epoch 2473
Epoch 2481
Epoch 2481, Loss: 0.000003294, Improvement: 0.000001003, Best Loss: 0.000000412 in Epoch 2473
Epoch 2482
Epoch 2482, Loss: 0.000004339, Improvement: 0.000001045, Best Loss: 0.000000412 in Epoch 2473
Epoch 2483
Epoch 2483, Loss: 0.000007852, Improvement: 0.000003513, Best Loss: 0.000000412 in Epoch 2473
Epoch 2484
Epoch 2484, Loss: 0.000006886, Improvement: -0.000000965, Best Loss: 0.000000412 in Epoch 2473
Epoch 2485
Epoch 2485, Loss: 0.000007739, Improvement: 0.000000853, Best Loss: 0.000000412 in Epoch 2473
Epoch 2486
Epoch 2486, Loss: 0.000007241, Improvement: -0.000000498, Best Loss: 0.000000412 in Epoch 2473
Epoch 2487
Epoch 2487, Loss: 0.000004644, Improvement: -0.000002597, Best Loss: 0.000000412 in Epoch 2473
Epoch 2488
Epoch 2488, Loss: 0.000005510, Improvement: 0.000000866, Best Loss: 0.000000412 in Epoch 2473
Epoch 2489
Epoch 2489, Loss: 0.000007760, Improvement: 0.000002250, Best Loss: 0.000000412 in Epoch 2473
Epoch 2490
Epoch 2490, Loss: 0.000010174, Improvement: 0.000002414, Best Loss: 0.000000412 in Epoch 2473
Epoch 2491
Epoch 2491, Loss: 0.000009777, Improvement: -0.000000397, Best Loss: 0.000000412 in Epoch 2473
Epoch 2492
Epoch 2492, Loss: 0.000004406, Improvement: -0.000005371, Best Loss: 0.000000412 in Epoch 2473
Epoch 2493
Epoch 2493, Loss: 0.000003308, Improvement: -0.000001098, Best Loss: 0.000000412 in Epoch 2473
Epoch 2494
Epoch 2494, Loss: 0.000002967, Improvement: -0.000000341, Best Loss: 0.000000412 in Epoch 2473
Epoch 2495
Epoch 2495, Loss: 0.000002118, Improvement: -0.000000849, Best Loss: 0.000000412 in Epoch 2473
Epoch 2496
Epoch 2496, Loss: 0.000001248, Improvement: -0.000000869, Best Loss: 0.000000412 in Epoch 2473
Epoch 2497
Epoch 2497, Loss: 0.000000946, Improvement: -0.000000303, Best Loss: 0.000000412 in Epoch 2473
Epoch 2498
Epoch 2498, Loss: 0.000000870, Improvement: -0.000000076, Best Loss: 0.000000412 in Epoch 2473
Epoch 2499
Epoch 2499, Loss: 0.000001124, Improvement: 0.000000254, Best Loss: 0.000000412 in Epoch 2473
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.000001475, Improvement: 0.000000351, Best Loss: 0.000000412 in Epoch 2473
Epoch 2501
Epoch 2501, Loss: 0.000001680, Improvement: 0.000000205, Best Loss: 0.000000412 in Epoch 2473
Epoch 2502
Epoch 2502, Loss: 0.000002865, Improvement: 0.000001185, Best Loss: 0.000000412 in Epoch 2473
Epoch 2503
Epoch 2503, Loss: 0.000002785, Improvement: -0.000000079, Best Loss: 0.000000412 in Epoch 2473
Epoch 2504
Epoch 2504, Loss: 0.000002753, Improvement: -0.000000032, Best Loss: 0.000000412 in Epoch 2473
Epoch 2505
Epoch 2505, Loss: 0.000002439, Improvement: -0.000000314, Best Loss: 0.000000412 in Epoch 2473
Epoch 2506
Epoch 2506, Loss: 0.000002347, Improvement: -0.000000092, Best Loss: 0.000000412 in Epoch 2473
Epoch 2507
Epoch 2507, Loss: 0.000002976, Improvement: 0.000000629, Best Loss: 0.000000412 in Epoch 2473
Epoch 2508
Epoch 2508, Loss: 0.000006007, Improvement: 0.000003031, Best Loss: 0.000000412 in Epoch 2473
Epoch 2509
Epoch 2509, Loss: 0.000006252, Improvement: 0.000000245, Best Loss: 0.000000412 in Epoch 2473
Epoch 2510
Epoch 2510, Loss: 0.000005542, Improvement: -0.000000710, Best Loss: 0.000000412 in Epoch 2473
Epoch 2511
Epoch 2511, Loss: 0.000007540, Improvement: 0.000001998, Best Loss: 0.000000412 in Epoch 2473
Epoch 2512
Epoch 2512, Loss: 0.000004096, Improvement: -0.000003443, Best Loss: 0.000000412 in Epoch 2473
Epoch 2513
Epoch 2513, Loss: 0.000002519, Improvement: -0.000001578, Best Loss: 0.000000412 in Epoch 2473
Epoch 2514
Epoch 2514, Loss: 0.000002095, Improvement: -0.000000424, Best Loss: 0.000000412 in Epoch 2473
Epoch 2515
Epoch 2515, Loss: 0.000001914, Improvement: -0.000000181, Best Loss: 0.000000412 in Epoch 2473
Epoch 2516
Epoch 2516, Loss: 0.000001862, Improvement: -0.000000052, Best Loss: 0.000000412 in Epoch 2473
Epoch 2517
Epoch 2517, Loss: 0.000002395, Improvement: 0.000000533, Best Loss: 0.000000412 in Epoch 2473
Epoch 2518
Epoch 2518, Loss: 0.000003293, Improvement: 0.000000898, Best Loss: 0.000000412 in Epoch 2473
Epoch 2519
Epoch 2519, Loss: 0.000004418, Improvement: 0.000001125, Best Loss: 0.000000412 in Epoch 2473
Epoch 2520
Epoch 2520, Loss: 0.000004167, Improvement: -0.000000251, Best Loss: 0.000000412 in Epoch 2473
Epoch 2521
Epoch 2521, Loss: 0.000005027, Improvement: 0.000000861, Best Loss: 0.000000412 in Epoch 2473
Epoch 2522
Epoch 2522, Loss: 0.000005334, Improvement: 0.000000307, Best Loss: 0.000000412 in Epoch 2473
Epoch 2523
Epoch 2523, Loss: 0.000003707, Improvement: -0.000001626, Best Loss: 0.000000412 in Epoch 2473
Epoch 2524
Epoch 2524, Loss: 0.000006675, Improvement: 0.000002968, Best Loss: 0.000000412 in Epoch 2473
Epoch 2525
Epoch 2525, Loss: 0.000005388, Improvement: -0.000001288, Best Loss: 0.000000412 in Epoch 2473
Epoch 2526
Epoch 2526, Loss: 0.000013511, Improvement: 0.000008123, Best Loss: 0.000000412 in Epoch 2473
Epoch 2527
Epoch 2527, Loss: 0.000007630, Improvement: -0.000005881, Best Loss: 0.000000412 in Epoch 2473
Epoch 2528
Epoch 2528, Loss: 0.000003997, Improvement: -0.000003633, Best Loss: 0.000000412 in Epoch 2473
Epoch 2529
Epoch 2529, Loss: 0.000005767, Improvement: 0.000001770, Best Loss: 0.000000412 in Epoch 2473
Epoch 2530
Epoch 2530, Loss: 0.000011231, Improvement: 0.000005463, Best Loss: 0.000000412 in Epoch 2473
Epoch 2531
Epoch 2531, Loss: 0.000008891, Improvement: -0.000002339, Best Loss: 0.000000412 in Epoch 2473
Epoch 2532
Epoch 2532, Loss: 0.000006122, Improvement: -0.000002769, Best Loss: 0.000000412 in Epoch 2473
Epoch 2533
Epoch 2533, Loss: 0.000005671, Improvement: -0.000000452, Best Loss: 0.000000412 in Epoch 2473
Epoch 2534
Epoch 2534, Loss: 0.000004670, Improvement: -0.000001001, Best Loss: 0.000000412 in Epoch 2473
Epoch 2535
Epoch 2535, Loss: 0.000004325, Improvement: -0.000000345, Best Loss: 0.000000412 in Epoch 2473
Epoch 2536
Epoch 2536, Loss: 0.000007094, Improvement: 0.000002768, Best Loss: 0.000000412 in Epoch 2473
Epoch 2537
Epoch 2537, Loss: 0.000006958, Improvement: -0.000000136, Best Loss: 0.000000412 in Epoch 2473
Epoch 2538
Epoch 2538, Loss: 0.000005383, Improvement: -0.000001575, Best Loss: 0.000000412 in Epoch 2473
Epoch 2539
Epoch 2539, Loss: 0.000004394, Improvement: -0.000000989, Best Loss: 0.000000412 in Epoch 2473
Epoch 2540
Epoch 2540, Loss: 0.000004360, Improvement: -0.000000034, Best Loss: 0.000000412 in Epoch 2473
Epoch 2541
Epoch 2541, Loss: 0.000003906, Improvement: -0.000000454, Best Loss: 0.000000412 in Epoch 2473
Epoch 2542
Epoch 2542, Loss: 0.000004050, Improvement: 0.000000144, Best Loss: 0.000000412 in Epoch 2473
Epoch 2543
Epoch 2543, Loss: 0.000002903, Improvement: -0.000001147, Best Loss: 0.000000412 in Epoch 2473
Epoch 2544
Epoch 2544, Loss: 0.000002211, Improvement: -0.000000692, Best Loss: 0.000000412 in Epoch 2473
Epoch 2545
Epoch 2545, Loss: 0.000001893, Improvement: -0.000000319, Best Loss: 0.000000412 in Epoch 2473
Epoch 2546
Epoch 2546, Loss: 0.000002056, Improvement: 0.000000163, Best Loss: 0.000000412 in Epoch 2473
Epoch 2547
Epoch 2547, Loss: 0.000010463, Improvement: 0.000008407, Best Loss: 0.000000412 in Epoch 2473
Epoch 2548
Epoch 2548, Loss: 0.000009879, Improvement: -0.000000584, Best Loss: 0.000000412 in Epoch 2473
Epoch 2549
Epoch 2549, Loss: 0.000005739, Improvement: -0.000004140, Best Loss: 0.000000412 in Epoch 2473
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.000006108, Improvement: 0.000000369, Best Loss: 0.000000412 in Epoch 2473
Epoch 2551
Epoch 2551, Loss: 0.000006235, Improvement: 0.000000126, Best Loss: 0.000000412 in Epoch 2473
Epoch 2552
Epoch 2552, Loss: 0.000003380, Improvement: -0.000002854, Best Loss: 0.000000412 in Epoch 2473
Epoch 2553
Epoch 2553, Loss: 0.000003240, Improvement: -0.000000141, Best Loss: 0.000000412 in Epoch 2473
Epoch 2554
Epoch 2554, Loss: 0.000004349, Improvement: 0.000001110, Best Loss: 0.000000412 in Epoch 2473
Epoch 2555
Epoch 2555, Loss: 0.000005382, Improvement: 0.000001033, Best Loss: 0.000000412 in Epoch 2473
Epoch 2556
Epoch 2556, Loss: 0.000004214, Improvement: -0.000001168, Best Loss: 0.000000412 in Epoch 2473
Epoch 2557
Epoch 2557, Loss: 0.000002706, Improvement: -0.000001508, Best Loss: 0.000000412 in Epoch 2473
Epoch 2558
Epoch 2558, Loss: 0.000002650, Improvement: -0.000000056, Best Loss: 0.000000412 in Epoch 2473
Epoch 2559
Epoch 2559, Loss: 0.000005174, Improvement: 0.000002524, Best Loss: 0.000000412 in Epoch 2473
Epoch 2560
Epoch 2560, Loss: 0.000004676, Improvement: -0.000000498, Best Loss: 0.000000412 in Epoch 2473
Epoch 2561
Epoch 2561, Loss: 0.000005424, Improvement: 0.000000748, Best Loss: 0.000000412 in Epoch 2473
Epoch 2562
Epoch 2562, Loss: 0.000014348, Improvement: 0.000008924, Best Loss: 0.000000412 in Epoch 2473
Epoch 2563
Epoch 2563, Loss: 0.000011827, Improvement: -0.000002522, Best Loss: 0.000000412 in Epoch 2473
Epoch 2564
Epoch 2564, Loss: 0.000005983, Improvement: -0.000005843, Best Loss: 0.000000412 in Epoch 2473
Epoch 2565
Epoch 2565, Loss: 0.000005151, Improvement: -0.000000833, Best Loss: 0.000000412 in Epoch 2473
Epoch 2566
Epoch 2566, Loss: 0.000003813, Improvement: -0.000001338, Best Loss: 0.000000412 in Epoch 2473
Epoch 2567
Epoch 2567, Loss: 0.000003013, Improvement: -0.000000799, Best Loss: 0.000000412 in Epoch 2473
Epoch 2568
Epoch 2568, Loss: 0.000001788, Improvement: -0.000001225, Best Loss: 0.000000412 in Epoch 2473
Epoch 2569
Epoch 2569, Loss: 0.000001356, Improvement: -0.000000432, Best Loss: 0.000000412 in Epoch 2473
Epoch 2570
Epoch 2570, Loss: 0.000001418, Improvement: 0.000000062, Best Loss: 0.000000412 in Epoch 2473
Epoch 2571
Epoch 2571, Loss: 0.000001117, Improvement: -0.000000301, Best Loss: 0.000000412 in Epoch 2473
Epoch 2572
Epoch 2572, Loss: 0.000001135, Improvement: 0.000000018, Best Loss: 0.000000412 in Epoch 2473
Epoch 2573
Epoch 2573, Loss: 0.000001021, Improvement: -0.000000114, Best Loss: 0.000000412 in Epoch 2473
Epoch 2574
Epoch 2574, Loss: 0.000001196, Improvement: 0.000000175, Best Loss: 0.000000412 in Epoch 2473
Epoch 2575
Epoch 2575, Loss: 0.000001181, Improvement: -0.000000015, Best Loss: 0.000000412 in Epoch 2473
Epoch 2576
Epoch 2576, Loss: 0.000001025, Improvement: -0.000000156, Best Loss: 0.000000412 in Epoch 2473
Epoch 2577
Epoch 2577, Loss: 0.000000996, Improvement: -0.000000029, Best Loss: 0.000000412 in Epoch 2473
Epoch 2578
Epoch 2578, Loss: 0.000001050, Improvement: 0.000000055, Best Loss: 0.000000412 in Epoch 2473
Epoch 2579
Epoch 2579, Loss: 0.000000924, Improvement: -0.000000126, Best Loss: 0.000000412 in Epoch 2473
Epoch 2580
Epoch 2580, Loss: 0.000000842, Improvement: -0.000000082, Best Loss: 0.000000412 in Epoch 2473
Epoch 2581
Epoch 2581, Loss: 0.000000772, Improvement: -0.000000070, Best Loss: 0.000000412 in Epoch 2473
Epoch 2582
Epoch 2582, Loss: 0.000000845, Improvement: 0.000000072, Best Loss: 0.000000412 in Epoch 2473
Epoch 2583
Epoch 2583, Loss: 0.000000910, Improvement: 0.000000066, Best Loss: 0.000000412 in Epoch 2473
Epoch 2584
Epoch 2584, Loss: 0.000001050, Improvement: 0.000000140, Best Loss: 0.000000412 in Epoch 2473
Epoch 2585
Epoch 2585, Loss: 0.000001078, Improvement: 0.000000028, Best Loss: 0.000000412 in Epoch 2473
Epoch 2586
Epoch 2586, Loss: 0.000000975, Improvement: -0.000000103, Best Loss: 0.000000412 in Epoch 2473
Epoch 2587
Epoch 2587, Loss: 0.000001126, Improvement: 0.000000151, Best Loss: 0.000000412 in Epoch 2473
Epoch 2588
Epoch 2588, Loss: 0.000001443, Improvement: 0.000000317, Best Loss: 0.000000412 in Epoch 2473
Epoch 2589
Epoch 2589, Loss: 0.000002449, Improvement: 0.000001006, Best Loss: 0.000000412 in Epoch 2473
Epoch 2590
Epoch 2590, Loss: 0.000002628, Improvement: 0.000000179, Best Loss: 0.000000412 in Epoch 2473
Epoch 2591
Epoch 2591, Loss: 0.000006324, Improvement: 0.000003696, Best Loss: 0.000000412 in Epoch 2473
Epoch 2592
Epoch 2592, Loss: 0.000008786, Improvement: 0.000002462, Best Loss: 0.000000412 in Epoch 2473
Epoch 2593
Epoch 2593, Loss: 0.000009156, Improvement: 0.000000369, Best Loss: 0.000000412 in Epoch 2473
Epoch 2594
Epoch 2594, Loss: 0.000007107, Improvement: -0.000002048, Best Loss: 0.000000412 in Epoch 2473
Epoch 2595
Epoch 2595, Loss: 0.000005374, Improvement: -0.000001734, Best Loss: 0.000000412 in Epoch 2473
Epoch 2596
Epoch 2596, Loss: 0.000004785, Improvement: -0.000000589, Best Loss: 0.000000412 in Epoch 2473
Epoch 2597
Epoch 2597, Loss: 0.000002446, Improvement: -0.000002339, Best Loss: 0.000000412 in Epoch 2473
Epoch 2598
Epoch 2598, Loss: 0.000001432, Improvement: -0.000001014, Best Loss: 0.000000412 in Epoch 2473
Epoch 2599
Epoch 2599, Loss: 0.000001341, Improvement: -0.000000091, Best Loss: 0.000000412 in Epoch 2473
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.000000987, Improvement: -0.000000354, Best Loss: 0.000000412 in Epoch 2473
Epoch 2601
Epoch 2601, Loss: 0.000000737, Improvement: -0.000000250, Best Loss: 0.000000412 in Epoch 2473
Epoch 2602
Epoch 2602, Loss: 0.000000904, Improvement: 0.000000167, Best Loss: 0.000000412 in Epoch 2473
Epoch 2603
Epoch 2603, Loss: 0.000001337, Improvement: 0.000000433, Best Loss: 0.000000412 in Epoch 2473
Epoch 2604
Epoch 2604, Loss: 0.000000909, Improvement: -0.000000428, Best Loss: 0.000000412 in Epoch 2473
Epoch 2605
Epoch 2605, Loss: 0.000001031, Improvement: 0.000000122, Best Loss: 0.000000412 in Epoch 2473
Epoch 2606
Epoch 2606, Loss: 0.000001752, Improvement: 0.000000721, Best Loss: 0.000000412 in Epoch 2473
Epoch 2607
Epoch 2607, Loss: 0.000002000, Improvement: 0.000000248, Best Loss: 0.000000412 in Epoch 2473
Epoch 2608
Epoch 2608, Loss: 0.000003359, Improvement: 0.000001359, Best Loss: 0.000000412 in Epoch 2473
Epoch 2609
Epoch 2609, Loss: 0.000006757, Improvement: 0.000003398, Best Loss: 0.000000412 in Epoch 2473
Epoch 2610
Epoch 2610, Loss: 0.000013248, Improvement: 0.000006492, Best Loss: 0.000000412 in Epoch 2473
Epoch 2611
Epoch 2611, Loss: 0.000023849, Improvement: 0.000010601, Best Loss: 0.000000412 in Epoch 2473
Epoch 2612
Epoch 2612, Loss: 0.000013990, Improvement: -0.000009859, Best Loss: 0.000000412 in Epoch 2473
Epoch 2613
Epoch 2613, Loss: 0.000008946, Improvement: -0.000005044, Best Loss: 0.000000412 in Epoch 2473
Epoch 2614
Epoch 2614, Loss: 0.000005102, Improvement: -0.000003844, Best Loss: 0.000000412 in Epoch 2473
Epoch 2615
Epoch 2615, Loss: 0.000001643, Improvement: -0.000003460, Best Loss: 0.000000412 in Epoch 2473
Epoch 2616
Epoch 2616, Loss: 0.000001426, Improvement: -0.000000217, Best Loss: 0.000000412 in Epoch 2473
Epoch 2617
Epoch 2617, Loss: 0.000000961, Improvement: -0.000000465, Best Loss: 0.000000412 in Epoch 2473
Epoch 2618
Epoch 2618, Loss: 0.000000647, Improvement: -0.000000314, Best Loss: 0.000000412 in Epoch 2473
Epoch 2619
Epoch 2619, Loss: 0.000000566, Improvement: -0.000000081, Best Loss: 0.000000412 in Epoch 2473
Epoch 2620
Epoch 2620, Loss: 0.000000546, Improvement: -0.000000019, Best Loss: 0.000000412 in Epoch 2473
Epoch 2621
Epoch 2621, Loss: 0.000000538, Improvement: -0.000000008, Best Loss: 0.000000412 in Epoch 2473
Epoch 2622
A best model at epoch 2622 has been saved with training error 0.000000407.
A best model at epoch 2622 has been saved with training error 0.000000366.
Epoch 2622, Loss: 0.000000503, Improvement: -0.000000035, Best Loss: 0.000000366 in Epoch 2622
Epoch 2623
A best model at epoch 2623 has been saved with training error 0.000000334.
Epoch 2623, Loss: 0.000000507, Improvement: 0.000000004, Best Loss: 0.000000334 in Epoch 2623
Epoch 2624
Epoch 2624, Loss: 0.000000644, Improvement: 0.000000137, Best Loss: 0.000000334 in Epoch 2623
Epoch 2625
Epoch 2625, Loss: 0.000000559, Improvement: -0.000000084, Best Loss: 0.000000334 in Epoch 2623
Epoch 2626
Epoch 2626, Loss: 0.000000530, Improvement: -0.000000029, Best Loss: 0.000000334 in Epoch 2623
Epoch 2627
Epoch 2627, Loss: 0.000000560, Improvement: 0.000000031, Best Loss: 0.000000334 in Epoch 2623
Epoch 2628
Epoch 2628, Loss: 0.000000692, Improvement: 0.000000132, Best Loss: 0.000000334 in Epoch 2623
Epoch 2629
Epoch 2629, Loss: 0.000000779, Improvement: 0.000000086, Best Loss: 0.000000334 in Epoch 2623
Epoch 2630
Epoch 2630, Loss: 0.000000728, Improvement: -0.000000051, Best Loss: 0.000000334 in Epoch 2623
Epoch 2631
Epoch 2631, Loss: 0.000000683, Improvement: -0.000000045, Best Loss: 0.000000334 in Epoch 2623
Epoch 2632
Epoch 2632, Loss: 0.000000641, Improvement: -0.000000042, Best Loss: 0.000000334 in Epoch 2623
Epoch 2633
Epoch 2633, Loss: 0.000000654, Improvement: 0.000000013, Best Loss: 0.000000334 in Epoch 2623
Epoch 2634
Epoch 2634, Loss: 0.000000616, Improvement: -0.000000039, Best Loss: 0.000000334 in Epoch 2623
Epoch 2635
Epoch 2635, Loss: 0.000000688, Improvement: 0.000000072, Best Loss: 0.000000334 in Epoch 2623
Epoch 2636
Epoch 2636, Loss: 0.000000891, Improvement: 0.000000203, Best Loss: 0.000000334 in Epoch 2623
Epoch 2637
Epoch 2637, Loss: 0.000000969, Improvement: 0.000000078, Best Loss: 0.000000334 in Epoch 2623
Epoch 2638
Epoch 2638, Loss: 0.000000757, Improvement: -0.000000212, Best Loss: 0.000000334 in Epoch 2623
Epoch 2639
Epoch 2639, Loss: 0.000000648, Improvement: -0.000000109, Best Loss: 0.000000334 in Epoch 2623
Epoch 2640
Epoch 2640, Loss: 0.000000810, Improvement: 0.000000162, Best Loss: 0.000000334 in Epoch 2623
Epoch 2641
Epoch 2641, Loss: 0.000000753, Improvement: -0.000000057, Best Loss: 0.000000334 in Epoch 2623
Epoch 2642
Epoch 2642, Loss: 0.000001044, Improvement: 0.000000291, Best Loss: 0.000000334 in Epoch 2623
Epoch 2643
Epoch 2643, Loss: 0.000000895, Improvement: -0.000000149, Best Loss: 0.000000334 in Epoch 2623
Epoch 2644
Epoch 2644, Loss: 0.000000763, Improvement: -0.000000132, Best Loss: 0.000000334 in Epoch 2623
Epoch 2645
Epoch 2645, Loss: 0.000000739, Improvement: -0.000000023, Best Loss: 0.000000334 in Epoch 2623
Epoch 2646
Epoch 2646, Loss: 0.000000830, Improvement: 0.000000091, Best Loss: 0.000000334 in Epoch 2623
Epoch 2647
Epoch 2647, Loss: 0.000000990, Improvement: 0.000000161, Best Loss: 0.000000334 in Epoch 2623
Epoch 2648
Epoch 2648, Loss: 0.000000835, Improvement: -0.000000156, Best Loss: 0.000000334 in Epoch 2623
Epoch 2649
Epoch 2649, Loss: 0.000001051, Improvement: 0.000000217, Best Loss: 0.000000334 in Epoch 2623
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.000001399, Improvement: 0.000000348, Best Loss: 0.000000334 in Epoch 2623
Epoch 2651
Epoch 2651, Loss: 0.000001778, Improvement: 0.000000379, Best Loss: 0.000000334 in Epoch 2623
Epoch 2652
Epoch 2652, Loss: 0.000002681, Improvement: 0.000000903, Best Loss: 0.000000334 in Epoch 2623
Epoch 2653
Epoch 2653, Loss: 0.000003836, Improvement: 0.000001155, Best Loss: 0.000000334 in Epoch 2623
Epoch 2654
Epoch 2654, Loss: 0.000004705, Improvement: 0.000000870, Best Loss: 0.000000334 in Epoch 2623
Epoch 2655
Epoch 2655, Loss: 0.000007838, Improvement: 0.000003133, Best Loss: 0.000000334 in Epoch 2623
Epoch 2656
Epoch 2656, Loss: 0.000007637, Improvement: -0.000000201, Best Loss: 0.000000334 in Epoch 2623
Epoch 2657
Epoch 2657, Loss: 0.000005607, Improvement: -0.000002030, Best Loss: 0.000000334 in Epoch 2623
Epoch 2658
Epoch 2658, Loss: 0.000008631, Improvement: 0.000003024, Best Loss: 0.000000334 in Epoch 2623
Epoch 2659
Epoch 2659, Loss: 0.000007308, Improvement: -0.000001323, Best Loss: 0.000000334 in Epoch 2623
Epoch 2660
Epoch 2660, Loss: 0.000009824, Improvement: 0.000002516, Best Loss: 0.000000334 in Epoch 2623
Epoch 2661
Epoch 2661, Loss: 0.000005266, Improvement: -0.000004558, Best Loss: 0.000000334 in Epoch 2623
Epoch 2662
Epoch 2662, Loss: 0.000003467, Improvement: -0.000001798, Best Loss: 0.000000334 in Epoch 2623
Epoch 2663
Epoch 2663, Loss: 0.000003492, Improvement: 0.000000025, Best Loss: 0.000000334 in Epoch 2623
Epoch 2664
Epoch 2664, Loss: 0.000002564, Improvement: -0.000000928, Best Loss: 0.000000334 in Epoch 2623
Epoch 2665
Epoch 2665, Loss: 0.000001525, Improvement: -0.000001039, Best Loss: 0.000000334 in Epoch 2623
Epoch 2666
Epoch 2666, Loss: 0.000001316, Improvement: -0.000000209, Best Loss: 0.000000334 in Epoch 2623
Epoch 2667
Epoch 2667, Loss: 0.000001263, Improvement: -0.000000052, Best Loss: 0.000000334 in Epoch 2623
Epoch 2668
Epoch 2668, Loss: 0.000000998, Improvement: -0.000000265, Best Loss: 0.000000334 in Epoch 2623
Epoch 2669
Epoch 2669, Loss: 0.000000855, Improvement: -0.000000143, Best Loss: 0.000000334 in Epoch 2623
Epoch 2670
Epoch 2670, Loss: 0.000001353, Improvement: 0.000000498, Best Loss: 0.000000334 in Epoch 2623
Epoch 2671
Epoch 2671, Loss: 0.000003033, Improvement: 0.000001680, Best Loss: 0.000000334 in Epoch 2623
Epoch 2672
Epoch 2672, Loss: 0.000008284, Improvement: 0.000005250, Best Loss: 0.000000334 in Epoch 2623
Epoch 2673
Epoch 2673, Loss: 0.000012889, Improvement: 0.000004605, Best Loss: 0.000000334 in Epoch 2623
Epoch 2674
Epoch 2674, Loss: 0.000007405, Improvement: -0.000005483, Best Loss: 0.000000334 in Epoch 2623
Epoch 2675
Epoch 2675, Loss: 0.000005999, Improvement: -0.000001407, Best Loss: 0.000000334 in Epoch 2623
Epoch 2676
Epoch 2676, Loss: 0.000004402, Improvement: -0.000001596, Best Loss: 0.000000334 in Epoch 2623
Epoch 2677
Epoch 2677, Loss: 0.000002897, Improvement: -0.000001505, Best Loss: 0.000000334 in Epoch 2623
Epoch 2678
Epoch 2678, Loss: 0.000002785, Improvement: -0.000000112, Best Loss: 0.000000334 in Epoch 2623
Epoch 2679
Epoch 2679, Loss: 0.000002066, Improvement: -0.000000719, Best Loss: 0.000000334 in Epoch 2623
Epoch 2680
Epoch 2680, Loss: 0.000001183, Improvement: -0.000000883, Best Loss: 0.000000334 in Epoch 2623
Epoch 2681
Epoch 2681, Loss: 0.000001098, Improvement: -0.000000084, Best Loss: 0.000000334 in Epoch 2623
Epoch 2682
Epoch 2682, Loss: 0.000001024, Improvement: -0.000000075, Best Loss: 0.000000334 in Epoch 2623
Epoch 2683
Epoch 2683, Loss: 0.000000759, Improvement: -0.000000265, Best Loss: 0.000000334 in Epoch 2623
Epoch 2684
Epoch 2684, Loss: 0.000000862, Improvement: 0.000000102, Best Loss: 0.000000334 in Epoch 2623
Epoch 2685
Epoch 2685, Loss: 0.000001087, Improvement: 0.000000225, Best Loss: 0.000000334 in Epoch 2623
Epoch 2686
Epoch 2686, Loss: 0.000001108, Improvement: 0.000000021, Best Loss: 0.000000334 in Epoch 2623
Epoch 2687
Epoch 2687, Loss: 0.000001037, Improvement: -0.000000071, Best Loss: 0.000000334 in Epoch 2623
Epoch 2688
Epoch 2688, Loss: 0.000000959, Improvement: -0.000000078, Best Loss: 0.000000334 in Epoch 2623
Epoch 2689
Epoch 2689, Loss: 0.000000811, Improvement: -0.000000148, Best Loss: 0.000000334 in Epoch 2623
Epoch 2690
Epoch 2690, Loss: 0.000000778, Improvement: -0.000000033, Best Loss: 0.000000334 in Epoch 2623
Epoch 2691
Epoch 2691, Loss: 0.000000974, Improvement: 0.000000196, Best Loss: 0.000000334 in Epoch 2623
Epoch 2692
Epoch 2692, Loss: 0.000001131, Improvement: 0.000000157, Best Loss: 0.000000334 in Epoch 2623
Epoch 2693
Epoch 2693, Loss: 0.000001284, Improvement: 0.000000152, Best Loss: 0.000000334 in Epoch 2623
Epoch 2694
Epoch 2694, Loss: 0.000001663, Improvement: 0.000000379, Best Loss: 0.000000334 in Epoch 2623
Epoch 2695
Epoch 2695, Loss: 0.000003800, Improvement: 0.000002137, Best Loss: 0.000000334 in Epoch 2623
Epoch 2696
Epoch 2696, Loss: 0.000002493, Improvement: -0.000001307, Best Loss: 0.000000334 in Epoch 2623
Epoch 2697
Epoch 2697, Loss: 0.000001728, Improvement: -0.000000765, Best Loss: 0.000000334 in Epoch 2623
Epoch 2698
Epoch 2698, Loss: 0.000001405, Improvement: -0.000000323, Best Loss: 0.000000334 in Epoch 2623
Epoch 2699
Epoch 2699, Loss: 0.000001345, Improvement: -0.000000060, Best Loss: 0.000000334 in Epoch 2623
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.000003390, Improvement: 0.000002045, Best Loss: 0.000000334 in Epoch 2623
Epoch 2701
Epoch 2701, Loss: 0.000004901, Improvement: 0.000001511, Best Loss: 0.000000334 in Epoch 2623
Epoch 2702
Epoch 2702, Loss: 0.000011172, Improvement: 0.000006271, Best Loss: 0.000000334 in Epoch 2623
Epoch 2703
Epoch 2703, Loss: 0.000007312, Improvement: -0.000003861, Best Loss: 0.000000334 in Epoch 2623
Epoch 2704
Epoch 2704, Loss: 0.000004925, Improvement: -0.000002386, Best Loss: 0.000000334 in Epoch 2623
Epoch 2705
Epoch 2705, Loss: 0.000005087, Improvement: 0.000000162, Best Loss: 0.000000334 in Epoch 2623
Epoch 2706
Epoch 2706, Loss: 0.000004604, Improvement: -0.000000483, Best Loss: 0.000000334 in Epoch 2623
Epoch 2707
Epoch 2707, Loss: 0.000003427, Improvement: -0.000001177, Best Loss: 0.000000334 in Epoch 2623
Epoch 2708
Epoch 2708, Loss: 0.000003768, Improvement: 0.000000340, Best Loss: 0.000000334 in Epoch 2623
Epoch 2709
Epoch 2709, Loss: 0.000005170, Improvement: 0.000001403, Best Loss: 0.000000334 in Epoch 2623
Epoch 2710
Epoch 2710, Loss: 0.000005143, Improvement: -0.000000028, Best Loss: 0.000000334 in Epoch 2623
Epoch 2711
Epoch 2711, Loss: 0.000007231, Improvement: 0.000002088, Best Loss: 0.000000334 in Epoch 2623
Epoch 2712
Epoch 2712, Loss: 0.000009062, Improvement: 0.000001831, Best Loss: 0.000000334 in Epoch 2623
Epoch 2713
Epoch 2713, Loss: 0.000003525, Improvement: -0.000005537, Best Loss: 0.000000334 in Epoch 2623
Epoch 2714
Epoch 2714, Loss: 0.000003102, Improvement: -0.000000422, Best Loss: 0.000000334 in Epoch 2623
Epoch 2715
Epoch 2715, Loss: 0.000003247, Improvement: 0.000000145, Best Loss: 0.000000334 in Epoch 2623
Epoch 2716
Epoch 2716, Loss: 0.000001893, Improvement: -0.000001355, Best Loss: 0.000000334 in Epoch 2623
Epoch 2717
Epoch 2717, Loss: 0.000001788, Improvement: -0.000000104, Best Loss: 0.000000334 in Epoch 2623
Epoch 2718
Epoch 2718, Loss: 0.000001428, Improvement: -0.000000360, Best Loss: 0.000000334 in Epoch 2623
Epoch 2719
Epoch 2719, Loss: 0.000001290, Improvement: -0.000000138, Best Loss: 0.000000334 in Epoch 2623
Epoch 2720
Epoch 2720, Loss: 0.000001070, Improvement: -0.000000220, Best Loss: 0.000000334 in Epoch 2623
Epoch 2721
Epoch 2721, Loss: 0.000000963, Improvement: -0.000000108, Best Loss: 0.000000334 in Epoch 2623
Epoch 2722
Epoch 2722, Loss: 0.000001012, Improvement: 0.000000050, Best Loss: 0.000000334 in Epoch 2623
Epoch 2723
Epoch 2723, Loss: 0.000000947, Improvement: -0.000000065, Best Loss: 0.000000334 in Epoch 2623
Epoch 2724
Epoch 2724, Loss: 0.000000834, Improvement: -0.000000114, Best Loss: 0.000000334 in Epoch 2623
Epoch 2725
Epoch 2725, Loss: 0.000000694, Improvement: -0.000000139, Best Loss: 0.000000334 in Epoch 2623
Epoch 2726
Epoch 2726, Loss: 0.000000680, Improvement: -0.000000014, Best Loss: 0.000000334 in Epoch 2623
Epoch 2727
Epoch 2727, Loss: 0.000000666, Improvement: -0.000000014, Best Loss: 0.000000334 in Epoch 2623
Epoch 2728
Epoch 2728, Loss: 0.000001299, Improvement: 0.000000633, Best Loss: 0.000000334 in Epoch 2623
Epoch 2729
Epoch 2729, Loss: 0.000002165, Improvement: 0.000000866, Best Loss: 0.000000334 in Epoch 2623
Epoch 2730
Epoch 2730, Loss: 0.000002003, Improvement: -0.000000162, Best Loss: 0.000000334 in Epoch 2623
Epoch 2731
Epoch 2731, Loss: 0.000001486, Improvement: -0.000000517, Best Loss: 0.000000334 in Epoch 2623
Epoch 2732
Epoch 2732, Loss: 0.000001261, Improvement: -0.000000224, Best Loss: 0.000000334 in Epoch 2623
Epoch 2733
Epoch 2733, Loss: 0.000001092, Improvement: -0.000000170, Best Loss: 0.000000334 in Epoch 2623
Epoch 2734
Epoch 2734, Loss: 0.000001154, Improvement: 0.000000062, Best Loss: 0.000000334 in Epoch 2623
Epoch 2735
Epoch 2735, Loss: 0.000001069, Improvement: -0.000000084, Best Loss: 0.000000334 in Epoch 2623
Epoch 2736
Epoch 2736, Loss: 0.000001952, Improvement: 0.000000883, Best Loss: 0.000000334 in Epoch 2623
Epoch 2737
Epoch 2737, Loss: 0.000003001, Improvement: 0.000001049, Best Loss: 0.000000334 in Epoch 2623
Epoch 2738
Epoch 2738, Loss: 0.000004318, Improvement: 0.000001317, Best Loss: 0.000000334 in Epoch 2623
Epoch 2739
Epoch 2739, Loss: 0.000004735, Improvement: 0.000000417, Best Loss: 0.000000334 in Epoch 2623
Epoch 2740
Epoch 2740, Loss: 0.000006405, Improvement: 0.000001670, Best Loss: 0.000000334 in Epoch 2623
Epoch 2741
Epoch 2741, Loss: 0.000007271, Improvement: 0.000000866, Best Loss: 0.000000334 in Epoch 2623
Epoch 2742
Epoch 2742, Loss: 0.000005644, Improvement: -0.000001627, Best Loss: 0.000000334 in Epoch 2623
Epoch 2743
Epoch 2743, Loss: 0.000004281, Improvement: -0.000001363, Best Loss: 0.000000334 in Epoch 2623
Epoch 2744
Epoch 2744, Loss: 0.000005443, Improvement: 0.000001162, Best Loss: 0.000000334 in Epoch 2623
Epoch 2745
Epoch 2745, Loss: 0.000004229, Improvement: -0.000001214, Best Loss: 0.000000334 in Epoch 2623
Epoch 2746
Epoch 2746, Loss: 0.000003637, Improvement: -0.000000592, Best Loss: 0.000000334 in Epoch 2623
Epoch 2747
Epoch 2747, Loss: 0.000004491, Improvement: 0.000000854, Best Loss: 0.000000334 in Epoch 2623
Epoch 2748
Epoch 2748, Loss: 0.000005509, Improvement: 0.000001018, Best Loss: 0.000000334 in Epoch 2623
Epoch 2749
Epoch 2749, Loss: 0.000004498, Improvement: -0.000001011, Best Loss: 0.000000334 in Epoch 2623
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.000005055, Improvement: 0.000000557, Best Loss: 0.000000334 in Epoch 2623
Epoch 2751
Epoch 2751, Loss: 0.000004075, Improvement: -0.000000980, Best Loss: 0.000000334 in Epoch 2623
Epoch 2752
Epoch 2752, Loss: 0.000002892, Improvement: -0.000001183, Best Loss: 0.000000334 in Epoch 2623
Epoch 2753
Epoch 2753, Loss: 0.000003034, Improvement: 0.000000142, Best Loss: 0.000000334 in Epoch 2623
Epoch 2754
Epoch 2754, Loss: 0.000003745, Improvement: 0.000000711, Best Loss: 0.000000334 in Epoch 2623
Epoch 2755
Epoch 2755, Loss: 0.000005281, Improvement: 0.000001536, Best Loss: 0.000000334 in Epoch 2623
Epoch 2756
Epoch 2756, Loss: 0.000012779, Improvement: 0.000007498, Best Loss: 0.000000334 in Epoch 2623
Epoch 2757
Epoch 2757, Loss: 0.000008159, Improvement: -0.000004620, Best Loss: 0.000000334 in Epoch 2623
Epoch 2758
Epoch 2758, Loss: 0.000004131, Improvement: -0.000004028, Best Loss: 0.000000334 in Epoch 2623
Epoch 2759
Epoch 2759, Loss: 0.000002647, Improvement: -0.000001484, Best Loss: 0.000000334 in Epoch 2623
Epoch 2760
Epoch 2760, Loss: 0.000002283, Improvement: -0.000000363, Best Loss: 0.000000334 in Epoch 2623
Epoch 2761
Epoch 2761, Loss: 0.000002407, Improvement: 0.000000124, Best Loss: 0.000000334 in Epoch 2623
Epoch 2762
Epoch 2762, Loss: 0.000002099, Improvement: -0.000000309, Best Loss: 0.000000334 in Epoch 2623
Epoch 2763
Epoch 2763, Loss: 0.000001815, Improvement: -0.000000284, Best Loss: 0.000000334 in Epoch 2623
Epoch 2764
Epoch 2764, Loss: 0.000001862, Improvement: 0.000000047, Best Loss: 0.000000334 in Epoch 2623
Epoch 2765
Epoch 2765, Loss: 0.000003020, Improvement: 0.000001158, Best Loss: 0.000000334 in Epoch 2623
Epoch 2766
Epoch 2766, Loss: 0.000003544, Improvement: 0.000000524, Best Loss: 0.000000334 in Epoch 2623
Epoch 2767
Epoch 2767, Loss: 0.000004609, Improvement: 0.000001065, Best Loss: 0.000000334 in Epoch 2623
Epoch 2768
Epoch 2768, Loss: 0.000003431, Improvement: -0.000001178, Best Loss: 0.000000334 in Epoch 2623
Epoch 2769
Epoch 2769, Loss: 0.000002766, Improvement: -0.000000664, Best Loss: 0.000000334 in Epoch 2623
Epoch 2770
Epoch 2770, Loss: 0.000002529, Improvement: -0.000000238, Best Loss: 0.000000334 in Epoch 2623
Epoch 2771
Epoch 2771, Loss: 0.000005791, Improvement: 0.000003263, Best Loss: 0.000000334 in Epoch 2623
Epoch 2772
Epoch 2772, Loss: 0.000003036, Improvement: -0.000002756, Best Loss: 0.000000334 in Epoch 2623
Epoch 2773
Epoch 2773, Loss: 0.000001840, Improvement: -0.000001196, Best Loss: 0.000000334 in Epoch 2623
Epoch 2774
Epoch 2774, Loss: 0.000001298, Improvement: -0.000000542, Best Loss: 0.000000334 in Epoch 2623
Epoch 2775
Epoch 2775, Loss: 0.000001837, Improvement: 0.000000539, Best Loss: 0.000000334 in Epoch 2623
Epoch 2776
Epoch 2776, Loss: 0.000002147, Improvement: 0.000000310, Best Loss: 0.000000334 in Epoch 2623
Epoch 2777
Epoch 2777, Loss: 0.000003298, Improvement: 0.000001151, Best Loss: 0.000000334 in Epoch 2623
Epoch 2778
Epoch 2778, Loss: 0.000003478, Improvement: 0.000000181, Best Loss: 0.000000334 in Epoch 2623
Epoch 2779
Epoch 2779, Loss: 0.000003149, Improvement: -0.000000330, Best Loss: 0.000000334 in Epoch 2623
Epoch 2780
Epoch 2780, Loss: 0.000005587, Improvement: 0.000002438, Best Loss: 0.000000334 in Epoch 2623
Epoch 2781
Epoch 2781, Loss: 0.000009292, Improvement: 0.000003705, Best Loss: 0.000000334 in Epoch 2623
Epoch 2782
Epoch 2782, Loss: 0.000007531, Improvement: -0.000001761, Best Loss: 0.000000334 in Epoch 2623
Epoch 2783
Epoch 2783, Loss: 0.000004945, Improvement: -0.000002586, Best Loss: 0.000000334 in Epoch 2623
Epoch 2784
Epoch 2784, Loss: 0.000004074, Improvement: -0.000000871, Best Loss: 0.000000334 in Epoch 2623
Epoch 2785
Epoch 2785, Loss: 0.000003660, Improvement: -0.000000415, Best Loss: 0.000000334 in Epoch 2623
Epoch 2786
Epoch 2786, Loss: 0.000005375, Improvement: 0.000001715, Best Loss: 0.000000334 in Epoch 2623
Epoch 2787
Epoch 2787, Loss: 0.000007545, Improvement: 0.000002170, Best Loss: 0.000000334 in Epoch 2623
Epoch 2788
Epoch 2788, Loss: 0.000006079, Improvement: -0.000001466, Best Loss: 0.000000334 in Epoch 2623
Epoch 2789
Epoch 2789, Loss: 0.000010813, Improvement: 0.000004734, Best Loss: 0.000000334 in Epoch 2623
Epoch 2790
Epoch 2790, Loss: 0.000006714, Improvement: -0.000004100, Best Loss: 0.000000334 in Epoch 2623
Epoch 2791
Epoch 2791, Loss: 0.000004401, Improvement: -0.000002312, Best Loss: 0.000000334 in Epoch 2623
Epoch 2792
Epoch 2792, Loss: 0.000002684, Improvement: -0.000001718, Best Loss: 0.000000334 in Epoch 2623
Epoch 2793
Epoch 2793, Loss: 0.000001336, Improvement: -0.000001348, Best Loss: 0.000000334 in Epoch 2623
Epoch 2794
Epoch 2794, Loss: 0.000001016, Improvement: -0.000000320, Best Loss: 0.000000334 in Epoch 2623
Epoch 2795
Epoch 2795, Loss: 0.000000883, Improvement: -0.000000133, Best Loss: 0.000000334 in Epoch 2623
Epoch 2796
Epoch 2796, Loss: 0.000001072, Improvement: 0.000000189, Best Loss: 0.000000334 in Epoch 2623
Epoch 2797
Epoch 2797, Loss: 0.000001085, Improvement: 0.000000013, Best Loss: 0.000000334 in Epoch 2623
Epoch 2798
Epoch 2798, Loss: 0.000000993, Improvement: -0.000000092, Best Loss: 0.000000334 in Epoch 2623
Epoch 2799
Epoch 2799, Loss: 0.000001186, Improvement: 0.000000193, Best Loss: 0.000000334 in Epoch 2623
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.000001519, Improvement: 0.000000332, Best Loss: 0.000000334 in Epoch 2623
Epoch 2801
Epoch 2801, Loss: 0.000001294, Improvement: -0.000000224, Best Loss: 0.000000334 in Epoch 2623
Epoch 2802
Epoch 2802, Loss: 0.000001474, Improvement: 0.000000180, Best Loss: 0.000000334 in Epoch 2623
Epoch 2803
Epoch 2803, Loss: 0.000001603, Improvement: 0.000000128, Best Loss: 0.000000334 in Epoch 2623
Epoch 2804
Epoch 2804, Loss: 0.000001683, Improvement: 0.000000080, Best Loss: 0.000000334 in Epoch 2623
Epoch 2805
Epoch 2805, Loss: 0.000001439, Improvement: -0.000000244, Best Loss: 0.000000334 in Epoch 2623
Epoch 2806
Epoch 2806, Loss: 0.000001633, Improvement: 0.000000194, Best Loss: 0.000000334 in Epoch 2623
Epoch 2807
Epoch 2807, Loss: 0.000002306, Improvement: 0.000000674, Best Loss: 0.000000334 in Epoch 2623
Epoch 2808
Epoch 2808, Loss: 0.000002509, Improvement: 0.000000203, Best Loss: 0.000000334 in Epoch 2623
Epoch 2809
Epoch 2809, Loss: 0.000002105, Improvement: -0.000000404, Best Loss: 0.000000334 in Epoch 2623
Epoch 2810
Epoch 2810, Loss: 0.000003279, Improvement: 0.000001174, Best Loss: 0.000000334 in Epoch 2623
Epoch 2811
Epoch 2811, Loss: 0.000004790, Improvement: 0.000001511, Best Loss: 0.000000334 in Epoch 2623
Epoch 2812
Epoch 2812, Loss: 0.000008632, Improvement: 0.000003842, Best Loss: 0.000000334 in Epoch 2623
Epoch 2813
Epoch 2813, Loss: 0.000009988, Improvement: 0.000001357, Best Loss: 0.000000334 in Epoch 2623
Epoch 2814
Epoch 2814, Loss: 0.000011677, Improvement: 0.000001689, Best Loss: 0.000000334 in Epoch 2623
Epoch 2815
Epoch 2815, Loss: 0.000010137, Improvement: -0.000001541, Best Loss: 0.000000334 in Epoch 2623
Epoch 2816
Epoch 2816, Loss: 0.000005513, Improvement: -0.000004623, Best Loss: 0.000000334 in Epoch 2623
Epoch 2817
Epoch 2817, Loss: 0.000002231, Improvement: -0.000003283, Best Loss: 0.000000334 in Epoch 2623
Epoch 2818
Epoch 2818, Loss: 0.000001463, Improvement: -0.000000768, Best Loss: 0.000000334 in Epoch 2623
Epoch 2819
Epoch 2819, Loss: 0.000000900, Improvement: -0.000000563, Best Loss: 0.000000334 in Epoch 2623
Epoch 2820
Epoch 2820, Loss: 0.000000870, Improvement: -0.000000031, Best Loss: 0.000000334 in Epoch 2623
Epoch 2821
Epoch 2821, Loss: 0.000000973, Improvement: 0.000000103, Best Loss: 0.000000334 in Epoch 2623
Epoch 2822
Epoch 2822, Loss: 0.000000945, Improvement: -0.000000027, Best Loss: 0.000000334 in Epoch 2623
Epoch 2823
Epoch 2823, Loss: 0.000001267, Improvement: 0.000000321, Best Loss: 0.000000334 in Epoch 2623
Epoch 2824
Epoch 2824, Loss: 0.000001261, Improvement: -0.000000006, Best Loss: 0.000000334 in Epoch 2623
Epoch 2825
Epoch 2825, Loss: 0.000001111, Improvement: -0.000000150, Best Loss: 0.000000334 in Epoch 2623
Epoch 2826
Epoch 2826, Loss: 0.000000967, Improvement: -0.000000143, Best Loss: 0.000000334 in Epoch 2623
Epoch 2827
Epoch 2827, Loss: 0.000001018, Improvement: 0.000000050, Best Loss: 0.000000334 in Epoch 2623
Epoch 2828
Epoch 2828, Loss: 0.000003364, Improvement: 0.000002346, Best Loss: 0.000000334 in Epoch 2623
Epoch 2829
Epoch 2829, Loss: 0.000004084, Improvement: 0.000000720, Best Loss: 0.000000334 in Epoch 2623
Epoch 2830
Epoch 2830, Loss: 0.000003768, Improvement: -0.000000315, Best Loss: 0.000000334 in Epoch 2623
Epoch 2831
Epoch 2831, Loss: 0.000003577, Improvement: -0.000000192, Best Loss: 0.000000334 in Epoch 2623
Epoch 2832
Epoch 2832, Loss: 0.000005297, Improvement: 0.000001720, Best Loss: 0.000000334 in Epoch 2623
Epoch 2833
Epoch 2833, Loss: 0.000005973, Improvement: 0.000000676, Best Loss: 0.000000334 in Epoch 2623
Epoch 2834
Epoch 2834, Loss: 0.000006427, Improvement: 0.000000455, Best Loss: 0.000000334 in Epoch 2623
Epoch 2835
Epoch 2835, Loss: 0.000004260, Improvement: -0.000002168, Best Loss: 0.000000334 in Epoch 2623
Epoch 2836
Epoch 2836, Loss: 0.000002303, Improvement: -0.000001957, Best Loss: 0.000000334 in Epoch 2623
Epoch 2837
Epoch 2837, Loss: 0.000002120, Improvement: -0.000000182, Best Loss: 0.000000334 in Epoch 2623
Epoch 2838
Epoch 2838, Loss: 0.000001730, Improvement: -0.000000390, Best Loss: 0.000000334 in Epoch 2623
Epoch 2839
Epoch 2839, Loss: 0.000005413, Improvement: 0.000003684, Best Loss: 0.000000334 in Epoch 2623
Epoch 2840
Epoch 2840, Loss: 0.000008009, Improvement: 0.000002595, Best Loss: 0.000000334 in Epoch 2623
Epoch 2841
Epoch 2841, Loss: 0.000008263, Improvement: 0.000000255, Best Loss: 0.000000334 in Epoch 2623
Epoch 2842
Epoch 2842, Loss: 0.000004214, Improvement: -0.000004049, Best Loss: 0.000000334 in Epoch 2623
Epoch 2843
Epoch 2843, Loss: 0.000004084, Improvement: -0.000000130, Best Loss: 0.000000334 in Epoch 2623
Epoch 2844
Epoch 2844, Loss: 0.000005537, Improvement: 0.000001454, Best Loss: 0.000000334 in Epoch 2623
Epoch 2845
Epoch 2845, Loss: 0.000008521, Improvement: 0.000002984, Best Loss: 0.000000334 in Epoch 2623
Epoch 2846
Epoch 2846, Loss: 0.000007256, Improvement: -0.000001265, Best Loss: 0.000000334 in Epoch 2623
Epoch 2847
Epoch 2847, Loss: 0.000003517, Improvement: -0.000003739, Best Loss: 0.000000334 in Epoch 2623
Epoch 2848
Epoch 2848, Loss: 0.000003066, Improvement: -0.000000450, Best Loss: 0.000000334 in Epoch 2623
Epoch 2849
Epoch 2849, Loss: 0.000002685, Improvement: -0.000000381, Best Loss: 0.000000334 in Epoch 2623
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.000004110, Improvement: 0.000001425, Best Loss: 0.000000334 in Epoch 2623
Epoch 2851
Epoch 2851, Loss: 0.000005696, Improvement: 0.000001586, Best Loss: 0.000000334 in Epoch 2623
Epoch 2852
Epoch 2852, Loss: 0.000007998, Improvement: 0.000002303, Best Loss: 0.000000334 in Epoch 2623
Epoch 2853
Epoch 2853, Loss: 0.000006795, Improvement: -0.000001204, Best Loss: 0.000000334 in Epoch 2623
Epoch 2854
Epoch 2854, Loss: 0.000011487, Improvement: 0.000004692, Best Loss: 0.000000334 in Epoch 2623
Epoch 2855
Epoch 2855, Loss: 0.000006317, Improvement: -0.000005169, Best Loss: 0.000000334 in Epoch 2623
Epoch 2856
Epoch 2856, Loss: 0.000003319, Improvement: -0.000002998, Best Loss: 0.000000334 in Epoch 2623
Epoch 2857
Epoch 2857, Loss: 0.000002852, Improvement: -0.000000467, Best Loss: 0.000000334 in Epoch 2623
Epoch 2858
Epoch 2858, Loss: 0.000002542, Improvement: -0.000000310, Best Loss: 0.000000334 in Epoch 2623
Epoch 2859
Epoch 2859, Loss: 0.000002496, Improvement: -0.000000046, Best Loss: 0.000000334 in Epoch 2623
Epoch 2860
Epoch 2860, Loss: 0.000002105, Improvement: -0.000000391, Best Loss: 0.000000334 in Epoch 2623
Epoch 2861
Epoch 2861, Loss: 0.000001365, Improvement: -0.000000740, Best Loss: 0.000000334 in Epoch 2623
Epoch 2862
Epoch 2862, Loss: 0.000001474, Improvement: 0.000000109, Best Loss: 0.000000334 in Epoch 2623
Epoch 2863
Epoch 2863, Loss: 0.000001198, Improvement: -0.000000276, Best Loss: 0.000000334 in Epoch 2623
Epoch 2864
Epoch 2864, Loss: 0.000001222, Improvement: 0.000000024, Best Loss: 0.000000334 in Epoch 2623
Epoch 2865
Epoch 2865, Loss: 0.000002395, Improvement: 0.000001173, Best Loss: 0.000000334 in Epoch 2623
Epoch 2866
Epoch 2866, Loss: 0.000002722, Improvement: 0.000000327, Best Loss: 0.000000334 in Epoch 2623
Epoch 2867
Epoch 2867, Loss: 0.000006757, Improvement: 0.000004034, Best Loss: 0.000000334 in Epoch 2623
Epoch 2868
Epoch 2868, Loss: 0.000009883, Improvement: 0.000003126, Best Loss: 0.000000334 in Epoch 2623
Epoch 2869
Epoch 2869, Loss: 0.000009737, Improvement: -0.000000146, Best Loss: 0.000000334 in Epoch 2623
Epoch 2870
Epoch 2870, Loss: 0.000005655, Improvement: -0.000004081, Best Loss: 0.000000334 in Epoch 2623
Epoch 2871
Epoch 2871, Loss: 0.000003605, Improvement: -0.000002050, Best Loss: 0.000000334 in Epoch 2623
Epoch 2872
Epoch 2872, Loss: 0.000002233, Improvement: -0.000001372, Best Loss: 0.000000334 in Epoch 2623
Epoch 2873
Epoch 2873, Loss: 0.000002256, Improvement: 0.000000023, Best Loss: 0.000000334 in Epoch 2623
Epoch 2874
Epoch 2874, Loss: 0.000001937, Improvement: -0.000000319, Best Loss: 0.000000334 in Epoch 2623
Epoch 2875
Epoch 2875, Loss: 0.000001919, Improvement: -0.000000018, Best Loss: 0.000000334 in Epoch 2623
Epoch 2876
Epoch 2876, Loss: 0.000001453, Improvement: -0.000000466, Best Loss: 0.000000334 in Epoch 2623
Epoch 2877
Epoch 2877, Loss: 0.000001316, Improvement: -0.000000137, Best Loss: 0.000000334 in Epoch 2623
Epoch 2878
Epoch 2878, Loss: 0.000000933, Improvement: -0.000000383, Best Loss: 0.000000334 in Epoch 2623
Epoch 2879
Epoch 2879, Loss: 0.000000933, Improvement: 0.000000000, Best Loss: 0.000000334 in Epoch 2623
Epoch 2880
Epoch 2880, Loss: 0.000000839, Improvement: -0.000000094, Best Loss: 0.000000334 in Epoch 2623
Epoch 2881
Epoch 2881, Loss: 0.000000860, Improvement: 0.000000021, Best Loss: 0.000000334 in Epoch 2623
Epoch 2882
Epoch 2882, Loss: 0.000001252, Improvement: 0.000000391, Best Loss: 0.000000334 in Epoch 2623
Epoch 2883
Epoch 2883, Loss: 0.000001275, Improvement: 0.000000023, Best Loss: 0.000000334 in Epoch 2623
Epoch 2884
Epoch 2884, Loss: 0.000002886, Improvement: 0.000001611, Best Loss: 0.000000334 in Epoch 2623
Epoch 2885
Epoch 2885, Loss: 0.000005108, Improvement: 0.000002222, Best Loss: 0.000000334 in Epoch 2623
Epoch 2886
Epoch 2886, Loss: 0.000006532, Improvement: 0.000001424, Best Loss: 0.000000334 in Epoch 2623
Epoch 2887
Epoch 2887, Loss: 0.000007673, Improvement: 0.000001141, Best Loss: 0.000000334 in Epoch 2623
Epoch 2888
Epoch 2888, Loss: 0.000003838, Improvement: -0.000003834, Best Loss: 0.000000334 in Epoch 2623
Epoch 2889
Epoch 2889, Loss: 0.000003498, Improvement: -0.000000340, Best Loss: 0.000000334 in Epoch 2623
Epoch 2890
Epoch 2890, Loss: 0.000003536, Improvement: 0.000000038, Best Loss: 0.000000334 in Epoch 2623
Epoch 2891
Epoch 2891, Loss: 0.000003741, Improvement: 0.000000205, Best Loss: 0.000000334 in Epoch 2623
Epoch 2892
Epoch 2892, Loss: 0.000003054, Improvement: -0.000000687, Best Loss: 0.000000334 in Epoch 2623
Epoch 2893
Epoch 2893, Loss: 0.000005335, Improvement: 0.000002281, Best Loss: 0.000000334 in Epoch 2623
Epoch 2894
Epoch 2894, Loss: 0.000004484, Improvement: -0.000000851, Best Loss: 0.000000334 in Epoch 2623
Epoch 2895
Epoch 2895, Loss: 0.000003344, Improvement: -0.000001140, Best Loss: 0.000000334 in Epoch 2623
Epoch 2896
Epoch 2896, Loss: 0.000003848, Improvement: 0.000000504, Best Loss: 0.000000334 in Epoch 2623
Epoch 2897
Epoch 2897, Loss: 0.000004317, Improvement: 0.000000469, Best Loss: 0.000000334 in Epoch 2623
Epoch 2898
Epoch 2898, Loss: 0.000004509, Improvement: 0.000000192, Best Loss: 0.000000334 in Epoch 2623
Epoch 2899
Epoch 2899, Loss: 0.000004189, Improvement: -0.000000320, Best Loss: 0.000000334 in Epoch 2623
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.000004195, Improvement: 0.000000006, Best Loss: 0.000000334 in Epoch 2623
Epoch 2901
Epoch 2901, Loss: 0.000009528, Improvement: 0.000005333, Best Loss: 0.000000334 in Epoch 2623
Epoch 2902
Epoch 2902, Loss: 0.000006492, Improvement: -0.000003037, Best Loss: 0.000000334 in Epoch 2623
Epoch 2903
Epoch 2903, Loss: 0.000003804, Improvement: -0.000002688, Best Loss: 0.000000334 in Epoch 2623
Epoch 2904
Epoch 2904, Loss: 0.000002337, Improvement: -0.000001467, Best Loss: 0.000000334 in Epoch 2623
Epoch 2905
Epoch 2905, Loss: 0.000001969, Improvement: -0.000000368, Best Loss: 0.000000334 in Epoch 2623
Epoch 2906
Epoch 2906, Loss: 0.000002085, Improvement: 0.000000116, Best Loss: 0.000000334 in Epoch 2623
Epoch 2907
Epoch 2907, Loss: 0.000002061, Improvement: -0.000000024, Best Loss: 0.000000334 in Epoch 2623
Epoch 2908
Epoch 2908, Loss: 0.000001383, Improvement: -0.000000678, Best Loss: 0.000000334 in Epoch 2623
Epoch 2909
Epoch 2909, Loss: 0.000001277, Improvement: -0.000000106, Best Loss: 0.000000334 in Epoch 2623
Epoch 2910
Epoch 2910, Loss: 0.000003074, Improvement: 0.000001797, Best Loss: 0.000000334 in Epoch 2623
Epoch 2911
Epoch 2911, Loss: 0.000005380, Improvement: 0.000002306, Best Loss: 0.000000334 in Epoch 2623
Epoch 2912
Epoch 2912, Loss: 0.000012015, Improvement: 0.000006635, Best Loss: 0.000000334 in Epoch 2623
Epoch 2913
Epoch 2913, Loss: 0.000008103, Improvement: -0.000003911, Best Loss: 0.000000334 in Epoch 2623
Epoch 2914
Epoch 2914, Loss: 0.000006408, Improvement: -0.000001696, Best Loss: 0.000000334 in Epoch 2623
Epoch 2915
Epoch 2915, Loss: 0.000002902, Improvement: -0.000003506, Best Loss: 0.000000334 in Epoch 2623
Epoch 2916
Epoch 2916, Loss: 0.000001838, Improvement: -0.000001064, Best Loss: 0.000000334 in Epoch 2623
Epoch 2917
Epoch 2917, Loss: 0.000001286, Improvement: -0.000000552, Best Loss: 0.000000334 in Epoch 2623
Epoch 2918
Epoch 2918, Loss: 0.000001305, Improvement: 0.000000019, Best Loss: 0.000000334 in Epoch 2623
Epoch 2919
Epoch 2919, Loss: 0.000001098, Improvement: -0.000000206, Best Loss: 0.000000334 in Epoch 2623
Epoch 2920
Epoch 2920, Loss: 0.000001150, Improvement: 0.000000051, Best Loss: 0.000000334 in Epoch 2623
Epoch 2921
Epoch 2921, Loss: 0.000000772, Improvement: -0.000000378, Best Loss: 0.000000334 in Epoch 2623
Epoch 2922
Epoch 2922, Loss: 0.000000598, Improvement: -0.000000174, Best Loss: 0.000000334 in Epoch 2623
Epoch 2923
Epoch 2923, Loss: 0.000000742, Improvement: 0.000000144, Best Loss: 0.000000334 in Epoch 2623
Epoch 2924
Epoch 2924, Loss: 0.000000636, Improvement: -0.000000106, Best Loss: 0.000000334 in Epoch 2623
Epoch 2925
Epoch 2925, Loss: 0.000000664, Improvement: 0.000000028, Best Loss: 0.000000334 in Epoch 2623
Epoch 2926
Epoch 2926, Loss: 0.000000725, Improvement: 0.000000061, Best Loss: 0.000000334 in Epoch 2623
Epoch 2927
Epoch 2927, Loss: 0.000000628, Improvement: -0.000000097, Best Loss: 0.000000334 in Epoch 2623
Epoch 2928
Epoch 2928, Loss: 0.000000758, Improvement: 0.000000130, Best Loss: 0.000000334 in Epoch 2623
Epoch 2929
Epoch 2929, Loss: 0.000000739, Improvement: -0.000000019, Best Loss: 0.000000334 in Epoch 2623
Epoch 2930
Epoch 2930, Loss: 0.000000708, Improvement: -0.000000031, Best Loss: 0.000000334 in Epoch 2623
Epoch 2931
Epoch 2931, Loss: 0.000000748, Improvement: 0.000000040, Best Loss: 0.000000334 in Epoch 2623
Epoch 2932
Epoch 2932, Loss: 0.000000521, Improvement: -0.000000226, Best Loss: 0.000000334 in Epoch 2623
Epoch 2933
Epoch 2933, Loss: 0.000000593, Improvement: 0.000000072, Best Loss: 0.000000334 in Epoch 2623
Epoch 2934
Epoch 2934, Loss: 0.000000577, Improvement: -0.000000016, Best Loss: 0.000000334 in Epoch 2623
Epoch 2935
Epoch 2935, Loss: 0.000000501, Improvement: -0.000000076, Best Loss: 0.000000334 in Epoch 2623
Epoch 2936
Epoch 2936, Loss: 0.000000593, Improvement: 0.000000092, Best Loss: 0.000000334 in Epoch 2623
Epoch 2937
Epoch 2937, Loss: 0.000000623, Improvement: 0.000000030, Best Loss: 0.000000334 in Epoch 2623
Epoch 2938
Epoch 2938, Loss: 0.000000508, Improvement: -0.000000114, Best Loss: 0.000000334 in Epoch 2623
Epoch 2939
A best model at epoch 2939 has been saved with training error 0.000000322.
Epoch 2939, Loss: 0.000000863, Improvement: 0.000000355, Best Loss: 0.000000322 in Epoch 2939
Epoch 2940
Epoch 2940, Loss: 0.000002818, Improvement: 0.000001955, Best Loss: 0.000000322 in Epoch 2939
Epoch 2941
Epoch 2941, Loss: 0.000004061, Improvement: 0.000001244, Best Loss: 0.000000322 in Epoch 2939
Epoch 2942
Epoch 2942, Loss: 0.000004129, Improvement: 0.000000068, Best Loss: 0.000000322 in Epoch 2939
Epoch 2943
Epoch 2943, Loss: 0.000003623, Improvement: -0.000000506, Best Loss: 0.000000322 in Epoch 2939
Epoch 2944
Epoch 2944, Loss: 0.000005323, Improvement: 0.000001700, Best Loss: 0.000000322 in Epoch 2939
Epoch 2945
Epoch 2945, Loss: 0.000007254, Improvement: 0.000001931, Best Loss: 0.000000322 in Epoch 2939
Epoch 2946
Epoch 2946, Loss: 0.000005249, Improvement: -0.000002004, Best Loss: 0.000000322 in Epoch 2939
Epoch 2947
Epoch 2947, Loss: 0.000004807, Improvement: -0.000000442, Best Loss: 0.000000322 in Epoch 2939
Epoch 2948
Epoch 2948, Loss: 0.000006982, Improvement: 0.000002174, Best Loss: 0.000000322 in Epoch 2939
Epoch 2949
Epoch 2949, Loss: 0.000006037, Improvement: -0.000000944, Best Loss: 0.000000322 in Epoch 2939
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.000009864, Improvement: 0.000003826, Best Loss: 0.000000322 in Epoch 2939
Epoch 2951
Epoch 2951, Loss: 0.000008022, Improvement: -0.000001841, Best Loss: 0.000000322 in Epoch 2939
Epoch 2952
Epoch 2952, Loss: 0.000006962, Improvement: -0.000001061, Best Loss: 0.000000322 in Epoch 2939
Epoch 2953
Epoch 2953, Loss: 0.000005555, Improvement: -0.000001407, Best Loss: 0.000000322 in Epoch 2939
Epoch 2954
Epoch 2954, Loss: 0.000002751, Improvement: -0.000002804, Best Loss: 0.000000322 in Epoch 2939
Epoch 2955
Epoch 2955, Loss: 0.000002377, Improvement: -0.000000374, Best Loss: 0.000000322 in Epoch 2939
Epoch 2956
Epoch 2956, Loss: 0.000002336, Improvement: -0.000000041, Best Loss: 0.000000322 in Epoch 2939
Epoch 2957
Epoch 2957, Loss: 0.000002246, Improvement: -0.000000090, Best Loss: 0.000000322 in Epoch 2939
Epoch 2958
Epoch 2958, Loss: 0.000001257, Improvement: -0.000000990, Best Loss: 0.000000322 in Epoch 2939
Epoch 2959
Epoch 2959, Loss: 0.000000756, Improvement: -0.000000500, Best Loss: 0.000000322 in Epoch 2939
Epoch 2960
Epoch 2960, Loss: 0.000000909, Improvement: 0.000000152, Best Loss: 0.000000322 in Epoch 2939
Epoch 2961
Epoch 2961, Loss: 0.000000776, Improvement: -0.000000133, Best Loss: 0.000000322 in Epoch 2939
Epoch 2962
Epoch 2962, Loss: 0.000000655, Improvement: -0.000000120, Best Loss: 0.000000322 in Epoch 2939
Epoch 2963
Epoch 2963, Loss: 0.000000753, Improvement: 0.000000098, Best Loss: 0.000000322 in Epoch 2939
Epoch 2964
Epoch 2964, Loss: 0.000000711, Improvement: -0.000000042, Best Loss: 0.000000322 in Epoch 2939
Epoch 2965
Epoch 2965, Loss: 0.000000743, Improvement: 0.000000032, Best Loss: 0.000000322 in Epoch 2939
Epoch 2966
Epoch 2966, Loss: 0.000000638, Improvement: -0.000000106, Best Loss: 0.000000322 in Epoch 2939
Epoch 2967
Epoch 2967, Loss: 0.000000730, Improvement: 0.000000093, Best Loss: 0.000000322 in Epoch 2939
Epoch 2968
Epoch 2968, Loss: 0.000000673, Improvement: -0.000000057, Best Loss: 0.000000322 in Epoch 2939
Epoch 2969
Epoch 2969, Loss: 0.000001168, Improvement: 0.000000495, Best Loss: 0.000000322 in Epoch 2939
Epoch 2970
Epoch 2970, Loss: 0.000001810, Improvement: 0.000000643, Best Loss: 0.000000322 in Epoch 2939
Epoch 2971
Epoch 2971, Loss: 0.000003684, Improvement: 0.000001874, Best Loss: 0.000000322 in Epoch 2939
Epoch 2972
Epoch 2972, Loss: 0.000003354, Improvement: -0.000000330, Best Loss: 0.000000322 in Epoch 2939
Epoch 2973
Epoch 2973, Loss: 0.000002544, Improvement: -0.000000810, Best Loss: 0.000000322 in Epoch 2939
Epoch 2974
Epoch 2974, Loss: 0.000002615, Improvement: 0.000000071, Best Loss: 0.000000322 in Epoch 2939
Epoch 2975
Epoch 2975, Loss: 0.000005498, Improvement: 0.000002883, Best Loss: 0.000000322 in Epoch 2939
Epoch 2976
Epoch 2976, Loss: 0.000006453, Improvement: 0.000000955, Best Loss: 0.000000322 in Epoch 2939
Epoch 2977
Epoch 2977, Loss: 0.000008224, Improvement: 0.000001771, Best Loss: 0.000000322 in Epoch 2939
Epoch 2978
Epoch 2978, Loss: 0.000009684, Improvement: 0.000001460, Best Loss: 0.000000322 in Epoch 2939
Epoch 2979
Epoch 2979, Loss: 0.000004716, Improvement: -0.000004968, Best Loss: 0.000000322 in Epoch 2939
Epoch 2980
Epoch 2980, Loss: 0.000003507, Improvement: -0.000001209, Best Loss: 0.000000322 in Epoch 2939
Epoch 2981
Epoch 2981, Loss: 0.000003865, Improvement: 0.000000358, Best Loss: 0.000000322 in Epoch 2939
Epoch 2982
Epoch 2982, Loss: 0.000003317, Improvement: -0.000000548, Best Loss: 0.000000322 in Epoch 2939
Epoch 2983
Epoch 2983, Loss: 0.000003117, Improvement: -0.000000200, Best Loss: 0.000000322 in Epoch 2939
Epoch 2984
Epoch 2984, Loss: 0.000002139, Improvement: -0.000000978, Best Loss: 0.000000322 in Epoch 2939
Epoch 2985
Epoch 2985, Loss: 0.000001177, Improvement: -0.000000962, Best Loss: 0.000000322 in Epoch 2939
Epoch 2986
Epoch 2986, Loss: 0.000001379, Improvement: 0.000000202, Best Loss: 0.000000322 in Epoch 2939
Epoch 2987
Epoch 2987, Loss: 0.000002745, Improvement: 0.000001367, Best Loss: 0.000000322 in Epoch 2939
Epoch 2988
Epoch 2988, Loss: 0.000002022, Improvement: -0.000000724, Best Loss: 0.000000322 in Epoch 2939
Epoch 2989
Epoch 2989, Loss: 0.000001673, Improvement: -0.000000349, Best Loss: 0.000000322 in Epoch 2939
Epoch 2990
Epoch 2990, Loss: 0.000003357, Improvement: 0.000001684, Best Loss: 0.000000322 in Epoch 2939
Epoch 2991
Epoch 2991, Loss: 0.000005282, Improvement: 0.000001926, Best Loss: 0.000000322 in Epoch 2939
Epoch 2992
Epoch 2992, Loss: 0.000005329, Improvement: 0.000000046, Best Loss: 0.000000322 in Epoch 2939
Epoch 2993
Epoch 2993, Loss: 0.000003810, Improvement: -0.000001519, Best Loss: 0.000000322 in Epoch 2939
Epoch 2994
Epoch 2994, Loss: 0.000003240, Improvement: -0.000000570, Best Loss: 0.000000322 in Epoch 2939
Epoch 2995
Epoch 2995, Loss: 0.000002441, Improvement: -0.000000800, Best Loss: 0.000000322 in Epoch 2939
Epoch 2996
Epoch 2996, Loss: 0.000001865, Improvement: -0.000000576, Best Loss: 0.000000322 in Epoch 2939
Epoch 2997
Epoch 2997, Loss: 0.000002521, Improvement: 0.000000656, Best Loss: 0.000000322 in Epoch 2939
Epoch 2998
Epoch 2998, Loss: 0.000003525, Improvement: 0.000001004, Best Loss: 0.000000322 in Epoch 2939
Epoch 2999
Epoch 2999, Loss: 0.000003110, Improvement: -0.000000415, Best Loss: 0.000000322 in Epoch 2939
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.000001884, Improvement: -0.000001226, Best Loss: 0.000000322 in Epoch 2939
