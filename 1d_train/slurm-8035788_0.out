The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00966496951878.
A best model at epoch 1 has been saved with training error 0.00231234473176.
A best model at epoch 1 has been saved with training error 0.00201799953356.
A best model at epoch 1 has been saved with training error 0.00041171346675.
A best model at epoch 1 has been saved with training error 0.00006413424853.
Epoch 1, Loss: 0.00114760699507, Improvement: 0.00114760699507, Best Loss: 0.00006413424853 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.00005721079651.
A best model at epoch 2 has been saved with training error 0.00003931020183.
Epoch 2, Loss: 0.00008080945026, Improvement: -0.00106679754481, Best Loss: 0.00003931020183 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.00003765846850.
Epoch 3, Loss: 0.00005483421428, Improvement: -0.00002597523599, Best Loss: 0.00003765846850 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.00003452497913.
A best model at epoch 4 has been saved with training error 0.00003366750025.
Epoch 4, Loss: 0.00005238520189, Improvement: -0.00000244901239, Best Loss: 0.00003366750025 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.00003340562762.
Epoch 5, Loss: 0.00005168705848, Improvement: -0.00000069814341, Best Loss: 0.00003340562762 in Epoch 5
Epoch 6
A best model at epoch 6 has been saved with training error 0.00002942574611.
Epoch 6, Loss: 0.00005141987231, Improvement: -0.00000026718617, Best Loss: 0.00002942574611 in Epoch 6
Epoch 7
Epoch 7, Loss: 0.00005119548532, Improvement: -0.00000022438699, Best Loss: 0.00002942574611 in Epoch 6
Epoch 8
Epoch 8, Loss: 0.00005093689560, Improvement: -0.00000025858972, Best Loss: 0.00002942574611 in Epoch 6
Epoch 9
Epoch 9, Loss: 0.00005066727044, Improvement: -0.00000026962516, Best Loss: 0.00002942574611 in Epoch 6
Epoch 10
Epoch 10, Loss: 0.00005026058698, Improvement: -0.00000040668347, Best Loss: 0.00002942574611 in Epoch 6
Epoch 11
Epoch 11, Loss: 0.00004986727090, Improvement: -0.00000039331608, Best Loss: 0.00002942574611 in Epoch 6
Epoch 12
Epoch 12, Loss: 0.00004937607737, Improvement: -0.00000049119353, Best Loss: 0.00002942574611 in Epoch 6
Epoch 13
Epoch 13, Loss: 0.00004879145527, Improvement: -0.00000058462210, Best Loss: 0.00002942574611 in Epoch 6
Epoch 14
A best model at epoch 14 has been saved with training error 0.00002851322643.
Epoch 14, Loss: 0.00004807580699, Improvement: -0.00000071564828, Best Loss: 0.00002851322643 in Epoch 14
Epoch 15
A best model at epoch 15 has been saved with training error 0.00002749388477.
Epoch 15, Loss: 0.00004722421318, Improvement: -0.00000085159381, Best Loss: 0.00002749388477 in Epoch 15
Epoch 16
A best model at epoch 16 has been saved with training error 0.00001840092045.
Epoch 16, Loss: 0.00004616248243, Improvement: -0.00000106173075, Best Loss: 0.00001840092045 in Epoch 16
Epoch 17
Epoch 17, Loss: 0.00004488919167, Improvement: -0.00000127329076, Best Loss: 0.00001840092045 in Epoch 16
Epoch 18
Epoch 18, Loss: 0.00004341662443, Improvement: -0.00000147256724, Best Loss: 0.00001840092045 in Epoch 16
Epoch 19
Epoch 19, Loss: 0.00004172018489, Improvement: -0.00000169643954, Best Loss: 0.00001840092045 in Epoch 16
Epoch 20
Epoch 20, Loss: 0.00004003858021, Improvement: -0.00000168160468, Best Loss: 0.00001840092045 in Epoch 16
Epoch 21
Epoch 21, Loss: 0.00003862238154, Improvement: -0.00000141619867, Best Loss: 0.00001840092045 in Epoch 16
Epoch 22
Epoch 22, Loss: 0.00003765037191, Improvement: -0.00000097200964, Best Loss: 0.00001840092045 in Epoch 16
Epoch 23
Epoch 23, Loss: 0.00003715367784, Improvement: -0.00000049669407, Best Loss: 0.00001840092045 in Epoch 16
Epoch 24
Epoch 24, Loss: 0.00003699927338, Improvement: -0.00000015440446, Best Loss: 0.00001840092045 in Epoch 16
Epoch 25
Epoch 25, Loss: 0.00003695193554, Improvement: -0.00000004733783, Best Loss: 0.00001840092045 in Epoch 16
Epoch 26
Epoch 26, Loss: 0.00003693074304, Improvement: -0.00000002119250, Best Loss: 0.00001840092045 in Epoch 16
Epoch 27
Epoch 27, Loss: 0.00003691601914, Improvement: -0.00000001472390, Best Loss: 0.00001840092045 in Epoch 16
Epoch 28
Epoch 28, Loss: 0.00003690464500, Improvement: -0.00000001137414, Best Loss: 0.00001840092045 in Epoch 16
Epoch 29
Epoch 29, Loss: 0.00003689583828, Improvement: -0.00000000880673, Best Loss: 0.00001840092045 in Epoch 16
Epoch 30
Epoch 30, Loss: 0.00003688800725, Improvement: -0.00000000783102, Best Loss: 0.00001840092045 in Epoch 16
Epoch 31
Epoch 31, Loss: 0.00003688159550, Improvement: -0.00000000641176, Best Loss: 0.00001840092045 in Epoch 16
Epoch 32
Epoch 32, Loss: 0.00003687578037, Improvement: -0.00000000581513, Best Loss: 0.00001840092045 in Epoch 16
Epoch 33
Epoch 33, Loss: 0.00003687053013, Improvement: -0.00000000525024, Best Loss: 0.00001840092045 in Epoch 16
Epoch 34
Epoch 34, Loss: 0.00003686553928, Improvement: -0.00000000499085, Best Loss: 0.00001840092045 in Epoch 16
Epoch 35
Epoch 35, Loss: 0.00003686041146, Improvement: -0.00000000512782, Best Loss: 0.00001840092045 in Epoch 16
Epoch 36
Epoch 36, Loss: 0.00003685531110, Improvement: -0.00000000510036, Best Loss: 0.00001840092045 in Epoch 16
Epoch 37
Epoch 37, Loss: 0.00003684993644, Improvement: -0.00000000537466, Best Loss: 0.00001840092045 in Epoch 16
Epoch 38
Epoch 38, Loss: 0.00003684521625, Improvement: -0.00000000472019, Best Loss: 0.00001840092045 in Epoch 16
Epoch 39
Epoch 39, Loss: 0.00003683957229, Improvement: -0.00000000564396, Best Loss: 0.00001840092045 in Epoch 16
Epoch 40
Epoch 40, Loss: 0.00003683429177, Improvement: -0.00000000528053, Best Loss: 0.00001840092045 in Epoch 16
Epoch 41
Epoch 41, Loss: 0.00003682902170, Improvement: -0.00000000527007, Best Loss: 0.00001840092045 in Epoch 16
Epoch 42
Epoch 42, Loss: 0.00003682308125, Improvement: -0.00000000594046, Best Loss: 0.00001840092045 in Epoch 16
Epoch 43
Epoch 43, Loss: 0.00003681730514, Improvement: -0.00000000577611, Best Loss: 0.00001840092045 in Epoch 16
Epoch 44
Epoch 44, Loss: 0.00003681123762, Improvement: -0.00000000606751, Best Loss: 0.00001840092045 in Epoch 16
Epoch 45
Epoch 45, Loss: 0.00003680499703, Improvement: -0.00000000624059, Best Loss: 0.00001840092045 in Epoch 16
Epoch 46
Epoch 46, Loss: 0.00003679816409, Improvement: -0.00000000683294, Best Loss: 0.00001840092045 in Epoch 16
Epoch 47
Epoch 47, Loss: 0.00003679123365, Improvement: -0.00000000693044, Best Loss: 0.00001840092045 in Epoch 16
Epoch 48
Epoch 48, Loss: 0.00003678432022, Improvement: -0.00000000691343, Best Loss: 0.00001840092045 in Epoch 16
Epoch 49
Epoch 49, Loss: 0.00003677695422, Improvement: -0.00000000736600, Best Loss: 0.00001840092045 in Epoch 16
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00003676851084, Improvement: -0.00000000844339, Best Loss: 0.00001840092045 in Epoch 16
Epoch 51
Epoch 51, Loss: 0.00003676046581, Improvement: -0.00000000804503, Best Loss: 0.00001840092045 in Epoch 16
Epoch 52
Epoch 52, Loss: 0.00003675206690, Improvement: -0.00000000839891, Best Loss: 0.00001840092045 in Epoch 16
Epoch 53
Epoch 53, Loss: 0.00003674283389, Improvement: -0.00000000923301, Best Loss: 0.00001840092045 in Epoch 16
Epoch 54
Epoch 54, Loss: 0.00003673332822, Improvement: -0.00000000950567, Best Loss: 0.00001840092045 in Epoch 16
Epoch 55
Epoch 55, Loss: 0.00003672356170, Improvement: -0.00000000976652, Best Loss: 0.00001840092045 in Epoch 16
Epoch 56
Epoch 56, Loss: 0.00003671294662, Improvement: -0.00000001061508, Best Loss: 0.00001840092045 in Epoch 16
Epoch 57
Epoch 57, Loss: 0.00003670212545, Improvement: -0.00000001082117, Best Loss: 0.00001840092045 in Epoch 16
Epoch 58
Epoch 58, Loss: 0.00003669088883, Improvement: -0.00000001123663, Best Loss: 0.00001840092045 in Epoch 16
Epoch 59
Epoch 59, Loss: 0.00003667812780, Improvement: -0.00000001276103, Best Loss: 0.00001840092045 in Epoch 16
Epoch 60
Epoch 60, Loss: 0.00003666424036, Improvement: -0.00000001388744, Best Loss: 0.00001840092045 in Epoch 16
Epoch 61
Epoch 61, Loss: 0.00003665017011, Improvement: -0.00000001407025, Best Loss: 0.00001840092045 in Epoch 16
Epoch 62
Epoch 62, Loss: 0.00003663494344, Improvement: -0.00000001522667, Best Loss: 0.00001840092045 in Epoch 16
Epoch 63
Epoch 63, Loss: 0.00003661884020, Improvement: -0.00000001610324, Best Loss: 0.00001840092045 in Epoch 16
Epoch 64
Epoch 64, Loss: 0.00003660148168, Improvement: -0.00000001735852, Best Loss: 0.00001840092045 in Epoch 16
Epoch 65
Epoch 65, Loss: 0.00003658241858, Improvement: -0.00000001906310, Best Loss: 0.00001840092045 in Epoch 16
Epoch 66
Epoch 66, Loss: 0.00003656275285, Improvement: -0.00000001966573, Best Loss: 0.00001840092045 in Epoch 16
Epoch 67
Epoch 67, Loss: 0.00003653980257, Improvement: -0.00000002295028, Best Loss: 0.00001840092045 in Epoch 16
Epoch 68
Epoch 68, Loss: 0.00003651603447, Improvement: -0.00000002376810, Best Loss: 0.00001840092045 in Epoch 16
Epoch 69
Epoch 69, Loss: 0.00003648993143, Improvement: -0.00000002610304, Best Loss: 0.00001840092045 in Epoch 16
Epoch 70
Epoch 70, Loss: 0.00003646064251, Improvement: -0.00000002928891, Best Loss: 0.00001840092045 in Epoch 16
Epoch 71
Epoch 71, Loss: 0.00003642919955, Improvement: -0.00000003144296, Best Loss: 0.00001840092045 in Epoch 16
Epoch 72
Epoch 72, Loss: 0.00003639348888, Improvement: -0.00000003571067, Best Loss: 0.00001840092045 in Epoch 16
Epoch 73
Epoch 73, Loss: 0.00003635474168, Improvement: -0.00000003874720, Best Loss: 0.00001840092045 in Epoch 16
Epoch 74
A best model at epoch 74 has been saved with training error 0.00001650329068.
Epoch 74, Loss: 0.00003631025165, Improvement: -0.00000004449003, Best Loss: 0.00001650329068 in Epoch 74
Epoch 75
Epoch 75, Loss: 0.00003625812806, Improvement: -0.00000005212360, Best Loss: 0.00001650329068 in Epoch 74
Epoch 76
Epoch 76, Loss: 0.00003620270418, Improvement: -0.00000005542388, Best Loss: 0.00001650329068 in Epoch 74
Epoch 77
Epoch 77, Loss: 0.00003613620138, Improvement: -0.00000006650280, Best Loss: 0.00001650329068 in Epoch 74
Epoch 78
Epoch 78, Loss: 0.00003606036080, Improvement: -0.00000007584058, Best Loss: 0.00001650329068 in Epoch 74
Epoch 79
Epoch 79, Loss: 0.00003597339410, Improvement: -0.00000008696670, Best Loss: 0.00001650329068 in Epoch 74
Epoch 80
Epoch 80, Loss: 0.00003586610937, Improvement: -0.00000010728472, Best Loss: 0.00001650329068 in Epoch 74
Epoch 81
Epoch 81, Loss: 0.00003573769527, Improvement: -0.00000012841410, Best Loss: 0.00001650329068 in Epoch 74
Epoch 82
A best model at epoch 82 has been saved with training error 0.00001622206037.
Epoch 82, Loss: 0.00003558559865, Improvement: -0.00000015209662, Best Loss: 0.00001622206037 in Epoch 82
Epoch 83
Epoch 83, Loss: 0.00003539561230, Improvement: -0.00000018998635, Best Loss: 0.00001622206037 in Epoch 82
Epoch 84
Epoch 84, Loss: 0.00003515610897, Improvement: -0.00000023950333, Best Loss: 0.00001622206037 in Epoch 82
Epoch 85
Epoch 85, Loss: 0.00003484378603, Improvement: -0.00000031232294, Best Loss: 0.00001622206037 in Epoch 82
Epoch 86
Epoch 86, Loss: 0.00003442620782, Improvement: -0.00000041757821, Best Loss: 0.00001622206037 in Epoch 82
Epoch 87
Epoch 87, Loss: 0.00003388384193, Improvement: -0.00000054236589, Best Loss: 0.00001622206037 in Epoch 82
Epoch 88
Epoch 88, Loss: 0.00003308629184, Improvement: -0.00000079755009, Best Loss: 0.00001622206037 in Epoch 82
Epoch 89
Epoch 89, Loss: 0.00003196611578, Improvement: -0.00000112017606, Best Loss: 0.00001622206037 in Epoch 82
Epoch 90
A best model at epoch 90 has been saved with training error 0.00001621088086.
Epoch 90, Loss: 0.00003031197220, Improvement: -0.00000165414358, Best Loss: 0.00001621088086 in Epoch 90
Epoch 91
A best model at epoch 91 has been saved with training error 0.00001562806756.
A best model at epoch 91 has been saved with training error 0.00001391098340.
Epoch 91, Loss: 0.00002768157419, Improvement: -0.00000263039801, Best Loss: 0.00001391098340 in Epoch 91
Epoch 92
Epoch 92, Loss: 0.00002373626403, Improvement: -0.00000394531016, Best Loss: 0.00001391098340 in Epoch 91
Epoch 93
A best model at epoch 93 has been saved with training error 0.00001358724148.
A best model at epoch 93 has been saved with training error 0.00000906255082.
Epoch 93, Loss: 0.00001849752466, Improvement: -0.00000523873937, Best Loss: 0.00000906255082 in Epoch 93
Epoch 94
A best model at epoch 94 has been saved with training error 0.00000899353108.
A best model at epoch 94 has been saved with training error 0.00000737807204.
A best model at epoch 94 has been saved with training error 0.00000597881626.
Epoch 94, Loss: 0.00001383705085, Improvement: -0.00000466047381, Best Loss: 0.00000597881626 in Epoch 94
Epoch 95
Epoch 95, Loss: 0.00001269582801, Improvement: -0.00000114122283, Best Loss: 0.00000597881626 in Epoch 94
Epoch 96
Epoch 96, Loss: 0.00001247822483, Improvement: -0.00000021760318, Best Loss: 0.00000597881626 in Epoch 94
Epoch 97
Epoch 97, Loss: 0.00001237151073, Improvement: -0.00000010671411, Best Loss: 0.00000597881626 in Epoch 94
Epoch 98
Epoch 98, Loss: 0.00001219807186, Improvement: -0.00000017343887, Best Loss: 0.00000597881626 in Epoch 94
Epoch 99
Epoch 99, Loss: 0.00001212311408, Improvement: -0.00000007495778, Best Loss: 0.00000597881626 in Epoch 94
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00001207850651, Improvement: -0.00000004460758, Best Loss: 0.00000597881626 in Epoch 94
Epoch 101
Epoch 101, Loss: 0.00001199105711, Improvement: -0.00000008744939, Best Loss: 0.00000597881626 in Epoch 94
Epoch 102
Epoch 102, Loss: 0.00001193362395, Improvement: -0.00000005743316, Best Loss: 0.00000597881626 in Epoch 94
Epoch 103
Epoch 103, Loss: 0.00001188227982, Improvement: -0.00000005134414, Best Loss: 0.00000597881626 in Epoch 94
Epoch 104
Epoch 104, Loss: 0.00001188563294, Improvement: 0.00000000335313, Best Loss: 0.00000597881626 in Epoch 94
Epoch 105
Epoch 105, Loss: 0.00001190660121, Improvement: 0.00000002096826, Best Loss: 0.00000597881626 in Epoch 94
Epoch 106
Epoch 106, Loss: 0.00001177615090, Improvement: -0.00000013045030, Best Loss: 0.00000597881626 in Epoch 94
Epoch 107
A best model at epoch 107 has been saved with training error 0.00000585766429.
Epoch 107, Loss: 0.00001171353579, Improvement: -0.00000006261512, Best Loss: 0.00000585766429 in Epoch 107
Epoch 108
Epoch 108, Loss: 0.00001170134260, Improvement: -0.00000001219319, Best Loss: 0.00000585766429 in Epoch 107
Epoch 109
Epoch 109, Loss: 0.00001165518497, Improvement: -0.00000004615763, Best Loss: 0.00000585766429 in Epoch 107
Epoch 110
Epoch 110, Loss: 0.00001173682399, Improvement: 0.00000008163902, Best Loss: 0.00000585766429 in Epoch 107
Epoch 111
Epoch 111, Loss: 0.00001158095979, Improvement: -0.00000015586420, Best Loss: 0.00000585766429 in Epoch 107
Epoch 112
A best model at epoch 112 has been saved with training error 0.00000574987689.
Epoch 112, Loss: 0.00001156578942, Improvement: -0.00000001517037, Best Loss: 0.00000574987689 in Epoch 112
Epoch 113
A best model at epoch 113 has been saved with training error 0.00000466491156.
Epoch 113, Loss: 0.00001160910097, Improvement: 0.00000004331155, Best Loss: 0.00000466491156 in Epoch 113
Epoch 114
Epoch 114, Loss: 0.00001147060971, Improvement: -0.00000013849126, Best Loss: 0.00000466491156 in Epoch 113
Epoch 115
Epoch 115, Loss: 0.00001135459099, Improvement: -0.00000011601871, Best Loss: 0.00000466491156 in Epoch 113
Epoch 116
Epoch 116, Loss: 0.00001134723982, Improvement: -0.00000000735117, Best Loss: 0.00000466491156 in Epoch 113
Epoch 117
Epoch 117, Loss: 0.00001125597373, Improvement: -0.00000009126609, Best Loss: 0.00000466491156 in Epoch 113
Epoch 118
Epoch 118, Loss: 0.00001121128946, Improvement: -0.00000004468427, Best Loss: 0.00000466491156 in Epoch 113
Epoch 119
Epoch 119, Loss: 0.00001119204417, Improvement: -0.00000001924529, Best Loss: 0.00000466491156 in Epoch 113
Epoch 120
Epoch 120, Loss: 0.00001113904102, Improvement: -0.00000005300315, Best Loss: 0.00000466491156 in Epoch 113
Epoch 121
Epoch 121, Loss: 0.00001108801846, Improvement: -0.00000005102256, Best Loss: 0.00000466491156 in Epoch 113
Epoch 122
Epoch 122, Loss: 0.00001103083805, Improvement: -0.00000005718041, Best Loss: 0.00000466491156 in Epoch 113
Epoch 123
Epoch 123, Loss: 0.00001096632191, Improvement: -0.00000006451614, Best Loss: 0.00000466491156 in Epoch 113
Epoch 124
Epoch 124, Loss: 0.00001089562313, Improvement: -0.00000007069877, Best Loss: 0.00000466491156 in Epoch 113
Epoch 125
A best model at epoch 125 has been saved with training error 0.00000423951633.
Epoch 125, Loss: 0.00001083868556, Improvement: -0.00000005693757, Best Loss: 0.00000423951633 in Epoch 125
Epoch 126
Epoch 126, Loss: 0.00001079652654, Improvement: -0.00000004215901, Best Loss: 0.00000423951633 in Epoch 125
Epoch 127
Epoch 127, Loss: 0.00001070617739, Improvement: -0.00000009034916, Best Loss: 0.00000423951633 in Epoch 125
Epoch 128
A best model at epoch 128 has been saved with training error 0.00000414849865.
Epoch 128, Loss: 0.00001067216460, Improvement: -0.00000003401278, Best Loss: 0.00000414849865 in Epoch 128
Epoch 129
Epoch 129, Loss: 0.00001067223450, Improvement: 0.00000000006989, Best Loss: 0.00000414849865 in Epoch 128
Epoch 130
Epoch 130, Loss: 0.00001149912102, Improvement: 0.00000082688653, Best Loss: 0.00000414849865 in Epoch 128
Epoch 131
Epoch 131, Loss: 0.00001076885228, Improvement: -0.00000073026874, Best Loss: 0.00000414849865 in Epoch 128
Epoch 132
Epoch 132, Loss: 0.00001042258843, Improvement: -0.00000034626385, Best Loss: 0.00000414849865 in Epoch 128
Epoch 133
Epoch 133, Loss: 0.00001037648758, Improvement: -0.00000004610085, Best Loss: 0.00000414849865 in Epoch 128
Epoch 134
Epoch 134, Loss: 0.00001033189776, Improvement: -0.00000004458982, Best Loss: 0.00000414849865 in Epoch 128
Epoch 135
Epoch 135, Loss: 0.00001018268704, Improvement: -0.00000014921072, Best Loss: 0.00000414849865 in Epoch 128
Epoch 136
Epoch 136, Loss: 0.00001011140341, Improvement: -0.00000007128363, Best Loss: 0.00000414849865 in Epoch 128
Epoch 137
Epoch 137, Loss: 0.00000994824829, Improvement: -0.00000016315512, Best Loss: 0.00000414849865 in Epoch 128
Epoch 138
Epoch 138, Loss: 0.00000993939141, Improvement: -0.00000000885689, Best Loss: 0.00000414849865 in Epoch 128
Epoch 139
Epoch 139, Loss: 0.00001044185601, Improvement: 0.00000050246460, Best Loss: 0.00000414849865 in Epoch 128
Epoch 140
Epoch 140, Loss: 0.00000986108198, Improvement: -0.00000058077403, Best Loss: 0.00000414849865 in Epoch 128
Epoch 141
Epoch 141, Loss: 0.00000965391478, Improvement: -0.00000020716720, Best Loss: 0.00000414849865 in Epoch 128
Epoch 142
Epoch 142, Loss: 0.00000948431245, Improvement: -0.00000016960232, Best Loss: 0.00000414849865 in Epoch 128
Epoch 143
Epoch 143, Loss: 0.00000973711392, Improvement: 0.00000025280146, Best Loss: 0.00000414849865 in Epoch 128
Epoch 144
Epoch 144, Loss: 0.00000911580416, Improvement: -0.00000062130975, Best Loss: 0.00000414849865 in Epoch 128
Epoch 145
A best model at epoch 145 has been saved with training error 0.00000342499175.
Epoch 145, Loss: 0.00000846658246, Improvement: -0.00000064922170, Best Loss: 0.00000342499175 in Epoch 145
Epoch 146
A best model at epoch 146 has been saved with training error 0.00000341400914.
Epoch 146, Loss: 0.00000802105902, Improvement: -0.00000044552344, Best Loss: 0.00000341400914 in Epoch 146
Epoch 147
Epoch 147, Loss: 0.00000796603733, Improvement: -0.00000005502169, Best Loss: 0.00000341400914 in Epoch 146
Epoch 148
A best model at epoch 148 has been saved with training error 0.00000321267339.
Epoch 148, Loss: 0.00000678159259, Improvement: -0.00000118444474, Best Loss: 0.00000321267339 in Epoch 148
Epoch 149
A best model at epoch 149 has been saved with training error 0.00000301916339.
Epoch 149, Loss: 0.00000581951243, Improvement: -0.00000096208016, Best Loss: 0.00000301916339 in Epoch 149
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00000692781448, Improvement: 0.00000110830205, Best Loss: 0.00000301916339 in Epoch 149
Epoch 151
A best model at epoch 151 has been saved with training error 0.00000262895333.
Epoch 151, Loss: 0.00000802109562, Improvement: 0.00000109328114, Best Loss: 0.00000262895333 in Epoch 151
Epoch 152
A best model at epoch 152 has been saved with training error 0.00000209132418.
Epoch 152, Loss: 0.00000404511626, Improvement: -0.00000397597936, Best Loss: 0.00000209132418 in Epoch 152
Epoch 153
A best model at epoch 153 has been saved with training error 0.00000189546654.
Epoch 153, Loss: 0.00000321193096, Improvement: -0.00000083318530, Best Loss: 0.00000189546654 in Epoch 153
Epoch 154
A best model at epoch 154 has been saved with training error 0.00000170973215.
Epoch 154, Loss: 0.00000307621839, Improvement: -0.00000013571257, Best Loss: 0.00000170973215 in Epoch 154
Epoch 155
Epoch 155, Loss: 0.00000298978451, Improvement: -0.00000008643389, Best Loss: 0.00000170973215 in Epoch 154
Epoch 156
A best model at epoch 156 has been saved with training error 0.00000166745815.
A best model at epoch 156 has been saved with training error 0.00000165441975.
Epoch 156, Loss: 0.00000246559491, Improvement: -0.00000052418960, Best Loss: 0.00000165441975 in Epoch 156
Epoch 157
A best model at epoch 157 has been saved with training error 0.00000160369439.
A best model at epoch 157 has been saved with training error 0.00000143206569.
Epoch 157, Loss: 0.00000220662511, Improvement: -0.00000025896980, Best Loss: 0.00000143206569 in Epoch 157
Epoch 158
A best model at epoch 158 has been saved with training error 0.00000137182087.
Epoch 158, Loss: 0.00000194290360, Improvement: -0.00000026372151, Best Loss: 0.00000137182087 in Epoch 158
Epoch 159
A best model at epoch 159 has been saved with training error 0.00000123363247.
Epoch 159, Loss: 0.00000173634081, Improvement: -0.00000020656279, Best Loss: 0.00000123363247 in Epoch 159
Epoch 160
Epoch 160, Loss: 0.00000225779548, Improvement: 0.00000052145467, Best Loss: 0.00000123363247 in Epoch 159
Epoch 161
Epoch 161, Loss: 0.00000231501025, Improvement: 0.00000005721477, Best Loss: 0.00000123363247 in Epoch 159
Epoch 162
A best model at epoch 162 has been saved with training error 0.00000123206030.
A best model at epoch 162 has been saved with training error 0.00000109884400.
Epoch 162, Loss: 0.00000152107002, Improvement: -0.00000079394023, Best Loss: 0.00000109884400 in Epoch 162
Epoch 163
A best model at epoch 163 has been saved with training error 0.00000099946806.
Epoch 163, Loss: 0.00000157381121, Improvement: 0.00000005274119, Best Loss: 0.00000099946806 in Epoch 163
Epoch 164
Epoch 164, Loss: 0.00000168904297, Improvement: 0.00000011523176, Best Loss: 0.00000099946806 in Epoch 163
Epoch 165
Epoch 165, Loss: 0.00000215468538, Improvement: 0.00000046564241, Best Loss: 0.00000099946806 in Epoch 163
Epoch 166
Epoch 166, Loss: 0.00000294040214, Improvement: 0.00000078571676, Best Loss: 0.00000099946806 in Epoch 163
Epoch 167
Epoch 167, Loss: 0.00000214984012, Improvement: -0.00000079056202, Best Loss: 0.00000099946806 in Epoch 163
Epoch 168
Epoch 168, Loss: 0.00000194446005, Improvement: -0.00000020538006, Best Loss: 0.00000099946806 in Epoch 163
Epoch 169
Epoch 169, Loss: 0.00000148584435, Improvement: -0.00000045861570, Best Loss: 0.00000099946806 in Epoch 163
Epoch 170
A best model at epoch 170 has been saved with training error 0.00000082382093.
Epoch 170, Loss: 0.00000144739368, Improvement: -0.00000003845066, Best Loss: 0.00000082382093 in Epoch 170
Epoch 171
Epoch 171, Loss: 0.00000138954717, Improvement: -0.00000005784651, Best Loss: 0.00000082382093 in Epoch 170
Epoch 172
Epoch 172, Loss: 0.00000191989683, Improvement: 0.00000053034966, Best Loss: 0.00000082382093 in Epoch 170
Epoch 173
Epoch 173, Loss: 0.00000173229778, Improvement: -0.00000018759905, Best Loss: 0.00000082382093 in Epoch 170
Epoch 174
Epoch 174, Loss: 0.00000282621652, Improvement: 0.00000109391875, Best Loss: 0.00000082382093 in Epoch 170
Epoch 175
Epoch 175, Loss: 0.00000255480018, Improvement: -0.00000027141634, Best Loss: 0.00000082382093 in Epoch 170
Epoch 176
Epoch 176, Loss: 0.00000351479512, Improvement: 0.00000095999494, Best Loss: 0.00000082382093 in Epoch 170
Epoch 177
Epoch 177, Loss: 0.00000299229970, Improvement: -0.00000052249542, Best Loss: 0.00000082382093 in Epoch 170
Epoch 178
Epoch 178, Loss: 0.00000271894938, Improvement: -0.00000027335032, Best Loss: 0.00000082382093 in Epoch 170
Epoch 179
Epoch 179, Loss: 0.00000243197443, Improvement: -0.00000028697496, Best Loss: 0.00000082382093 in Epoch 170
Epoch 180
Epoch 180, Loss: 0.00000229927814, Improvement: -0.00000013269629, Best Loss: 0.00000082382093 in Epoch 170
Epoch 181
Epoch 181, Loss: 0.00000217062251, Improvement: -0.00000012865563, Best Loss: 0.00000082382093 in Epoch 170
Epoch 182
Epoch 182, Loss: 0.00000222056740, Improvement: 0.00000004994490, Best Loss: 0.00000082382093 in Epoch 170
Epoch 183
Epoch 183, Loss: 0.00000205064251, Improvement: -0.00000016992489, Best Loss: 0.00000082382093 in Epoch 170
Epoch 184
Epoch 184, Loss: 0.00000274182835, Improvement: 0.00000069118584, Best Loss: 0.00000082382093 in Epoch 170
Epoch 185
Epoch 185, Loss: 0.00000292391817, Improvement: 0.00000018208981, Best Loss: 0.00000082382093 in Epoch 170
Epoch 186
Epoch 186, Loss: 0.00000340039600, Improvement: 0.00000047647783, Best Loss: 0.00000082382093 in Epoch 170
Epoch 187
Epoch 187, Loss: 0.00000278158819, Improvement: -0.00000061880781, Best Loss: 0.00000082382093 in Epoch 170
Epoch 188
Epoch 188, Loss: 0.00000348055832, Improvement: 0.00000069897013, Best Loss: 0.00000082382093 in Epoch 170
Epoch 189
Epoch 189, Loss: 0.00000682633296, Improvement: 0.00000334577464, Best Loss: 0.00000082382093 in Epoch 170
Epoch 190
Epoch 190, Loss: 0.00000316285364, Improvement: -0.00000366347932, Best Loss: 0.00000082382093 in Epoch 170
Epoch 191
Epoch 191, Loss: 0.00000215642698, Improvement: -0.00000100642666, Best Loss: 0.00000082382093 in Epoch 170
Epoch 192
Epoch 192, Loss: 0.00000159950068, Improvement: -0.00000055692630, Best Loss: 0.00000082382093 in Epoch 170
Epoch 193
Epoch 193, Loss: 0.00000154644595, Improvement: -0.00000005305473, Best Loss: 0.00000082382093 in Epoch 170
Epoch 194
Epoch 194, Loss: 0.00000149676980, Improvement: -0.00000004967615, Best Loss: 0.00000082382093 in Epoch 170
Epoch 195
Epoch 195, Loss: 0.00000210596690, Improvement: 0.00000060919710, Best Loss: 0.00000082382093 in Epoch 170
Epoch 196
Epoch 196, Loss: 0.00000166578395, Improvement: -0.00000044018295, Best Loss: 0.00000082382093 in Epoch 170
Epoch 197
Epoch 197, Loss: 0.00000128991758, Improvement: -0.00000037586637, Best Loss: 0.00000082382093 in Epoch 170
Epoch 198
A best model at epoch 198 has been saved with training error 0.00000073851845.
Epoch 198, Loss: 0.00000117708322, Improvement: -0.00000011283436, Best Loss: 0.00000073851845 in Epoch 198
Epoch 199
Epoch 199, Loss: 0.00000102283168, Improvement: -0.00000015425154, Best Loss: 0.00000073851845 in Epoch 198
Epoch 200
A best model at epoch 200 has been saved with training error 0.00000068147159.
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00000097807276, Improvement: -0.00000004475892, Best Loss: 0.00000068147159 in Epoch 200
Epoch 201
Epoch 201, Loss: 0.00000097794607, Improvement: -0.00000000012669, Best Loss: 0.00000068147159 in Epoch 200
Epoch 202
Epoch 202, Loss: 0.00000098614804, Improvement: 0.00000000820197, Best Loss: 0.00000068147159 in Epoch 200
Epoch 203
Epoch 203, Loss: 0.00000108104886, Improvement: 0.00000009490081, Best Loss: 0.00000068147159 in Epoch 200
Epoch 204
A best model at epoch 204 has been saved with training error 0.00000067560410.
Epoch 204, Loss: 0.00000092204513, Improvement: -0.00000015900372, Best Loss: 0.00000067560410 in Epoch 204
Epoch 205
A best model at epoch 205 has been saved with training error 0.00000066549353.
Epoch 205, Loss: 0.00000092542521, Improvement: 0.00000000338007, Best Loss: 0.00000066549353 in Epoch 205
Epoch 206
Epoch 206, Loss: 0.00000097122419, Improvement: 0.00000004579898, Best Loss: 0.00000066549353 in Epoch 205
Epoch 207
Epoch 207, Loss: 0.00000123697498, Improvement: 0.00000026575079, Best Loss: 0.00000066549353 in Epoch 205
Epoch 208
Epoch 208, Loss: 0.00000115170836, Improvement: -0.00000008526662, Best Loss: 0.00000066549353 in Epoch 205
Epoch 209
Epoch 209, Loss: 0.00000193130604, Improvement: 0.00000077959768, Best Loss: 0.00000066549353 in Epoch 205
Epoch 210
Epoch 210, Loss: 0.00000195679112, Improvement: 0.00000002548508, Best Loss: 0.00000066549353 in Epoch 205
Epoch 211
Epoch 211, Loss: 0.00000135416845, Improvement: -0.00000060262267, Best Loss: 0.00000066549353 in Epoch 205
Epoch 212
Epoch 212, Loss: 0.00000154792478, Improvement: 0.00000019375634, Best Loss: 0.00000066549353 in Epoch 205
Epoch 213
Epoch 213, Loss: 0.00000133940227, Improvement: -0.00000020852251, Best Loss: 0.00000066549353 in Epoch 205
Epoch 214
Epoch 214, Loss: 0.00000127595002, Improvement: -0.00000006345226, Best Loss: 0.00000066549353 in Epoch 205
Epoch 215
Epoch 215, Loss: 0.00000179014929, Improvement: 0.00000051419928, Best Loss: 0.00000066549353 in Epoch 205
Epoch 216
Epoch 216, Loss: 0.00000886570115, Improvement: 0.00000707555185, Best Loss: 0.00000066549353 in Epoch 205
Epoch 217
Epoch 217, Loss: 0.00000894680438, Improvement: 0.00000008110324, Best Loss: 0.00000066549353 in Epoch 205
Epoch 218
Epoch 218, Loss: 0.00000616020701, Improvement: -0.00000278659737, Best Loss: 0.00000066549353 in Epoch 205
Epoch 219
Epoch 219, Loss: 0.00000217619230, Improvement: -0.00000398401471, Best Loss: 0.00000066549353 in Epoch 205
Epoch 220
Epoch 220, Loss: 0.00000140803044, Improvement: -0.00000076816186, Best Loss: 0.00000066549353 in Epoch 205
Epoch 221
Epoch 221, Loss: 0.00000110960660, Improvement: -0.00000029842384, Best Loss: 0.00000066549353 in Epoch 205
Epoch 222
Epoch 222, Loss: 0.00000103526577, Improvement: -0.00000007434083, Best Loss: 0.00000066549353 in Epoch 205
Epoch 223
Epoch 223, Loss: 0.00000097444470, Improvement: -0.00000006082107, Best Loss: 0.00000066549353 in Epoch 205
Epoch 224
A best model at epoch 224 has been saved with training error 0.00000060765825.
Epoch 224, Loss: 0.00000092753250, Improvement: -0.00000004691220, Best Loss: 0.00000060765825 in Epoch 224
Epoch 225
Epoch 225, Loss: 0.00000092249371, Improvement: -0.00000000503879, Best Loss: 0.00000060765825 in Epoch 224
Epoch 226
A best model at epoch 226 has been saved with training error 0.00000052145509.
Epoch 226, Loss: 0.00000089182153, Improvement: -0.00000003067217, Best Loss: 0.00000052145509 in Epoch 226
Epoch 227
Epoch 227, Loss: 0.00000087737091, Improvement: -0.00000001445062, Best Loss: 0.00000052145509 in Epoch 226
Epoch 228
Epoch 228, Loss: 0.00000086291275, Improvement: -0.00000001445816, Best Loss: 0.00000052145509 in Epoch 226
Epoch 229
Epoch 229, Loss: 0.00000087206516, Improvement: 0.00000000915240, Best Loss: 0.00000052145509 in Epoch 226
Epoch 230
Epoch 230, Loss: 0.00000085539694, Improvement: -0.00000001666822, Best Loss: 0.00000052145509 in Epoch 226
Epoch 231
Epoch 231, Loss: 0.00000086488320, Improvement: 0.00000000948626, Best Loss: 0.00000052145509 in Epoch 226
Epoch 232
Epoch 232, Loss: 0.00000086345847, Improvement: -0.00000000142473, Best Loss: 0.00000052145509 in Epoch 226
Epoch 233
Epoch 233, Loss: 0.00000081953984, Improvement: -0.00000004391864, Best Loss: 0.00000052145509 in Epoch 226
Epoch 234
Epoch 234, Loss: 0.00000079919676, Improvement: -0.00000002034308, Best Loss: 0.00000052145509 in Epoch 226
Epoch 235
Epoch 235, Loss: 0.00000077178379, Improvement: -0.00000002741297, Best Loss: 0.00000052145509 in Epoch 226
Epoch 236
A best model at epoch 236 has been saved with training error 0.00000048681693.
Epoch 236, Loss: 0.00000076076240, Improvement: -0.00000001102139, Best Loss: 0.00000048681693 in Epoch 236
Epoch 237
Epoch 237, Loss: 0.00000076884601, Improvement: 0.00000000808361, Best Loss: 0.00000048681693 in Epoch 236
Epoch 238
Epoch 238, Loss: 0.00000081707927, Improvement: 0.00000004823326, Best Loss: 0.00000048681693 in Epoch 236
Epoch 239
Epoch 239, Loss: 0.00000077332713, Improvement: -0.00000004375214, Best Loss: 0.00000048681693 in Epoch 236
Epoch 240
Epoch 240, Loss: 0.00000086574959, Improvement: 0.00000009242247, Best Loss: 0.00000048681693 in Epoch 236
Epoch 241
Epoch 241, Loss: 0.00000094331547, Improvement: 0.00000007756587, Best Loss: 0.00000048681693 in Epoch 236
Epoch 242
Epoch 242, Loss: 0.00000162858046, Improvement: 0.00000068526499, Best Loss: 0.00000048681693 in Epoch 236
Epoch 243
Epoch 243, Loss: 0.00000357393867, Improvement: 0.00000194535821, Best Loss: 0.00000048681693 in Epoch 236
Epoch 244
Epoch 244, Loss: 0.00000410690917, Improvement: 0.00000053297050, Best Loss: 0.00000048681693 in Epoch 236
Epoch 245
Epoch 245, Loss: 0.00000694406383, Improvement: 0.00000283715466, Best Loss: 0.00000048681693 in Epoch 236
Epoch 246
Epoch 246, Loss: 0.00000474262025, Improvement: -0.00000220144358, Best Loss: 0.00000048681693 in Epoch 236
Epoch 247
Epoch 247, Loss: 0.00000187993924, Improvement: -0.00000286268101, Best Loss: 0.00000048681693 in Epoch 236
Epoch 248
Epoch 248, Loss: 0.00000104418135, Improvement: -0.00000083575789, Best Loss: 0.00000048681693 in Epoch 236
Epoch 249
Epoch 249, Loss: 0.00000090841368, Improvement: -0.00000013576768, Best Loss: 0.00000048681693 in Epoch 236
Epoch 250
A best model at epoch 250 has been saved with training error 0.00000046254198.
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000082153343, Improvement: -0.00000008688025, Best Loss: 0.00000046254198 in Epoch 250
Epoch 251
Epoch 251, Loss: 0.00000078738215, Improvement: -0.00000003415127, Best Loss: 0.00000046254198 in Epoch 250
Epoch 252
Epoch 252, Loss: 0.00000077439283, Improvement: -0.00000001298932, Best Loss: 0.00000046254198 in Epoch 250
Epoch 253
Epoch 253, Loss: 0.00000076490699, Improvement: -0.00000000948584, Best Loss: 0.00000046254198 in Epoch 250
Epoch 254
Epoch 254, Loss: 0.00000077708918, Improvement: 0.00000001218219, Best Loss: 0.00000046254198 in Epoch 250
Epoch 255
Epoch 255, Loss: 0.00000073561416, Improvement: -0.00000004147502, Best Loss: 0.00000046254198 in Epoch 250
Epoch 256
Epoch 256, Loss: 0.00000070738553, Improvement: -0.00000002822863, Best Loss: 0.00000046254198 in Epoch 250
Epoch 257
Epoch 257, Loss: 0.00000071962600, Improvement: 0.00000001224047, Best Loss: 0.00000046254198 in Epoch 250
Epoch 258
Epoch 258, Loss: 0.00000071894341, Improvement: -0.00000000068259, Best Loss: 0.00000046254198 in Epoch 250
Epoch 259
Epoch 259, Loss: 0.00000067362750, Improvement: -0.00000004531591, Best Loss: 0.00000046254198 in Epoch 250
Epoch 260
Epoch 260, Loss: 0.00000066789061, Improvement: -0.00000000573690, Best Loss: 0.00000046254198 in Epoch 250
Epoch 261
Epoch 261, Loss: 0.00000066632947, Improvement: -0.00000000156114, Best Loss: 0.00000046254198 in Epoch 250
Epoch 262
Epoch 262, Loss: 0.00000065281282, Improvement: -0.00000001351665, Best Loss: 0.00000046254198 in Epoch 250
Epoch 263
Epoch 263, Loss: 0.00000066266149, Improvement: 0.00000000984867, Best Loss: 0.00000046254198 in Epoch 250
Epoch 264
A best model at epoch 264 has been saved with training error 0.00000038193738.
Epoch 264, Loss: 0.00000071727379, Improvement: 0.00000005461230, Best Loss: 0.00000038193738 in Epoch 264
Epoch 265
Epoch 265, Loss: 0.00000070600511, Improvement: -0.00000001126868, Best Loss: 0.00000038193738 in Epoch 264
Epoch 266
Epoch 266, Loss: 0.00000065800477, Improvement: -0.00000004800034, Best Loss: 0.00000038193738 in Epoch 264
Epoch 267
Epoch 267, Loss: 0.00000067410149, Improvement: 0.00000001609672, Best Loss: 0.00000038193738 in Epoch 264
Epoch 268
Epoch 268, Loss: 0.00000065002897, Improvement: -0.00000002407252, Best Loss: 0.00000038193738 in Epoch 264
Epoch 269
Epoch 269, Loss: 0.00000067026123, Improvement: 0.00000002023226, Best Loss: 0.00000038193738 in Epoch 264
Epoch 270
Epoch 270, Loss: 0.00000062314798, Improvement: -0.00000004711326, Best Loss: 0.00000038193738 in Epoch 264
Epoch 271
Epoch 271, Loss: 0.00000062635461, Improvement: 0.00000000320664, Best Loss: 0.00000038193738 in Epoch 264
Epoch 272
Epoch 272, Loss: 0.00000070611740, Improvement: 0.00000007976278, Best Loss: 0.00000038193738 in Epoch 264
Epoch 273
Epoch 273, Loss: 0.00000077583273, Improvement: 0.00000006971533, Best Loss: 0.00000038193738 in Epoch 264
Epoch 274
Epoch 274, Loss: 0.00000076307908, Improvement: -0.00000001275365, Best Loss: 0.00000038193738 in Epoch 264
Epoch 275
Epoch 275, Loss: 0.00000131994381, Improvement: 0.00000055686473, Best Loss: 0.00000038193738 in Epoch 264
Epoch 276
Epoch 276, Loss: 0.00000158217401, Improvement: 0.00000026223020, Best Loss: 0.00000038193738 in Epoch 264
Epoch 277
Epoch 277, Loss: 0.00000195084108, Improvement: 0.00000036866707, Best Loss: 0.00000038193738 in Epoch 264
Epoch 278
Epoch 278, Loss: 0.00000249021333, Improvement: 0.00000053937225, Best Loss: 0.00000038193738 in Epoch 264
Epoch 279
Epoch 279, Loss: 0.00000159271286, Improvement: -0.00000089750047, Best Loss: 0.00000038193738 in Epoch 264
Epoch 280
Epoch 280, Loss: 0.00000169980840, Improvement: 0.00000010709554, Best Loss: 0.00000038193738 in Epoch 264
Epoch 281
Epoch 281, Loss: 0.00000204817789, Improvement: 0.00000034836949, Best Loss: 0.00000038193738 in Epoch 264
Epoch 282
Epoch 282, Loss: 0.00000164322831, Improvement: -0.00000040494958, Best Loss: 0.00000038193738 in Epoch 264
Epoch 283
Epoch 283, Loss: 0.00000286383853, Improvement: 0.00000122061022, Best Loss: 0.00000038193738 in Epoch 264
Epoch 284
Epoch 284, Loss: 0.00000310641902, Improvement: 0.00000024258050, Best Loss: 0.00000038193738 in Epoch 264
Epoch 285
Epoch 285, Loss: 0.00000396363390, Improvement: 0.00000085721488, Best Loss: 0.00000038193738 in Epoch 264
Epoch 286
Epoch 286, Loss: 0.00000200843965, Improvement: -0.00000195519425, Best Loss: 0.00000038193738 in Epoch 264
Epoch 287
Epoch 287, Loss: 0.00000103860592, Improvement: -0.00000096983373, Best Loss: 0.00000038193738 in Epoch 264
Epoch 288
Epoch 288, Loss: 0.00000081156142, Improvement: -0.00000022704451, Best Loss: 0.00000038193738 in Epoch 264
Epoch 289
Epoch 289, Loss: 0.00000070934971, Improvement: -0.00000010221171, Best Loss: 0.00000038193738 in Epoch 264
Epoch 290
Epoch 290, Loss: 0.00000065926968, Improvement: -0.00000005008003, Best Loss: 0.00000038193738 in Epoch 264
Epoch 291
Epoch 291, Loss: 0.00000064681067, Improvement: -0.00000001245901, Best Loss: 0.00000038193738 in Epoch 264
Epoch 292
A best model at epoch 292 has been saved with training error 0.00000032472286.
Epoch 292, Loss: 0.00000064748415, Improvement: 0.00000000067348, Best Loss: 0.00000032472286 in Epoch 292
Epoch 293
Epoch 293, Loss: 0.00000064289914, Improvement: -0.00000000458501, Best Loss: 0.00000032472286 in Epoch 292
Epoch 294
Epoch 294, Loss: 0.00000065536107, Improvement: 0.00000001246194, Best Loss: 0.00000032472286 in Epoch 292
Epoch 295
Epoch 295, Loss: 0.00000064325321, Improvement: -0.00000001210787, Best Loss: 0.00000032472286 in Epoch 292
Epoch 296
Epoch 296, Loss: 0.00000061451736, Improvement: -0.00000002873585, Best Loss: 0.00000032472286 in Epoch 292
Epoch 297
Epoch 297, Loss: 0.00000061956969, Improvement: 0.00000000505233, Best Loss: 0.00000032472286 in Epoch 292
Epoch 298
Epoch 298, Loss: 0.00000066637716, Improvement: 0.00000004680747, Best Loss: 0.00000032472286 in Epoch 292
Epoch 299
Epoch 299, Loss: 0.00000064275947, Improvement: -0.00000002361769, Best Loss: 0.00000032472286 in Epoch 292
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000080816684, Improvement: 0.00000016540737, Best Loss: 0.00000032472286 in Epoch 292
Epoch 301
Epoch 301, Loss: 0.00000086881861, Improvement: 0.00000006065177, Best Loss: 0.00000032472286 in Epoch 292
Epoch 302
Epoch 302, Loss: 0.00000105120600, Improvement: 0.00000018238739, Best Loss: 0.00000032472286 in Epoch 292
Epoch 303
Epoch 303, Loss: 0.00000089549899, Improvement: -0.00000015570701, Best Loss: 0.00000032472286 in Epoch 292
Epoch 304
Epoch 304, Loss: 0.00000108446585, Improvement: 0.00000018896686, Best Loss: 0.00000032472286 in Epoch 292
Epoch 305
Epoch 305, Loss: 0.00000142829790, Improvement: 0.00000034383205, Best Loss: 0.00000032472286 in Epoch 292
Epoch 306
Epoch 306, Loss: 0.00000129717263, Improvement: -0.00000013112526, Best Loss: 0.00000032472286 in Epoch 292
Epoch 307
Epoch 307, Loss: 0.00000157380254, Improvement: 0.00000027662990, Best Loss: 0.00000032472286 in Epoch 292
Epoch 308
Epoch 308, Loss: 0.00000158800922, Improvement: 0.00000001420669, Best Loss: 0.00000032472286 in Epoch 292
Epoch 309
Epoch 309, Loss: 0.00000100292142, Improvement: -0.00000058508781, Best Loss: 0.00000032472286 in Epoch 292
Epoch 310
Epoch 310, Loss: 0.00000095529241, Improvement: -0.00000004762901, Best Loss: 0.00000032472286 in Epoch 292
Epoch 311
Epoch 311, Loss: 0.00000087597219, Improvement: -0.00000007932022, Best Loss: 0.00000032472286 in Epoch 292
Epoch 312
Epoch 312, Loss: 0.00000073061554, Improvement: -0.00000014535665, Best Loss: 0.00000032472286 in Epoch 292
Epoch 313
Epoch 313, Loss: 0.00000098821742, Improvement: 0.00000025760187, Best Loss: 0.00000032472286 in Epoch 292
Epoch 314
Epoch 314, Loss: 0.00000090179818, Improvement: -0.00000008641924, Best Loss: 0.00000032472286 in Epoch 292
Epoch 315
Epoch 315, Loss: 0.00000101621122, Improvement: 0.00000011441305, Best Loss: 0.00000032472286 in Epoch 292
Epoch 316
Epoch 316, Loss: 0.00000081712135, Improvement: -0.00000019908987, Best Loss: 0.00000032472286 in Epoch 292
Epoch 317
Epoch 317, Loss: 0.00000160718226, Improvement: 0.00000079006091, Best Loss: 0.00000032472286 in Epoch 292
Epoch 318
Epoch 318, Loss: 0.00000187468941, Improvement: 0.00000026750715, Best Loss: 0.00000032472286 in Epoch 292
Epoch 319
Epoch 319, Loss: 0.00000379215182, Improvement: 0.00000191746241, Best Loss: 0.00000032472286 in Epoch 292
Epoch 320
Epoch 320, Loss: 0.00000308563611, Improvement: -0.00000070651570, Best Loss: 0.00000032472286 in Epoch 292
Epoch 321
Epoch 321, Loss: 0.00000138850484, Improvement: -0.00000169713128, Best Loss: 0.00000032472286 in Epoch 292
Epoch 322
Epoch 322, Loss: 0.00000104731216, Improvement: -0.00000034119268, Best Loss: 0.00000032472286 in Epoch 292
Epoch 323
Epoch 323, Loss: 0.00000102044308, Improvement: -0.00000002686908, Best Loss: 0.00000032472286 in Epoch 292
Epoch 324
Epoch 324, Loss: 0.00000096057679, Improvement: -0.00000005986630, Best Loss: 0.00000032472286 in Epoch 292
Epoch 325
Epoch 325, Loss: 0.00000073620124, Improvement: -0.00000022437555, Best Loss: 0.00000032472286 in Epoch 292
Epoch 326
Epoch 326, Loss: 0.00000063852879, Improvement: -0.00000009767245, Best Loss: 0.00000032472286 in Epoch 292
Epoch 327
Epoch 327, Loss: 0.00000063788646, Improvement: -0.00000000064233, Best Loss: 0.00000032472286 in Epoch 292
Epoch 328
Epoch 328, Loss: 0.00000061773943, Improvement: -0.00000002014703, Best Loss: 0.00000032472286 in Epoch 292
Epoch 329
Epoch 329, Loss: 0.00000063305665, Improvement: 0.00000001531722, Best Loss: 0.00000032472286 in Epoch 292
Epoch 330
Epoch 330, Loss: 0.00000057512073, Improvement: -0.00000005793592, Best Loss: 0.00000032472286 in Epoch 292
Epoch 331
Epoch 331, Loss: 0.00000055911414, Improvement: -0.00000001600659, Best Loss: 0.00000032472286 in Epoch 292
Epoch 332
Epoch 332, Loss: 0.00000058048460, Improvement: 0.00000002137046, Best Loss: 0.00000032472286 in Epoch 292
Epoch 333
Epoch 333, Loss: 0.00000070582508, Improvement: 0.00000012534048, Best Loss: 0.00000032472286 in Epoch 292
Epoch 334
Epoch 334, Loss: 0.00000063532163, Improvement: -0.00000007050346, Best Loss: 0.00000032472286 in Epoch 292
Epoch 335
Epoch 335, Loss: 0.00000060742039, Improvement: -0.00000002790124, Best Loss: 0.00000032472286 in Epoch 292
Epoch 336
Epoch 336, Loss: 0.00000066518014, Improvement: 0.00000005775974, Best Loss: 0.00000032472286 in Epoch 292
Epoch 337
Epoch 337, Loss: 0.00000070350730, Improvement: 0.00000003832716, Best Loss: 0.00000032472286 in Epoch 292
Epoch 338
Epoch 338, Loss: 0.00000063467946, Improvement: -0.00000006882783, Best Loss: 0.00000032472286 in Epoch 292
Epoch 339
Epoch 339, Loss: 0.00000066895934, Improvement: 0.00000003427988, Best Loss: 0.00000032472286 in Epoch 292
Epoch 340
Epoch 340, Loss: 0.00000059758981, Improvement: -0.00000007136954, Best Loss: 0.00000032472286 in Epoch 292
Epoch 341
Epoch 341, Loss: 0.00000070137108, Improvement: 0.00000010378127, Best Loss: 0.00000032472286 in Epoch 292
Epoch 342
Epoch 342, Loss: 0.00000119242348, Improvement: 0.00000049105240, Best Loss: 0.00000032472286 in Epoch 292
Epoch 343
Epoch 343, Loss: 0.00000083929542, Improvement: -0.00000035312806, Best Loss: 0.00000032472286 in Epoch 292
Epoch 344
Epoch 344, Loss: 0.00000061604943, Improvement: -0.00000022324600, Best Loss: 0.00000032472286 in Epoch 292
Epoch 345
Epoch 345, Loss: 0.00000060526882, Improvement: -0.00000001078061, Best Loss: 0.00000032472286 in Epoch 292
Epoch 346
Epoch 346, Loss: 0.00000056265572, Improvement: -0.00000004261309, Best Loss: 0.00000032472286 in Epoch 292
Epoch 347
Epoch 347, Loss: 0.00000060114011, Improvement: 0.00000003848439, Best Loss: 0.00000032472286 in Epoch 292
Epoch 348
Epoch 348, Loss: 0.00000072610438, Improvement: 0.00000012496427, Best Loss: 0.00000032472286 in Epoch 292
Epoch 349
Epoch 349, Loss: 0.00000059235042, Improvement: -0.00000013375396, Best Loss: 0.00000032472286 in Epoch 292
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000054324455, Improvement: -0.00000004910587, Best Loss: 0.00000032472286 in Epoch 292
Epoch 351
Epoch 351, Loss: 0.00000071149020, Improvement: 0.00000016824565, Best Loss: 0.00000032472286 in Epoch 292
Epoch 352
Epoch 352, Loss: 0.00000124308777, Improvement: 0.00000053159757, Best Loss: 0.00000032472286 in Epoch 292
Epoch 353
Epoch 353, Loss: 0.00000078208942, Improvement: -0.00000046099835, Best Loss: 0.00000032472286 in Epoch 292
Epoch 354
Epoch 354, Loss: 0.00000103308871, Improvement: 0.00000025099930, Best Loss: 0.00000032472286 in Epoch 292
Epoch 355
Epoch 355, Loss: 0.00000126627169, Improvement: 0.00000023318297, Best Loss: 0.00000032472286 in Epoch 292
Epoch 356
Epoch 356, Loss: 0.00000148277704, Improvement: 0.00000021650535, Best Loss: 0.00000032472286 in Epoch 292
Epoch 357
Epoch 357, Loss: 0.00000095089689, Improvement: -0.00000053188014, Best Loss: 0.00000032472286 in Epoch 292
Epoch 358
Epoch 358, Loss: 0.00000064648740, Improvement: -0.00000030440950, Best Loss: 0.00000032472286 in Epoch 292
Epoch 359
Epoch 359, Loss: 0.00000058952494, Improvement: -0.00000005696245, Best Loss: 0.00000032472286 in Epoch 292
Epoch 360
Epoch 360, Loss: 0.00000084145931, Improvement: 0.00000025193436, Best Loss: 0.00000032472286 in Epoch 292
Epoch 361
Epoch 361, Loss: 0.00000069339999, Improvement: -0.00000014805931, Best Loss: 0.00000032472286 in Epoch 292
Epoch 362
Epoch 362, Loss: 0.00000060465959, Improvement: -0.00000008874040, Best Loss: 0.00000032472286 in Epoch 292
Epoch 363
Epoch 363, Loss: 0.00000054061766, Improvement: -0.00000006404193, Best Loss: 0.00000032472286 in Epoch 292
Epoch 364
Epoch 364, Loss: 0.00000050880026, Improvement: -0.00000003181740, Best Loss: 0.00000032472286 in Epoch 292
Epoch 365
Epoch 365, Loss: 0.00000050764278, Improvement: -0.00000000115747, Best Loss: 0.00000032472286 in Epoch 292
Epoch 366
Epoch 366, Loss: 0.00000048738541, Improvement: -0.00000002025737, Best Loss: 0.00000032472286 in Epoch 292
Epoch 367
Epoch 367, Loss: 0.00000053013522, Improvement: 0.00000004274981, Best Loss: 0.00000032472286 in Epoch 292
Epoch 368
Epoch 368, Loss: 0.00000053172319, Improvement: 0.00000000158797, Best Loss: 0.00000032472286 in Epoch 292
Epoch 369
Epoch 369, Loss: 0.00000050861439, Improvement: -0.00000002310881, Best Loss: 0.00000032472286 in Epoch 292
Epoch 370
Epoch 370, Loss: 0.00000053886246, Improvement: 0.00000003024807, Best Loss: 0.00000032472286 in Epoch 292
Epoch 371
Epoch 371, Loss: 0.00000056267424, Improvement: 0.00000002381179, Best Loss: 0.00000032472286 in Epoch 292
Epoch 372
Epoch 372, Loss: 0.00000077031234, Improvement: 0.00000020763810, Best Loss: 0.00000032472286 in Epoch 292
Epoch 373
Epoch 373, Loss: 0.00000118815303, Improvement: 0.00000041784069, Best Loss: 0.00000032472286 in Epoch 292
Epoch 374
Epoch 374, Loss: 0.00000109983807, Improvement: -0.00000008831495, Best Loss: 0.00000032472286 in Epoch 292
Epoch 375
Epoch 375, Loss: 0.00000136640903, Improvement: 0.00000026657096, Best Loss: 0.00000032472286 in Epoch 292
Epoch 376
Epoch 376, Loss: 0.00000099746998, Improvement: -0.00000036893905, Best Loss: 0.00000032472286 in Epoch 292
Epoch 377
Epoch 377, Loss: 0.00000060864121, Improvement: -0.00000038882877, Best Loss: 0.00000032472286 in Epoch 292
Epoch 378
Epoch 378, Loss: 0.00000052165191, Improvement: -0.00000008698931, Best Loss: 0.00000032472286 in Epoch 292
Epoch 379
A best model at epoch 379 has been saved with training error 0.00000032412245.
Epoch 379, Loss: 0.00000048015415, Improvement: -0.00000004149776, Best Loss: 0.00000032412245 in Epoch 379
Epoch 380
A best model at epoch 380 has been saved with training error 0.00000032209999.
Epoch 380, Loss: 0.00000048395632, Improvement: 0.00000000380217, Best Loss: 0.00000032209999 in Epoch 380
Epoch 381
Epoch 381, Loss: 0.00000050048364, Improvement: 0.00000001652733, Best Loss: 0.00000032209999 in Epoch 380
Epoch 382
Epoch 382, Loss: 0.00000049951935, Improvement: -0.00000000096429, Best Loss: 0.00000032209999 in Epoch 380
Epoch 383
Epoch 383, Loss: 0.00000054827414, Improvement: 0.00000004875479, Best Loss: 0.00000032209999 in Epoch 380
Epoch 384
Epoch 384, Loss: 0.00000050910099, Improvement: -0.00000003917316, Best Loss: 0.00000032209999 in Epoch 380
Epoch 385
Epoch 385, Loss: 0.00000048809627, Improvement: -0.00000002100471, Best Loss: 0.00000032209999 in Epoch 380
Epoch 386
Epoch 386, Loss: 0.00000051479268, Improvement: 0.00000002669640, Best Loss: 0.00000032209999 in Epoch 380
Epoch 387
Epoch 387, Loss: 0.00000069528690, Improvement: 0.00000018049422, Best Loss: 0.00000032209999 in Epoch 380
Epoch 388
Epoch 388, Loss: 0.00000124266648, Improvement: 0.00000054737958, Best Loss: 0.00000032209999 in Epoch 380
Epoch 389
Epoch 389, Loss: 0.00000121453638, Improvement: -0.00000002813010, Best Loss: 0.00000032209999 in Epoch 380
Epoch 390
Epoch 390, Loss: 0.00000277259739, Improvement: 0.00000155806101, Best Loss: 0.00000032209999 in Epoch 380
Epoch 391
Epoch 391, Loss: 0.00000275126315, Improvement: -0.00000002133424, Best Loss: 0.00000032209999 in Epoch 380
Epoch 392
Epoch 392, Loss: 0.00000108867472, Improvement: -0.00000166258843, Best Loss: 0.00000032209999 in Epoch 380
Epoch 393
Epoch 393, Loss: 0.00000065775706, Improvement: -0.00000043091766, Best Loss: 0.00000032209999 in Epoch 380
Epoch 394
Epoch 394, Loss: 0.00000054166475, Improvement: -0.00000011609232, Best Loss: 0.00000032209999 in Epoch 380
Epoch 395
Epoch 395, Loss: 0.00000050329037, Improvement: -0.00000003837437, Best Loss: 0.00000032209999 in Epoch 380
Epoch 396
Epoch 396, Loss: 0.00000046787088, Improvement: -0.00000003541949, Best Loss: 0.00000032209999 in Epoch 380
Epoch 397
A best model at epoch 397 has been saved with training error 0.00000031987275.
Epoch 397, Loss: 0.00000045782042, Improvement: -0.00000001005046, Best Loss: 0.00000031987275 in Epoch 397
Epoch 398
Epoch 398, Loss: 0.00000045764658, Improvement: -0.00000000017384, Best Loss: 0.00000031987275 in Epoch 397
Epoch 399
Epoch 399, Loss: 0.00000044752867, Improvement: -0.00000001011791, Best Loss: 0.00000031987275 in Epoch 397
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000044476747, Improvement: -0.00000000276120, Best Loss: 0.00000031987275 in Epoch 397
Epoch 401
Epoch 401, Loss: 0.00000046350037, Improvement: 0.00000001873290, Best Loss: 0.00000031987275 in Epoch 397
Epoch 402
Epoch 402, Loss: 0.00000047245323, Improvement: 0.00000000895286, Best Loss: 0.00000031987275 in Epoch 397
Epoch 403
Epoch 403, Loss: 0.00000045150166, Improvement: -0.00000002095156, Best Loss: 0.00000031987275 in Epoch 397
Epoch 404
Epoch 404, Loss: 0.00000047151563, Improvement: 0.00000002001397, Best Loss: 0.00000031987275 in Epoch 397
Epoch 405
A best model at epoch 405 has been saved with training error 0.00000031193531.
Epoch 405, Loss: 0.00000044349264, Improvement: -0.00000002802299, Best Loss: 0.00000031193531 in Epoch 405
Epoch 406
Epoch 406, Loss: 0.00000046418866, Improvement: 0.00000002069602, Best Loss: 0.00000031193531 in Epoch 405
Epoch 407
Epoch 407, Loss: 0.00000062230044, Improvement: 0.00000015811178, Best Loss: 0.00000031193531 in Epoch 405
Epoch 408
Epoch 408, Loss: 0.00000077148317, Improvement: 0.00000014918273, Best Loss: 0.00000031193531 in Epoch 405
Epoch 409
Epoch 409, Loss: 0.00000125374141, Improvement: 0.00000048225824, Best Loss: 0.00000031193531 in Epoch 405
Epoch 410
Epoch 410, Loss: 0.00000080214363, Improvement: -0.00000045159779, Best Loss: 0.00000031193531 in Epoch 405
Epoch 411
Epoch 411, Loss: 0.00000058747477, Improvement: -0.00000021466885, Best Loss: 0.00000031193531 in Epoch 405
Epoch 412
Epoch 412, Loss: 0.00000052050692, Improvement: -0.00000006696785, Best Loss: 0.00000031193531 in Epoch 405
Epoch 413
Epoch 413, Loss: 0.00000048370654, Improvement: -0.00000003680038, Best Loss: 0.00000031193531 in Epoch 405
Epoch 414
Epoch 414, Loss: 0.00000052101697, Improvement: 0.00000003731043, Best Loss: 0.00000031193531 in Epoch 405
Epoch 415
Epoch 415, Loss: 0.00000050526150, Improvement: -0.00000001575547, Best Loss: 0.00000031193531 in Epoch 405
Epoch 416
Epoch 416, Loss: 0.00000047335356, Improvement: -0.00000003190794, Best Loss: 0.00000031193531 in Epoch 405
Epoch 417
Epoch 417, Loss: 0.00000045782786, Improvement: -0.00000001552569, Best Loss: 0.00000031193531 in Epoch 405
Epoch 418
A best model at epoch 418 has been saved with training error 0.00000030800643.
Epoch 418, Loss: 0.00000047597462, Improvement: 0.00000001814676, Best Loss: 0.00000030800643 in Epoch 418
Epoch 419
Epoch 419, Loss: 0.00000046634588, Improvement: -0.00000000962874, Best Loss: 0.00000030800643 in Epoch 418
Epoch 420
Epoch 420, Loss: 0.00000043630821, Improvement: -0.00000003003767, Best Loss: 0.00000030800643 in Epoch 418
Epoch 421
Epoch 421, Loss: 0.00000046210890, Improvement: 0.00000002580069, Best Loss: 0.00000030800643 in Epoch 418
Epoch 422
Epoch 422, Loss: 0.00000046380301, Improvement: 0.00000000169411, Best Loss: 0.00000030800643 in Epoch 418
Epoch 423
Epoch 423, Loss: 0.00000051795334, Improvement: 0.00000005415033, Best Loss: 0.00000030800643 in Epoch 418
Epoch 424
Epoch 424, Loss: 0.00000095174350, Improvement: 0.00000043379016, Best Loss: 0.00000030800643 in Epoch 418
Epoch 425
Epoch 425, Loss: 0.00000083112723, Improvement: -0.00000012061627, Best Loss: 0.00000030800643 in Epoch 418
Epoch 426
Epoch 426, Loss: 0.00000079820923, Improvement: -0.00000003291799, Best Loss: 0.00000030800643 in Epoch 418
Epoch 427
Epoch 427, Loss: 0.00000065232520, Improvement: -0.00000014588404, Best Loss: 0.00000030800643 in Epoch 418
Epoch 428
Epoch 428, Loss: 0.00000063550226, Improvement: -0.00000001682293, Best Loss: 0.00000030800643 in Epoch 418
Epoch 429
Epoch 429, Loss: 0.00000050261006, Improvement: -0.00000013289220, Best Loss: 0.00000030800643 in Epoch 418
Epoch 430
Epoch 430, Loss: 0.00000044331009, Improvement: -0.00000005929997, Best Loss: 0.00000030800643 in Epoch 418
Epoch 431
Epoch 431, Loss: 0.00000046999301, Improvement: 0.00000002668292, Best Loss: 0.00000030800643 in Epoch 418
Epoch 432
Epoch 432, Loss: 0.00000048799101, Improvement: 0.00000001799800, Best Loss: 0.00000030800643 in Epoch 418
Epoch 433
Epoch 433, Loss: 0.00000048769011, Improvement: -0.00000000030090, Best Loss: 0.00000030800643 in Epoch 418
Epoch 434
A best model at epoch 434 has been saved with training error 0.00000029735074.
Epoch 434, Loss: 0.00000042975620, Improvement: -0.00000005793390, Best Loss: 0.00000029735074 in Epoch 434
Epoch 435
A best model at epoch 435 has been saved with training error 0.00000029264245.
A best model at epoch 435 has been saved with training error 0.00000027402902.
Epoch 435, Loss: 0.00000043790111, Improvement: 0.00000000814491, Best Loss: 0.00000027402902 in Epoch 435
Epoch 436
Epoch 436, Loss: 0.00000045604037, Improvement: 0.00000001813926, Best Loss: 0.00000027402902 in Epoch 435
Epoch 437
Epoch 437, Loss: 0.00000046601304, Improvement: 0.00000000997267, Best Loss: 0.00000027402902 in Epoch 435
Epoch 438
Epoch 438, Loss: 0.00000103446213, Improvement: 0.00000056844909, Best Loss: 0.00000027402902 in Epoch 435
Epoch 439
Epoch 439, Loss: 0.00000123267347, Improvement: 0.00000019821134, Best Loss: 0.00000027402902 in Epoch 435
Epoch 440
Epoch 440, Loss: 0.00000094860797, Improvement: -0.00000028406550, Best Loss: 0.00000027402902 in Epoch 435
Epoch 441
Epoch 441, Loss: 0.00000055325221, Improvement: -0.00000039535576, Best Loss: 0.00000027402902 in Epoch 435
Epoch 442
Epoch 442, Loss: 0.00000045135594, Improvement: -0.00000010189627, Best Loss: 0.00000027402902 in Epoch 435
Epoch 443
Epoch 443, Loss: 0.00000044087791, Improvement: -0.00000001047803, Best Loss: 0.00000027402902 in Epoch 435
Epoch 444
Epoch 444, Loss: 0.00000042043043, Improvement: -0.00000002044747, Best Loss: 0.00000027402902 in Epoch 435
Epoch 445
Epoch 445, Loss: 0.00000045211315, Improvement: 0.00000003168271, Best Loss: 0.00000027402902 in Epoch 435
Epoch 446
Epoch 446, Loss: 0.00000051048859, Improvement: 0.00000005837545, Best Loss: 0.00000027402902 in Epoch 435
Epoch 447
Epoch 447, Loss: 0.00000046407195, Improvement: -0.00000004641664, Best Loss: 0.00000027402902 in Epoch 435
Epoch 448
Epoch 448, Loss: 0.00000041387137, Improvement: -0.00000005020058, Best Loss: 0.00000027402902 in Epoch 435
Epoch 449
Epoch 449, Loss: 0.00000041749195, Improvement: 0.00000000362057, Best Loss: 0.00000027402902 in Epoch 435
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000040613442, Improvement: -0.00000001135753, Best Loss: 0.00000027402902 in Epoch 435
Epoch 451
Epoch 451, Loss: 0.00000041168362, Improvement: 0.00000000554920, Best Loss: 0.00000027402902 in Epoch 435
Epoch 452
Epoch 452, Loss: 0.00000041858750, Improvement: 0.00000000690388, Best Loss: 0.00000027402902 in Epoch 435
Epoch 453
Epoch 453, Loss: 0.00000046375743, Improvement: 0.00000004516993, Best Loss: 0.00000027402902 in Epoch 435
Epoch 454
Epoch 454, Loss: 0.00000054202481, Improvement: 0.00000007826738, Best Loss: 0.00000027402902 in Epoch 435
Epoch 455
Epoch 455, Loss: 0.00000048641981, Improvement: -0.00000005560500, Best Loss: 0.00000027402902 in Epoch 435
Epoch 456
Epoch 456, Loss: 0.00000091336111, Improvement: 0.00000042694130, Best Loss: 0.00000027402902 in Epoch 435
Epoch 457
Epoch 457, Loss: 0.00000071871443, Improvement: -0.00000019464669, Best Loss: 0.00000027402902 in Epoch 435
Epoch 458
Epoch 458, Loss: 0.00000058523176, Improvement: -0.00000013348267, Best Loss: 0.00000027402902 in Epoch 435
Epoch 459
Epoch 459, Loss: 0.00000079196160, Improvement: 0.00000020672984, Best Loss: 0.00000027402902 in Epoch 435
Epoch 460
Epoch 460, Loss: 0.00000072210611, Improvement: -0.00000006985550, Best Loss: 0.00000027402902 in Epoch 435
Epoch 461
Epoch 461, Loss: 0.00000055640569, Improvement: -0.00000016570041, Best Loss: 0.00000027402902 in Epoch 435
Epoch 462
Epoch 462, Loss: 0.00000045311405, Improvement: -0.00000010329164, Best Loss: 0.00000027402902 in Epoch 435
Epoch 463
A best model at epoch 463 has been saved with training error 0.00000026540047.
Epoch 463, Loss: 0.00000041121270, Improvement: -0.00000004190135, Best Loss: 0.00000026540047 in Epoch 463
Epoch 464
Epoch 464, Loss: 0.00000040080562, Improvement: -0.00000001040708, Best Loss: 0.00000026540047 in Epoch 463
Epoch 465
Epoch 465, Loss: 0.00000038239584, Improvement: -0.00000001840978, Best Loss: 0.00000026540047 in Epoch 463
Epoch 466
Epoch 466, Loss: 0.00000038363849, Improvement: 0.00000000124265, Best Loss: 0.00000026540047 in Epoch 463
Epoch 467
Epoch 467, Loss: 0.00000037678855, Improvement: -0.00000000684994, Best Loss: 0.00000026540047 in Epoch 463
Epoch 468
A best model at epoch 468 has been saved with training error 0.00000025569202.
Epoch 468, Loss: 0.00000039006804, Improvement: 0.00000001327949, Best Loss: 0.00000025569202 in Epoch 468
Epoch 469
Epoch 469, Loss: 0.00000038850615, Improvement: -0.00000000156189, Best Loss: 0.00000025569202 in Epoch 468
Epoch 470
Epoch 470, Loss: 0.00000039474495, Improvement: 0.00000000623880, Best Loss: 0.00000025569202 in Epoch 468
Epoch 471
Epoch 471, Loss: 0.00000039570246, Improvement: 0.00000000095751, Best Loss: 0.00000025569202 in Epoch 468
Epoch 472
Epoch 472, Loss: 0.00000044001381, Improvement: 0.00000004431135, Best Loss: 0.00000025569202 in Epoch 468
Epoch 473
Epoch 473, Loss: 0.00000046659671, Improvement: 0.00000002658290, Best Loss: 0.00000025569202 in Epoch 468
Epoch 474
Epoch 474, Loss: 0.00000059223678, Improvement: 0.00000012564007, Best Loss: 0.00000025569202 in Epoch 468
Epoch 475
Epoch 475, Loss: 0.00000054847098, Improvement: -0.00000004376580, Best Loss: 0.00000025569202 in Epoch 468
Epoch 476
Epoch 476, Loss: 0.00000070047067, Improvement: 0.00000015199969, Best Loss: 0.00000025569202 in Epoch 468
Epoch 477
Epoch 477, Loss: 0.00000081578528, Improvement: 0.00000011531461, Best Loss: 0.00000025569202 in Epoch 468
Epoch 478
Epoch 478, Loss: 0.00000069269291, Improvement: -0.00000012309237, Best Loss: 0.00000025569202 in Epoch 468
Epoch 479
Epoch 479, Loss: 0.00000056732582, Improvement: -0.00000012536709, Best Loss: 0.00000025569202 in Epoch 468
Epoch 480
Epoch 480, Loss: 0.00000050898513, Improvement: -0.00000005834070, Best Loss: 0.00000025569202 in Epoch 468
Epoch 481
Epoch 481, Loss: 0.00000063537538, Improvement: 0.00000012639025, Best Loss: 0.00000025569202 in Epoch 468
Epoch 482
Epoch 482, Loss: 0.00000064217557, Improvement: 0.00000000680020, Best Loss: 0.00000025569202 in Epoch 468
Epoch 483
Epoch 483, Loss: 0.00000098944734, Improvement: 0.00000034727177, Best Loss: 0.00000025569202 in Epoch 468
Epoch 484
Epoch 484, Loss: 0.00000069162483, Improvement: -0.00000029782251, Best Loss: 0.00000025569202 in Epoch 468
Epoch 485
Epoch 485, Loss: 0.00000047256719, Improvement: -0.00000021905764, Best Loss: 0.00000025569202 in Epoch 468
Epoch 486
A best model at epoch 486 has been saved with training error 0.00000024304347.
Epoch 486, Loss: 0.00000038550194, Improvement: -0.00000008706525, Best Loss: 0.00000024304347 in Epoch 486
Epoch 487
A best model at epoch 487 has been saved with training error 0.00000022783658.
Epoch 487, Loss: 0.00000033654367, Improvement: -0.00000004895827, Best Loss: 0.00000022783658 in Epoch 487
Epoch 488
Epoch 488, Loss: 0.00000034588195, Improvement: 0.00000000933829, Best Loss: 0.00000022783658 in Epoch 487
Epoch 489
A best model at epoch 489 has been saved with training error 0.00000022226796.
Epoch 489, Loss: 0.00000035949038, Improvement: 0.00000001360843, Best Loss: 0.00000022226796 in Epoch 489
Epoch 490
A best model at epoch 490 has been saved with training error 0.00000021821822.
Epoch 490, Loss: 0.00000036280568, Improvement: 0.00000000331530, Best Loss: 0.00000021821822 in Epoch 490
Epoch 491
Epoch 491, Loss: 0.00000038865393, Improvement: 0.00000002584825, Best Loss: 0.00000021821822 in Epoch 490
Epoch 492
Epoch 492, Loss: 0.00000045177537, Improvement: 0.00000006312144, Best Loss: 0.00000021821822 in Epoch 490
Epoch 493
Epoch 493, Loss: 0.00000045285332, Improvement: 0.00000000107795, Best Loss: 0.00000021821822 in Epoch 490
Epoch 494
Epoch 494, Loss: 0.00000047871988, Improvement: 0.00000002586656, Best Loss: 0.00000021821822 in Epoch 490
Epoch 495
Epoch 495, Loss: 0.00000057932844, Improvement: 0.00000010060856, Best Loss: 0.00000021821822 in Epoch 490
Epoch 496
Epoch 496, Loss: 0.00000059463930, Improvement: 0.00000001531087, Best Loss: 0.00000021821822 in Epoch 490
Epoch 497
Epoch 497, Loss: 0.00000043311278, Improvement: -0.00000016152653, Best Loss: 0.00000021821822 in Epoch 490
Epoch 498
Epoch 498, Loss: 0.00000036725738, Improvement: -0.00000006585539, Best Loss: 0.00000021821822 in Epoch 490
Epoch 499
Epoch 499, Loss: 0.00000038271637, Improvement: 0.00000001545899, Best Loss: 0.00000021821822 in Epoch 490
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000049290219, Improvement: 0.00000011018582, Best Loss: 0.00000021821822 in Epoch 490
Epoch 501
Epoch 501, Loss: 0.00000095032925, Improvement: 0.00000045742706, Best Loss: 0.00000021821822 in Epoch 490
Epoch 502
Epoch 502, Loss: 0.00000054734313, Improvement: -0.00000040298613, Best Loss: 0.00000021821822 in Epoch 490
Epoch 503
Epoch 503, Loss: 0.00000069342197, Improvement: 0.00000014607884, Best Loss: 0.00000021821822 in Epoch 490
Epoch 504
Epoch 504, Loss: 0.00000055864747, Improvement: -0.00000013477450, Best Loss: 0.00000021821822 in Epoch 490
Epoch 505
Epoch 505, Loss: 0.00000039206427, Improvement: -0.00000016658320, Best Loss: 0.00000021821822 in Epoch 490
Epoch 506
Epoch 506, Loss: 0.00000036594660, Improvement: -0.00000002611767, Best Loss: 0.00000021821822 in Epoch 490
Epoch 507
Epoch 507, Loss: 0.00000030587725, Improvement: -0.00000006006935, Best Loss: 0.00000021821822 in Epoch 490
Epoch 508
A best model at epoch 508 has been saved with training error 0.00000019174159.
Epoch 508, Loss: 0.00000028341438, Improvement: -0.00000002246287, Best Loss: 0.00000019174159 in Epoch 508
Epoch 509
Epoch 509, Loss: 0.00000028448147, Improvement: 0.00000000106709, Best Loss: 0.00000019174159 in Epoch 508
Epoch 510
Epoch 510, Loss: 0.00000026026381, Improvement: -0.00000002421766, Best Loss: 0.00000019174159 in Epoch 508
Epoch 511
Epoch 511, Loss: 0.00000025788137, Improvement: -0.00000000238244, Best Loss: 0.00000019174159 in Epoch 508
Epoch 512
A best model at epoch 512 has been saved with training error 0.00000018221088.
A best model at epoch 512 has been saved with training error 0.00000015510864.
Epoch 512, Loss: 0.00000026398292, Improvement: 0.00000000610155, Best Loss: 0.00000015510864 in Epoch 512
Epoch 513
Epoch 513, Loss: 0.00000027074520, Improvement: 0.00000000676228, Best Loss: 0.00000015510864 in Epoch 512
Epoch 514
Epoch 514, Loss: 0.00000035255716, Improvement: 0.00000008181197, Best Loss: 0.00000015510864 in Epoch 512
Epoch 515
Epoch 515, Loss: 0.00000036328433, Improvement: 0.00000001072716, Best Loss: 0.00000015510864 in Epoch 512
Epoch 516
Epoch 516, Loss: 0.00000079077265, Improvement: 0.00000042748832, Best Loss: 0.00000015510864 in Epoch 512
Epoch 517
Epoch 517, Loss: 0.00000055267426, Improvement: -0.00000023809839, Best Loss: 0.00000015510864 in Epoch 512
Epoch 518
Epoch 518, Loss: 0.00000033343514, Improvement: -0.00000021923912, Best Loss: 0.00000015510864 in Epoch 512
Epoch 519
Epoch 519, Loss: 0.00000027897875, Improvement: -0.00000005445639, Best Loss: 0.00000015510864 in Epoch 512
Epoch 520
Epoch 520, Loss: 0.00000032863994, Improvement: 0.00000004966119, Best Loss: 0.00000015510864 in Epoch 512
Epoch 521
Epoch 521, Loss: 0.00000030949254, Improvement: -0.00000001914740, Best Loss: 0.00000015510864 in Epoch 512
Epoch 522
A best model at epoch 522 has been saved with training error 0.00000015390830.
Epoch 522, Loss: 0.00000037458248, Improvement: 0.00000006508994, Best Loss: 0.00000015390830 in Epoch 522
Epoch 523
Epoch 523, Loss: 0.00000044999394, Improvement: 0.00000007541146, Best Loss: 0.00000015390830 in Epoch 522
Epoch 524
Epoch 524, Loss: 0.00000071091180, Improvement: 0.00000026091786, Best Loss: 0.00000015390830 in Epoch 522
Epoch 525
Epoch 525, Loss: 0.00000053297587, Improvement: -0.00000017793593, Best Loss: 0.00000015390830 in Epoch 522
Epoch 526
Epoch 526, Loss: 0.00000061394145, Improvement: 0.00000008096558, Best Loss: 0.00000015390830 in Epoch 522
Epoch 527
Epoch 527, Loss: 0.00000041958164, Improvement: -0.00000019435982, Best Loss: 0.00000015390830 in Epoch 522
Epoch 528
Epoch 528, Loss: 0.00000055217110, Improvement: 0.00000013258946, Best Loss: 0.00000015390830 in Epoch 522
Epoch 529
Epoch 529, Loss: 0.00000050900644, Improvement: -0.00000004316465, Best Loss: 0.00000015390830 in Epoch 522
Epoch 530
Epoch 530, Loss: 0.00000054236328, Improvement: 0.00000003335684, Best Loss: 0.00000015390830 in Epoch 522
Epoch 531
Epoch 531, Loss: 0.00000047033616, Improvement: -0.00000007202712, Best Loss: 0.00000015390830 in Epoch 522
Epoch 532
Epoch 532, Loss: 0.00000035990938, Improvement: -0.00000011042678, Best Loss: 0.00000015390830 in Epoch 522
Epoch 533
Epoch 533, Loss: 0.00000033000828, Improvement: -0.00000002990110, Best Loss: 0.00000015390830 in Epoch 522
Epoch 534
Epoch 534, Loss: 0.00000037514752, Improvement: 0.00000004513924, Best Loss: 0.00000015390830 in Epoch 522
Epoch 535
Epoch 535, Loss: 0.00000035312091, Improvement: -0.00000002202662, Best Loss: 0.00000015390830 in Epoch 522
Epoch 536
Epoch 536, Loss: 0.00000042940077, Improvement: 0.00000007627986, Best Loss: 0.00000015390830 in Epoch 522
Epoch 537
Epoch 537, Loss: 0.00000057708535, Improvement: 0.00000014768459, Best Loss: 0.00000015390830 in Epoch 522
Epoch 538
Epoch 538, Loss: 0.00000049119113, Improvement: -0.00000008589423, Best Loss: 0.00000015390830 in Epoch 522
Epoch 539
Epoch 539, Loss: 0.00000046659501, Improvement: -0.00000002459612, Best Loss: 0.00000015390830 in Epoch 522
Epoch 540
Epoch 540, Loss: 0.00000039577668, Improvement: -0.00000007081833, Best Loss: 0.00000015390830 in Epoch 522
Epoch 541
Epoch 541, Loss: 0.00000032734254, Improvement: -0.00000006843413, Best Loss: 0.00000015390830 in Epoch 522
Epoch 542
Epoch 542, Loss: 0.00000036574972, Improvement: 0.00000003840718, Best Loss: 0.00000015390830 in Epoch 522
Epoch 543
Epoch 543, Loss: 0.00000030990486, Improvement: -0.00000005584486, Best Loss: 0.00000015390830 in Epoch 522
Epoch 544
Epoch 544, Loss: 0.00000028597922, Improvement: -0.00000002392564, Best Loss: 0.00000015390830 in Epoch 522
Epoch 545
Epoch 545, Loss: 0.00000024723707, Improvement: -0.00000003874216, Best Loss: 0.00000015390830 in Epoch 522
Epoch 546
Epoch 546, Loss: 0.00000032111928, Improvement: 0.00000007388222, Best Loss: 0.00000015390830 in Epoch 522
Epoch 547
Epoch 547, Loss: 0.00000041091671, Improvement: 0.00000008979743, Best Loss: 0.00000015390830 in Epoch 522
Epoch 548
Epoch 548, Loss: 0.00000044802015, Improvement: 0.00000003710344, Best Loss: 0.00000015390830 in Epoch 522
Epoch 549
Epoch 549, Loss: 0.00000040435064, Improvement: -0.00000004366951, Best Loss: 0.00000015390830 in Epoch 522
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000038652521, Improvement: -0.00000001782544, Best Loss: 0.00000015390830 in Epoch 522
Epoch 551
Epoch 551, Loss: 0.00000038727974, Improvement: 0.00000000075453, Best Loss: 0.00000015390830 in Epoch 522
Epoch 552
Epoch 552, Loss: 0.00000103578030, Improvement: 0.00000064850057, Best Loss: 0.00000015390830 in Epoch 522
Epoch 553
Epoch 553, Loss: 0.00000065902298, Improvement: -0.00000037675732, Best Loss: 0.00000015390830 in Epoch 522
Epoch 554
Epoch 554, Loss: 0.00000047834959, Improvement: -0.00000018067340, Best Loss: 0.00000015390830 in Epoch 522
Epoch 555
Epoch 555, Loss: 0.00000029756673, Improvement: -0.00000018078286, Best Loss: 0.00000015390830 in Epoch 522
Epoch 556
Epoch 556, Loss: 0.00000024725240, Improvement: -0.00000005031433, Best Loss: 0.00000015390830 in Epoch 522
Epoch 557
Epoch 557, Loss: 0.00000023484606, Improvement: -0.00000001240634, Best Loss: 0.00000015390830 in Epoch 522
Epoch 558
A best model at epoch 558 has been saved with training error 0.00000014519280.
Epoch 558, Loss: 0.00000022918543, Improvement: -0.00000000566063, Best Loss: 0.00000014519280 in Epoch 558
Epoch 559
Epoch 559, Loss: 0.00000021916169, Improvement: -0.00000001002374, Best Loss: 0.00000014519280 in Epoch 558
Epoch 560
A best model at epoch 560 has been saved with training error 0.00000014486119.
Epoch 560, Loss: 0.00000021343939, Improvement: -0.00000000572230, Best Loss: 0.00000014486119 in Epoch 560
Epoch 561
Epoch 561, Loss: 0.00000020777327, Improvement: -0.00000000566612, Best Loss: 0.00000014486119 in Epoch 560
Epoch 562
Epoch 562, Loss: 0.00000021358587, Improvement: 0.00000000581259, Best Loss: 0.00000014486119 in Epoch 560
Epoch 563
Epoch 563, Loss: 0.00000020805161, Improvement: -0.00000000553425, Best Loss: 0.00000014486119 in Epoch 560
Epoch 564
Epoch 564, Loss: 0.00000020474482, Improvement: -0.00000000330679, Best Loss: 0.00000014486119 in Epoch 560
Epoch 565
Epoch 565, Loss: 0.00000020081698, Improvement: -0.00000000392784, Best Loss: 0.00000014486119 in Epoch 560
Epoch 566
Epoch 566, Loss: 0.00000021887970, Improvement: 0.00000001806272, Best Loss: 0.00000014486119 in Epoch 560
Epoch 567
Epoch 567, Loss: 0.00000023771024, Improvement: 0.00000001883054, Best Loss: 0.00000014486119 in Epoch 560
Epoch 568
Epoch 568, Loss: 0.00000038409491, Improvement: 0.00000014638467, Best Loss: 0.00000014486119 in Epoch 560
Epoch 569
Epoch 569, Loss: 0.00000025194394, Improvement: -0.00000013215098, Best Loss: 0.00000014486119 in Epoch 560
Epoch 570
Epoch 570, Loss: 0.00000026746857, Improvement: 0.00000001552464, Best Loss: 0.00000014486119 in Epoch 560
Epoch 571
Epoch 571, Loss: 0.00000030892958, Improvement: 0.00000004146101, Best Loss: 0.00000014486119 in Epoch 560
Epoch 572
Epoch 572, Loss: 0.00000039972890, Improvement: 0.00000009079932, Best Loss: 0.00000014486119 in Epoch 560
Epoch 573
Epoch 573, Loss: 0.00000094043198, Improvement: 0.00000054070308, Best Loss: 0.00000014486119 in Epoch 560
Epoch 574
Epoch 574, Loss: 0.00000057708442, Improvement: -0.00000036334757, Best Loss: 0.00000014486119 in Epoch 560
Epoch 575
Epoch 575, Loss: 0.00000044301334, Improvement: -0.00000013407108, Best Loss: 0.00000014486119 in Epoch 560
Epoch 576
Epoch 576, Loss: 0.00000024004590, Improvement: -0.00000020296744, Best Loss: 0.00000014486119 in Epoch 560
Epoch 577
Epoch 577, Loss: 0.00000021165523, Improvement: -0.00000002839067, Best Loss: 0.00000014486119 in Epoch 560
Epoch 578
Epoch 578, Loss: 0.00000022152159, Improvement: 0.00000000986636, Best Loss: 0.00000014486119 in Epoch 560
Epoch 579
A best model at epoch 579 has been saved with training error 0.00000013836804.
A best model at epoch 579 has been saved with training error 0.00000011137163.
Epoch 579, Loss: 0.00000020257728, Improvement: -0.00000001894431, Best Loss: 0.00000011137163 in Epoch 579
Epoch 580
Epoch 580, Loss: 0.00000018926469, Improvement: -0.00000001331258, Best Loss: 0.00000011137163 in Epoch 579
Epoch 581
Epoch 581, Loss: 0.00000018664840, Improvement: -0.00000000261629, Best Loss: 0.00000011137163 in Epoch 579
Epoch 582
Epoch 582, Loss: 0.00000018593472, Improvement: -0.00000000071368, Best Loss: 0.00000011137163 in Epoch 579
Epoch 583
Epoch 583, Loss: 0.00000020424414, Improvement: 0.00000001830943, Best Loss: 0.00000011137163 in Epoch 579
Epoch 584
Epoch 584, Loss: 0.00000025179255, Improvement: 0.00000004754841, Best Loss: 0.00000011137163 in Epoch 579
Epoch 585
Epoch 585, Loss: 0.00000031105339, Improvement: 0.00000005926084, Best Loss: 0.00000011137163 in Epoch 579
Epoch 586
Epoch 586, Loss: 0.00000024923346, Improvement: -0.00000006181993, Best Loss: 0.00000011137163 in Epoch 579
Epoch 587
Epoch 587, Loss: 0.00000026554007, Improvement: 0.00000001630661, Best Loss: 0.00000011137163 in Epoch 579
Epoch 588
Epoch 588, Loss: 0.00000031099011, Improvement: 0.00000004545004, Best Loss: 0.00000011137163 in Epoch 579
Epoch 589
Epoch 589, Loss: 0.00000028402527, Improvement: -0.00000002696483, Best Loss: 0.00000011137163 in Epoch 579
Epoch 590
Epoch 590, Loss: 0.00000030785013, Improvement: 0.00000002382485, Best Loss: 0.00000011137163 in Epoch 579
Epoch 591
Epoch 591, Loss: 0.00000031285869, Improvement: 0.00000000500857, Best Loss: 0.00000011137163 in Epoch 579
Epoch 592
Epoch 592, Loss: 0.00000034372095, Improvement: 0.00000003086226, Best Loss: 0.00000011137163 in Epoch 579
Epoch 593
Epoch 593, Loss: 0.00000028157329, Improvement: -0.00000006214766, Best Loss: 0.00000011137163 in Epoch 579
Epoch 594
Epoch 594, Loss: 0.00000020873419, Improvement: -0.00000007283910, Best Loss: 0.00000011137163 in Epoch 579
Epoch 595
Epoch 595, Loss: 0.00000024761469, Improvement: 0.00000003888050, Best Loss: 0.00000011137163 in Epoch 579
Epoch 596
Epoch 596, Loss: 0.00000054198955, Improvement: 0.00000029437486, Best Loss: 0.00000011137163 in Epoch 579
Epoch 597
Epoch 597, Loss: 0.00000072696658, Improvement: 0.00000018497703, Best Loss: 0.00000011137163 in Epoch 579
Epoch 598
Epoch 598, Loss: 0.00000091132759, Improvement: 0.00000018436101, Best Loss: 0.00000011137163 in Epoch 579
Epoch 599
Epoch 599, Loss: 0.00000061327086, Improvement: -0.00000029805672, Best Loss: 0.00000011137163 in Epoch 579
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000034229289, Improvement: -0.00000027097797, Best Loss: 0.00000011137163 in Epoch 579
Epoch 601
Epoch 601, Loss: 0.00000024755293, Improvement: -0.00000009473995, Best Loss: 0.00000011137163 in Epoch 579
Epoch 602
Epoch 602, Loss: 0.00000021393238, Improvement: -0.00000003362056, Best Loss: 0.00000011137163 in Epoch 579
Epoch 603
Epoch 603, Loss: 0.00000016988404, Improvement: -0.00000004404834, Best Loss: 0.00000011137163 in Epoch 579
Epoch 604
Epoch 604, Loss: 0.00000016321154, Improvement: -0.00000000667249, Best Loss: 0.00000011137163 in Epoch 579
Epoch 605
A best model at epoch 605 has been saved with training error 0.00000010946133.
Epoch 605, Loss: 0.00000015919779, Improvement: -0.00000000401376, Best Loss: 0.00000010946133 in Epoch 605
Epoch 606
Epoch 606, Loss: 0.00000016856788, Improvement: 0.00000000937010, Best Loss: 0.00000010946133 in Epoch 605
Epoch 607
Epoch 607, Loss: 0.00000018613390, Improvement: 0.00000001756602, Best Loss: 0.00000010946133 in Epoch 605
Epoch 608
Epoch 608, Loss: 0.00000017808398, Improvement: -0.00000000804992, Best Loss: 0.00000010946133 in Epoch 605
Epoch 609
Epoch 609, Loss: 0.00000020191161, Improvement: 0.00000002382763, Best Loss: 0.00000010946133 in Epoch 605
Epoch 610
Epoch 610, Loss: 0.00000019619643, Improvement: -0.00000000571518, Best Loss: 0.00000010946133 in Epoch 605
Epoch 611
Epoch 611, Loss: 0.00000019883300, Improvement: 0.00000000263657, Best Loss: 0.00000010946133 in Epoch 605
Epoch 612
Epoch 612, Loss: 0.00000019623123, Improvement: -0.00000000260177, Best Loss: 0.00000010946133 in Epoch 605
Epoch 613
Epoch 613, Loss: 0.00000016077909, Improvement: -0.00000003545215, Best Loss: 0.00000010946133 in Epoch 605
Epoch 614
Epoch 614, Loss: 0.00000017737030, Improvement: 0.00000001659122, Best Loss: 0.00000010946133 in Epoch 605
Epoch 615
A best model at epoch 615 has been saved with training error 0.00000009132009.
Epoch 615, Loss: 0.00000016478457, Improvement: -0.00000001258573, Best Loss: 0.00000009132009 in Epoch 615
Epoch 616
Epoch 616, Loss: 0.00000020134196, Improvement: 0.00000003655740, Best Loss: 0.00000009132009 in Epoch 615
Epoch 617
Epoch 617, Loss: 0.00000027011318, Improvement: 0.00000006877121, Best Loss: 0.00000009132009 in Epoch 615
Epoch 618
Epoch 618, Loss: 0.00000049925450, Improvement: 0.00000022914132, Best Loss: 0.00000009132009 in Epoch 615
Epoch 619
Epoch 619, Loss: 0.00000120021317, Improvement: 0.00000070095867, Best Loss: 0.00000009132009 in Epoch 615
Epoch 620
Epoch 620, Loss: 0.00000100955786, Improvement: -0.00000019065531, Best Loss: 0.00000009132009 in Epoch 615
Epoch 621
Epoch 621, Loss: 0.00000051390344, Improvement: -0.00000049565441, Best Loss: 0.00000009132009 in Epoch 615
Epoch 622
Epoch 622, Loss: 0.00000026311818, Improvement: -0.00000025078526, Best Loss: 0.00000009132009 in Epoch 615
Epoch 623
Epoch 623, Loss: 0.00000019676328, Improvement: -0.00000006635490, Best Loss: 0.00000009132009 in Epoch 615
Epoch 624
Epoch 624, Loss: 0.00000017820902, Improvement: -0.00000001855426, Best Loss: 0.00000009132009 in Epoch 615
Epoch 625
Epoch 625, Loss: 0.00000019696298, Improvement: 0.00000001875396, Best Loss: 0.00000009132009 in Epoch 615
Epoch 626
Epoch 626, Loss: 0.00000018967250, Improvement: -0.00000000729049, Best Loss: 0.00000009132009 in Epoch 615
Epoch 627
Epoch 627, Loss: 0.00000020779935, Improvement: 0.00000001812685, Best Loss: 0.00000009132009 in Epoch 615
Epoch 628
Epoch 628, Loss: 0.00000016516920, Improvement: -0.00000004263015, Best Loss: 0.00000009132009 in Epoch 615
Epoch 629
Epoch 629, Loss: 0.00000015538531, Improvement: -0.00000000978389, Best Loss: 0.00000009132009 in Epoch 615
Epoch 630
Epoch 630, Loss: 0.00000015525058, Improvement: -0.00000000013473, Best Loss: 0.00000009132009 in Epoch 615
Epoch 631
Epoch 631, Loss: 0.00000015880060, Improvement: 0.00000000355002, Best Loss: 0.00000009132009 in Epoch 615
Epoch 632
Epoch 632, Loss: 0.00000015897981, Improvement: 0.00000000017920, Best Loss: 0.00000009132009 in Epoch 615
Epoch 633
Epoch 633, Loss: 0.00000014165236, Improvement: -0.00000001732745, Best Loss: 0.00000009132009 in Epoch 615
Epoch 634
A best model at epoch 634 has been saved with training error 0.00000008211850.
Epoch 634, Loss: 0.00000013595638, Improvement: -0.00000000569598, Best Loss: 0.00000008211850 in Epoch 634
Epoch 635
Epoch 635, Loss: 0.00000013658424, Improvement: 0.00000000062786, Best Loss: 0.00000008211850 in Epoch 634
Epoch 636
Epoch 636, Loss: 0.00000014049123, Improvement: 0.00000000390698, Best Loss: 0.00000008211850 in Epoch 634
Epoch 637
Epoch 637, Loss: 0.00000015078114, Improvement: 0.00000001028992, Best Loss: 0.00000008211850 in Epoch 634
Epoch 638
Epoch 638, Loss: 0.00000014257670, Improvement: -0.00000000820444, Best Loss: 0.00000008211850 in Epoch 634
Epoch 639
Epoch 639, Loss: 0.00000016690389, Improvement: 0.00000002432719, Best Loss: 0.00000008211850 in Epoch 634
Epoch 640
Epoch 640, Loss: 0.00000015874186, Improvement: -0.00000000816203, Best Loss: 0.00000008211850 in Epoch 634
Epoch 641
Epoch 641, Loss: 0.00000014790314, Improvement: -0.00000001083872, Best Loss: 0.00000008211850 in Epoch 634
Epoch 642
Epoch 642, Loss: 0.00000015577696, Improvement: 0.00000000787382, Best Loss: 0.00000008211850 in Epoch 634
Epoch 643
Epoch 643, Loss: 0.00000015199535, Improvement: -0.00000000378161, Best Loss: 0.00000008211850 in Epoch 634
Epoch 644
Epoch 644, Loss: 0.00000018231040, Improvement: 0.00000003031506, Best Loss: 0.00000008211850 in Epoch 634
Epoch 645
Epoch 645, Loss: 0.00000013607020, Improvement: -0.00000004624020, Best Loss: 0.00000008211850 in Epoch 634
Epoch 646
Epoch 646, Loss: 0.00000015410973, Improvement: 0.00000001803952, Best Loss: 0.00000008211850 in Epoch 634
Epoch 647
Epoch 647, Loss: 0.00000017725784, Improvement: 0.00000002314811, Best Loss: 0.00000008211850 in Epoch 634
Epoch 648
Epoch 648, Loss: 0.00000014769751, Improvement: -0.00000002956033, Best Loss: 0.00000008211850 in Epoch 634
Epoch 649
Epoch 649, Loss: 0.00000016468758, Improvement: 0.00000001699007, Best Loss: 0.00000008211850 in Epoch 634
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000017918047, Improvement: 0.00000001449290, Best Loss: 0.00000008211850 in Epoch 634
Epoch 651
Epoch 651, Loss: 0.00000015336344, Improvement: -0.00000002581703, Best Loss: 0.00000008211850 in Epoch 634
Epoch 652
Epoch 652, Loss: 0.00000013433603, Improvement: -0.00000001902742, Best Loss: 0.00000008211850 in Epoch 634
Epoch 653
Epoch 653, Loss: 0.00000013818228, Improvement: 0.00000000384625, Best Loss: 0.00000008211850 in Epoch 634
Epoch 654
Epoch 654, Loss: 0.00000015872790, Improvement: 0.00000002054562, Best Loss: 0.00000008211850 in Epoch 634
Epoch 655
Epoch 655, Loss: 0.00000016863665, Improvement: 0.00000000990875, Best Loss: 0.00000008211850 in Epoch 634
Epoch 656
Epoch 656, Loss: 0.00000020936932, Improvement: 0.00000004073266, Best Loss: 0.00000008211850 in Epoch 634
Epoch 657
Epoch 657, Loss: 0.00000052449209, Improvement: 0.00000031512277, Best Loss: 0.00000008211850 in Epoch 634
Epoch 658
Epoch 658, Loss: 0.00000053435612, Improvement: 0.00000000986403, Best Loss: 0.00000008211850 in Epoch 634
Epoch 659
Epoch 659, Loss: 0.00000037810813, Improvement: -0.00000015624800, Best Loss: 0.00000008211850 in Epoch 634
Epoch 660
Epoch 660, Loss: 0.00000022254280, Improvement: -0.00000015556532, Best Loss: 0.00000008211850 in Epoch 634
Epoch 661
Epoch 661, Loss: 0.00000018034344, Improvement: -0.00000004219937, Best Loss: 0.00000008211850 in Epoch 634
Epoch 662
Epoch 662, Loss: 0.00000015371684, Improvement: -0.00000002662660, Best Loss: 0.00000008211850 in Epoch 634
Epoch 663
Epoch 663, Loss: 0.00000014703070, Improvement: -0.00000000668614, Best Loss: 0.00000008211850 in Epoch 634
Epoch 664
Epoch 664, Loss: 0.00000016084939, Improvement: 0.00000001381869, Best Loss: 0.00000008211850 in Epoch 634
Epoch 665
Epoch 665, Loss: 0.00000015330789, Improvement: -0.00000000754150, Best Loss: 0.00000008211850 in Epoch 634
Epoch 666
Epoch 666, Loss: 0.00000016540705, Improvement: 0.00000001209916, Best Loss: 0.00000008211850 in Epoch 634
Epoch 667
Epoch 667, Loss: 0.00000018752032, Improvement: 0.00000002211327, Best Loss: 0.00000008211850 in Epoch 634
Epoch 668
Epoch 668, Loss: 0.00000023842714, Improvement: 0.00000005090682, Best Loss: 0.00000008211850 in Epoch 634
Epoch 669
Epoch 669, Loss: 0.00000054624187, Improvement: 0.00000030781473, Best Loss: 0.00000008211850 in Epoch 634
Epoch 670
Epoch 670, Loss: 0.00000048224045, Improvement: -0.00000006400142, Best Loss: 0.00000008211850 in Epoch 634
Epoch 671
Epoch 671, Loss: 0.00000036512841, Improvement: -0.00000011711204, Best Loss: 0.00000008211850 in Epoch 634
Epoch 672
Epoch 672, Loss: 0.00000020565921, Improvement: -0.00000015946920, Best Loss: 0.00000008211850 in Epoch 634
Epoch 673
Epoch 673, Loss: 0.00000016579364, Improvement: -0.00000003986557, Best Loss: 0.00000008211850 in Epoch 634
Epoch 674
Epoch 674, Loss: 0.00000019064956, Improvement: 0.00000002485591, Best Loss: 0.00000008211850 in Epoch 634
Epoch 675
Epoch 675, Loss: 0.00000027271228, Improvement: 0.00000008206273, Best Loss: 0.00000008211850 in Epoch 634
Epoch 676
Epoch 676, Loss: 0.00000017916546, Improvement: -0.00000009354682, Best Loss: 0.00000008211850 in Epoch 634
Epoch 677
Epoch 677, Loss: 0.00000018545459, Improvement: 0.00000000628913, Best Loss: 0.00000008211850 in Epoch 634
Epoch 678
Epoch 678, Loss: 0.00000028263218, Improvement: 0.00000009717759, Best Loss: 0.00000008211850 in Epoch 634
Epoch 679
Epoch 679, Loss: 0.00000028956685, Improvement: 0.00000000693467, Best Loss: 0.00000008211850 in Epoch 634
Epoch 680
Epoch 680, Loss: 0.00000022841562, Improvement: -0.00000006115123, Best Loss: 0.00000008211850 in Epoch 634
Epoch 681
Epoch 681, Loss: 0.00000023336904, Improvement: 0.00000000495342, Best Loss: 0.00000008211850 in Epoch 634
Epoch 682
Epoch 682, Loss: 0.00000041233097, Improvement: 0.00000017896193, Best Loss: 0.00000008211850 in Epoch 634
Epoch 683
Epoch 683, Loss: 0.00000052306575, Improvement: 0.00000011073479, Best Loss: 0.00000008211850 in Epoch 634
Epoch 684
Epoch 684, Loss: 0.00000028680856, Improvement: -0.00000023625719, Best Loss: 0.00000008211850 in Epoch 634
Epoch 685
Epoch 685, Loss: 0.00000020650979, Improvement: -0.00000008029877, Best Loss: 0.00000008211850 in Epoch 634
Epoch 686
Epoch 686, Loss: 0.00000014933441, Improvement: -0.00000005717537, Best Loss: 0.00000008211850 in Epoch 634
Epoch 687
Epoch 687, Loss: 0.00000014078705, Improvement: -0.00000000854736, Best Loss: 0.00000008211850 in Epoch 634
Epoch 688
Epoch 688, Loss: 0.00000030978594, Improvement: 0.00000016899889, Best Loss: 0.00000008211850 in Epoch 634
Epoch 689
Epoch 689, Loss: 0.00000034009437, Improvement: 0.00000003030843, Best Loss: 0.00000008211850 in Epoch 634
Epoch 690
Epoch 690, Loss: 0.00000020108207, Improvement: -0.00000013901230, Best Loss: 0.00000008211850 in Epoch 634
Epoch 691
Epoch 691, Loss: 0.00000013327883, Improvement: -0.00000006780324, Best Loss: 0.00000008211850 in Epoch 634
Epoch 692
Epoch 692, Loss: 0.00000014136667, Improvement: 0.00000000808785, Best Loss: 0.00000008211850 in Epoch 634
Epoch 693
Epoch 693, Loss: 0.00000016033081, Improvement: 0.00000001896414, Best Loss: 0.00000008211850 in Epoch 634
Epoch 694
Epoch 694, Loss: 0.00000018078835, Improvement: 0.00000002045755, Best Loss: 0.00000008211850 in Epoch 634
Epoch 695
Epoch 695, Loss: 0.00000017287617, Improvement: -0.00000000791218, Best Loss: 0.00000008211850 in Epoch 634
Epoch 696
Epoch 696, Loss: 0.00000045444502, Improvement: 0.00000028156885, Best Loss: 0.00000008211850 in Epoch 634
Epoch 697
Epoch 697, Loss: 0.00000045350087, Improvement: -0.00000000094415, Best Loss: 0.00000008211850 in Epoch 634
Epoch 698
Epoch 698, Loss: 0.00000029597331, Improvement: -0.00000015752755, Best Loss: 0.00000008211850 in Epoch 634
Epoch 699
Epoch 699, Loss: 0.00000022226658, Improvement: -0.00000007370673, Best Loss: 0.00000008211850 in Epoch 634
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000020627041, Improvement: -0.00000001599617, Best Loss: 0.00000008211850 in Epoch 634
Epoch 701
Epoch 701, Loss: 0.00000021127955, Improvement: 0.00000000500914, Best Loss: 0.00000008211850 in Epoch 634
Epoch 702
Epoch 702, Loss: 0.00000015752926, Improvement: -0.00000005375029, Best Loss: 0.00000008211850 in Epoch 634
Epoch 703
Epoch 703, Loss: 0.00000013349667, Improvement: -0.00000002403259, Best Loss: 0.00000008211850 in Epoch 634
Epoch 704
Epoch 704, Loss: 0.00000012436184, Improvement: -0.00000000913483, Best Loss: 0.00000008211850 in Epoch 634
Epoch 705
Epoch 705, Loss: 0.00000012858966, Improvement: 0.00000000422782, Best Loss: 0.00000008211850 in Epoch 634
Epoch 706
Epoch 706, Loss: 0.00000017856502, Improvement: 0.00000004997536, Best Loss: 0.00000008211850 in Epoch 634
Epoch 707
Epoch 707, Loss: 0.00000014678932, Improvement: -0.00000003177571, Best Loss: 0.00000008211850 in Epoch 634
Epoch 708
Epoch 708, Loss: 0.00000019196592, Improvement: 0.00000004517661, Best Loss: 0.00000008211850 in Epoch 634
Epoch 709
Epoch 709, Loss: 0.00000023997671, Improvement: 0.00000004801078, Best Loss: 0.00000008211850 in Epoch 634
Epoch 710
Epoch 710, Loss: 0.00000039544815, Improvement: 0.00000015547144, Best Loss: 0.00000008211850 in Epoch 634
Epoch 711
Epoch 711, Loss: 0.00000036270584, Improvement: -0.00000003274231, Best Loss: 0.00000008211850 in Epoch 634
Epoch 712
Epoch 712, Loss: 0.00000019273059, Improvement: -0.00000016997525, Best Loss: 0.00000008211850 in Epoch 634
Epoch 713
Epoch 713, Loss: 0.00000015034731, Improvement: -0.00000004238328, Best Loss: 0.00000008211850 in Epoch 634
Epoch 714
Epoch 714, Loss: 0.00000020147725, Improvement: 0.00000005112994, Best Loss: 0.00000008211850 in Epoch 634
Epoch 715
Epoch 715, Loss: 0.00000025693863, Improvement: 0.00000005546138, Best Loss: 0.00000008211850 in Epoch 634
Epoch 716
Epoch 716, Loss: 0.00000045118266, Improvement: 0.00000019424403, Best Loss: 0.00000008211850 in Epoch 634
Epoch 717
Epoch 717, Loss: 0.00000048291582, Improvement: 0.00000003173316, Best Loss: 0.00000008211850 in Epoch 634
Epoch 718
Epoch 718, Loss: 0.00000024019819, Improvement: -0.00000024271763, Best Loss: 0.00000008211850 in Epoch 634
Epoch 719
Epoch 719, Loss: 0.00000021722388, Improvement: -0.00000002297431, Best Loss: 0.00000008211850 in Epoch 634
Epoch 720
Epoch 720, Loss: 0.00000016078527, Improvement: -0.00000005643861, Best Loss: 0.00000008211850 in Epoch 634
Epoch 721
Epoch 721, Loss: 0.00000013074922, Improvement: -0.00000003003605, Best Loss: 0.00000008211850 in Epoch 634
Epoch 722
Epoch 722, Loss: 0.00000013486348, Improvement: 0.00000000411426, Best Loss: 0.00000008211850 in Epoch 634
Epoch 723
Epoch 723, Loss: 0.00000021551938, Improvement: 0.00000008065590, Best Loss: 0.00000008211850 in Epoch 634
Epoch 724
Epoch 724, Loss: 0.00000030590333, Improvement: 0.00000009038395, Best Loss: 0.00000008211850 in Epoch 634
Epoch 725
Epoch 725, Loss: 0.00000023556723, Improvement: -0.00000007033611, Best Loss: 0.00000008211850 in Epoch 634
Epoch 726
Epoch 726, Loss: 0.00000018483869, Improvement: -0.00000005072853, Best Loss: 0.00000008211850 in Epoch 634
Epoch 727
Epoch 727, Loss: 0.00000017324942, Improvement: -0.00000001158928, Best Loss: 0.00000008211850 in Epoch 634
Epoch 728
Epoch 728, Loss: 0.00000021479209, Improvement: 0.00000004154267, Best Loss: 0.00000008211850 in Epoch 634
Epoch 729
Epoch 729, Loss: 0.00000016077874, Improvement: -0.00000005401335, Best Loss: 0.00000008211850 in Epoch 634
Epoch 730
Epoch 730, Loss: 0.00000013047448, Improvement: -0.00000003030426, Best Loss: 0.00000008211850 in Epoch 634
Epoch 731
Epoch 731, Loss: 0.00000012238600, Improvement: -0.00000000808848, Best Loss: 0.00000008211850 in Epoch 634
Epoch 732
Epoch 732, Loss: 0.00000014144368, Improvement: 0.00000001905768, Best Loss: 0.00000008211850 in Epoch 634
Epoch 733
Epoch 733, Loss: 0.00000018878267, Improvement: 0.00000004733899, Best Loss: 0.00000008211850 in Epoch 634
Epoch 734
Epoch 734, Loss: 0.00000018279843, Improvement: -0.00000000598424, Best Loss: 0.00000008211850 in Epoch 634
Epoch 735
Epoch 735, Loss: 0.00000014910485, Improvement: -0.00000003369358, Best Loss: 0.00000008211850 in Epoch 634
Epoch 736
Epoch 736, Loss: 0.00000012355847, Improvement: -0.00000002554637, Best Loss: 0.00000008211850 in Epoch 634
Epoch 737
Epoch 737, Loss: 0.00000016652821, Improvement: 0.00000004296974, Best Loss: 0.00000008211850 in Epoch 634
Epoch 738
Epoch 738, Loss: 0.00000026011709, Improvement: 0.00000009358888, Best Loss: 0.00000008211850 in Epoch 634
Epoch 739
Epoch 739, Loss: 0.00000040050400, Improvement: 0.00000014038691, Best Loss: 0.00000008211850 in Epoch 634
Epoch 740
Epoch 740, Loss: 0.00000039590177, Improvement: -0.00000000460223, Best Loss: 0.00000008211850 in Epoch 634
Epoch 741
Epoch 741, Loss: 0.00000031226846, Improvement: -0.00000008363331, Best Loss: 0.00000008211850 in Epoch 634
Epoch 742
Epoch 742, Loss: 0.00000028540057, Improvement: -0.00000002686789, Best Loss: 0.00000008211850 in Epoch 634
Epoch 743
Epoch 743, Loss: 0.00000031452744, Improvement: 0.00000002912686, Best Loss: 0.00000008211850 in Epoch 634
Epoch 744
Epoch 744, Loss: 0.00000026121910, Improvement: -0.00000005330833, Best Loss: 0.00000008211850 in Epoch 634
Epoch 745
Epoch 745, Loss: 0.00000015590616, Improvement: -0.00000010531294, Best Loss: 0.00000008211850 in Epoch 634
Epoch 746
Epoch 746, Loss: 0.00000013567303, Improvement: -0.00000002023313, Best Loss: 0.00000008211850 in Epoch 634
Epoch 747
Epoch 747, Loss: 0.00000011937029, Improvement: -0.00000001630274, Best Loss: 0.00000008211850 in Epoch 634
Epoch 748
Epoch 748, Loss: 0.00000013673396, Improvement: 0.00000001736367, Best Loss: 0.00000008211850 in Epoch 634
Epoch 749
A best model at epoch 749 has been saved with training error 0.00000007992265.
Epoch 749, Loss: 0.00000012161352, Improvement: -0.00000001512044, Best Loss: 0.00000007992265 in Epoch 749
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000013870775, Improvement: 0.00000001709423, Best Loss: 0.00000007992265 in Epoch 749
Epoch 751
Epoch 751, Loss: 0.00000018645025, Improvement: 0.00000004774250, Best Loss: 0.00000007992265 in Epoch 749
Epoch 752
Epoch 752, Loss: 0.00000017424411, Improvement: -0.00000001220614, Best Loss: 0.00000007992265 in Epoch 749
Epoch 753
Epoch 753, Loss: 0.00000020814077, Improvement: 0.00000003389666, Best Loss: 0.00000007992265 in Epoch 749
Epoch 754
Epoch 754, Loss: 0.00000021537827, Improvement: 0.00000000723750, Best Loss: 0.00000007992265 in Epoch 749
Epoch 755
Epoch 755, Loss: 0.00000021600468, Improvement: 0.00000000062641, Best Loss: 0.00000007992265 in Epoch 749
Epoch 756
Epoch 756, Loss: 0.00000018671980, Improvement: -0.00000002928488, Best Loss: 0.00000007992265 in Epoch 749
Epoch 757
Epoch 757, Loss: 0.00000013773744, Improvement: -0.00000004898236, Best Loss: 0.00000007992265 in Epoch 749
Epoch 758
Epoch 758, Loss: 0.00000017942360, Improvement: 0.00000004168615, Best Loss: 0.00000007992265 in Epoch 749
Epoch 759
Epoch 759, Loss: 0.00000025156400, Improvement: 0.00000007214040, Best Loss: 0.00000007992265 in Epoch 749
Epoch 760
Epoch 760, Loss: 0.00000021066073, Improvement: -0.00000004090327, Best Loss: 0.00000007992265 in Epoch 749
Epoch 761
Epoch 761, Loss: 0.00000017597981, Improvement: -0.00000003468092, Best Loss: 0.00000007992265 in Epoch 749
Epoch 762
Epoch 762, Loss: 0.00000020290687, Improvement: 0.00000002692706, Best Loss: 0.00000007992265 in Epoch 749
Epoch 763
Epoch 763, Loss: 0.00000050925234, Improvement: 0.00000030634546, Best Loss: 0.00000007992265 in Epoch 749
Epoch 764
Epoch 764, Loss: 0.00000028139701, Improvement: -0.00000022785532, Best Loss: 0.00000007992265 in Epoch 749
Epoch 765
Epoch 765, Loss: 0.00000017351657, Improvement: -0.00000010788044, Best Loss: 0.00000007992265 in Epoch 749
Epoch 766
Epoch 766, Loss: 0.00000013834920, Improvement: -0.00000003516737, Best Loss: 0.00000007992265 in Epoch 749
Epoch 767
Epoch 767, Loss: 0.00000013199191, Improvement: -0.00000000635730, Best Loss: 0.00000007992265 in Epoch 749
Epoch 768
Epoch 768, Loss: 0.00000013205483, Improvement: 0.00000000006292, Best Loss: 0.00000007992265 in Epoch 749
Epoch 769
Epoch 769, Loss: 0.00000012273465, Improvement: -0.00000000932018, Best Loss: 0.00000007992265 in Epoch 749
Epoch 770
Epoch 770, Loss: 0.00000013346547, Improvement: 0.00000001073082, Best Loss: 0.00000007992265 in Epoch 749
Epoch 771
Epoch 771, Loss: 0.00000013962282, Improvement: 0.00000000615735, Best Loss: 0.00000007992265 in Epoch 749
Epoch 772
Epoch 772, Loss: 0.00000017100944, Improvement: 0.00000003138662, Best Loss: 0.00000007992265 in Epoch 749
Epoch 773
Epoch 773, Loss: 0.00000014919636, Improvement: -0.00000002181308, Best Loss: 0.00000007992265 in Epoch 749
Epoch 774
A best model at epoch 774 has been saved with training error 0.00000007962872.
Epoch 774, Loss: 0.00000011294198, Improvement: -0.00000003625438, Best Loss: 0.00000007962872 in Epoch 774
Epoch 775
Epoch 775, Loss: 0.00000014704109, Improvement: 0.00000003409911, Best Loss: 0.00000007962872 in Epoch 774
Epoch 776
Epoch 776, Loss: 0.00000011622928, Improvement: -0.00000003081181, Best Loss: 0.00000007962872 in Epoch 774
Epoch 777
Epoch 777, Loss: 0.00000014512427, Improvement: 0.00000002889499, Best Loss: 0.00000007962872 in Epoch 774
Epoch 778
Epoch 778, Loss: 0.00000016147951, Improvement: 0.00000001635524, Best Loss: 0.00000007962872 in Epoch 774
Epoch 779
Epoch 779, Loss: 0.00000011831219, Improvement: -0.00000004316731, Best Loss: 0.00000007962872 in Epoch 774
Epoch 780
Epoch 780, Loss: 0.00000019075565, Improvement: 0.00000007244346, Best Loss: 0.00000007962872 in Epoch 774
Epoch 781
Epoch 781, Loss: 0.00000027363836, Improvement: 0.00000008288270, Best Loss: 0.00000007962872 in Epoch 774
Epoch 782
Epoch 782, Loss: 0.00000028758480, Improvement: 0.00000001394645, Best Loss: 0.00000007962872 in Epoch 774
Epoch 783
Epoch 783, Loss: 0.00000022026672, Improvement: -0.00000006731808, Best Loss: 0.00000007962872 in Epoch 774
Epoch 784
Epoch 784, Loss: 0.00000021868021, Improvement: -0.00000000158652, Best Loss: 0.00000007962872 in Epoch 774
Epoch 785
Epoch 785, Loss: 0.00000021347457, Improvement: -0.00000000520564, Best Loss: 0.00000007962872 in Epoch 774
Epoch 786
Epoch 786, Loss: 0.00000020203276, Improvement: -0.00000001144181, Best Loss: 0.00000007962872 in Epoch 774
Epoch 787
Epoch 787, Loss: 0.00000035909022, Improvement: 0.00000015705746, Best Loss: 0.00000007962872 in Epoch 774
Epoch 788
Epoch 788, Loss: 0.00000026510573, Improvement: -0.00000009398449, Best Loss: 0.00000007962872 in Epoch 774
Epoch 789
Epoch 789, Loss: 0.00000019740231, Improvement: -0.00000006770342, Best Loss: 0.00000007962872 in Epoch 774
Epoch 790
Epoch 790, Loss: 0.00000017528832, Improvement: -0.00000002211398, Best Loss: 0.00000007962872 in Epoch 774
Epoch 791
Epoch 791, Loss: 0.00000014618034, Improvement: -0.00000002910798, Best Loss: 0.00000007962872 in Epoch 774
Epoch 792
Epoch 792, Loss: 0.00000013776838, Improvement: -0.00000000841196, Best Loss: 0.00000007962872 in Epoch 774
Epoch 793
Epoch 793, Loss: 0.00000013592136, Improvement: -0.00000000184702, Best Loss: 0.00000007962872 in Epoch 774
Epoch 794
A best model at epoch 794 has been saved with training error 0.00000007717522.
Epoch 794, Loss: 0.00000011670013, Improvement: -0.00000001922124, Best Loss: 0.00000007717522 in Epoch 794
Epoch 795
A best model at epoch 795 has been saved with training error 0.00000007713295.
Epoch 795, Loss: 0.00000011154086, Improvement: -0.00000000515926, Best Loss: 0.00000007713295 in Epoch 795
Epoch 796
Epoch 796, Loss: 0.00000019153777, Improvement: 0.00000007999691, Best Loss: 0.00000007713295 in Epoch 795
Epoch 797
Epoch 797, Loss: 0.00000016621566, Improvement: -0.00000002532212, Best Loss: 0.00000007713295 in Epoch 795
Epoch 798
Epoch 798, Loss: 0.00000015543664, Improvement: -0.00000001077902, Best Loss: 0.00000007713295 in Epoch 795
Epoch 799
Epoch 799, Loss: 0.00000013688867, Improvement: -0.00000001854796, Best Loss: 0.00000007713295 in Epoch 795
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000021056123, Improvement: 0.00000007367255, Best Loss: 0.00000007713295 in Epoch 795
Epoch 801
Epoch 801, Loss: 0.00000029148329, Improvement: 0.00000008092206, Best Loss: 0.00000007713295 in Epoch 795
Epoch 802
Epoch 802, Loss: 0.00000036863365, Improvement: 0.00000007715036, Best Loss: 0.00000007713295 in Epoch 795
Epoch 803
Epoch 803, Loss: 0.00000025023401, Improvement: -0.00000011839964, Best Loss: 0.00000007713295 in Epoch 795
Epoch 804
Epoch 804, Loss: 0.00000019492040, Improvement: -0.00000005531361, Best Loss: 0.00000007713295 in Epoch 795
Epoch 805
Epoch 805, Loss: 0.00000013557284, Improvement: -0.00000005934756, Best Loss: 0.00000007713295 in Epoch 795
Epoch 806
Epoch 806, Loss: 0.00000013128316, Improvement: -0.00000000428967, Best Loss: 0.00000007713295 in Epoch 795
Epoch 807
Epoch 807, Loss: 0.00000011516229, Improvement: -0.00000001612088, Best Loss: 0.00000007713295 in Epoch 795
Epoch 808
A best model at epoch 808 has been saved with training error 0.00000007661135.
A best model at epoch 808 has been saved with training error 0.00000007440697.
A best model at epoch 808 has been saved with training error 0.00000006968307.
Epoch 808, Loss: 0.00000010321645, Improvement: -0.00000001194583, Best Loss: 0.00000006968307 in Epoch 808
Epoch 809
Epoch 809, Loss: 0.00000011841773, Improvement: 0.00000001520128, Best Loss: 0.00000006968307 in Epoch 808
Epoch 810
Epoch 810, Loss: 0.00000014773055, Improvement: 0.00000002931282, Best Loss: 0.00000006968307 in Epoch 808
Epoch 811
Epoch 811, Loss: 0.00000026953776, Improvement: 0.00000012180722, Best Loss: 0.00000006968307 in Epoch 808
Epoch 812
Epoch 812, Loss: 0.00000039337872, Improvement: 0.00000012384095, Best Loss: 0.00000006968307 in Epoch 808
Epoch 813
Epoch 813, Loss: 0.00000029649149, Improvement: -0.00000009688722, Best Loss: 0.00000006968307 in Epoch 808
Epoch 814
Epoch 814, Loss: 0.00000021316164, Improvement: -0.00000008332985, Best Loss: 0.00000006968307 in Epoch 808
Epoch 815
Epoch 815, Loss: 0.00000020524564, Improvement: -0.00000000791600, Best Loss: 0.00000006968307 in Epoch 808
Epoch 816
Epoch 816, Loss: 0.00000031971029, Improvement: 0.00000011446466, Best Loss: 0.00000006968307 in Epoch 808
Epoch 817
Epoch 817, Loss: 0.00000042285763, Improvement: 0.00000010314734, Best Loss: 0.00000006968307 in Epoch 808
Epoch 818
Epoch 818, Loss: 0.00000033990086, Improvement: -0.00000008295677, Best Loss: 0.00000006968307 in Epoch 808
Epoch 819
Epoch 819, Loss: 0.00000029564230, Improvement: -0.00000004425856, Best Loss: 0.00000006968307 in Epoch 808
Epoch 820
Epoch 820, Loss: 0.00000019341486, Improvement: -0.00000010222744, Best Loss: 0.00000006968307 in Epoch 808
Epoch 821
Epoch 821, Loss: 0.00000014723994, Improvement: -0.00000004617492, Best Loss: 0.00000006968307 in Epoch 808
Epoch 822
Epoch 822, Loss: 0.00000012671231, Improvement: -0.00000002052763, Best Loss: 0.00000006968307 in Epoch 808
Epoch 823
Epoch 823, Loss: 0.00000013319263, Improvement: 0.00000000648032, Best Loss: 0.00000006968307 in Epoch 808
Epoch 824
Epoch 824, Loss: 0.00000015970793, Improvement: 0.00000002651530, Best Loss: 0.00000006968307 in Epoch 808
Epoch 825
Epoch 825, Loss: 0.00000014541610, Improvement: -0.00000001429183, Best Loss: 0.00000006968307 in Epoch 808
Epoch 826
Epoch 826, Loss: 0.00000014743654, Improvement: 0.00000000202044, Best Loss: 0.00000006968307 in Epoch 808
Epoch 827
Epoch 827, Loss: 0.00000013126218, Improvement: -0.00000001617436, Best Loss: 0.00000006968307 in Epoch 808
Epoch 828
Epoch 828, Loss: 0.00000013966479, Improvement: 0.00000000840261, Best Loss: 0.00000006968307 in Epoch 808
Epoch 829
Epoch 829, Loss: 0.00000031201297, Improvement: 0.00000017234818, Best Loss: 0.00000006968307 in Epoch 808
Epoch 830
Epoch 830, Loss: 0.00000017681710, Improvement: -0.00000013519587, Best Loss: 0.00000006968307 in Epoch 808
Epoch 831
Epoch 831, Loss: 0.00000016696153, Improvement: -0.00000000985557, Best Loss: 0.00000006968307 in Epoch 808
Epoch 832
Epoch 832, Loss: 0.00000013959405, Improvement: -0.00000002736748, Best Loss: 0.00000006968307 in Epoch 808
Epoch 833
Epoch 833, Loss: 0.00000015456869, Improvement: 0.00000001497465, Best Loss: 0.00000006968307 in Epoch 808
Epoch 834
Epoch 834, Loss: 0.00000018913021, Improvement: 0.00000003456151, Best Loss: 0.00000006968307 in Epoch 808
Epoch 835
Epoch 835, Loss: 0.00000011228209, Improvement: -0.00000007684812, Best Loss: 0.00000006968307 in Epoch 808
Epoch 836
Epoch 836, Loss: 0.00000010416609, Improvement: -0.00000000811600, Best Loss: 0.00000006968307 in Epoch 808
Epoch 837
Epoch 837, Loss: 0.00000010242251, Improvement: -0.00000000174358, Best Loss: 0.00000006968307 in Epoch 808
Epoch 838
Epoch 838, Loss: 0.00000015876871, Improvement: 0.00000005634620, Best Loss: 0.00000006968307 in Epoch 808
Epoch 839
Epoch 839, Loss: 0.00000020865753, Improvement: 0.00000004988882, Best Loss: 0.00000006968307 in Epoch 808
Epoch 840
Epoch 840, Loss: 0.00000018714703, Improvement: -0.00000002151049, Best Loss: 0.00000006968307 in Epoch 808
Epoch 841
Epoch 841, Loss: 0.00000031413933, Improvement: 0.00000012699230, Best Loss: 0.00000006968307 in Epoch 808
Epoch 842
Epoch 842, Loss: 0.00000028645921, Improvement: -0.00000002768012, Best Loss: 0.00000006968307 in Epoch 808
Epoch 843
Epoch 843, Loss: 0.00000015536946, Improvement: -0.00000013108976, Best Loss: 0.00000006968307 in Epoch 808
Epoch 844
Epoch 844, Loss: 0.00000013219244, Improvement: -0.00000002317701, Best Loss: 0.00000006968307 in Epoch 808
Epoch 845
Epoch 845, Loss: 0.00000017280239, Improvement: 0.00000004060995, Best Loss: 0.00000006968307 in Epoch 808
Epoch 846
Epoch 846, Loss: 0.00000038323511, Improvement: 0.00000021043272, Best Loss: 0.00000006968307 in Epoch 808
Epoch 847
Epoch 847, Loss: 0.00000032454756, Improvement: -0.00000005868755, Best Loss: 0.00000006968307 in Epoch 808
Epoch 848
Epoch 848, Loss: 0.00000016319437, Improvement: -0.00000016135319, Best Loss: 0.00000006968307 in Epoch 808
Epoch 849
Epoch 849, Loss: 0.00000012424759, Improvement: -0.00000003894678, Best Loss: 0.00000006968307 in Epoch 808
Epoch 850
A best model at epoch 850 has been saved with training error 0.00000006546713.
A best model at epoch 850 has been saved with training error 0.00000006380613.
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000010381262, Improvement: -0.00000002043497, Best Loss: 0.00000006380613 in Epoch 850
Epoch 851
Epoch 851, Loss: 0.00000009494522, Improvement: -0.00000000886740, Best Loss: 0.00000006380613 in Epoch 850
Epoch 852
Epoch 852, Loss: 0.00000010980283, Improvement: 0.00000001485761, Best Loss: 0.00000006380613 in Epoch 850
Epoch 853
Epoch 853, Loss: 0.00000020207900, Improvement: 0.00000009227617, Best Loss: 0.00000006380613 in Epoch 850
Epoch 854
Epoch 854, Loss: 0.00000029622913, Improvement: 0.00000009415012, Best Loss: 0.00000006380613 in Epoch 850
Epoch 855
Epoch 855, Loss: 0.00000016784771, Improvement: -0.00000012838142, Best Loss: 0.00000006380613 in Epoch 850
Epoch 856
Epoch 856, Loss: 0.00000014706865, Improvement: -0.00000002077906, Best Loss: 0.00000006380613 in Epoch 850
Epoch 857
Epoch 857, Loss: 0.00000012729535, Improvement: -0.00000001977330, Best Loss: 0.00000006380613 in Epoch 850
Epoch 858
Epoch 858, Loss: 0.00000012876798, Improvement: 0.00000000147263, Best Loss: 0.00000006380613 in Epoch 850
Epoch 859
Epoch 859, Loss: 0.00000014783287, Improvement: 0.00000001906489, Best Loss: 0.00000006380613 in Epoch 850
Epoch 860
Epoch 860, Loss: 0.00000015917006, Improvement: 0.00000001133719, Best Loss: 0.00000006380613 in Epoch 850
Epoch 861
Epoch 861, Loss: 0.00000014241571, Improvement: -0.00000001675435, Best Loss: 0.00000006380613 in Epoch 850
Epoch 862
Epoch 862, Loss: 0.00000026475225, Improvement: 0.00000012233654, Best Loss: 0.00000006380613 in Epoch 850
Epoch 863
Epoch 863, Loss: 0.00000022949914, Improvement: -0.00000003525311, Best Loss: 0.00000006380613 in Epoch 850
Epoch 864
Epoch 864, Loss: 0.00000017329653, Improvement: -0.00000005620261, Best Loss: 0.00000006380613 in Epoch 850
Epoch 865
Epoch 865, Loss: 0.00000019645166, Improvement: 0.00000002315513, Best Loss: 0.00000006380613 in Epoch 850
Epoch 866
Epoch 866, Loss: 0.00000014912423, Improvement: -0.00000004732743, Best Loss: 0.00000006380613 in Epoch 850
Epoch 867
Epoch 867, Loss: 0.00000020796432, Improvement: 0.00000005884008, Best Loss: 0.00000006380613 in Epoch 850
Epoch 868
Epoch 868, Loss: 0.00000031480501, Improvement: 0.00000010684069, Best Loss: 0.00000006380613 in Epoch 850
Epoch 869
Epoch 869, Loss: 0.00000015053703, Improvement: -0.00000016426798, Best Loss: 0.00000006380613 in Epoch 850
Epoch 870
Epoch 870, Loss: 0.00000017705559, Improvement: 0.00000002651856, Best Loss: 0.00000006380613 in Epoch 850
Epoch 871
Epoch 871, Loss: 0.00000022421611, Improvement: 0.00000004716052, Best Loss: 0.00000006380613 in Epoch 850
Epoch 872
Epoch 872, Loss: 0.00000020837203, Improvement: -0.00000001584408, Best Loss: 0.00000006380613 in Epoch 850
Epoch 873
Epoch 873, Loss: 0.00000016445516, Improvement: -0.00000004391686, Best Loss: 0.00000006380613 in Epoch 850
Epoch 874
Epoch 874, Loss: 0.00000013342796, Improvement: -0.00000003102720, Best Loss: 0.00000006380613 in Epoch 850
Epoch 875
Epoch 875, Loss: 0.00000017872022, Improvement: 0.00000004529226, Best Loss: 0.00000006380613 in Epoch 850
Epoch 876
Epoch 876, Loss: 0.00000021640374, Improvement: 0.00000003768352, Best Loss: 0.00000006380613 in Epoch 850
Epoch 877
Epoch 877, Loss: 0.00000014982052, Improvement: -0.00000006658322, Best Loss: 0.00000006380613 in Epoch 850
Epoch 878
Epoch 878, Loss: 0.00000017039136, Improvement: 0.00000002057084, Best Loss: 0.00000006380613 in Epoch 850
Epoch 879
Epoch 879, Loss: 0.00000014591302, Improvement: -0.00000002447834, Best Loss: 0.00000006380613 in Epoch 850
Epoch 880
Epoch 880, Loss: 0.00000012585492, Improvement: -0.00000002005810, Best Loss: 0.00000006380613 in Epoch 850
Epoch 881
A best model at epoch 881 has been saved with training error 0.00000005793384.
Epoch 881, Loss: 0.00000011089016, Improvement: -0.00000001496476, Best Loss: 0.00000005793384 in Epoch 881
Epoch 882
Epoch 882, Loss: 0.00000013195913, Improvement: 0.00000002106897, Best Loss: 0.00000005793384 in Epoch 881
Epoch 883
Epoch 883, Loss: 0.00000011519545, Improvement: -0.00000001676368, Best Loss: 0.00000005793384 in Epoch 881
Epoch 884
A best model at epoch 884 has been saved with training error 0.00000005680078.
Epoch 884, Loss: 0.00000009883435, Improvement: -0.00000001636110, Best Loss: 0.00000005680078 in Epoch 884
Epoch 885
Epoch 885, Loss: 0.00000014993174, Improvement: 0.00000005109739, Best Loss: 0.00000005680078 in Epoch 884
Epoch 886
Epoch 886, Loss: 0.00000016261455, Improvement: 0.00000001268281, Best Loss: 0.00000005680078 in Epoch 884
Epoch 887
Epoch 887, Loss: 0.00000011805522, Improvement: -0.00000004455933, Best Loss: 0.00000005680078 in Epoch 884
Epoch 888
A best model at epoch 888 has been saved with training error 0.00000005603879.
Epoch 888, Loss: 0.00000010565741, Improvement: -0.00000001239781, Best Loss: 0.00000005603879 in Epoch 888
Epoch 889
Epoch 889, Loss: 0.00000020190476, Improvement: 0.00000009624735, Best Loss: 0.00000005603879 in Epoch 888
Epoch 890
Epoch 890, Loss: 0.00000025118505, Improvement: 0.00000004928029, Best Loss: 0.00000005603879 in Epoch 888
Epoch 891
Epoch 891, Loss: 0.00000035453669, Improvement: 0.00000010335164, Best Loss: 0.00000005603879 in Epoch 888
Epoch 892
Epoch 892, Loss: 0.00000037991353, Improvement: 0.00000002537683, Best Loss: 0.00000005603879 in Epoch 888
Epoch 893
Epoch 893, Loss: 0.00000026537093, Improvement: -0.00000011454260, Best Loss: 0.00000005603879 in Epoch 888
Epoch 894
Epoch 894, Loss: 0.00000013532754, Improvement: -0.00000013004339, Best Loss: 0.00000005603879 in Epoch 888
Epoch 895
A best model at epoch 895 has been saved with training error 0.00000005549200.
Epoch 895, Loss: 0.00000010162062, Improvement: -0.00000003370692, Best Loss: 0.00000005549200 in Epoch 895
Epoch 896
Epoch 896, Loss: 0.00000009699346, Improvement: -0.00000000462715, Best Loss: 0.00000005549200 in Epoch 895
Epoch 897
Epoch 897, Loss: 0.00000008682370, Improvement: -0.00000001016977, Best Loss: 0.00000005549200 in Epoch 895
Epoch 898
Epoch 898, Loss: 0.00000009063161, Improvement: 0.00000000380792, Best Loss: 0.00000005549200 in Epoch 895
Epoch 899
A best model at epoch 899 has been saved with training error 0.00000005261824.
Epoch 899, Loss: 0.00000008192894, Improvement: -0.00000000870267, Best Loss: 0.00000005261824 in Epoch 899
Epoch 900
A best model at epoch 900 has been saved with training error 0.00000005093695.
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000007039357, Improvement: -0.00000001153537, Best Loss: 0.00000005093695 in Epoch 900
Epoch 901
Epoch 901, Loss: 0.00000007739252, Improvement: 0.00000000699895, Best Loss: 0.00000005093695 in Epoch 900
Epoch 902
Epoch 902, Loss: 0.00000008693539, Improvement: 0.00000000954287, Best Loss: 0.00000005093695 in Epoch 900
Epoch 903
Epoch 903, Loss: 0.00000008637715, Improvement: -0.00000000055824, Best Loss: 0.00000005093695 in Epoch 900
Epoch 904
Epoch 904, Loss: 0.00000008510282, Improvement: -0.00000000127433, Best Loss: 0.00000005093695 in Epoch 900
Epoch 905
A best model at epoch 905 has been saved with training error 0.00000004819400.
Epoch 905, Loss: 0.00000007857675, Improvement: -0.00000000652607, Best Loss: 0.00000004819400 in Epoch 905
Epoch 906
Epoch 906, Loss: 0.00000011844630, Improvement: 0.00000003986955, Best Loss: 0.00000004819400 in Epoch 905
Epoch 907
Epoch 907, Loss: 0.00000019814522, Improvement: 0.00000007969891, Best Loss: 0.00000004819400 in Epoch 905
Epoch 908
Epoch 908, Loss: 0.00000033728385, Improvement: 0.00000013913864, Best Loss: 0.00000004819400 in Epoch 905
Epoch 909
Epoch 909, Loss: 0.00000045074252, Improvement: 0.00000011345867, Best Loss: 0.00000004819400 in Epoch 905
Epoch 910
Epoch 910, Loss: 0.00000016371802, Improvement: -0.00000028702450, Best Loss: 0.00000004819400 in Epoch 905
Epoch 911
Epoch 911, Loss: 0.00000009656260, Improvement: -0.00000006715543, Best Loss: 0.00000004819400 in Epoch 905
Epoch 912
Epoch 912, Loss: 0.00000008822157, Improvement: -0.00000000834102, Best Loss: 0.00000004819400 in Epoch 905
Epoch 913
Epoch 913, Loss: 0.00000009611221, Improvement: 0.00000000789064, Best Loss: 0.00000004819400 in Epoch 905
Epoch 914
Epoch 914, Loss: 0.00000008486351, Improvement: -0.00000001124870, Best Loss: 0.00000004819400 in Epoch 905
Epoch 915
Epoch 915, Loss: 0.00000007097046, Improvement: -0.00000001389305, Best Loss: 0.00000004819400 in Epoch 905
Epoch 916
Epoch 916, Loss: 0.00000009237174, Improvement: 0.00000002140128, Best Loss: 0.00000004819400 in Epoch 905
Epoch 917
Epoch 917, Loss: 0.00000009817904, Improvement: 0.00000000580730, Best Loss: 0.00000004819400 in Epoch 905
Epoch 918
Epoch 918, Loss: 0.00000008839728, Improvement: -0.00000000978176, Best Loss: 0.00000004819400 in Epoch 905
Epoch 919
Epoch 919, Loss: 0.00000008756283, Improvement: -0.00000000083445, Best Loss: 0.00000004819400 in Epoch 905
Epoch 920
Epoch 920, Loss: 0.00000008984142, Improvement: 0.00000000227859, Best Loss: 0.00000004819400 in Epoch 905
Epoch 921
Epoch 921, Loss: 0.00000008078019, Improvement: -0.00000000906123, Best Loss: 0.00000004819400 in Epoch 905
Epoch 922
Epoch 922, Loss: 0.00000012971227, Improvement: 0.00000004893208, Best Loss: 0.00000004819400 in Epoch 905
Epoch 923
Epoch 923, Loss: 0.00000021715861, Improvement: 0.00000008744634, Best Loss: 0.00000004819400 in Epoch 905
Epoch 924
Epoch 924, Loss: 0.00000023309854, Improvement: 0.00000001593993, Best Loss: 0.00000004819400 in Epoch 905
Epoch 925
Epoch 925, Loss: 0.00000019342330, Improvement: -0.00000003967524, Best Loss: 0.00000004819400 in Epoch 905
Epoch 926
Epoch 926, Loss: 0.00000024399501, Improvement: 0.00000005057171, Best Loss: 0.00000004819400 in Epoch 905
Epoch 927
Epoch 927, Loss: 0.00000012937004, Improvement: -0.00000011462496, Best Loss: 0.00000004819400 in Epoch 905
Epoch 928
Epoch 928, Loss: 0.00000010190363, Improvement: -0.00000002746642, Best Loss: 0.00000004819400 in Epoch 905
Epoch 929
Epoch 929, Loss: 0.00000011699609, Improvement: 0.00000001509247, Best Loss: 0.00000004819400 in Epoch 905
Epoch 930
Epoch 930, Loss: 0.00000010287781, Improvement: -0.00000001411828, Best Loss: 0.00000004819400 in Epoch 905
Epoch 931
Epoch 931, Loss: 0.00000009113419, Improvement: -0.00000001174362, Best Loss: 0.00000004819400 in Epoch 905
Epoch 932
Epoch 932, Loss: 0.00000009355842, Improvement: 0.00000000242422, Best Loss: 0.00000004819400 in Epoch 905
Epoch 933
Epoch 933, Loss: 0.00000010548504, Improvement: 0.00000001192662, Best Loss: 0.00000004819400 in Epoch 905
Epoch 934
Epoch 934, Loss: 0.00000012553927, Improvement: 0.00000002005424, Best Loss: 0.00000004819400 in Epoch 905
Epoch 935
Epoch 935, Loss: 0.00000017637500, Improvement: 0.00000005083572, Best Loss: 0.00000004819400 in Epoch 905
Epoch 936
Epoch 936, Loss: 0.00000020009343, Improvement: 0.00000002371843, Best Loss: 0.00000004819400 in Epoch 905
Epoch 937
Epoch 937, Loss: 0.00000022884063, Improvement: 0.00000002874720, Best Loss: 0.00000004819400 in Epoch 905
Epoch 938
Epoch 938, Loss: 0.00000020223617, Improvement: -0.00000002660445, Best Loss: 0.00000004819400 in Epoch 905
Epoch 939
Epoch 939, Loss: 0.00000013693444, Improvement: -0.00000006530173, Best Loss: 0.00000004819400 in Epoch 905
Epoch 940
Epoch 940, Loss: 0.00000008710749, Improvement: -0.00000004982695, Best Loss: 0.00000004819400 in Epoch 905
Epoch 941
Epoch 941, Loss: 0.00000008631891, Improvement: -0.00000000078858, Best Loss: 0.00000004819400 in Epoch 905
Epoch 942
Epoch 942, Loss: 0.00000007477364, Improvement: -0.00000001154527, Best Loss: 0.00000004819400 in Epoch 905
Epoch 943
Epoch 943, Loss: 0.00000007886718, Improvement: 0.00000000409354, Best Loss: 0.00000004819400 in Epoch 905
Epoch 944
Epoch 944, Loss: 0.00000007785862, Improvement: -0.00000000100856, Best Loss: 0.00000004819400 in Epoch 905
Epoch 945
Epoch 945, Loss: 0.00000020052235, Improvement: 0.00000012266373, Best Loss: 0.00000004819400 in Epoch 905
Epoch 946
Epoch 946, Loss: 0.00000038854433, Improvement: 0.00000018802199, Best Loss: 0.00000004819400 in Epoch 905
Epoch 947
Epoch 947, Loss: 0.00000022960009, Improvement: -0.00000015894424, Best Loss: 0.00000004819400 in Epoch 905
Epoch 948
Epoch 948, Loss: 0.00000019727525, Improvement: -0.00000003232484, Best Loss: 0.00000004819400 in Epoch 905
Epoch 949
Epoch 949, Loss: 0.00000017177956, Improvement: -0.00000002549569, Best Loss: 0.00000004819400 in Epoch 905
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000012049326, Improvement: -0.00000005128630, Best Loss: 0.00000004819400 in Epoch 905
Epoch 951
Epoch 951, Loss: 0.00000008692683, Improvement: -0.00000003356642, Best Loss: 0.00000004819400 in Epoch 905
Epoch 952
A best model at epoch 952 has been saved with training error 0.00000003904402.
Epoch 952, Loss: 0.00000006460801, Improvement: -0.00000002231882, Best Loss: 0.00000003904402 in Epoch 952
Epoch 953
Epoch 953, Loss: 0.00000006914671, Improvement: 0.00000000453870, Best Loss: 0.00000003904402 in Epoch 952
Epoch 954
Epoch 954, Loss: 0.00000006607385, Improvement: -0.00000000307286, Best Loss: 0.00000003904402 in Epoch 952
Epoch 955
Epoch 955, Loss: 0.00000006287601, Improvement: -0.00000000319784, Best Loss: 0.00000003904402 in Epoch 952
Epoch 956
Epoch 956, Loss: 0.00000006956504, Improvement: 0.00000000668904, Best Loss: 0.00000003904402 in Epoch 952
Epoch 957
Epoch 957, Loss: 0.00000006717646, Improvement: -0.00000000238859, Best Loss: 0.00000003904402 in Epoch 952
Epoch 958
Epoch 958, Loss: 0.00000006495861, Improvement: -0.00000000221785, Best Loss: 0.00000003904402 in Epoch 952
Epoch 959
Epoch 959, Loss: 0.00000007473888, Improvement: 0.00000000978028, Best Loss: 0.00000003904402 in Epoch 952
Epoch 960
Epoch 960, Loss: 0.00000012463638, Improvement: 0.00000004989750, Best Loss: 0.00000003904402 in Epoch 952
Epoch 961
Epoch 961, Loss: 0.00000013579459, Improvement: 0.00000001115821, Best Loss: 0.00000003904402 in Epoch 952
Epoch 962
Epoch 962, Loss: 0.00000013553861, Improvement: -0.00000000025599, Best Loss: 0.00000003904402 in Epoch 952
Epoch 963
Epoch 963, Loss: 0.00000029209367, Improvement: 0.00000015655506, Best Loss: 0.00000003904402 in Epoch 952
Epoch 964
Epoch 964, Loss: 0.00000020563315, Improvement: -0.00000008646052, Best Loss: 0.00000003904402 in Epoch 952
Epoch 965
Epoch 965, Loss: 0.00000016362235, Improvement: -0.00000004201080, Best Loss: 0.00000003904402 in Epoch 952
Epoch 966
Epoch 966, Loss: 0.00000010380941, Improvement: -0.00000005981294, Best Loss: 0.00000003904402 in Epoch 952
Epoch 967
Epoch 967, Loss: 0.00000006947583, Improvement: -0.00000003433358, Best Loss: 0.00000003904402 in Epoch 952
Epoch 968
Epoch 968, Loss: 0.00000008938248, Improvement: 0.00000001990665, Best Loss: 0.00000003904402 in Epoch 952
Epoch 969
Epoch 969, Loss: 0.00000006976608, Improvement: -0.00000001961640, Best Loss: 0.00000003904402 in Epoch 952
Epoch 970
Epoch 970, Loss: 0.00000007872501, Improvement: 0.00000000895893, Best Loss: 0.00000003904402 in Epoch 952
Epoch 971
Epoch 971, Loss: 0.00000008242666, Improvement: 0.00000000370165, Best Loss: 0.00000003904402 in Epoch 952
Epoch 972
Epoch 972, Loss: 0.00000008114837, Improvement: -0.00000000127829, Best Loss: 0.00000003904402 in Epoch 952
Epoch 973
Epoch 973, Loss: 0.00000007747798, Improvement: -0.00000000367040, Best Loss: 0.00000003904402 in Epoch 952
Epoch 974
Epoch 974, Loss: 0.00000009904660, Improvement: 0.00000002156862, Best Loss: 0.00000003904402 in Epoch 952
Epoch 975
Epoch 975, Loss: 0.00000010862617, Improvement: 0.00000000957957, Best Loss: 0.00000003904402 in Epoch 952
Epoch 976
Epoch 976, Loss: 0.00000008597919, Improvement: -0.00000002264698, Best Loss: 0.00000003904402 in Epoch 952
Epoch 977
Epoch 977, Loss: 0.00000009872750, Improvement: 0.00000001274831, Best Loss: 0.00000003904402 in Epoch 952
Epoch 978
Epoch 978, Loss: 0.00000009840623, Improvement: -0.00000000032127, Best Loss: 0.00000003904402 in Epoch 952
Epoch 979
Epoch 979, Loss: 0.00000019189827, Improvement: 0.00000009349203, Best Loss: 0.00000003904402 in Epoch 952
Epoch 980
Epoch 980, Loss: 0.00000020890176, Improvement: 0.00000001700350, Best Loss: 0.00000003904402 in Epoch 952
Epoch 981
Epoch 981, Loss: 0.00000023734733, Improvement: 0.00000002844556, Best Loss: 0.00000003904402 in Epoch 952
Epoch 982
Epoch 982, Loss: 0.00000023487001, Improvement: -0.00000000247732, Best Loss: 0.00000003904402 in Epoch 952
Epoch 983
Epoch 983, Loss: 0.00000024679204, Improvement: 0.00000001192203, Best Loss: 0.00000003904402 in Epoch 952
Epoch 984
Epoch 984, Loss: 0.00000025118077, Improvement: 0.00000000438873, Best Loss: 0.00000003904402 in Epoch 952
Epoch 985
Epoch 985, Loss: 0.00000036193715, Improvement: 0.00000011075638, Best Loss: 0.00000003904402 in Epoch 952
Epoch 986
Epoch 986, Loss: 0.00000025047316, Improvement: -0.00000011146399, Best Loss: 0.00000003904402 in Epoch 952
Epoch 987
Epoch 987, Loss: 0.00000021261318, Improvement: -0.00000003785998, Best Loss: 0.00000003904402 in Epoch 952
Epoch 988
Epoch 988, Loss: 0.00000014134904, Improvement: -0.00000007126414, Best Loss: 0.00000003904402 in Epoch 952
Epoch 989
Epoch 989, Loss: 0.00000009253638, Improvement: -0.00000004881266, Best Loss: 0.00000003904402 in Epoch 952
Epoch 990
Epoch 990, Loss: 0.00000007695588, Improvement: -0.00000001558049, Best Loss: 0.00000003904402 in Epoch 952
Epoch 991
Epoch 991, Loss: 0.00000006102089, Improvement: -0.00000001593499, Best Loss: 0.00000003904402 in Epoch 952
Epoch 992
Epoch 992, Loss: 0.00000006124921, Improvement: 0.00000000022832, Best Loss: 0.00000003904402 in Epoch 952
Epoch 993
Epoch 993, Loss: 0.00000006494791, Improvement: 0.00000000369869, Best Loss: 0.00000003904402 in Epoch 952
Epoch 994
Epoch 994, Loss: 0.00000006999702, Improvement: 0.00000000504911, Best Loss: 0.00000003904402 in Epoch 952
Epoch 995
Epoch 995, Loss: 0.00000010620314, Improvement: 0.00000003620612, Best Loss: 0.00000003904402 in Epoch 952
Epoch 996
Epoch 996, Loss: 0.00000011099893, Improvement: 0.00000000479579, Best Loss: 0.00000003904402 in Epoch 952
Epoch 997
Epoch 997, Loss: 0.00000007842324, Improvement: -0.00000003257569, Best Loss: 0.00000003904402 in Epoch 952
Epoch 998
Epoch 998, Loss: 0.00000009268778, Improvement: 0.00000001426454, Best Loss: 0.00000003904402 in Epoch 952
Epoch 999
Epoch 999, Loss: 0.00000009780432, Improvement: 0.00000000511654, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000020723100, Improvement: 0.00000010942668, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1001
Epoch 1001, Loss: 0.00000021349533, Improvement: 0.00000000626432, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1002
Epoch 1002, Loss: 0.00000013710859, Improvement: -0.00000007638674, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1003
Epoch 1003, Loss: 0.00000008838452, Improvement: -0.00000004872407, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1004
Epoch 1004, Loss: 0.00000013109528, Improvement: 0.00000004271076, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1005
Epoch 1005, Loss: 0.00000028252188, Improvement: 0.00000015142660, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1006
Epoch 1006, Loss: 0.00000043385745, Improvement: 0.00000015133557, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1007
Epoch 1007, Loss: 0.00000030485715, Improvement: -0.00000012900030, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1008
Epoch 1008, Loss: 0.00000018162805, Improvement: -0.00000012322910, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1009
Epoch 1009, Loss: 0.00000012493051, Improvement: -0.00000005669754, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1010
Epoch 1010, Loss: 0.00000009499864, Improvement: -0.00000002993187, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1011
Epoch 1011, Loss: 0.00000008904687, Improvement: -0.00000000595177, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1012
Epoch 1012, Loss: 0.00000008596651, Improvement: -0.00000000308036, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1013
Epoch 1013, Loss: 0.00000006884561, Improvement: -0.00000001712090, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1014
Epoch 1014, Loss: 0.00000006331647, Improvement: -0.00000000552914, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1015
Epoch 1015, Loss: 0.00000006767487, Improvement: 0.00000000435840, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1016
Epoch 1016, Loss: 0.00000006940999, Improvement: 0.00000000173513, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1017
Epoch 1017, Loss: 0.00000005695220, Improvement: -0.00000001245779, Best Loss: 0.00000003904402 in Epoch 952
Epoch 1018
A best model at epoch 1018 has been saved with training error 0.00000003865582.
Epoch 1018, Loss: 0.00000005378646, Improvement: -0.00000000316574, Best Loss: 0.00000003865582 in Epoch 1018
Epoch 1019
Epoch 1019, Loss: 0.00000005480088, Improvement: 0.00000000101442, Best Loss: 0.00000003865582 in Epoch 1018
Epoch 1020
Epoch 1020, Loss: 0.00000006489343, Improvement: 0.00000001009255, Best Loss: 0.00000003865582 in Epoch 1018
Epoch 1021
Epoch 1021, Loss: 0.00000008991462, Improvement: 0.00000002502120, Best Loss: 0.00000003865582 in Epoch 1018
Epoch 1022
Epoch 1022, Loss: 0.00000011080112, Improvement: 0.00000002088650, Best Loss: 0.00000003865582 in Epoch 1018
Epoch 1023
A best model at epoch 1023 has been saved with training error 0.00000003647574.
Epoch 1023, Loss: 0.00000006999313, Improvement: -0.00000004080799, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1024
Epoch 1024, Loss: 0.00000006018922, Improvement: -0.00000000980392, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1025
Epoch 1025, Loss: 0.00000007418905, Improvement: 0.00000001399984, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1026
Epoch 1026, Loss: 0.00000007608499, Improvement: 0.00000000189594, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1027
Epoch 1027, Loss: 0.00000007066041, Improvement: -0.00000000542458, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1028
Epoch 1028, Loss: 0.00000014715104, Improvement: 0.00000007649063, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1029
Epoch 1029, Loss: 0.00000025598772, Improvement: 0.00000010883668, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1030
Epoch 1030, Loss: 0.00000013227264, Improvement: -0.00000012371508, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1031
Epoch 1031, Loss: 0.00000010438105, Improvement: -0.00000002789159, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1032
Epoch 1032, Loss: 0.00000011572275, Improvement: 0.00000001134170, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1033
Epoch 1033, Loss: 0.00000009177351, Improvement: -0.00000002394924, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1034
Epoch 1034, Loss: 0.00000009709651, Improvement: 0.00000000532301, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1035
Epoch 1035, Loss: 0.00000008403529, Improvement: -0.00000001306122, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1036
Epoch 1036, Loss: 0.00000009994974, Improvement: 0.00000001591445, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1037
Epoch 1037, Loss: 0.00000008738188, Improvement: -0.00000001256787, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1038
Epoch 1038, Loss: 0.00000007789726, Improvement: -0.00000000948462, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1039
Epoch 1039, Loss: 0.00000011186850, Improvement: 0.00000003397125, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1040
Epoch 1040, Loss: 0.00000017929932, Improvement: 0.00000006743082, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1041
Epoch 1041, Loss: 0.00000013488782, Improvement: -0.00000004441150, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1042
Epoch 1042, Loss: 0.00000007865577, Improvement: -0.00000005623205, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1043
Epoch 1043, Loss: 0.00000006379432, Improvement: -0.00000001486145, Best Loss: 0.00000003647574 in Epoch 1023
Epoch 1044
A best model at epoch 1044 has been saved with training error 0.00000003463942.
Epoch 1044, Loss: 0.00000005941639, Improvement: -0.00000000437792, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1045
Epoch 1045, Loss: 0.00000007017608, Improvement: 0.00000001075969, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1046
Epoch 1046, Loss: 0.00000008853750, Improvement: 0.00000001836142, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1047
Epoch 1047, Loss: 0.00000009158703, Improvement: 0.00000000304953, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1048
Epoch 1048, Loss: 0.00000014386647, Improvement: 0.00000005227944, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1049
Epoch 1049, Loss: 0.00000015046990, Improvement: 0.00000000660343, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000015719294, Improvement: 0.00000000672304, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1051
Epoch 1051, Loss: 0.00000008641363, Improvement: -0.00000007077930, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1052
Epoch 1052, Loss: 0.00000008603040, Improvement: -0.00000000038323, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1053
Epoch 1053, Loss: 0.00000010522854, Improvement: 0.00000001919814, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1054
Epoch 1054, Loss: 0.00000010401709, Improvement: -0.00000000121144, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1055
Epoch 1055, Loss: 0.00000008529649, Improvement: -0.00000001872061, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1056
Epoch 1056, Loss: 0.00000013864849, Improvement: 0.00000005335201, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1057
Epoch 1057, Loss: 0.00000014603097, Improvement: 0.00000000738247, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1058
Epoch 1058, Loss: 0.00000017655483, Improvement: 0.00000003052386, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1059
Epoch 1059, Loss: 0.00000013506826, Improvement: -0.00000004148657, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1060
Epoch 1060, Loss: 0.00000013436373, Improvement: -0.00000000070453, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1061
Epoch 1061, Loss: 0.00000013662823, Improvement: 0.00000000226450, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1062
Epoch 1062, Loss: 0.00000017334599, Improvement: 0.00000003671776, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1063
Epoch 1063, Loss: 0.00000016958426, Improvement: -0.00000000376173, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1064
Epoch 1064, Loss: 0.00000014444373, Improvement: -0.00000002514053, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1065
Epoch 1065, Loss: 0.00000009303078, Improvement: -0.00000005141295, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1066
Epoch 1066, Loss: 0.00000010341368, Improvement: 0.00000001038291, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1067
Epoch 1067, Loss: 0.00000009994989, Improvement: -0.00000000346379, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1068
Epoch 1068, Loss: 0.00000012270126, Improvement: 0.00000002275137, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1069
Epoch 1069, Loss: 0.00000009559796, Improvement: -0.00000002710329, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1070
Epoch 1070, Loss: 0.00000007339481, Improvement: -0.00000002220316, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1071
Epoch 1071, Loss: 0.00000007533010, Improvement: 0.00000000193529, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1072
Epoch 1072, Loss: 0.00000018640961, Improvement: 0.00000011107951, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1073
Epoch 1073, Loss: 0.00000024851876, Improvement: 0.00000006210916, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1074
Epoch 1074, Loss: 0.00000014884084, Improvement: -0.00000009967793, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1075
Epoch 1075, Loss: 0.00000014354163, Improvement: -0.00000000529921, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1076
Epoch 1076, Loss: 0.00000008772796, Improvement: -0.00000005581367, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1077
Epoch 1077, Loss: 0.00000008110393, Improvement: -0.00000000662403, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1078
Epoch 1078, Loss: 0.00000006252642, Improvement: -0.00000001857751, Best Loss: 0.00000003463942 in Epoch 1044
Epoch 1079
A best model at epoch 1079 has been saved with training error 0.00000002619056.
Epoch 1079, Loss: 0.00000005814337, Improvement: -0.00000000438304, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1080
Epoch 1080, Loss: 0.00000006041191, Improvement: 0.00000000226853, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1081
Epoch 1081, Loss: 0.00000007199829, Improvement: 0.00000001158638, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1082
Epoch 1082, Loss: 0.00000012832406, Improvement: 0.00000005632577, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1083
Epoch 1083, Loss: 0.00000014698521, Improvement: 0.00000001866115, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1084
Epoch 1084, Loss: 0.00000010933198, Improvement: -0.00000003765323, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1085
Epoch 1085, Loss: 0.00000007272472, Improvement: -0.00000003660726, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1086
Epoch 1086, Loss: 0.00000009897151, Improvement: 0.00000002624680, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1087
Epoch 1087, Loss: 0.00000012370675, Improvement: 0.00000002473524, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1088
Epoch 1088, Loss: 0.00000034170400, Improvement: 0.00000021799725, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1089
Epoch 1089, Loss: 0.00000014004451, Improvement: -0.00000020165949, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1090
Epoch 1090, Loss: 0.00000011347198, Improvement: -0.00000002657253, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1091
Epoch 1091, Loss: 0.00000019985941, Improvement: 0.00000008638743, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1092
Epoch 1092, Loss: 0.00000014522143, Improvement: -0.00000005463798, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1093
Epoch 1093, Loss: 0.00000009682609, Improvement: -0.00000004839534, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1094
Epoch 1094, Loss: 0.00000006101078, Improvement: -0.00000003581532, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1095
Epoch 1095, Loss: 0.00000006478027, Improvement: 0.00000000376949, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1096
Epoch 1096, Loss: 0.00000007983112, Improvement: 0.00000001505086, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1097
Epoch 1097, Loss: 0.00000007180455, Improvement: -0.00000000802658, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1098
Epoch 1098, Loss: 0.00000005831721, Improvement: -0.00000001348734, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1099
Epoch 1099, Loss: 0.00000007253766, Improvement: 0.00000001422045, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000006040454, Improvement: -0.00000001213312, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1101
Epoch 1101, Loss: 0.00000006853413, Improvement: 0.00000000812960, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1102
Epoch 1102, Loss: 0.00000012508886, Improvement: 0.00000005655473, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1103
Epoch 1103, Loss: 0.00000010780525, Improvement: -0.00000001728362, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1104
Epoch 1104, Loss: 0.00000008692000, Improvement: -0.00000002088525, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1105
Epoch 1105, Loss: 0.00000006951277, Improvement: -0.00000001740723, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1106
Epoch 1106, Loss: 0.00000008565635, Improvement: 0.00000001614358, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1107
Epoch 1107, Loss: 0.00000011661398, Improvement: 0.00000003095763, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1108
Epoch 1108, Loss: 0.00000010573276, Improvement: -0.00000001088122, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1109
Epoch 1109, Loss: 0.00000007122752, Improvement: -0.00000003450523, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1110
Epoch 1110, Loss: 0.00000007544198, Improvement: 0.00000000421446, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1111
Epoch 1111, Loss: 0.00000008859932, Improvement: 0.00000001315734, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1112
Epoch 1112, Loss: 0.00000013348343, Improvement: 0.00000004488410, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1113
Epoch 1113, Loss: 0.00000016305245, Improvement: 0.00000002956902, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1114
Epoch 1114, Loss: 0.00000017085115, Improvement: 0.00000000779870, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1115
Epoch 1115, Loss: 0.00000008969169, Improvement: -0.00000008115946, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1116
Epoch 1116, Loss: 0.00000006249671, Improvement: -0.00000002719498, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1117
Epoch 1117, Loss: 0.00000010304036, Improvement: 0.00000004054365, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1118
Epoch 1118, Loss: 0.00000016546253, Improvement: 0.00000006242217, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1119
Epoch 1119, Loss: 0.00000012693144, Improvement: -0.00000003853109, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1120
Epoch 1120, Loss: 0.00000010020688, Improvement: -0.00000002672456, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1121
Epoch 1121, Loss: 0.00000006902119, Improvement: -0.00000003118569, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1122
Epoch 1122, Loss: 0.00000007529562, Improvement: 0.00000000627443, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1123
Epoch 1123, Loss: 0.00000007069516, Improvement: -0.00000000460045, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1124
Epoch 1124, Loss: 0.00000006546751, Improvement: -0.00000000522766, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1125
Epoch 1125, Loss: 0.00000005763853, Improvement: -0.00000000782898, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1126
Epoch 1126, Loss: 0.00000009026787, Improvement: 0.00000003262935, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1127
Epoch 1127, Loss: 0.00000018650158, Improvement: 0.00000009623370, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1128
Epoch 1128, Loss: 0.00000025265380, Improvement: 0.00000006615222, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1129
Epoch 1129, Loss: 0.00000013572924, Improvement: -0.00000011692456, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1130
Epoch 1130, Loss: 0.00000015384482, Improvement: 0.00000001811558, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1131
Epoch 1131, Loss: 0.00000011617133, Improvement: -0.00000003767349, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1132
Epoch 1132, Loss: 0.00000009868970, Improvement: -0.00000001748163, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1133
Epoch 1133, Loss: 0.00000009070303, Improvement: -0.00000000798667, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1134
Epoch 1134, Loss: 0.00000008345657, Improvement: -0.00000000724646, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1135
Epoch 1135, Loss: 0.00000008101276, Improvement: -0.00000000244380, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1136
Epoch 1136, Loss: 0.00000006966241, Improvement: -0.00000001135035, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1137
Epoch 1137, Loss: 0.00000006341345, Improvement: -0.00000000624896, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1138
Epoch 1138, Loss: 0.00000008423442, Improvement: 0.00000002082097, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1139
Epoch 1139, Loss: 0.00000014057755, Improvement: 0.00000005634313, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1140
Epoch 1140, Loss: 0.00000010343621, Improvement: -0.00000003714133, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1141
Epoch 1141, Loss: 0.00000008606482, Improvement: -0.00000001737139, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1142
Epoch 1142, Loss: 0.00000017990375, Improvement: 0.00000009383892, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1143
Epoch 1143, Loss: 0.00000012998138, Improvement: -0.00000004992236, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1144
Epoch 1144, Loss: 0.00000016407298, Improvement: 0.00000003409159, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1145
Epoch 1145, Loss: 0.00000013672919, Improvement: -0.00000002734379, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1146
Epoch 1146, Loss: 0.00000013962768, Improvement: 0.00000000289849, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1147
Epoch 1147, Loss: 0.00000010661538, Improvement: -0.00000003301230, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1148
Epoch 1148, Loss: 0.00000010591784, Improvement: -0.00000000069754, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1149
Epoch 1149, Loss: 0.00000006349364, Improvement: -0.00000004242420, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000005315973, Improvement: -0.00000001033391, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1151
Epoch 1151, Loss: 0.00000005699229, Improvement: 0.00000000383256, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1152
Epoch 1152, Loss: 0.00000006517130, Improvement: 0.00000000817900, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1153
Epoch 1153, Loss: 0.00000006188976, Improvement: -0.00000000328153, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1154
Epoch 1154, Loss: 0.00000010993265, Improvement: 0.00000004804289, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1155
Epoch 1155, Loss: 0.00000024729208, Improvement: 0.00000013735943, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1156
Epoch 1156, Loss: 0.00000013398441, Improvement: -0.00000011330767, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1157
Epoch 1157, Loss: 0.00000009255440, Improvement: -0.00000004143001, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1158
Epoch 1158, Loss: 0.00000006554285, Improvement: -0.00000002701155, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1159
Epoch 1159, Loss: 0.00000004804294, Improvement: -0.00000001749991, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1160
Epoch 1160, Loss: 0.00000005350498, Improvement: 0.00000000546204, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1161
Epoch 1161, Loss: 0.00000004432007, Improvement: -0.00000000918491, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1162
Epoch 1162, Loss: 0.00000004855966, Improvement: 0.00000000423958, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1163
Epoch 1163, Loss: 0.00000006502923, Improvement: 0.00000001646957, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1164
Epoch 1164, Loss: 0.00000012550203, Improvement: 0.00000006047280, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1165
Epoch 1165, Loss: 0.00000017921179, Improvement: 0.00000005370977, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1166
Epoch 1166, Loss: 0.00000012250347, Improvement: -0.00000005670832, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1167
Epoch 1167, Loss: 0.00000006927622, Improvement: -0.00000005322726, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1168
Epoch 1168, Loss: 0.00000005327891, Improvement: -0.00000001599730, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1169
Epoch 1169, Loss: 0.00000005646100, Improvement: 0.00000000318209, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1170
Epoch 1170, Loss: 0.00000006262379, Improvement: 0.00000000616279, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1171
Epoch 1171, Loss: 0.00000010203631, Improvement: 0.00000003941252, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1172
Epoch 1172, Loss: 0.00000013120147, Improvement: 0.00000002916515, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1173
Epoch 1173, Loss: 0.00000026502722, Improvement: 0.00000013382576, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1174
Epoch 1174, Loss: 0.00000009418077, Improvement: -0.00000017084646, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1175
Epoch 1175, Loss: 0.00000006211874, Improvement: -0.00000003206203, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1176
Epoch 1176, Loss: 0.00000006342281, Improvement: 0.00000000130407, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1177
Epoch 1177, Loss: 0.00000006271602, Improvement: -0.00000000070679, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1178
Epoch 1178, Loss: 0.00000004757963, Improvement: -0.00000001513639, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1179
Epoch 1179, Loss: 0.00000005407819, Improvement: 0.00000000649856, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1180
Epoch 1180, Loss: 0.00000005121883, Improvement: -0.00000000285936, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1181
Epoch 1181, Loss: 0.00000004996044, Improvement: -0.00000000125839, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1182
Epoch 1182, Loss: 0.00000004942908, Improvement: -0.00000000053135, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1183
Epoch 1183, Loss: 0.00000005934353, Improvement: 0.00000000991445, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1184
Epoch 1184, Loss: 0.00000005992762, Improvement: 0.00000000058409, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1185
Epoch 1185, Loss: 0.00000006307917, Improvement: 0.00000000315154, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1186
Epoch 1186, Loss: 0.00000008524140, Improvement: 0.00000002216223, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1187
Epoch 1187, Loss: 0.00000009198228, Improvement: 0.00000000674088, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1188
Epoch 1188, Loss: 0.00000009303573, Improvement: 0.00000000105345, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1189
Epoch 1189, Loss: 0.00000008152901, Improvement: -0.00000001150672, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1190
Epoch 1190, Loss: 0.00000010701568, Improvement: 0.00000002548666, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1191
Epoch 1191, Loss: 0.00000009766418, Improvement: -0.00000000935150, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1192
Epoch 1192, Loss: 0.00000006970666, Improvement: -0.00000002795752, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1193
Epoch 1193, Loss: 0.00000010846517, Improvement: 0.00000003875851, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1194
Epoch 1194, Loss: 0.00000013499973, Improvement: 0.00000002653456, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1195
Epoch 1195, Loss: 0.00000024787672, Improvement: 0.00000011287699, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1196
Epoch 1196, Loss: 0.00000024442894, Improvement: -0.00000000344777, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1197
Epoch 1197, Loss: 0.00000022917398, Improvement: -0.00000001525496, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1198
Epoch 1198, Loss: 0.00000010283997, Improvement: -0.00000012633401, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1199
Epoch 1199, Loss: 0.00000007942160, Improvement: -0.00000002341837, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000004815525, Improvement: -0.00000003126635, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1201
Epoch 1201, Loss: 0.00000005231684, Improvement: 0.00000000416159, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1202
Epoch 1202, Loss: 0.00000004864414, Improvement: -0.00000000367270, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1203
Epoch 1203, Loss: 0.00000004321668, Improvement: -0.00000000542746, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1204
Epoch 1204, Loss: 0.00000004930993, Improvement: 0.00000000609325, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1205
Epoch 1205, Loss: 0.00000006791569, Improvement: 0.00000001860576, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1206
Epoch 1206, Loss: 0.00000006452213, Improvement: -0.00000000339356, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1207
Epoch 1207, Loss: 0.00000004358707, Improvement: -0.00000002093505, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1208
Epoch 1208, Loss: 0.00000005425220, Improvement: 0.00000001066513, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1209
Epoch 1209, Loss: 0.00000007221961, Improvement: 0.00000001796741, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1210
Epoch 1210, Loss: 0.00000012120692, Improvement: 0.00000004898731, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1211
Epoch 1211, Loss: 0.00000009980014, Improvement: -0.00000002140679, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1212
Epoch 1212, Loss: 0.00000015848398, Improvement: 0.00000005868385, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1213
Epoch 1213, Loss: 0.00000011342915, Improvement: -0.00000004505483, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1214
Epoch 1214, Loss: 0.00000016006853, Improvement: 0.00000004663938, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1215
Epoch 1215, Loss: 0.00000010794963, Improvement: -0.00000005211891, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1216
Epoch 1216, Loss: 0.00000007654263, Improvement: -0.00000003140700, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1217
Epoch 1217, Loss: 0.00000005787796, Improvement: -0.00000001866467, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1218
Epoch 1218, Loss: 0.00000005451905, Improvement: -0.00000000335892, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1219
Epoch 1219, Loss: 0.00000006127188, Improvement: 0.00000000675283, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1220
Epoch 1220, Loss: 0.00000004963675, Improvement: -0.00000001163513, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1221
Epoch 1221, Loss: 0.00000014906954, Improvement: 0.00000009943279, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1222
Epoch 1222, Loss: 0.00000018647349, Improvement: 0.00000003740395, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1223
Epoch 1223, Loss: 0.00000014269842, Improvement: -0.00000004377506, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1224
Epoch 1224, Loss: 0.00000009082441, Improvement: -0.00000005187402, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1225
Epoch 1225, Loss: 0.00000013094310, Improvement: 0.00000004011869, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1226
Epoch 1226, Loss: 0.00000010408951, Improvement: -0.00000002685359, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1227
Epoch 1227, Loss: 0.00000006440400, Improvement: -0.00000003968551, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1228
Epoch 1228, Loss: 0.00000005524098, Improvement: -0.00000000916301, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1229
Epoch 1229, Loss: 0.00000005967006, Improvement: 0.00000000442907, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1230
Epoch 1230, Loss: 0.00000005923137, Improvement: -0.00000000043869, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1231
Epoch 1231, Loss: 0.00000004246281, Improvement: -0.00000001676857, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1232
Epoch 1232, Loss: 0.00000005330682, Improvement: 0.00000001084401, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1233
Epoch 1233, Loss: 0.00000007669119, Improvement: 0.00000002338437, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1234
Epoch 1234, Loss: 0.00000009097786, Improvement: 0.00000001428667, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1235
Epoch 1235, Loss: 0.00000009334057, Improvement: 0.00000000236271, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1236
Epoch 1236, Loss: 0.00000015887850, Improvement: 0.00000006553793, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1237
Epoch 1237, Loss: 0.00000017040307, Improvement: 0.00000001152457, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1238
Epoch 1238, Loss: 0.00000017903708, Improvement: 0.00000000863401, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1239
Epoch 1239, Loss: 0.00000008595547, Improvement: -0.00000009308161, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1240
Epoch 1240, Loss: 0.00000007222857, Improvement: -0.00000001372689, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1241
Epoch 1241, Loss: 0.00000005050046, Improvement: -0.00000002172811, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1242
Epoch 1242, Loss: 0.00000004543917, Improvement: -0.00000000506129, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1243
Epoch 1243, Loss: 0.00000005342445, Improvement: 0.00000000798528, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1244
Epoch 1244, Loss: 0.00000005944232, Improvement: 0.00000000601787, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1245
Epoch 1245, Loss: 0.00000005787593, Improvement: -0.00000000156639, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1246
Epoch 1246, Loss: 0.00000004992396, Improvement: -0.00000000795198, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1247
Epoch 1247, Loss: 0.00000005378713, Improvement: 0.00000000386317, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1248
Epoch 1248, Loss: 0.00000004364116, Improvement: -0.00000001014597, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1249
Epoch 1249, Loss: 0.00000006568864, Improvement: 0.00000002204748, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000006869611, Improvement: 0.00000000300746, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1251
Epoch 1251, Loss: 0.00000015785081, Improvement: 0.00000008915470, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1252
Epoch 1252, Loss: 0.00000018281999, Improvement: 0.00000002496918, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1253
Epoch 1253, Loss: 0.00000010225348, Improvement: -0.00000008056651, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1254
Epoch 1254, Loss: 0.00000005722636, Improvement: -0.00000004502712, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1255
Epoch 1255, Loss: 0.00000005149876, Improvement: -0.00000000572760, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1256
Epoch 1256, Loss: 0.00000005994135, Improvement: 0.00000000844259, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1257
Epoch 1257, Loss: 0.00000005157323, Improvement: -0.00000000836812, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1258
Epoch 1258, Loss: 0.00000003885241, Improvement: -0.00000001272082, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1259
Epoch 1259, Loss: 0.00000003949675, Improvement: 0.00000000064434, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1260
Epoch 1260, Loss: 0.00000004183726, Improvement: 0.00000000234051, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1261
Epoch 1261, Loss: 0.00000005666579, Improvement: 0.00000001482853, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1262
Epoch 1262, Loss: 0.00000011085898, Improvement: 0.00000005419320, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1263
Epoch 1263, Loss: 0.00000008141919, Improvement: -0.00000002943979, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1264
Epoch 1264, Loss: 0.00000006725873, Improvement: -0.00000001416046, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1265
Epoch 1265, Loss: 0.00000005516258, Improvement: -0.00000001209615, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1266
Epoch 1266, Loss: 0.00000006483852, Improvement: 0.00000000967594, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1267
Epoch 1267, Loss: 0.00000013066859, Improvement: 0.00000006583007, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1268
Epoch 1268, Loss: 0.00000020849095, Improvement: 0.00000007782236, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1269
Epoch 1269, Loss: 0.00000010968951, Improvement: -0.00000009880144, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1270
Epoch 1270, Loss: 0.00000007180891, Improvement: -0.00000003788060, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1271
Epoch 1271, Loss: 0.00000006266094, Improvement: -0.00000000914797, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1272
Epoch 1272, Loss: 0.00000008057588, Improvement: 0.00000001791494, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1273
Epoch 1273, Loss: 0.00000008255720, Improvement: 0.00000000198132, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1274
Epoch 1274, Loss: 0.00000007587259, Improvement: -0.00000000668461, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1275
Epoch 1275, Loss: 0.00000005593587, Improvement: -0.00000001993672, Best Loss: 0.00000002619056 in Epoch 1079
Epoch 1276
A best model at epoch 1276 has been saved with training error 0.00000002387388.
Epoch 1276, Loss: 0.00000004789716, Improvement: -0.00000000803871, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1277
Epoch 1277, Loss: 0.00000004465037, Improvement: -0.00000000324679, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1278
Epoch 1278, Loss: 0.00000005219529, Improvement: 0.00000000754492, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1279
Epoch 1279, Loss: 0.00000007213458, Improvement: 0.00000001993928, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1280
Epoch 1280, Loss: 0.00000007000629, Improvement: -0.00000000212828, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1281
Epoch 1281, Loss: 0.00000007697779, Improvement: 0.00000000697150, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1282
Epoch 1282, Loss: 0.00000020238322, Improvement: 0.00000012540543, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1283
Epoch 1283, Loss: 0.00000017044900, Improvement: -0.00000003193422, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1284
Epoch 1284, Loss: 0.00000008296061, Improvement: -0.00000008748838, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1285
Epoch 1285, Loss: 0.00000009864923, Improvement: 0.00000001568862, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1286
Epoch 1286, Loss: 0.00000013106355, Improvement: 0.00000003241432, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1287
Epoch 1287, Loss: 0.00000011633198, Improvement: -0.00000001473158, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1288
Epoch 1288, Loss: 0.00000006303879, Improvement: -0.00000005329319, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1289
Epoch 1289, Loss: 0.00000005699307, Improvement: -0.00000000604572, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1290
Epoch 1290, Loss: 0.00000005463411, Improvement: -0.00000000235896, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1291
Epoch 1291, Loss: 0.00000005375116, Improvement: -0.00000000088295, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1292
Epoch 1292, Loss: 0.00000008504249, Improvement: 0.00000003129133, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1293
Epoch 1293, Loss: 0.00000008217485, Improvement: -0.00000000286764, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1294
Epoch 1294, Loss: 0.00000010165136, Improvement: 0.00000001947651, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1295
Epoch 1295, Loss: 0.00000010904513, Improvement: 0.00000000739378, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1296
Epoch 1296, Loss: 0.00000025954264, Improvement: 0.00000015049751, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1297
Epoch 1297, Loss: 0.00000019998536, Improvement: -0.00000005955728, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1298
Epoch 1298, Loss: 0.00000010176739, Improvement: -0.00000009821797, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1299
Epoch 1299, Loss: 0.00000007666497, Improvement: -0.00000002510242, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000008082385, Improvement: 0.00000000415888, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1301
Epoch 1301, Loss: 0.00000005393298, Improvement: -0.00000002689087, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1302
Epoch 1302, Loss: 0.00000004808801, Improvement: -0.00000000584497, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1303
Epoch 1303, Loss: 0.00000004498244, Improvement: -0.00000000310557, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1304
Epoch 1304, Loss: 0.00000003868745, Improvement: -0.00000000629498, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1305
Epoch 1305, Loss: 0.00000003968800, Improvement: 0.00000000100054, Best Loss: 0.00000002387388 in Epoch 1276
Epoch 1306
A best model at epoch 1306 has been saved with training error 0.00000002380122.
Epoch 1306, Loss: 0.00000003756536, Improvement: -0.00000000212263, Best Loss: 0.00000002380122 in Epoch 1306
Epoch 1307
Epoch 1307, Loss: 0.00000004131790, Improvement: 0.00000000375254, Best Loss: 0.00000002380122 in Epoch 1306
Epoch 1308
Epoch 1308, Loss: 0.00000004239808, Improvement: 0.00000000108018, Best Loss: 0.00000002380122 in Epoch 1306
Epoch 1309
A best model at epoch 1309 has been saved with training error 0.00000002254287.
Epoch 1309, Loss: 0.00000003813949, Improvement: -0.00000000425859, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1310
Epoch 1310, Loss: 0.00000003676358, Improvement: -0.00000000137591, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1311
Epoch 1311, Loss: 0.00000005225930, Improvement: 0.00000001549572, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1312
Epoch 1312, Loss: 0.00000005221164, Improvement: -0.00000000004767, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1313
Epoch 1313, Loss: 0.00000004095239, Improvement: -0.00000001125925, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1314
Epoch 1314, Loss: 0.00000004230855, Improvement: 0.00000000135617, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1315
Epoch 1315, Loss: 0.00000005211771, Improvement: 0.00000000980916, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1316
Epoch 1316, Loss: 0.00000008024689, Improvement: 0.00000002812917, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1317
Epoch 1317, Loss: 0.00000010459575, Improvement: 0.00000002434886, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1318
Epoch 1318, Loss: 0.00000006525536, Improvement: -0.00000003934039, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1319
Epoch 1319, Loss: 0.00000005268547, Improvement: -0.00000001256989, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1320
Epoch 1320, Loss: 0.00000009251123, Improvement: 0.00000003982576, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1321
Epoch 1321, Loss: 0.00000022603946, Improvement: 0.00000013352823, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1322
Epoch 1322, Loss: 0.00000013368859, Improvement: -0.00000009235087, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1323
Epoch 1323, Loss: 0.00000009109226, Improvement: -0.00000004259633, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1324
Epoch 1324, Loss: 0.00000006733895, Improvement: -0.00000002375331, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1325
Epoch 1325, Loss: 0.00000007970930, Improvement: 0.00000001237035, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1326
Epoch 1326, Loss: 0.00000006286196, Improvement: -0.00000001684734, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1327
Epoch 1327, Loss: 0.00000005404003, Improvement: -0.00000000882192, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1328
Epoch 1328, Loss: 0.00000004809166, Improvement: -0.00000000594838, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1329
Epoch 1329, Loss: 0.00000005392383, Improvement: 0.00000000583217, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1330
Epoch 1330, Loss: 0.00000004778943, Improvement: -0.00000000613440, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1331
Epoch 1331, Loss: 0.00000004756460, Improvement: -0.00000000022483, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1332
Epoch 1332, Loss: 0.00000007510255, Improvement: 0.00000002753796, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1333
Epoch 1333, Loss: 0.00000008520725, Improvement: 0.00000001010470, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1334
Epoch 1334, Loss: 0.00000007141169, Improvement: -0.00000001379556, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1335
Epoch 1335, Loss: 0.00000003922814, Improvement: -0.00000003218356, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1336
Epoch 1336, Loss: 0.00000004195928, Improvement: 0.00000000273115, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1337
Epoch 1337, Loss: 0.00000003634617, Improvement: -0.00000000561311, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1338
Epoch 1338, Loss: 0.00000007061976, Improvement: 0.00000003427359, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1339
Epoch 1339, Loss: 0.00000019388353, Improvement: 0.00000012326377, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1340
Epoch 1340, Loss: 0.00000020053154, Improvement: 0.00000000664801, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1341
Epoch 1341, Loss: 0.00000011796215, Improvement: -0.00000008256939, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1342
Epoch 1342, Loss: 0.00000010849447, Improvement: -0.00000000946768, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1343
Epoch 1343, Loss: 0.00000008285532, Improvement: -0.00000002563916, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1344
Epoch 1344, Loss: 0.00000006137571, Improvement: -0.00000002147960, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1345
Epoch 1345, Loss: 0.00000004434133, Improvement: -0.00000001703439, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1346
Epoch 1346, Loss: 0.00000003943377, Improvement: -0.00000000490756, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1347
Epoch 1347, Loss: 0.00000003966269, Improvement: 0.00000000022892, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1348
Epoch 1348, Loss: 0.00000003871022, Improvement: -0.00000000095247, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1349
Epoch 1349, Loss: 0.00000004945912, Improvement: 0.00000001074890, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000004049195, Improvement: -0.00000000896717, Best Loss: 0.00000002254287 in Epoch 1309
Epoch 1351
A best model at epoch 1351 has been saved with training error 0.00000002153576.
Epoch 1351, Loss: 0.00000003408032, Improvement: -0.00000000641163, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1352
Epoch 1352, Loss: 0.00000003975671, Improvement: 0.00000000567639, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1353
Epoch 1353, Loss: 0.00000003659678, Improvement: -0.00000000315993, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1354
Epoch 1354, Loss: 0.00000003606503, Improvement: -0.00000000053175, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1355
Epoch 1355, Loss: 0.00000004241046, Improvement: 0.00000000634543, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1356
Epoch 1356, Loss: 0.00000007732976, Improvement: 0.00000003491931, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1357
Epoch 1357, Loss: 0.00000008590804, Improvement: 0.00000000857828, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1358
Epoch 1358, Loss: 0.00000013935410, Improvement: 0.00000005344606, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1359
Epoch 1359, Loss: 0.00000017257276, Improvement: 0.00000003321866, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1360
Epoch 1360, Loss: 0.00000016680272, Improvement: -0.00000000577004, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1361
Epoch 1361, Loss: 0.00000009615215, Improvement: -0.00000007065057, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1362
Epoch 1362, Loss: 0.00000006218283, Improvement: -0.00000003396932, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1363
Epoch 1363, Loss: 0.00000006307243, Improvement: 0.00000000088960, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1364
Epoch 1364, Loss: 0.00000005348297, Improvement: -0.00000000958946, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1365
Epoch 1365, Loss: 0.00000004840658, Improvement: -0.00000000507640, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1366
Epoch 1366, Loss: 0.00000004611947, Improvement: -0.00000000228710, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1367
Epoch 1367, Loss: 0.00000005135338, Improvement: 0.00000000523391, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1368
Epoch 1368, Loss: 0.00000004548140, Improvement: -0.00000000587198, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1369
Epoch 1369, Loss: 0.00000003612883, Improvement: -0.00000000935257, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1370
Epoch 1370, Loss: 0.00000004223890, Improvement: 0.00000000611007, Best Loss: 0.00000002153576 in Epoch 1351
Epoch 1371
A best model at epoch 1371 has been saved with training error 0.00000002104619.
Epoch 1371, Loss: 0.00000003207873, Improvement: -0.00000001016017, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1372
Epoch 1372, Loss: 0.00000003991707, Improvement: 0.00000000783834, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1373
Epoch 1373, Loss: 0.00000007269997, Improvement: 0.00000003278289, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1374
Epoch 1374, Loss: 0.00000011715253, Improvement: 0.00000004445256, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1375
Epoch 1375, Loss: 0.00000008721467, Improvement: -0.00000002993785, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1376
Epoch 1376, Loss: 0.00000005224659, Improvement: -0.00000003496809, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1377
Epoch 1377, Loss: 0.00000005416084, Improvement: 0.00000000191425, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1378
Epoch 1378, Loss: 0.00000005584661, Improvement: 0.00000000168577, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1379
Epoch 1379, Loss: 0.00000004216854, Improvement: -0.00000001367807, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1380
Epoch 1380, Loss: 0.00000003137770, Improvement: -0.00000001079084, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1381
Epoch 1381, Loss: 0.00000003261628, Improvement: 0.00000000123858, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1382
Epoch 1382, Loss: 0.00000004008264, Improvement: 0.00000000746635, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1383
Epoch 1383, Loss: 0.00000004859556, Improvement: 0.00000000851292, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1384
Epoch 1384, Loss: 0.00000005253113, Improvement: 0.00000000393557, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1385
Epoch 1385, Loss: 0.00000006006503, Improvement: 0.00000000753390, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1386
Epoch 1386, Loss: 0.00000006948682, Improvement: 0.00000000942179, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1387
Epoch 1387, Loss: 0.00000017247608, Improvement: 0.00000010298926, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1388
Epoch 1388, Loss: 0.00000021261199, Improvement: 0.00000004013591, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1389
Epoch 1389, Loss: 0.00000013941766, Improvement: -0.00000007319433, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1390
Epoch 1390, Loss: 0.00000007689456, Improvement: -0.00000006252310, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1391
Epoch 1391, Loss: 0.00000005174561, Improvement: -0.00000002514895, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1392
Epoch 1392, Loss: 0.00000005176244, Improvement: 0.00000000001683, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1393
Epoch 1393, Loss: 0.00000004705576, Improvement: -0.00000000470668, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1394
Epoch 1394, Loss: 0.00000004705916, Improvement: 0.00000000000339, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1395
Epoch 1395, Loss: 0.00000007748513, Improvement: 0.00000003042597, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1396
Epoch 1396, Loss: 0.00000008674911, Improvement: 0.00000000926399, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1397
Epoch 1397, Loss: 0.00000011971970, Improvement: 0.00000003297059, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1398
Epoch 1398, Loss: 0.00000011952775, Improvement: -0.00000000019195, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1399
Epoch 1399, Loss: 0.00000007150633, Improvement: -0.00000004802142, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000004265350, Improvement: -0.00000002885283, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1401
Epoch 1401, Loss: 0.00000004486430, Improvement: 0.00000000221080, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1402
Epoch 1402, Loss: 0.00000004323248, Improvement: -0.00000000163182, Best Loss: 0.00000002104619 in Epoch 1371
Epoch 1403
A best model at epoch 1403 has been saved with training error 0.00000001664580.
Epoch 1403, Loss: 0.00000003444512, Improvement: -0.00000000878737, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1404
Epoch 1404, Loss: 0.00000003985963, Improvement: 0.00000000541452, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1405
Epoch 1405, Loss: 0.00000005994164, Improvement: 0.00000002008201, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1406
Epoch 1406, Loss: 0.00000006958589, Improvement: 0.00000000964425, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1407
Epoch 1407, Loss: 0.00000006562482, Improvement: -0.00000000396107, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1408
Epoch 1408, Loss: 0.00000007573938, Improvement: 0.00000001011456, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1409
Epoch 1409, Loss: 0.00000019315971, Improvement: 0.00000011742033, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1410
Epoch 1410, Loss: 0.00000014598626, Improvement: -0.00000004717345, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1411
Epoch 1411, Loss: 0.00000008174906, Improvement: -0.00000006423720, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1412
Epoch 1412, Loss: 0.00000006743993, Improvement: -0.00000001430913, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1413
Epoch 1413, Loss: 0.00000004897581, Improvement: -0.00000001846411, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1414
Epoch 1414, Loss: 0.00000003730882, Improvement: -0.00000001166699, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1415
Epoch 1415, Loss: 0.00000003415220, Improvement: -0.00000000315662, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1416
Epoch 1416, Loss: 0.00000003432870, Improvement: 0.00000000017650, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1417
Epoch 1417, Loss: 0.00000006503179, Improvement: 0.00000003070309, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1418
Epoch 1418, Loss: 0.00000005151721, Improvement: -0.00000001351458, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1419
Epoch 1419, Loss: 0.00000004400714, Improvement: -0.00000000751007, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1420
Epoch 1420, Loss: 0.00000003274440, Improvement: -0.00000001126274, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1421
Epoch 1421, Loss: 0.00000004189614, Improvement: 0.00000000915173, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1422
Epoch 1422, Loss: 0.00000003958306, Improvement: -0.00000000231308, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1423
Epoch 1423, Loss: 0.00000006296987, Improvement: 0.00000002338681, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1424
Epoch 1424, Loss: 0.00000011943142, Improvement: 0.00000005646155, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1425
Epoch 1425, Loss: 0.00000008815478, Improvement: -0.00000003127664, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1426
Epoch 1426, Loss: 0.00000007903418, Improvement: -0.00000000912059, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1427
Epoch 1427, Loss: 0.00000006504811, Improvement: -0.00000001398607, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1428
Epoch 1428, Loss: 0.00000010401748, Improvement: 0.00000003896936, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1429
Epoch 1429, Loss: 0.00000012506058, Improvement: 0.00000002104311, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1430
Epoch 1430, Loss: 0.00000018526811, Improvement: 0.00000006020753, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1431
Epoch 1431, Loss: 0.00000009422790, Improvement: -0.00000009104021, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1432
Epoch 1432, Loss: 0.00000005585614, Improvement: -0.00000003837176, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1433
Epoch 1433, Loss: 0.00000004988860, Improvement: -0.00000000596754, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1434
Epoch 1434, Loss: 0.00000005655513, Improvement: 0.00000000666653, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1435
Epoch 1435, Loss: 0.00000008637192, Improvement: 0.00000002981680, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1436
Epoch 1436, Loss: 0.00000008998395, Improvement: 0.00000000361203, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1437
Epoch 1437, Loss: 0.00000008095111, Improvement: -0.00000000903284, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1438
Epoch 1438, Loss: 0.00000007229934, Improvement: -0.00000000865177, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1439
Epoch 1439, Loss: 0.00000006133232, Improvement: -0.00000001096702, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1440
Epoch 1440, Loss: 0.00000008253918, Improvement: 0.00000002120686, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1441
Epoch 1441, Loss: 0.00000006388624, Improvement: -0.00000001865294, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1442
Epoch 1442, Loss: 0.00000005547059, Improvement: -0.00000000841565, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1443
Epoch 1443, Loss: 0.00000006557307, Improvement: 0.00000001010248, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1444
Epoch 1444, Loss: 0.00000009026413, Improvement: 0.00000002469106, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1445
Epoch 1445, Loss: 0.00000012726198, Improvement: 0.00000003699785, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1446
Epoch 1446, Loss: 0.00000009603757, Improvement: -0.00000003122441, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1447
Epoch 1447, Loss: 0.00000010338240, Improvement: 0.00000000734482, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1448
Epoch 1448, Loss: 0.00000014176235, Improvement: 0.00000003837995, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1449
Epoch 1449, Loss: 0.00000015317690, Improvement: 0.00000001141455, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000011179172, Improvement: -0.00000004138518, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1451
Epoch 1451, Loss: 0.00000006541180, Improvement: -0.00000004637992, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1452
Epoch 1452, Loss: 0.00000012536754, Improvement: 0.00000005995574, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1453
Epoch 1453, Loss: 0.00000013714464, Improvement: 0.00000001177709, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1454
Epoch 1454, Loss: 0.00000007227078, Improvement: -0.00000006487386, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1455
Epoch 1455, Loss: 0.00000005251623, Improvement: -0.00000001975454, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1456
Epoch 1456, Loss: 0.00000005728969, Improvement: 0.00000000477345, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1457
Epoch 1457, Loss: 0.00000005410149, Improvement: -0.00000000318819, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1458
Epoch 1458, Loss: 0.00000004498569, Improvement: -0.00000000911581, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1459
Epoch 1459, Loss: 0.00000004625856, Improvement: 0.00000000127287, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1460
Epoch 1460, Loss: 0.00000003722738, Improvement: -0.00000000903117, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1461
Epoch 1461, Loss: 0.00000004140801, Improvement: 0.00000000418063, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1462
Epoch 1462, Loss: 0.00000005146492, Improvement: 0.00000001005691, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1463
Epoch 1463, Loss: 0.00000007993902, Improvement: 0.00000002847410, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1464
Epoch 1464, Loss: 0.00000009647969, Improvement: 0.00000001654067, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1465
Epoch 1465, Loss: 0.00000009597230, Improvement: -0.00000000050739, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1466
Epoch 1466, Loss: 0.00000008888488, Improvement: -0.00000000708742, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1467
Epoch 1467, Loss: 0.00000012793881, Improvement: 0.00000003905393, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1468
Epoch 1468, Loss: 0.00000009747441, Improvement: -0.00000003046440, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1469
Epoch 1469, Loss: 0.00000008080869, Improvement: -0.00000001666572, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1470
Epoch 1470, Loss: 0.00000005953676, Improvement: -0.00000002127193, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1471
Epoch 1471, Loss: 0.00000006377918, Improvement: 0.00000000424242, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1472
Epoch 1472, Loss: 0.00000004879149, Improvement: -0.00000001498769, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1473
Epoch 1473, Loss: 0.00000004601195, Improvement: -0.00000000277954, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1474
Epoch 1474, Loss: 0.00000004017774, Improvement: -0.00000000583420, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1475
Epoch 1475, Loss: 0.00000003405256, Improvement: -0.00000000612519, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1476
Epoch 1476, Loss: 0.00000004324007, Improvement: 0.00000000918752, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1477
Epoch 1477, Loss: 0.00000005051546, Improvement: 0.00000000727539, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1478
Epoch 1478, Loss: 0.00000006690872, Improvement: 0.00000001639326, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1479
Epoch 1479, Loss: 0.00000007252510, Improvement: 0.00000000561638, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1480
Epoch 1480, Loss: 0.00000004719166, Improvement: -0.00000002533344, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1481
Epoch 1481, Loss: 0.00000010893846, Improvement: 0.00000006174680, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1482
Epoch 1482, Loss: 0.00000010159452, Improvement: -0.00000000734394, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1483
Epoch 1483, Loss: 0.00000006963252, Improvement: -0.00000003196200, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1484
Epoch 1484, Loss: 0.00000011099440, Improvement: 0.00000004136189, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1485
Epoch 1485, Loss: 0.00000009181036, Improvement: -0.00000001918404, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1486
Epoch 1486, Loss: 0.00000008382391, Improvement: -0.00000000798646, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1487
Epoch 1487, Loss: 0.00000005445251, Improvement: -0.00000002937139, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1488
Epoch 1488, Loss: 0.00000004021765, Improvement: -0.00000001423487, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1489
Epoch 1489, Loss: 0.00000006236759, Improvement: 0.00000002214995, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1490
Epoch 1490, Loss: 0.00000010463937, Improvement: 0.00000004227177, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1491
Epoch 1491, Loss: 0.00000008047378, Improvement: -0.00000002416559, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1492
Epoch 1492, Loss: 0.00000011611520, Improvement: 0.00000003564142, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1493
Epoch 1493, Loss: 0.00000022923564, Improvement: 0.00000011312044, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1494
Epoch 1494, Loss: 0.00000011084238, Improvement: -0.00000011839326, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1495
Epoch 1495, Loss: 0.00000005804053, Improvement: -0.00000005280185, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1496
Epoch 1496, Loss: 0.00000004111855, Improvement: -0.00000001692198, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1497
Epoch 1497, Loss: 0.00000003544559, Improvement: -0.00000000567295, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1498
Epoch 1498, Loss: 0.00000003630921, Improvement: 0.00000000086362, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1499
Epoch 1499, Loss: 0.00000004544344, Improvement: 0.00000000913422, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000006095997, Improvement: 0.00000001551653, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1501
Epoch 1501, Loss: 0.00000003803240, Improvement: -0.00000002292757, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1502
Epoch 1502, Loss: 0.00000003471486, Improvement: -0.00000000331754, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1503
Epoch 1503, Loss: 0.00000003147439, Improvement: -0.00000000324047, Best Loss: 0.00000001664580 in Epoch 1403
Epoch 1504
A best model at epoch 1504 has been saved with training error 0.00000001657876.
Epoch 1504, Loss: 0.00000003031413, Improvement: -0.00000000116026, Best Loss: 0.00000001657876 in Epoch 1504
Epoch 1505
Epoch 1505, Loss: 0.00000002767013, Improvement: -0.00000000264400, Best Loss: 0.00000001657876 in Epoch 1504
Epoch 1506
Epoch 1506, Loss: 0.00000002877754, Improvement: 0.00000000110741, Best Loss: 0.00000001657876 in Epoch 1504
Epoch 1507
Epoch 1507, Loss: 0.00000002716129, Improvement: -0.00000000161624, Best Loss: 0.00000001657876 in Epoch 1504
Epoch 1508
A best model at epoch 1508 has been saved with training error 0.00000001585617.
Epoch 1508, Loss: 0.00000002470467, Improvement: -0.00000000245662, Best Loss: 0.00000001585617 in Epoch 1508
Epoch 1509
Epoch 1509, Loss: 0.00000002633190, Improvement: 0.00000000162723, Best Loss: 0.00000001585617 in Epoch 1508
Epoch 1510
Epoch 1510, Loss: 0.00000003029785, Improvement: 0.00000000396595, Best Loss: 0.00000001585617 in Epoch 1508
Epoch 1511
A best model at epoch 1511 has been saved with training error 0.00000001472454.
Epoch 1511, Loss: 0.00000003607533, Improvement: 0.00000000577748, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1512
Epoch 1512, Loss: 0.00000004346136, Improvement: 0.00000000738603, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1513
Epoch 1513, Loss: 0.00000004801635, Improvement: 0.00000000455499, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1514
Epoch 1514, Loss: 0.00000008185084, Improvement: 0.00000003383448, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1515
Epoch 1515, Loss: 0.00000007345423, Improvement: -0.00000000839661, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1516
Epoch 1516, Loss: 0.00000007427553, Improvement: 0.00000000082130, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1517
Epoch 1517, Loss: 0.00000005457236, Improvement: -0.00000001970316, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1518
Epoch 1518, Loss: 0.00000004307735, Improvement: -0.00000001149501, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1519
Epoch 1519, Loss: 0.00000003697994, Improvement: -0.00000000609741, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1520
Epoch 1520, Loss: 0.00000004267386, Improvement: 0.00000000569392, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1521
Epoch 1521, Loss: 0.00000006043793, Improvement: 0.00000001776407, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1522
Epoch 1522, Loss: 0.00000010360101, Improvement: 0.00000004316309, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1523
Epoch 1523, Loss: 0.00000006429579, Improvement: -0.00000003930522, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1524
Epoch 1524, Loss: 0.00000006216085, Improvement: -0.00000000213494, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1525
Epoch 1525, Loss: 0.00000011086992, Improvement: 0.00000004870907, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1526
Epoch 1526, Loss: 0.00000017404800, Improvement: 0.00000006317808, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1527
Epoch 1527, Loss: 0.00000021395461, Improvement: 0.00000003990661, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1528
Epoch 1528, Loss: 0.00000008709287, Improvement: -0.00000012686175, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1529
Epoch 1529, Loss: 0.00000006236561, Improvement: -0.00000002472726, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1530
Epoch 1530, Loss: 0.00000004978957, Improvement: -0.00000001257604, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1531
Epoch 1531, Loss: 0.00000004382013, Improvement: -0.00000000596943, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1532
Epoch 1532, Loss: 0.00000003847598, Improvement: -0.00000000534415, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1533
Epoch 1533, Loss: 0.00000003813400, Improvement: -0.00000000034199, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1534
Epoch 1534, Loss: 0.00000003908788, Improvement: 0.00000000095389, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1535
Epoch 1535, Loss: 0.00000003138350, Improvement: -0.00000000770439, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1536
Epoch 1536, Loss: 0.00000003253190, Improvement: 0.00000000114840, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1537
Epoch 1537, Loss: 0.00000003001392, Improvement: -0.00000000251798, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1538
Epoch 1538, Loss: 0.00000003056948, Improvement: 0.00000000055555, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1539
Epoch 1539, Loss: 0.00000003273494, Improvement: 0.00000000216546, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1540
Epoch 1540, Loss: 0.00000003475513, Improvement: 0.00000000202019, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1541
Epoch 1541, Loss: 0.00000003705413, Improvement: 0.00000000229900, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1542
Epoch 1542, Loss: 0.00000003879247, Improvement: 0.00000000173834, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1543
Epoch 1543, Loss: 0.00000004903241, Improvement: 0.00000001023994, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1544
Epoch 1544, Loss: 0.00000011066738, Improvement: 0.00000006163497, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1545
Epoch 1545, Loss: 0.00000011487691, Improvement: 0.00000000420953, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1546
Epoch 1546, Loss: 0.00000007734037, Improvement: -0.00000003753654, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1547
Epoch 1547, Loss: 0.00000015953308, Improvement: 0.00000008219272, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1548
Epoch 1548, Loss: 0.00000018501128, Improvement: 0.00000002547820, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1549
Epoch 1549, Loss: 0.00000007773576, Improvement: -0.00000010727552, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000004353803, Improvement: -0.00000003419773, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1551
Epoch 1551, Loss: 0.00000002973107, Improvement: -0.00000001380696, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1552
Epoch 1552, Loss: 0.00000002851013, Improvement: -0.00000000122094, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1553
Epoch 1553, Loss: 0.00000002774916, Improvement: -0.00000000076096, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1554
Epoch 1554, Loss: 0.00000002566738, Improvement: -0.00000000208178, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1555
Epoch 1555, Loss: 0.00000002854060, Improvement: 0.00000000287322, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1556
Epoch 1556, Loss: 0.00000003289515, Improvement: 0.00000000435455, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1557
Epoch 1557, Loss: 0.00000006958926, Improvement: 0.00000003669411, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1558
Epoch 1558, Loss: 0.00000005819336, Improvement: -0.00000001139590, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1559
Epoch 1559, Loss: 0.00000004556803, Improvement: -0.00000001262533, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1560
Epoch 1560, Loss: 0.00000003618579, Improvement: -0.00000000938224, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1561
Epoch 1561, Loss: 0.00000003361955, Improvement: -0.00000000256624, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1562
Epoch 1562, Loss: 0.00000003326934, Improvement: -0.00000000035020, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1563
Epoch 1563, Loss: 0.00000003826111, Improvement: 0.00000000499177, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1564
Epoch 1564, Loss: 0.00000003840593, Improvement: 0.00000000014482, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1565
Epoch 1565, Loss: 0.00000003391110, Improvement: -0.00000000449483, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1566
Epoch 1566, Loss: 0.00000003175012, Improvement: -0.00000000216098, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1567
Epoch 1567, Loss: 0.00000003758533, Improvement: 0.00000000583520, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1568
Epoch 1568, Loss: 0.00000005191056, Improvement: 0.00000001432524, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1569
Epoch 1569, Loss: 0.00000004022542, Improvement: -0.00000001168514, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1570
Epoch 1570, Loss: 0.00000010160648, Improvement: 0.00000006138105, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1571
Epoch 1571, Loss: 0.00000009536993, Improvement: -0.00000000623655, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1572
Epoch 1572, Loss: 0.00000010045475, Improvement: 0.00000000508482, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1573
Epoch 1573, Loss: 0.00000009363498, Improvement: -0.00000000681976, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1574
Epoch 1574, Loss: 0.00000012136470, Improvement: 0.00000002772971, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1575
Epoch 1575, Loss: 0.00000017436086, Improvement: 0.00000005299616, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1576
Epoch 1576, Loss: 0.00000018771478, Improvement: 0.00000001335392, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1577
Epoch 1577, Loss: 0.00000010688443, Improvement: -0.00000008083035, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1578
Epoch 1578, Loss: 0.00000005172856, Improvement: -0.00000005515588, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1579
Epoch 1579, Loss: 0.00000004754717, Improvement: -0.00000000418138, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1580
Epoch 1580, Loss: 0.00000003656495, Improvement: -0.00000001098223, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1581
Epoch 1581, Loss: 0.00000003303573, Improvement: -0.00000000352922, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1582
Epoch 1582, Loss: 0.00000003135665, Improvement: -0.00000000167908, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1583
Epoch 1583, Loss: 0.00000003037184, Improvement: -0.00000000098481, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1584
Epoch 1584, Loss: 0.00000002773836, Improvement: -0.00000000263348, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1585
Epoch 1585, Loss: 0.00000002859239, Improvement: 0.00000000085403, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1586
Epoch 1586, Loss: 0.00000002445934, Improvement: -0.00000000413305, Best Loss: 0.00000001472454 in Epoch 1511
Epoch 1587
A best model at epoch 1587 has been saved with training error 0.00000001318140.
Epoch 1587, Loss: 0.00000002521043, Improvement: 0.00000000075109, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1588
Epoch 1588, Loss: 0.00000003020756, Improvement: 0.00000000499713, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1589
Epoch 1589, Loss: 0.00000003783755, Improvement: 0.00000000762999, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1590
Epoch 1590, Loss: 0.00000003498632, Improvement: -0.00000000285123, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1591
Epoch 1591, Loss: 0.00000002955239, Improvement: -0.00000000543392, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1592
Epoch 1592, Loss: 0.00000002624104, Improvement: -0.00000000331135, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1593
Epoch 1593, Loss: 0.00000003227125, Improvement: 0.00000000603021, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1594
Epoch 1594, Loss: 0.00000003203627, Improvement: -0.00000000023498, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1595
Epoch 1595, Loss: 0.00000003369955, Improvement: 0.00000000166327, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1596
Epoch 1596, Loss: 0.00000004036759, Improvement: 0.00000000666805, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1597
Epoch 1597, Loss: 0.00000003603814, Improvement: -0.00000000432946, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1598
Epoch 1598, Loss: 0.00000005262945, Improvement: 0.00000001659131, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1599
Epoch 1599, Loss: 0.00000006583972, Improvement: 0.00000001321027, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000012119916, Improvement: 0.00000005535944, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1601
Epoch 1601, Loss: 0.00000018228365, Improvement: 0.00000006108449, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1602
Epoch 1602, Loss: 0.00000008038306, Improvement: -0.00000010190059, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1603
Epoch 1603, Loss: 0.00000004463723, Improvement: -0.00000003574583, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1604
Epoch 1604, Loss: 0.00000004141530, Improvement: -0.00000000322193, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1605
Epoch 1605, Loss: 0.00000003396022, Improvement: -0.00000000745508, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1606
Epoch 1606, Loss: 0.00000004675558, Improvement: 0.00000001279535, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1607
Epoch 1607, Loss: 0.00000002870170, Improvement: -0.00000001805388, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1608
Epoch 1608, Loss: 0.00000002797554, Improvement: -0.00000000072616, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1609
Epoch 1609, Loss: 0.00000002931055, Improvement: 0.00000000133501, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1610
Epoch 1610, Loss: 0.00000005003866, Improvement: 0.00000002072811, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1611
Epoch 1611, Loss: 0.00000007531212, Improvement: 0.00000002527346, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1612
Epoch 1612, Loss: 0.00000005847550, Improvement: -0.00000001683663, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1613
Epoch 1613, Loss: 0.00000003610619, Improvement: -0.00000002236931, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1614
Epoch 1614, Loss: 0.00000003505784, Improvement: -0.00000000104835, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1615
Epoch 1615, Loss: 0.00000003150857, Improvement: -0.00000000354927, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1616
Epoch 1616, Loss: 0.00000002675287, Improvement: -0.00000000475569, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1617
Epoch 1617, Loss: 0.00000002482654, Improvement: -0.00000000192633, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1618
Epoch 1618, Loss: 0.00000004611620, Improvement: 0.00000002128965, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1619
Epoch 1619, Loss: 0.00000003737598, Improvement: -0.00000000874022, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1620
Epoch 1620, Loss: 0.00000008399309, Improvement: 0.00000004661712, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1621
Epoch 1621, Loss: 0.00000009368469, Improvement: 0.00000000969159, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1622
Epoch 1622, Loss: 0.00000007589214, Improvement: -0.00000001779255, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1623
Epoch 1623, Loss: 0.00000005445575, Improvement: -0.00000002143639, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1624
Epoch 1624, Loss: 0.00000008145405, Improvement: 0.00000002699830, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1625
Epoch 1625, Loss: 0.00000008446692, Improvement: 0.00000000301287, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1626
Epoch 1626, Loss: 0.00000006395639, Improvement: -0.00000002051053, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1627
Epoch 1627, Loss: 0.00000005754667, Improvement: -0.00000000640972, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1628
Epoch 1628, Loss: 0.00000005623028, Improvement: -0.00000000131639, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1629
Epoch 1629, Loss: 0.00000005714949, Improvement: 0.00000000091920, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1630
Epoch 1630, Loss: 0.00000007156119, Improvement: 0.00000001441170, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1631
Epoch 1631, Loss: 0.00000008711447, Improvement: 0.00000001555329, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1632
Epoch 1632, Loss: 0.00000006648467, Improvement: -0.00000002062980, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1633
Epoch 1633, Loss: 0.00000013253831, Improvement: 0.00000006605364, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1634
Epoch 1634, Loss: 0.00000014397702, Improvement: 0.00000001143871, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1635
Epoch 1635, Loss: 0.00000009325075, Improvement: -0.00000005072627, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1636
Epoch 1636, Loss: 0.00000005582693, Improvement: -0.00000003742382, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1637
Epoch 1637, Loss: 0.00000003583073, Improvement: -0.00000001999619, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1638
Epoch 1638, Loss: 0.00000002846285, Improvement: -0.00000000736788, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1639
Epoch 1639, Loss: 0.00000002684908, Improvement: -0.00000000161377, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1640
Epoch 1640, Loss: 0.00000003025193, Improvement: 0.00000000340286, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1641
Epoch 1641, Loss: 0.00000003153937, Improvement: 0.00000000128743, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1642
Epoch 1642, Loss: 0.00000003801974, Improvement: 0.00000000648038, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1643
Epoch 1643, Loss: 0.00000003232032, Improvement: -0.00000000569942, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1644
Epoch 1644, Loss: 0.00000003095321, Improvement: -0.00000000136711, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1645
Epoch 1645, Loss: 0.00000003901339, Improvement: 0.00000000806018, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1646
Epoch 1646, Loss: 0.00000004162287, Improvement: 0.00000000260948, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1647
Epoch 1647, Loss: 0.00000005039699, Improvement: 0.00000000877412, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1648
Epoch 1648, Loss: 0.00000006419996, Improvement: 0.00000001380297, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1649
Epoch 1649, Loss: 0.00000005110880, Improvement: -0.00000001309116, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000005576819, Improvement: 0.00000000465938, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1651
Epoch 1651, Loss: 0.00000004506978, Improvement: -0.00000001069840, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1652
Epoch 1652, Loss: 0.00000003894792, Improvement: -0.00000000612186, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1653
Epoch 1653, Loss: 0.00000009438856, Improvement: 0.00000005544064, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1654
Epoch 1654, Loss: 0.00000006704865, Improvement: -0.00000002733992, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1655
Epoch 1655, Loss: 0.00000005532420, Improvement: -0.00000001172444, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1656
Epoch 1656, Loss: 0.00000004367675, Improvement: -0.00000001164745, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1657
Epoch 1657, Loss: 0.00000006207943, Improvement: 0.00000001840268, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1658
Epoch 1658, Loss: 0.00000007054803, Improvement: 0.00000000846861, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1659
Epoch 1659, Loss: 0.00000005971353, Improvement: -0.00000001083451, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1660
Epoch 1660, Loss: 0.00000011440906, Improvement: 0.00000005469553, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1661
Epoch 1661, Loss: 0.00000020336774, Improvement: 0.00000008895868, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1662
Epoch 1662, Loss: 0.00000013507686, Improvement: -0.00000006829088, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1663
Epoch 1663, Loss: 0.00000008631718, Improvement: -0.00000004875967, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1664
Epoch 1664, Loss: 0.00000005968681, Improvement: -0.00000002663038, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1665
Epoch 1665, Loss: 0.00000004066530, Improvement: -0.00000001902150, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1666
Epoch 1666, Loss: 0.00000003617284, Improvement: -0.00000000449247, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1667
Epoch 1667, Loss: 0.00000004464751, Improvement: 0.00000000847467, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1668
Epoch 1668, Loss: 0.00000003917714, Improvement: -0.00000000547037, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1669
Epoch 1669, Loss: 0.00000003224702, Improvement: -0.00000000693012, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1670
Epoch 1670, Loss: 0.00000002937875, Improvement: -0.00000000286826, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1671
Epoch 1671, Loss: 0.00000002655229, Improvement: -0.00000000282647, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1672
Epoch 1672, Loss: 0.00000002526466, Improvement: -0.00000000128763, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1673
Epoch 1673, Loss: 0.00000003394635, Improvement: 0.00000000868169, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1674
Epoch 1674, Loss: 0.00000004625817, Improvement: 0.00000001231182, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1675
Epoch 1675, Loss: 0.00000004501012, Improvement: -0.00000000124805, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1676
Epoch 1676, Loss: 0.00000005355268, Improvement: 0.00000000854256, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1677
Epoch 1677, Loss: 0.00000004662418, Improvement: -0.00000000692850, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1678
Epoch 1678, Loss: 0.00000004818323, Improvement: 0.00000000155905, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1679
Epoch 1679, Loss: 0.00000003802581, Improvement: -0.00000001015743, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1680
Epoch 1680, Loss: 0.00000005010282, Improvement: 0.00000001207701, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1681
Epoch 1681, Loss: 0.00000008556723, Improvement: 0.00000003546441, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1682
Epoch 1682, Loss: 0.00000011518364, Improvement: 0.00000002961641, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1683
Epoch 1683, Loss: 0.00000012245261, Improvement: 0.00000000726897, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1684
Epoch 1684, Loss: 0.00000007492272, Improvement: -0.00000004752988, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1685
Epoch 1685, Loss: 0.00000004655499, Improvement: -0.00000002836773, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1686
Epoch 1686, Loss: 0.00000004304382, Improvement: -0.00000000351117, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1687
Epoch 1687, Loss: 0.00000003645955, Improvement: -0.00000000658426, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1688
Epoch 1688, Loss: 0.00000003320630, Improvement: -0.00000000325325, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1689
Epoch 1689, Loss: 0.00000004477309, Improvement: 0.00000001156679, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1690
Epoch 1690, Loss: 0.00000004266956, Improvement: -0.00000000210354, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1691
Epoch 1691, Loss: 0.00000004224729, Improvement: -0.00000000042227, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1692
Epoch 1692, Loss: 0.00000004347802, Improvement: 0.00000000123073, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1693
Epoch 1693, Loss: 0.00000007227415, Improvement: 0.00000002879613, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1694
Epoch 1694, Loss: 0.00000010798027, Improvement: 0.00000003570611, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1695
Epoch 1695, Loss: 0.00000008304996, Improvement: -0.00000002493030, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1696
Epoch 1696, Loss: 0.00000006563303, Improvement: -0.00000001741693, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1697
Epoch 1697, Loss: 0.00000004347064, Improvement: -0.00000002216240, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1698
Epoch 1698, Loss: 0.00000003593194, Improvement: -0.00000000753869, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1699
Epoch 1699, Loss: 0.00000003014116, Improvement: -0.00000000579079, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000002679085, Improvement: -0.00000000335030, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1701
Epoch 1701, Loss: 0.00000003004555, Improvement: 0.00000000325470, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1702
Epoch 1702, Loss: 0.00000004119807, Improvement: 0.00000001115251, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1703
Epoch 1703, Loss: 0.00000003188710, Improvement: -0.00000000931097, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1704
Epoch 1704, Loss: 0.00000002795724, Improvement: -0.00000000392986, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1705
Epoch 1705, Loss: 0.00000002945972, Improvement: 0.00000000150248, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1706
Epoch 1706, Loss: 0.00000004657542, Improvement: 0.00000001711570, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1707
Epoch 1707, Loss: 0.00000004255355, Improvement: -0.00000000402187, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1708
Epoch 1708, Loss: 0.00000004093929, Improvement: -0.00000000161426, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1709
Epoch 1709, Loss: 0.00000006760168, Improvement: 0.00000002666239, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1710
Epoch 1710, Loss: 0.00000007102764, Improvement: 0.00000000342596, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1711
Epoch 1711, Loss: 0.00000008478541, Improvement: 0.00000001375777, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1712
Epoch 1712, Loss: 0.00000009409976, Improvement: 0.00000000931436, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1713
Epoch 1713, Loss: 0.00000016944930, Improvement: 0.00000007534953, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1714
Epoch 1714, Loss: 0.00000008639686, Improvement: -0.00000008305243, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1715
Epoch 1715, Loss: 0.00000004950347, Improvement: -0.00000003689339, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1716
Epoch 1716, Loss: 0.00000004368035, Improvement: -0.00000000582312, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1717
Epoch 1717, Loss: 0.00000003428694, Improvement: -0.00000000939342, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1718
Epoch 1718, Loss: 0.00000002777010, Improvement: -0.00000000651683, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1719
Epoch 1719, Loss: 0.00000002406074, Improvement: -0.00000000370936, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1720
Epoch 1720, Loss: 0.00000003470362, Improvement: 0.00000001064288, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1721
Epoch 1721, Loss: 0.00000003312862, Improvement: -0.00000000157499, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1722
Epoch 1722, Loss: 0.00000003067120, Improvement: -0.00000000245742, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1723
Epoch 1723, Loss: 0.00000003188495, Improvement: 0.00000000121376, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1724
Epoch 1724, Loss: 0.00000003511119, Improvement: 0.00000000322624, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1725
Epoch 1725, Loss: 0.00000003180555, Improvement: -0.00000000330564, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1726
Epoch 1726, Loss: 0.00000003598053, Improvement: 0.00000000417498, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1727
Epoch 1727, Loss: 0.00000006633678, Improvement: 0.00000003035625, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1728
Epoch 1728, Loss: 0.00000008633979, Improvement: 0.00000002000300, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1729
Epoch 1729, Loss: 0.00000006783508, Improvement: -0.00000001850471, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1730
Epoch 1730, Loss: 0.00000007232508, Improvement: 0.00000000449000, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1731
Epoch 1731, Loss: 0.00000003665836, Improvement: -0.00000003566672, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1732
Epoch 1732, Loss: 0.00000003943441, Improvement: 0.00000000277605, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1733
Epoch 1733, Loss: 0.00000004141885, Improvement: 0.00000000198444, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1734
Epoch 1734, Loss: 0.00000003644225, Improvement: -0.00000000497660, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1735
Epoch 1735, Loss: 0.00000004164078, Improvement: 0.00000000519853, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1736
Epoch 1736, Loss: 0.00000004261123, Improvement: 0.00000000097045, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1737
Epoch 1737, Loss: 0.00000003736947, Improvement: -0.00000000524177, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1738
Epoch 1738, Loss: 0.00000003808030, Improvement: 0.00000000071084, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1739
Epoch 1739, Loss: 0.00000003712609, Improvement: -0.00000000095422, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1740
Epoch 1740, Loss: 0.00000005938130, Improvement: 0.00000002225521, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1741
Epoch 1741, Loss: 0.00000004784370, Improvement: -0.00000001153760, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1742
Epoch 1742, Loss: 0.00000003072379, Improvement: -0.00000001711991, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1743
Epoch 1743, Loss: 0.00000003161909, Improvement: 0.00000000089531, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1744
Epoch 1744, Loss: 0.00000004318805, Improvement: 0.00000001156896, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1745
Epoch 1745, Loss: 0.00000006847384, Improvement: 0.00000002528580, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1746
Epoch 1746, Loss: 0.00000007955680, Improvement: 0.00000001108296, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1747
Epoch 1747, Loss: 0.00000005290319, Improvement: -0.00000002665362, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1748
Epoch 1748, Loss: 0.00000003830481, Improvement: -0.00000001459837, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1749
Epoch 1749, Loss: 0.00000002966789, Improvement: -0.00000000863692, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000003429856, Improvement: 0.00000000463067, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1751
Epoch 1751, Loss: 0.00000004771267, Improvement: 0.00000001341410, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1752
Epoch 1752, Loss: 0.00000005175949, Improvement: 0.00000000404682, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1753
Epoch 1753, Loss: 0.00000004470851, Improvement: -0.00000000705098, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1754
Epoch 1754, Loss: 0.00000002988149, Improvement: -0.00000001482702, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1755
Epoch 1755, Loss: 0.00000005061248, Improvement: 0.00000002073100, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1756
Epoch 1756, Loss: 0.00000007214251, Improvement: 0.00000002153003, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1757
Epoch 1757, Loss: 0.00000013199103, Improvement: 0.00000005984852, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1758
Epoch 1758, Loss: 0.00000012788451, Improvement: -0.00000000410652, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1759
Epoch 1759, Loss: 0.00000007041975, Improvement: -0.00000005746476, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1760
Epoch 1760, Loss: 0.00000003156023, Improvement: -0.00000003885952, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1761
Epoch 1761, Loss: 0.00000002724188, Improvement: -0.00000000431836, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1762
Epoch 1762, Loss: 0.00000003013132, Improvement: 0.00000000288945, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1763
Epoch 1763, Loss: 0.00000003140352, Improvement: 0.00000000127220, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1764
Epoch 1764, Loss: 0.00000004738611, Improvement: 0.00000001598258, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1765
Epoch 1765, Loss: 0.00000005838932, Improvement: 0.00000001100321, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1766
Epoch 1766, Loss: 0.00000004212100, Improvement: -0.00000001626832, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1767
Epoch 1767, Loss: 0.00000004040148, Improvement: -0.00000000171952, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1768
Epoch 1768, Loss: 0.00000005313597, Improvement: 0.00000001273449, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1769
Epoch 1769, Loss: 0.00000009597025, Improvement: 0.00000004283428, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1770
Epoch 1770, Loss: 0.00000008931219, Improvement: -0.00000000665806, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1771
Epoch 1771, Loss: 0.00000007828162, Improvement: -0.00000001103057, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1772
Epoch 1772, Loss: 0.00000007150170, Improvement: -0.00000000677992, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1773
Epoch 1773, Loss: 0.00000005625689, Improvement: -0.00000001524481, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1774
Epoch 1774, Loss: 0.00000004933347, Improvement: -0.00000000692342, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1775
Epoch 1775, Loss: 0.00000004412050, Improvement: -0.00000000521297, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1776
Epoch 1776, Loss: 0.00000006315752, Improvement: 0.00000001903702, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1777
Epoch 1777, Loss: 0.00000006040737, Improvement: -0.00000000275015, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1778
Epoch 1778, Loss: 0.00000004304306, Improvement: -0.00000001736431, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1779
Epoch 1779, Loss: 0.00000004578488, Improvement: 0.00000000274183, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1780
Epoch 1780, Loss: 0.00000007131926, Improvement: 0.00000002553438, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1781
Epoch 1781, Loss: 0.00000005136751, Improvement: -0.00000001995175, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1782
Epoch 1782, Loss: 0.00000005206310, Improvement: 0.00000000069559, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1783
Epoch 1783, Loss: 0.00000003724014, Improvement: -0.00000001482296, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1784
Epoch 1784, Loss: 0.00000004033269, Improvement: 0.00000000309254, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1785
Epoch 1785, Loss: 0.00000004805638, Improvement: 0.00000000772370, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1786
Epoch 1786, Loss: 0.00000004485673, Improvement: -0.00000000319965, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1787
Epoch 1787, Loss: 0.00000005203139, Improvement: 0.00000000717466, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1788
Epoch 1788, Loss: 0.00000006284177, Improvement: 0.00000001081038, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1789
Epoch 1789, Loss: 0.00000010599313, Improvement: 0.00000004315136, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1790
Epoch 1790, Loss: 0.00000008883983, Improvement: -0.00000001715330, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1791
Epoch 1791, Loss: 0.00000009024850, Improvement: 0.00000000140867, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1792
Epoch 1792, Loss: 0.00000007741143, Improvement: -0.00000001283707, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1793
Epoch 1793, Loss: 0.00000010293004, Improvement: 0.00000002551861, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1794
Epoch 1794, Loss: 0.00000008732862, Improvement: -0.00000001560142, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1795
Epoch 1795, Loss: 0.00000008324299, Improvement: -0.00000000408563, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1796
Epoch 1796, Loss: 0.00000010911435, Improvement: 0.00000002587136, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1797
Epoch 1797, Loss: 0.00000008914194, Improvement: -0.00000001997241, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1798
Epoch 1798, Loss: 0.00000007383792, Improvement: -0.00000001530402, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1799
Epoch 1799, Loss: 0.00000004233050, Improvement: -0.00000003150741, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000003024290, Improvement: -0.00000001208761, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1801
Epoch 1801, Loss: 0.00000002795552, Improvement: -0.00000000228738, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1802
Epoch 1802, Loss: 0.00000003269121, Improvement: 0.00000000473569, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1803
Epoch 1803, Loss: 0.00000004176384, Improvement: 0.00000000907264, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1804
Epoch 1804, Loss: 0.00000003570306, Improvement: -0.00000000606078, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1805
Epoch 1805, Loss: 0.00000003288907, Improvement: -0.00000000281399, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1806
Epoch 1806, Loss: 0.00000004728028, Improvement: 0.00000001439121, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1807
Epoch 1807, Loss: 0.00000005573586, Improvement: 0.00000000845558, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1808
Epoch 1808, Loss: 0.00000005465422, Improvement: -0.00000000108164, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1809
Epoch 1809, Loss: 0.00000003842296, Improvement: -0.00000001623126, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1810
Epoch 1810, Loss: 0.00000004350734, Improvement: 0.00000000508439, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1811
Epoch 1811, Loss: 0.00000005613474, Improvement: 0.00000001262740, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1812
Epoch 1812, Loss: 0.00000003642702, Improvement: -0.00000001970772, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1813
Epoch 1813, Loss: 0.00000003581533, Improvement: -0.00000000061169, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1814
Epoch 1814, Loss: 0.00000002876253, Improvement: -0.00000000705280, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1815
Epoch 1815, Loss: 0.00000003327059, Improvement: 0.00000000450806, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1816
Epoch 1816, Loss: 0.00000003292200, Improvement: -0.00000000034860, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1817
Epoch 1817, Loss: 0.00000004292048, Improvement: 0.00000000999848, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1818
Epoch 1818, Loss: 0.00000017848101, Improvement: 0.00000013556053, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1819
Epoch 1819, Loss: 0.00000012050585, Improvement: -0.00000005797516, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1820
Epoch 1820, Loss: 0.00000013267036, Improvement: 0.00000001216451, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1821
Epoch 1821, Loss: 0.00000011542303, Improvement: -0.00000001724733, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1822
Epoch 1822, Loss: 0.00000004984669, Improvement: -0.00000006557633, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1823
Epoch 1823, Loss: 0.00000003160534, Improvement: -0.00000001824136, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1824
Epoch 1824, Loss: 0.00000002282512, Improvement: -0.00000000878022, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1825
Epoch 1825, Loss: 0.00000002127554, Improvement: -0.00000000154958, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1826
Epoch 1826, Loss: 0.00000002060759, Improvement: -0.00000000066795, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1827
Epoch 1827, Loss: 0.00000002344738, Improvement: 0.00000000283979, Best Loss: 0.00000001318140 in Epoch 1587
Epoch 1828
A best model at epoch 1828 has been saved with training error 0.00000001228726.
Epoch 1828, Loss: 0.00000002130087, Improvement: -0.00000000214651, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1829
Epoch 1829, Loss: 0.00000002124773, Improvement: -0.00000000005314, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1830
Epoch 1830, Loss: 0.00000002726410, Improvement: 0.00000000601637, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1831
Epoch 1831, Loss: 0.00000003254509, Improvement: 0.00000000528099, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1832
Epoch 1832, Loss: 0.00000003738427, Improvement: 0.00000000483918, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1833
Epoch 1833, Loss: 0.00000003351286, Improvement: -0.00000000387141, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1834
Epoch 1834, Loss: 0.00000003852320, Improvement: 0.00000000501034, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1835
Epoch 1835, Loss: 0.00000003268844, Improvement: -0.00000000583476, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1836
Epoch 1836, Loss: 0.00000002815082, Improvement: -0.00000000453762, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1837
Epoch 1837, Loss: 0.00000003308765, Improvement: 0.00000000493683, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1838
Epoch 1838, Loss: 0.00000005894444, Improvement: 0.00000002585679, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1839
Epoch 1839, Loss: 0.00000006353660, Improvement: 0.00000000459215, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1840
Epoch 1840, Loss: 0.00000003702598, Improvement: -0.00000002651062, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1841
Epoch 1841, Loss: 0.00000002969036, Improvement: -0.00000000733562, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1842
Epoch 1842, Loss: 0.00000003050114, Improvement: 0.00000000081078, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1843
Epoch 1843, Loss: 0.00000003258826, Improvement: 0.00000000208712, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1844
Epoch 1844, Loss: 0.00000003794582, Improvement: 0.00000000535756, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1845
Epoch 1845, Loss: 0.00000003871461, Improvement: 0.00000000076879, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1846
Epoch 1846, Loss: 0.00000003940054, Improvement: 0.00000000068594, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1847
Epoch 1847, Loss: 0.00000004324943, Improvement: 0.00000000384889, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1848
Epoch 1848, Loss: 0.00000011690628, Improvement: 0.00000007365685, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1849
Epoch 1849, Loss: 0.00000008384839, Improvement: -0.00000003305789, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000005390833, Improvement: -0.00000002994006, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1851
Epoch 1851, Loss: 0.00000004457500, Improvement: -0.00000000933333, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1852
Epoch 1852, Loss: 0.00000005425918, Improvement: 0.00000000968419, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1853
Epoch 1853, Loss: 0.00000003840787, Improvement: -0.00000001585131, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1854
Epoch 1854, Loss: 0.00000002633311, Improvement: -0.00000001207476, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1855
Epoch 1855, Loss: 0.00000002373782, Improvement: -0.00000000259529, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1856
Epoch 1856, Loss: 0.00000002764884, Improvement: 0.00000000391102, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1857
Epoch 1857, Loss: 0.00000002766236, Improvement: 0.00000000001352, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1858
Epoch 1858, Loss: 0.00000003163735, Improvement: 0.00000000397499, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1859
Epoch 1859, Loss: 0.00000007131342, Improvement: 0.00000003967607, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1860
Epoch 1860, Loss: 0.00000005135170, Improvement: -0.00000001996172, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1861
Epoch 1861, Loss: 0.00000004866121, Improvement: -0.00000000269049, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1862
Epoch 1862, Loss: 0.00000005500658, Improvement: 0.00000000634537, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1863
Epoch 1863, Loss: 0.00000003400765, Improvement: -0.00000002099893, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1864
Epoch 1864, Loss: 0.00000004457918, Improvement: 0.00000001057153, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1865
Epoch 1865, Loss: 0.00000004437231, Improvement: -0.00000000020687, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1866
Epoch 1866, Loss: 0.00000004917373, Improvement: 0.00000000480141, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1867
Epoch 1867, Loss: 0.00000007169287, Improvement: 0.00000002251914, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1868
Epoch 1868, Loss: 0.00000004333507, Improvement: -0.00000002835780, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1869
Epoch 1869, Loss: 0.00000004062224, Improvement: -0.00000000271283, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1870
Epoch 1870, Loss: 0.00000004333958, Improvement: 0.00000000271733, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1871
Epoch 1871, Loss: 0.00000003328978, Improvement: -0.00000001004979, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1872
Epoch 1872, Loss: 0.00000004023665, Improvement: 0.00000000694687, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1873
Epoch 1873, Loss: 0.00000003359388, Improvement: -0.00000000664277, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1874
Epoch 1874, Loss: 0.00000003542556, Improvement: 0.00000000183168, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1875
Epoch 1875, Loss: 0.00000003709063, Improvement: 0.00000000166507, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1876
Epoch 1876, Loss: 0.00000006105646, Improvement: 0.00000002396583, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1877
Epoch 1877, Loss: 0.00000006112060, Improvement: 0.00000000006414, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1878
Epoch 1878, Loss: 0.00000003168315, Improvement: -0.00000002943745, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1879
Epoch 1879, Loss: 0.00000005359917, Improvement: 0.00000002191602, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1880
Epoch 1880, Loss: 0.00000012268119, Improvement: 0.00000006908202, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1881
Epoch 1881, Loss: 0.00000009300576, Improvement: -0.00000002967543, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1882
Epoch 1882, Loss: 0.00000004488392, Improvement: -0.00000004812184, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1883
Epoch 1883, Loss: 0.00000005678858, Improvement: 0.00000001190467, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1884
Epoch 1884, Loss: 0.00000007349477, Improvement: 0.00000001670619, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1885
Epoch 1885, Loss: 0.00000005351720, Improvement: -0.00000001997757, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1886
Epoch 1886, Loss: 0.00000003508211, Improvement: -0.00000001843509, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1887
Epoch 1887, Loss: 0.00000002216213, Improvement: -0.00000001291998, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1888
Epoch 1888, Loss: 0.00000002294072, Improvement: 0.00000000077859, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1889
Epoch 1889, Loss: 0.00000002583168, Improvement: 0.00000000289095, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1890
Epoch 1890, Loss: 0.00000002664465, Improvement: 0.00000000081298, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1891
Epoch 1891, Loss: 0.00000003842303, Improvement: 0.00000001177838, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1892
Epoch 1892, Loss: 0.00000003134517, Improvement: -0.00000000707787, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1893
Epoch 1893, Loss: 0.00000003717622, Improvement: 0.00000000583106, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1894
Epoch 1894, Loss: 0.00000005545880, Improvement: 0.00000001828258, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1895
Epoch 1895, Loss: 0.00000005231070, Improvement: -0.00000000314811, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1896
Epoch 1896, Loss: 0.00000004170930, Improvement: -0.00000001060139, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1897
Epoch 1897, Loss: 0.00000004166331, Improvement: -0.00000000004600, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1898
Epoch 1898, Loss: 0.00000004143549, Improvement: -0.00000000022782, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1899
Epoch 1899, Loss: 0.00000003631783, Improvement: -0.00000000511766, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000005676631, Improvement: 0.00000002044849, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1901
Epoch 1901, Loss: 0.00000006027677, Improvement: 0.00000000351045, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1902
Epoch 1902, Loss: 0.00000003940095, Improvement: -0.00000002087582, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1903
Epoch 1903, Loss: 0.00000009025977, Improvement: 0.00000005085882, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1904
Epoch 1904, Loss: 0.00000012935635, Improvement: 0.00000003909658, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1905
Epoch 1905, Loss: 0.00000018380303, Improvement: 0.00000005444668, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1906
Epoch 1906, Loss: 0.00000012032639, Improvement: -0.00000006347665, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1907
Epoch 1907, Loss: 0.00000005660310, Improvement: -0.00000006372328, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1908
Epoch 1908, Loss: 0.00000005147066, Improvement: -0.00000000513244, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1909
Epoch 1909, Loss: 0.00000004072610, Improvement: -0.00000001074456, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1910
Epoch 1910, Loss: 0.00000002485053, Improvement: -0.00000001587558, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1911
Epoch 1911, Loss: 0.00000002313777, Improvement: -0.00000000171276, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1912
Epoch 1912, Loss: 0.00000002233673, Improvement: -0.00000000080103, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1913
Epoch 1913, Loss: 0.00000002390113, Improvement: 0.00000000156440, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1914
Epoch 1914, Loss: 0.00000002760966, Improvement: 0.00000000370853, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1915
Epoch 1915, Loss: 0.00000003989542, Improvement: 0.00000001228576, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1916
Epoch 1916, Loss: 0.00000002785913, Improvement: -0.00000001203629, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1917
Epoch 1917, Loss: 0.00000003056450, Improvement: 0.00000000270538, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1918
Epoch 1918, Loss: 0.00000002848689, Improvement: -0.00000000207761, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1919
Epoch 1919, Loss: 0.00000002631054, Improvement: -0.00000000217635, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1920
Epoch 1920, Loss: 0.00000002632805, Improvement: 0.00000000001751, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1921
Epoch 1921, Loss: 0.00000003486219, Improvement: 0.00000000853414, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1922
Epoch 1922, Loss: 0.00000002934191, Improvement: -0.00000000552028, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1923
Epoch 1923, Loss: 0.00000002393864, Improvement: -0.00000000540327, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1924
Epoch 1924, Loss: 0.00000002285982, Improvement: -0.00000000107882, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1925
Epoch 1925, Loss: 0.00000003158317, Improvement: 0.00000000872335, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1926
Epoch 1926, Loss: 0.00000004209878, Improvement: 0.00000001051561, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1927
Epoch 1927, Loss: 0.00000005189105, Improvement: 0.00000000979227, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1928
Epoch 1928, Loss: 0.00000005167123, Improvement: -0.00000000021982, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1929
Epoch 1929, Loss: 0.00000007044549, Improvement: 0.00000001877425, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1930
Epoch 1930, Loss: 0.00000012121544, Improvement: 0.00000005076996, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1931
Epoch 1931, Loss: 0.00000011137662, Improvement: -0.00000000983882, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1932
Epoch 1932, Loss: 0.00000007883289, Improvement: -0.00000003254373, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1933
Epoch 1933, Loss: 0.00000004344078, Improvement: -0.00000003539211, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1934
Epoch 1934, Loss: 0.00000002316570, Improvement: -0.00000002027508, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1935
Epoch 1935, Loss: 0.00000002212744, Improvement: -0.00000000103826, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1936
Epoch 1936, Loss: 0.00000003491197, Improvement: 0.00000001278453, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1937
Epoch 1937, Loss: 0.00000003063995, Improvement: -0.00000000427202, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1938
Epoch 1938, Loss: 0.00000004073954, Improvement: 0.00000001009959, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1939
Epoch 1939, Loss: 0.00000002761167, Improvement: -0.00000001312786, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1940
Epoch 1940, Loss: 0.00000003222243, Improvement: 0.00000000461076, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1941
Epoch 1941, Loss: 0.00000004102201, Improvement: 0.00000000879958, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1942
Epoch 1942, Loss: 0.00000002632991, Improvement: -0.00000001469211, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1943
Epoch 1943, Loss: 0.00000002377305, Improvement: -0.00000000255686, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1944
Epoch 1944, Loss: 0.00000002372578, Improvement: -0.00000000004727, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1945
Epoch 1945, Loss: 0.00000002317762, Improvement: -0.00000000054816, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1946
Epoch 1946, Loss: 0.00000003133576, Improvement: 0.00000000815814, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1947
Epoch 1947, Loss: 0.00000003459955, Improvement: 0.00000000326379, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1948
Epoch 1948, Loss: 0.00000005948436, Improvement: 0.00000002488481, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1949
Epoch 1949, Loss: 0.00000007735214, Improvement: 0.00000001786778, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000004954433, Improvement: -0.00000002780780, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1951
Epoch 1951, Loss: 0.00000003088437, Improvement: -0.00000001865997, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1952
Epoch 1952, Loss: 0.00000003534919, Improvement: 0.00000000446482, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1953
Epoch 1953, Loss: 0.00000005031102, Improvement: 0.00000001496183, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1954
Epoch 1954, Loss: 0.00000007062756, Improvement: 0.00000002031655, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1955
Epoch 1955, Loss: 0.00000007039382, Improvement: -0.00000000023374, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1956
Epoch 1956, Loss: 0.00000004629450, Improvement: -0.00000002409932, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1957
Epoch 1957, Loss: 0.00000003305665, Improvement: -0.00000001323785, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1958
Epoch 1958, Loss: 0.00000003435853, Improvement: 0.00000000130188, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1959
Epoch 1959, Loss: 0.00000003402328, Improvement: -0.00000000033525, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1960
Epoch 1960, Loss: 0.00000003320080, Improvement: -0.00000000082248, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1961
Epoch 1961, Loss: 0.00000004503939, Improvement: 0.00000001183859, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1962
Epoch 1962, Loss: 0.00000009797322, Improvement: 0.00000005293384, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1963
Epoch 1963, Loss: 0.00000009920569, Improvement: 0.00000000123247, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1964
Epoch 1964, Loss: 0.00000005815053, Improvement: -0.00000004105517, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1965
Epoch 1965, Loss: 0.00000009740689, Improvement: 0.00000003925637, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1966
Epoch 1966, Loss: 0.00000009567430, Improvement: -0.00000000173260, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1967
Epoch 1967, Loss: 0.00000004776313, Improvement: -0.00000004791117, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1968
Epoch 1968, Loss: 0.00000004182329, Improvement: -0.00000000593983, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1969
Epoch 1969, Loss: 0.00000003124031, Improvement: -0.00000001058298, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1970
Epoch 1970, Loss: 0.00000002362364, Improvement: -0.00000000761667, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1971
Epoch 1971, Loss: 0.00000002702953, Improvement: 0.00000000340589, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1972
Epoch 1972, Loss: 0.00000003082946, Improvement: 0.00000000379993, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1973
Epoch 1973, Loss: 0.00000003045306, Improvement: -0.00000000037641, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1974
Epoch 1974, Loss: 0.00000003093826, Improvement: 0.00000000048520, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1975
Epoch 1975, Loss: 0.00000002899725, Improvement: -0.00000000194100, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1976
Epoch 1976, Loss: 0.00000002226711, Improvement: -0.00000000673014, Best Loss: 0.00000001228726 in Epoch 1828
Epoch 1977
A best model at epoch 1977 has been saved with training error 0.00000001152754.
Epoch 1977, Loss: 0.00000002140771, Improvement: -0.00000000085940, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1978
Epoch 1978, Loss: 0.00000002228868, Improvement: 0.00000000088097, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1979
Epoch 1979, Loss: 0.00000002281128, Improvement: 0.00000000052260, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1980
Epoch 1980, Loss: 0.00000002943078, Improvement: 0.00000000661950, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1981
Epoch 1981, Loss: 0.00000003606861, Improvement: 0.00000000663783, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1982
Epoch 1982, Loss: 0.00000003170071, Improvement: -0.00000000436790, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1983
Epoch 1983, Loss: 0.00000002609983, Improvement: -0.00000000560088, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1984
Epoch 1984, Loss: 0.00000003328649, Improvement: 0.00000000718665, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1985
Epoch 1985, Loss: 0.00000005402578, Improvement: 0.00000002073930, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1986
Epoch 1986, Loss: 0.00000006117490, Improvement: 0.00000000714912, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1987
Epoch 1987, Loss: 0.00000010139918, Improvement: 0.00000004022428, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1988
Epoch 1988, Loss: 0.00000009277109, Improvement: -0.00000000862809, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1989
Epoch 1989, Loss: 0.00000007699010, Improvement: -0.00000001578098, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1990
Epoch 1990, Loss: 0.00000014814670, Improvement: 0.00000007115660, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1991
Epoch 1991, Loss: 0.00000007519419, Improvement: -0.00000007295251, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1992
Epoch 1992, Loss: 0.00000003600565, Improvement: -0.00000003918854, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1993
Epoch 1993, Loss: 0.00000002833374, Improvement: -0.00000000767191, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1994
Epoch 1994, Loss: 0.00000002723589, Improvement: -0.00000000109785, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1995
Epoch 1995, Loss: 0.00000002558423, Improvement: -0.00000000165166, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1996
Epoch 1996, Loss: 0.00000002404618, Improvement: -0.00000000153805, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1997
Epoch 1997, Loss: 0.00000002562089, Improvement: 0.00000000157471, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1998
Epoch 1998, Loss: 0.00000002045035, Improvement: -0.00000000517054, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 1999
Epoch 1999, Loss: 0.00000002295454, Improvement: 0.00000000250419, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000003220319, Improvement: 0.00000000924865, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2001
Epoch 2001, Loss: 0.00000005637266, Improvement: 0.00000002416947, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2002
Epoch 2002, Loss: 0.00000003653826, Improvement: -0.00000001983440, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2003
Epoch 2003, Loss: 0.00000002520833, Improvement: -0.00000001132993, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2004
Epoch 2004, Loss: 0.00000002252338, Improvement: -0.00000000268495, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2005
Epoch 2005, Loss: 0.00000003071395, Improvement: 0.00000000819057, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2006
Epoch 2006, Loss: 0.00000003459181, Improvement: 0.00000000387785, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2007
Epoch 2007, Loss: 0.00000005162973, Improvement: 0.00000001703792, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2008
Epoch 2008, Loss: 0.00000004021926, Improvement: -0.00000001141047, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2009
Epoch 2009, Loss: 0.00000003882570, Improvement: -0.00000000139356, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2010
Epoch 2010, Loss: 0.00000005198596, Improvement: 0.00000001316025, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2011
Epoch 2011, Loss: 0.00000006026669, Improvement: 0.00000000828073, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2012
Epoch 2012, Loss: 0.00000006269044, Improvement: 0.00000000242375, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2013
Epoch 2013, Loss: 0.00000006108866, Improvement: -0.00000000160178, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2014
Epoch 2014, Loss: 0.00000004200756, Improvement: -0.00000001908110, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2015
Epoch 2015, Loss: 0.00000003724295, Improvement: -0.00000000476460, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2016
Epoch 2016, Loss: 0.00000006215280, Improvement: 0.00000002490985, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2017
Epoch 2017, Loss: 0.00000006245358, Improvement: 0.00000000030078, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2018
Epoch 2018, Loss: 0.00000003006755, Improvement: -0.00000003238603, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2019
Epoch 2019, Loss: 0.00000002761706, Improvement: -0.00000000245049, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2020
Epoch 2020, Loss: 0.00000002656530, Improvement: -0.00000000105176, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2021
Epoch 2021, Loss: 0.00000002125161, Improvement: -0.00000000531369, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2022
Epoch 2022, Loss: 0.00000002321952, Improvement: 0.00000000196791, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2023
Epoch 2023, Loss: 0.00000002664635, Improvement: 0.00000000342683, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2024
Epoch 2024, Loss: 0.00000004816460, Improvement: 0.00000002151825, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2025
Epoch 2025, Loss: 0.00000003079273, Improvement: -0.00000001737187, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2026
Epoch 2026, Loss: 0.00000004260780, Improvement: 0.00000001181507, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2027
Epoch 2027, Loss: 0.00000005686405, Improvement: 0.00000001425625, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2028
Epoch 2028, Loss: 0.00000004788405, Improvement: -0.00000000898000, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2029
Epoch 2029, Loss: 0.00000004243790, Improvement: -0.00000000544615, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2030
Epoch 2030, Loss: 0.00000003547373, Improvement: -0.00000000696418, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2031
Epoch 2031, Loss: 0.00000002623832, Improvement: -0.00000000923541, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2032
Epoch 2032, Loss: 0.00000005841623, Improvement: 0.00000003217792, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2033
Epoch 2033, Loss: 0.00000008941795, Improvement: 0.00000003100172, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2034
Epoch 2034, Loss: 0.00000015663811, Improvement: 0.00000006722016, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2035
Epoch 2035, Loss: 0.00000011437720, Improvement: -0.00000004226091, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2036
Epoch 2036, Loss: 0.00000006103816, Improvement: -0.00000005333904, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2037
Epoch 2037, Loss: 0.00000003112366, Improvement: -0.00000002991450, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2038
Epoch 2038, Loss: 0.00000002805814, Improvement: -0.00000000306551, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2039
Epoch 2039, Loss: 0.00000003000865, Improvement: 0.00000000195050, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2040
Epoch 2040, Loss: 0.00000002573392, Improvement: -0.00000000427472, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2041
Epoch 2041, Loss: 0.00000002797206, Improvement: 0.00000000223814, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2042
Epoch 2042, Loss: 0.00000003016393, Improvement: 0.00000000219187, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2043
Epoch 2043, Loss: 0.00000004294121, Improvement: 0.00000001277728, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2044
Epoch 2044, Loss: 0.00000004017332, Improvement: -0.00000000276789, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2045
Epoch 2045, Loss: 0.00000004114670, Improvement: 0.00000000097338, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2046
Epoch 2046, Loss: 0.00000002993171, Improvement: -0.00000001121499, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2047
Epoch 2047, Loss: 0.00000003191554, Improvement: 0.00000000198383, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2048
Epoch 2048, Loss: 0.00000002407262, Improvement: -0.00000000784291, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2049
Epoch 2049, Loss: 0.00000002467356, Improvement: 0.00000000060094, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000002221377, Improvement: -0.00000000245979, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2051
Epoch 2051, Loss: 0.00000002493065, Improvement: 0.00000000271687, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2052
Epoch 2052, Loss: 0.00000002811247, Improvement: 0.00000000318182, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2053
Epoch 2053, Loss: 0.00000002947671, Improvement: 0.00000000136424, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2054
Epoch 2054, Loss: 0.00000003638355, Improvement: 0.00000000690684, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2055
Epoch 2055, Loss: 0.00000003841150, Improvement: 0.00000000202795, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2056
Epoch 2056, Loss: 0.00000004062273, Improvement: 0.00000000221122, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2057
Epoch 2057, Loss: 0.00000004888575, Improvement: 0.00000000826303, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2058
Epoch 2058, Loss: 0.00000010027538, Improvement: 0.00000005138963, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2059
Epoch 2059, Loss: 0.00000008673972, Improvement: -0.00000001353567, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2060
Epoch 2060, Loss: 0.00000005440297, Improvement: -0.00000003233675, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2061
Epoch 2061, Loss: 0.00000003634649, Improvement: -0.00000001805648, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2062
Epoch 2062, Loss: 0.00000002598239, Improvement: -0.00000001036411, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2063
Epoch 2063, Loss: 0.00000002256035, Improvement: -0.00000000342203, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2064
Epoch 2064, Loss: 0.00000002139779, Improvement: -0.00000000116257, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2065
Epoch 2065, Loss: 0.00000001979683, Improvement: -0.00000000160095, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2066
Epoch 2066, Loss: 0.00000002220271, Improvement: 0.00000000240588, Best Loss: 0.00000001152754 in Epoch 1977
Epoch 2067
A best model at epoch 2067 has been saved with training error 0.00000001006838.
Epoch 2067, Loss: 0.00000002097751, Improvement: -0.00000000122520, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2068
Epoch 2068, Loss: 0.00000002046065, Improvement: -0.00000000051686, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2069
Epoch 2069, Loss: 0.00000002177327, Improvement: 0.00000000131261, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2070
Epoch 2070, Loss: 0.00000003738177, Improvement: 0.00000001560851, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2071
Epoch 2071, Loss: 0.00000005755287, Improvement: 0.00000002017110, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2072
Epoch 2072, Loss: 0.00000005977582, Improvement: 0.00000000222295, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2073
Epoch 2073, Loss: 0.00000004678908, Improvement: -0.00000001298675, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2074
Epoch 2074, Loss: 0.00000005243380, Improvement: 0.00000000564472, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2075
Epoch 2075, Loss: 0.00000003772232, Improvement: -0.00000001471148, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2076
Epoch 2076, Loss: 0.00000003188325, Improvement: -0.00000000583907, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2077
Epoch 2077, Loss: 0.00000003175973, Improvement: -0.00000000012353, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2078
Epoch 2078, Loss: 0.00000002956810, Improvement: -0.00000000219162, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2079
Epoch 2079, Loss: 0.00000004492417, Improvement: 0.00000001535607, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2080
Epoch 2080, Loss: 0.00000004952849, Improvement: 0.00000000460432, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2081
Epoch 2081, Loss: 0.00000007706559, Improvement: 0.00000002753710, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2082
Epoch 2082, Loss: 0.00000005850869, Improvement: -0.00000001855690, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2083
Epoch 2083, Loss: 0.00000005144984, Improvement: -0.00000000705886, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2084
Epoch 2084, Loss: 0.00000004739445, Improvement: -0.00000000405539, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2085
Epoch 2085, Loss: 0.00000006139254, Improvement: 0.00000001399809, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2086
Epoch 2086, Loss: 0.00000003453058, Improvement: -0.00000002686195, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2087
Epoch 2087, Loss: 0.00000002287745, Improvement: -0.00000001165314, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2088
Epoch 2088, Loss: 0.00000002677187, Improvement: 0.00000000389442, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2089
Epoch 2089, Loss: 0.00000003767723, Improvement: 0.00000001090536, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2090
Epoch 2090, Loss: 0.00000003640480, Improvement: -0.00000000127243, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2091
Epoch 2091, Loss: 0.00000003404964, Improvement: -0.00000000235516, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2092
Epoch 2092, Loss: 0.00000003269036, Improvement: -0.00000000135928, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2093
Epoch 2093, Loss: 0.00000006092364, Improvement: 0.00000002823328, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2094
Epoch 2094, Loss: 0.00000011751880, Improvement: 0.00000005659516, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2095
Epoch 2095, Loss: 0.00000008428294, Improvement: -0.00000003323586, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2096
Epoch 2096, Loss: 0.00000003576224, Improvement: -0.00000004852069, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2097
Epoch 2097, Loss: 0.00000003191958, Improvement: -0.00000000384266, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2098
Epoch 2098, Loss: 0.00000002960890, Improvement: -0.00000000231068, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2099
Epoch 2099, Loss: 0.00000002546647, Improvement: -0.00000000414244, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000002286004, Improvement: -0.00000000260643, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2101
Epoch 2101, Loss: 0.00000003099984, Improvement: 0.00000000813980, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2102
Epoch 2102, Loss: 0.00000002698394, Improvement: -0.00000000401590, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2103
Epoch 2103, Loss: 0.00000002530550, Improvement: -0.00000000167845, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2104
Epoch 2104, Loss: 0.00000002546467, Improvement: 0.00000000015918, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2105
Epoch 2105, Loss: 0.00000002820197, Improvement: 0.00000000273730, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2106
Epoch 2106, Loss: 0.00000002784963, Improvement: -0.00000000035234, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2107
Epoch 2107, Loss: 0.00000004273930, Improvement: 0.00000001488967, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2108
Epoch 2108, Loss: 0.00000003369798, Improvement: -0.00000000904132, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2109
Epoch 2109, Loss: 0.00000002435362, Improvement: -0.00000000934436, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2110
Epoch 2110, Loss: 0.00000002632362, Improvement: 0.00000000197000, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2111
Epoch 2111, Loss: 0.00000003734791, Improvement: 0.00000001102429, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2112
Epoch 2112, Loss: 0.00000005960256, Improvement: 0.00000002225465, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2113
Epoch 2113, Loss: 0.00000004105000, Improvement: -0.00000001855256, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2114
Epoch 2114, Loss: 0.00000007659046, Improvement: 0.00000003554046, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2115
Epoch 2115, Loss: 0.00000007122824, Improvement: -0.00000000536221, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2116
Epoch 2116, Loss: 0.00000004320785, Improvement: -0.00000002802040, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2117
Epoch 2117, Loss: 0.00000006100719, Improvement: 0.00000001779934, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2118
Epoch 2118, Loss: 0.00000004846107, Improvement: -0.00000001254612, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2119
Epoch 2119, Loss: 0.00000003786362, Improvement: -0.00000001059745, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2120
Epoch 2120, Loss: 0.00000003261706, Improvement: -0.00000000524656, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2121
Epoch 2121, Loss: 0.00000002979715, Improvement: -0.00000000281991, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2122
Epoch 2122, Loss: 0.00000003131424, Improvement: 0.00000000151709, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2123
Epoch 2123, Loss: 0.00000003759464, Improvement: 0.00000000628040, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2124
Epoch 2124, Loss: 0.00000003850598, Improvement: 0.00000000091134, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2125
Epoch 2125, Loss: 0.00000003529269, Improvement: -0.00000000321329, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2126
Epoch 2126, Loss: 0.00000002782058, Improvement: -0.00000000747211, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2127
Epoch 2127, Loss: 0.00000002475978, Improvement: -0.00000000306080, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2128
Epoch 2128, Loss: 0.00000003699400, Improvement: 0.00000001223422, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2129
Epoch 2129, Loss: 0.00000003286158, Improvement: -0.00000000413242, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2130
Epoch 2130, Loss: 0.00000004971840, Improvement: 0.00000001685682, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2131
Epoch 2131, Loss: 0.00000006066939, Improvement: 0.00000001095099, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2132
Epoch 2132, Loss: 0.00000004367203, Improvement: -0.00000001699736, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2133
Epoch 2133, Loss: 0.00000007990761, Improvement: 0.00000003623558, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2134
Epoch 2134, Loss: 0.00000007687466, Improvement: -0.00000000303295, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2135
Epoch 2135, Loss: 0.00000003437926, Improvement: -0.00000004249540, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2136
Epoch 2136, Loss: 0.00000002867208, Improvement: -0.00000000570718, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2137
Epoch 2137, Loss: 0.00000002988136, Improvement: 0.00000000120928, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2138
Epoch 2138, Loss: 0.00000003490908, Improvement: 0.00000000502773, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2139
Epoch 2139, Loss: 0.00000006972693, Improvement: 0.00000003481784, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2140
Epoch 2140, Loss: 0.00000005973982, Improvement: -0.00000000998711, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2141
Epoch 2141, Loss: 0.00000004933613, Improvement: -0.00000001040369, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2142
Epoch 2142, Loss: 0.00000006151896, Improvement: 0.00000001218282, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2143
Epoch 2143, Loss: 0.00000004546703, Improvement: -0.00000001605192, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2144
Epoch 2144, Loss: 0.00000003029851, Improvement: -0.00000001516852, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2145
Epoch 2145, Loss: 0.00000002803476, Improvement: -0.00000000226375, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2146
Epoch 2146, Loss: 0.00000002684038, Improvement: -0.00000000119437, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2147
Epoch 2147, Loss: 0.00000005404453, Improvement: 0.00000002720415, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2148
Epoch 2148, Loss: 0.00000004840077, Improvement: -0.00000000564376, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2149
Epoch 2149, Loss: 0.00000003431301, Improvement: -0.00000001408777, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000003016229, Improvement: -0.00000000415071, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2151
Epoch 2151, Loss: 0.00000002985385, Improvement: -0.00000000030844, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2152
Epoch 2152, Loss: 0.00000002300097, Improvement: -0.00000000685288, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2153
Epoch 2153, Loss: 0.00000002429565, Improvement: 0.00000000129467, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2154
Epoch 2154, Loss: 0.00000002518991, Improvement: 0.00000000089427, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2155
Epoch 2155, Loss: 0.00000003013754, Improvement: 0.00000000494762, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2156
Epoch 2156, Loss: 0.00000005439023, Improvement: 0.00000002425270, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2157
Epoch 2157, Loss: 0.00000004468191, Improvement: -0.00000000970832, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2158
Epoch 2158, Loss: 0.00000005694925, Improvement: 0.00000001226734, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2159
Epoch 2159, Loss: 0.00000009006129, Improvement: 0.00000003311204, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2160
Epoch 2160, Loss: 0.00000007456651, Improvement: -0.00000001549479, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2161
Epoch 2161, Loss: 0.00000005409520, Improvement: -0.00000002047130, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2162
Epoch 2162, Loss: 0.00000005722380, Improvement: 0.00000000312860, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2163
Epoch 2163, Loss: 0.00000003147522, Improvement: -0.00000002574858, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2164
Epoch 2164, Loss: 0.00000003489018, Improvement: 0.00000000341496, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2165
Epoch 2165, Loss: 0.00000003515699, Improvement: 0.00000000026681, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2166
Epoch 2166, Loss: 0.00000002722620, Improvement: -0.00000000793079, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2167
Epoch 2167, Loss: 0.00000002850220, Improvement: 0.00000000127600, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2168
Epoch 2168, Loss: 0.00000002818134, Improvement: -0.00000000032086, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2169
Epoch 2169, Loss: 0.00000005290146, Improvement: 0.00000002472012, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2170
Epoch 2170, Loss: 0.00000005140137, Improvement: -0.00000000150008, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2171
Epoch 2171, Loss: 0.00000003638641, Improvement: -0.00000001501497, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2172
Epoch 2172, Loss: 0.00000002999507, Improvement: -0.00000000639133, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2173
Epoch 2173, Loss: 0.00000005318321, Improvement: 0.00000002318814, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2174
Epoch 2174, Loss: 0.00000004154743, Improvement: -0.00000001163579, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2175
Epoch 2175, Loss: 0.00000004314223, Improvement: 0.00000000159481, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2176
Epoch 2176, Loss: 0.00000005524751, Improvement: 0.00000001210527, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2177
Epoch 2177, Loss: 0.00000007488276, Improvement: 0.00000001963525, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2178
Epoch 2178, Loss: 0.00000007199311, Improvement: -0.00000000288965, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2179
Epoch 2179, Loss: 0.00000006198519, Improvement: -0.00000001000792, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2180
Epoch 2180, Loss: 0.00000004738932, Improvement: -0.00000001459587, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2181
Epoch 2181, Loss: 0.00000002998444, Improvement: -0.00000001740489, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2182
Epoch 2182, Loss: 0.00000002427858, Improvement: -0.00000000570586, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2183
Epoch 2183, Loss: 0.00000002406101, Improvement: -0.00000000021757, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2184
Epoch 2184, Loss: 0.00000003514913, Improvement: 0.00000001108812, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2185
Epoch 2185, Loss: 0.00000005733752, Improvement: 0.00000002218839, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2186
Epoch 2186, Loss: 0.00000004615179, Improvement: -0.00000001118573, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2187
Epoch 2187, Loss: 0.00000005497149, Improvement: 0.00000000881970, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2188
Epoch 2188, Loss: 0.00000003198491, Improvement: -0.00000002298658, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2189
Epoch 2189, Loss: 0.00000002523728, Improvement: -0.00000000674763, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2190
Epoch 2190, Loss: 0.00000002167115, Improvement: -0.00000000356612, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2191
Epoch 2191, Loss: 0.00000001936817, Improvement: -0.00000000230298, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2192
Epoch 2192, Loss: 0.00000002345058, Improvement: 0.00000000408241, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2193
Epoch 2193, Loss: 0.00000001977679, Improvement: -0.00000000367379, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2194
Epoch 2194, Loss: 0.00000002088251, Improvement: 0.00000000110572, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2195
Epoch 2195, Loss: 0.00000003118493, Improvement: 0.00000001030242, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2196
Epoch 2196, Loss: 0.00000003634177, Improvement: 0.00000000515684, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2197
Epoch 2197, Loss: 0.00000004332780, Improvement: 0.00000000698602, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2198
Epoch 2198, Loss: 0.00000003729031, Improvement: -0.00000000603749, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2199
Epoch 2199, Loss: 0.00000004181108, Improvement: 0.00000000452078, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000006768378, Improvement: 0.00000002587270, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2201
Epoch 2201, Loss: 0.00000007431861, Improvement: 0.00000000663483, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2202
Epoch 2202, Loss: 0.00000008592337, Improvement: 0.00000001160477, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2203
Epoch 2203, Loss: 0.00000006217666, Improvement: -0.00000002374672, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2204
Epoch 2204, Loss: 0.00000003514897, Improvement: -0.00000002702769, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2205
Epoch 2205, Loss: 0.00000004371231, Improvement: 0.00000000856335, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2206
Epoch 2206, Loss: 0.00000003608064, Improvement: -0.00000000763168, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2207
Epoch 2207, Loss: 0.00000002457091, Improvement: -0.00000001150973, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2208
Epoch 2208, Loss: 0.00000002104032, Improvement: -0.00000000353059, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2209
Epoch 2209, Loss: 0.00000002724466, Improvement: 0.00000000620435, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2210
Epoch 2210, Loss: 0.00000003383978, Improvement: 0.00000000659512, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2211
Epoch 2211, Loss: 0.00000003638898, Improvement: 0.00000000254920, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2212
Epoch 2212, Loss: 0.00000003656321, Improvement: 0.00000000017422, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2213
Epoch 2213, Loss: 0.00000003941458, Improvement: 0.00000000285137, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2214
Epoch 2214, Loss: 0.00000006147003, Improvement: 0.00000002205545, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2215
Epoch 2215, Loss: 0.00000007021764, Improvement: 0.00000000874761, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2216
Epoch 2216, Loss: 0.00000006338812, Improvement: -0.00000000682953, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2217
Epoch 2217, Loss: 0.00000003505185, Improvement: -0.00000002833626, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2218
Epoch 2218, Loss: 0.00000003408166, Improvement: -0.00000000097019, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2219
Epoch 2219, Loss: 0.00000003202189, Improvement: -0.00000000205977, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2220
Epoch 2220, Loss: 0.00000003943728, Improvement: 0.00000000741539, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2221
Epoch 2221, Loss: 0.00000003822748, Improvement: -0.00000000120980, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2222
Epoch 2222, Loss: 0.00000003262687, Improvement: -0.00000000560060, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2223
Epoch 2223, Loss: 0.00000002360973, Improvement: -0.00000000901714, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2224
Epoch 2224, Loss: 0.00000005456810, Improvement: 0.00000003095837, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2225
Epoch 2225, Loss: 0.00000010605162, Improvement: 0.00000005148352, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2226
Epoch 2226, Loss: 0.00000010766667, Improvement: 0.00000000161505, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2227
Epoch 2227, Loss: 0.00000006037890, Improvement: -0.00000004728777, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2228
Epoch 2228, Loss: 0.00000003891004, Improvement: -0.00000002146886, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2229
Epoch 2229, Loss: 0.00000004411225, Improvement: 0.00000000520221, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2230
Epoch 2230, Loss: 0.00000002308264, Improvement: -0.00000002102961, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2231
Epoch 2231, Loss: 0.00000002055690, Improvement: -0.00000000252574, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2232
Epoch 2232, Loss: 0.00000002028012, Improvement: -0.00000000027677, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2233
Epoch 2233, Loss: 0.00000002358156, Improvement: 0.00000000330143, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2234
Epoch 2234, Loss: 0.00000002709921, Improvement: 0.00000000351765, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2235
Epoch 2235, Loss: 0.00000002422031, Improvement: -0.00000000287889, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2236
Epoch 2236, Loss: 0.00000002873041, Improvement: 0.00000000451010, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2237
Epoch 2237, Loss: 0.00000002731008, Improvement: -0.00000000142033, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2238
Epoch 2238, Loss: 0.00000002445975, Improvement: -0.00000000285034, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2239
Epoch 2239, Loss: 0.00000003784017, Improvement: 0.00000001338042, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2240
Epoch 2240, Loss: 0.00000003569966, Improvement: -0.00000000214050, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2241
Epoch 2241, Loss: 0.00000003539365, Improvement: -0.00000000030601, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2242
Epoch 2242, Loss: 0.00000002956633, Improvement: -0.00000000582732, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2243
Epoch 2243, Loss: 0.00000003880390, Improvement: 0.00000000923757, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2244
Epoch 2244, Loss: 0.00000005802928, Improvement: 0.00000001922538, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2245
Epoch 2245, Loss: 0.00000007775301, Improvement: 0.00000001972373, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2246
Epoch 2246, Loss: 0.00000004250881, Improvement: -0.00000003524420, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2247
Epoch 2247, Loss: 0.00000001944501, Improvement: -0.00000002306380, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2248
Epoch 2248, Loss: 0.00000001677289, Improvement: -0.00000000267212, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2249
Epoch 2249, Loss: 0.00000002821777, Improvement: 0.00000001144487, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000003917365, Improvement: 0.00000001095588, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2251
Epoch 2251, Loss: 0.00000004830197, Improvement: 0.00000000912832, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2252
Epoch 2252, Loss: 0.00000003429123, Improvement: -0.00000001401074, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2253
Epoch 2253, Loss: 0.00000004473760, Improvement: 0.00000001044636, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2254
Epoch 2254, Loss: 0.00000004208241, Improvement: -0.00000000265519, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2255
Epoch 2255, Loss: 0.00000002686371, Improvement: -0.00000001521870, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2256
Epoch 2256, Loss: 0.00000002865655, Improvement: 0.00000000179284, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2257
Epoch 2257, Loss: 0.00000003575732, Improvement: 0.00000000710077, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2258
Epoch 2258, Loss: 0.00000003343880, Improvement: -0.00000000231852, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2259
Epoch 2259, Loss: 0.00000003327856, Improvement: -0.00000000016024, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2260
Epoch 2260, Loss: 0.00000003383495, Improvement: 0.00000000055639, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2261
Epoch 2261, Loss: 0.00000002461571, Improvement: -0.00000000921925, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2262
Epoch 2262, Loss: 0.00000002958783, Improvement: 0.00000000497213, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2263
Epoch 2263, Loss: 0.00000004621375, Improvement: 0.00000001662592, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2264
Epoch 2264, Loss: 0.00000003489694, Improvement: -0.00000001131681, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2265
Epoch 2265, Loss: 0.00000003271957, Improvement: -0.00000000217737, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2266
Epoch 2266, Loss: 0.00000002557081, Improvement: -0.00000000714876, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2267
Epoch 2267, Loss: 0.00000003065843, Improvement: 0.00000000508762, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2268
Epoch 2268, Loss: 0.00000002315549, Improvement: -0.00000000750294, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2269
Epoch 2269, Loss: 0.00000002765772, Improvement: 0.00000000450223, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2270
Epoch 2270, Loss: 0.00000002742779, Improvement: -0.00000000022994, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2271
Epoch 2271, Loss: 0.00000003324881, Improvement: 0.00000000582102, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2272
Epoch 2272, Loss: 0.00000005521496, Improvement: 0.00000002196615, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2273
Epoch 2273, Loss: 0.00000005138944, Improvement: -0.00000000382553, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2274
Epoch 2274, Loss: 0.00000006315919, Improvement: 0.00000001176975, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2275
Epoch 2275, Loss: 0.00000004381044, Improvement: -0.00000001934875, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2276
Epoch 2276, Loss: 0.00000003380526, Improvement: -0.00000001000518, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2277
Epoch 2277, Loss: 0.00000002992401, Improvement: -0.00000000388125, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2278
Epoch 2278, Loss: 0.00000004562655, Improvement: 0.00000001570254, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2279
Epoch 2279, Loss: 0.00000004757919, Improvement: 0.00000000195264, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2280
Epoch 2280, Loss: 0.00000003747968, Improvement: -0.00000001009951, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2281
Epoch 2281, Loss: 0.00000004935516, Improvement: 0.00000001187548, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2282
Epoch 2282, Loss: 0.00000003916091, Improvement: -0.00000001019425, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2283
Epoch 2283, Loss: 0.00000003041159, Improvement: -0.00000000874932, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2284
Epoch 2284, Loss: 0.00000004747734, Improvement: 0.00000001706575, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2285
Epoch 2285, Loss: 0.00000004711060, Improvement: -0.00000000036674, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2286
Epoch 2286, Loss: 0.00000005367046, Improvement: 0.00000000655986, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2287
Epoch 2287, Loss: 0.00000005737219, Improvement: 0.00000000370173, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2288
Epoch 2288, Loss: 0.00000005102324, Improvement: -0.00000000634895, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2289
Epoch 2289, Loss: 0.00000005707681, Improvement: 0.00000000605357, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2290
Epoch 2290, Loss: 0.00000004942359, Improvement: -0.00000000765322, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2291
Epoch 2291, Loss: 0.00000003378796, Improvement: -0.00000001563564, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2292
Epoch 2292, Loss: 0.00000003261545, Improvement: -0.00000000117251, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2293
Epoch 2293, Loss: 0.00000005460841, Improvement: 0.00000002199297, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2294
Epoch 2294, Loss: 0.00000004332390, Improvement: -0.00000001128451, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2295
Epoch 2295, Loss: 0.00000003776133, Improvement: -0.00000000556257, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2296
Epoch 2296, Loss: 0.00000002811492, Improvement: -0.00000000964641, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2297
Epoch 2297, Loss: 0.00000003455266, Improvement: 0.00000000643774, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2298
Epoch 2298, Loss: 0.00000003098866, Improvement: -0.00000000356400, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2299
Epoch 2299, Loss: 0.00000003250586, Improvement: 0.00000000151721, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000004426037, Improvement: 0.00000001175451, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2301
Epoch 2301, Loss: 0.00000007211655, Improvement: 0.00000002785618, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2302
Epoch 2302, Loss: 0.00000013875411, Improvement: 0.00000006663756, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2303
Epoch 2303, Loss: 0.00000009939561, Improvement: -0.00000003935850, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2304
Epoch 2304, Loss: 0.00000005358999, Improvement: -0.00000004580562, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2305
Epoch 2305, Loss: 0.00000003046548, Improvement: -0.00000002312451, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2306
Epoch 2306, Loss: 0.00000002346655, Improvement: -0.00000000699893, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2307
Epoch 2307, Loss: 0.00000001905658, Improvement: -0.00000000440997, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2308
Epoch 2308, Loss: 0.00000001781517, Improvement: -0.00000000124141, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2309
Epoch 2309, Loss: 0.00000001569740, Improvement: -0.00000000211777, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2310
Epoch 2310, Loss: 0.00000001619870, Improvement: 0.00000000050130, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2311
Epoch 2311, Loss: 0.00000001659197, Improvement: 0.00000000039327, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2312
Epoch 2312, Loss: 0.00000001823937, Improvement: 0.00000000164740, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2313
Epoch 2313, Loss: 0.00000002134572, Improvement: 0.00000000310635, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2314
Epoch 2314, Loss: 0.00000001782902, Improvement: -0.00000000351671, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2315
Epoch 2315, Loss: 0.00000002197754, Improvement: 0.00000000414853, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2316
Epoch 2316, Loss: 0.00000001919799, Improvement: -0.00000000277955, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2317
Epoch 2317, Loss: 0.00000001760249, Improvement: -0.00000000159550, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2318
Epoch 2318, Loss: 0.00000001651529, Improvement: -0.00000000108720, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2319
Epoch 2319, Loss: 0.00000002103264, Improvement: 0.00000000451735, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2320
Epoch 2320, Loss: 0.00000002666570, Improvement: 0.00000000563305, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2321
Epoch 2321, Loss: 0.00000001883259, Improvement: -0.00000000783311, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2322
Epoch 2322, Loss: 0.00000002227804, Improvement: 0.00000000344545, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2323
Epoch 2323, Loss: 0.00000001968102, Improvement: -0.00000000259701, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2324
Epoch 2324, Loss: 0.00000001800431, Improvement: -0.00000000167671, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2325
Epoch 2325, Loss: 0.00000002381362, Improvement: 0.00000000580931, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2326
Epoch 2326, Loss: 0.00000002531205, Improvement: 0.00000000149842, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2327
Epoch 2327, Loss: 0.00000003522990, Improvement: 0.00000000991785, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2328
Epoch 2328, Loss: 0.00000005748786, Improvement: 0.00000002225796, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2329
Epoch 2329, Loss: 0.00000003675957, Improvement: -0.00000002072830, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2330
Epoch 2330, Loss: 0.00000002986510, Improvement: -0.00000000689447, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2331
Epoch 2331, Loss: 0.00000002841512, Improvement: -0.00000000144998, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2332
Epoch 2332, Loss: 0.00000004501234, Improvement: 0.00000001659722, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2333
Epoch 2333, Loss: 0.00000006420630, Improvement: 0.00000001919396, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2334
Epoch 2334, Loss: 0.00000003764328, Improvement: -0.00000002656303, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2335
Epoch 2335, Loss: 0.00000002571699, Improvement: -0.00000001192628, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2336
Epoch 2336, Loss: 0.00000002233456, Improvement: -0.00000000338243, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2337
Epoch 2337, Loss: 0.00000001977755, Improvement: -0.00000000255702, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2338
Epoch 2338, Loss: 0.00000002086193, Improvement: 0.00000000108439, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2339
Epoch 2339, Loss: 0.00000001855567, Improvement: -0.00000000230626, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2340
Epoch 2340, Loss: 0.00000003293901, Improvement: 0.00000001438334, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2341
Epoch 2341, Loss: 0.00000002541051, Improvement: -0.00000000752850, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2342
Epoch 2342, Loss: 0.00000003061846, Improvement: 0.00000000520795, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2343
Epoch 2343, Loss: 0.00000002938154, Improvement: -0.00000000123692, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2344
Epoch 2344, Loss: 0.00000002069922, Improvement: -0.00000000868232, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2345
Epoch 2345, Loss: 0.00000003181052, Improvement: 0.00000001111131, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2346
Epoch 2346, Loss: 0.00000005300698, Improvement: 0.00000002119645, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2347
Epoch 2347, Loss: 0.00000005566210, Improvement: 0.00000000265513, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2348
Epoch 2348, Loss: 0.00000003516297, Improvement: -0.00000002049913, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2349
Epoch 2349, Loss: 0.00000003720470, Improvement: 0.00000000204173, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000005715129, Improvement: 0.00000001994659, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2351
Epoch 2351, Loss: 0.00000008096904, Improvement: 0.00000002381775, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2352
Epoch 2352, Loss: 0.00000006491568, Improvement: -0.00000001605336, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2353
Epoch 2353, Loss: 0.00000008890502, Improvement: 0.00000002398935, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2354
Epoch 2354, Loss: 0.00000009990482, Improvement: 0.00000001099979, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2355
Epoch 2355, Loss: 0.00000008735603, Improvement: -0.00000001254879, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2356
Epoch 2356, Loss: 0.00000005881906, Improvement: -0.00000002853697, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2357
Epoch 2357, Loss: 0.00000002882615, Improvement: -0.00000002999291, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2358
Epoch 2358, Loss: 0.00000001855190, Improvement: -0.00000001027424, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2359
Epoch 2359, Loss: 0.00000001594294, Improvement: -0.00000000260896, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2360
Epoch 2360, Loss: 0.00000001751205, Improvement: 0.00000000156911, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2361
Epoch 2361, Loss: 0.00000001525435, Improvement: -0.00000000225770, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2362
Epoch 2362, Loss: 0.00000001750134, Improvement: 0.00000000224699, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2363
Epoch 2363, Loss: 0.00000002374095, Improvement: 0.00000000623961, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2364
Epoch 2364, Loss: 0.00000002012713, Improvement: -0.00000000361382, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2365
Epoch 2365, Loss: 0.00000002557942, Improvement: 0.00000000545229, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2366
Epoch 2366, Loss: 0.00000002047340, Improvement: -0.00000000510603, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2367
Epoch 2367, Loss: 0.00000002128373, Improvement: 0.00000000081033, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2368
Epoch 2368, Loss: 0.00000003548285, Improvement: 0.00000001419912, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2369
Epoch 2369, Loss: 0.00000004486999, Improvement: 0.00000000938715, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2370
Epoch 2370, Loss: 0.00000005602580, Improvement: 0.00000001115581, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2371
Epoch 2371, Loss: 0.00000004964938, Improvement: -0.00000000637643, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2372
Epoch 2372, Loss: 0.00000002802783, Improvement: -0.00000002162155, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2373
Epoch 2373, Loss: 0.00000002007559, Improvement: -0.00000000795223, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2374
Epoch 2374, Loss: 0.00000003034552, Improvement: 0.00000001026992, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2375
Epoch 2375, Loss: 0.00000003342472, Improvement: 0.00000000307921, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2376
Epoch 2376, Loss: 0.00000003086825, Improvement: -0.00000000255647, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2377
Epoch 2377, Loss: 0.00000002900112, Improvement: -0.00000000186713, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2378
Epoch 2378, Loss: 0.00000004636281, Improvement: 0.00000001736169, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2379
Epoch 2379, Loss: 0.00000004054962, Improvement: -0.00000000581319, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2380
Epoch 2380, Loss: 0.00000003644046, Improvement: -0.00000000410916, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2381
Epoch 2381, Loss: 0.00000003140424, Improvement: -0.00000000503622, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2382
Epoch 2382, Loss: 0.00000003576190, Improvement: 0.00000000435766, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2383
Epoch 2383, Loss: 0.00000003970364, Improvement: 0.00000000394174, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2384
Epoch 2384, Loss: 0.00000002510069, Improvement: -0.00000001460295, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2385
Epoch 2385, Loss: 0.00000002559804, Improvement: 0.00000000049734, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2386
Epoch 2386, Loss: 0.00000003704984, Improvement: 0.00000001145181, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2387
Epoch 2387, Loss: 0.00000003239164, Improvement: -0.00000000465821, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2388
Epoch 2388, Loss: 0.00000006353710, Improvement: 0.00000003114546, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2389
Epoch 2389, Loss: 0.00000013756685, Improvement: 0.00000007402975, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2390
Epoch 2390, Loss: 0.00000008220081, Improvement: -0.00000005536604, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2391
Epoch 2391, Loss: 0.00000007689611, Improvement: -0.00000000530470, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2392
Epoch 2392, Loss: 0.00000007088377, Improvement: -0.00000000601234, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2393
Epoch 2393, Loss: 0.00000004446162, Improvement: -0.00000002642215, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2394
Epoch 2394, Loss: 0.00000002894427, Improvement: -0.00000001551735, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2395
Epoch 2395, Loss: 0.00000001941778, Improvement: -0.00000000952649, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2396
Epoch 2396, Loss: 0.00000001774004, Improvement: -0.00000000167774, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2397
Epoch 2397, Loss: 0.00000002245774, Improvement: 0.00000000471770, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2398
Epoch 2398, Loss: 0.00000002674974, Improvement: 0.00000000429200, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2399
Epoch 2399, Loss: 0.00000001981515, Improvement: -0.00000000693459, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000001699928, Improvement: -0.00000000281587, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2401
Epoch 2401, Loss: 0.00000001726140, Improvement: 0.00000000026211, Best Loss: 0.00000001006838 in Epoch 2067
Epoch 2402
A best model at epoch 2402 has been saved with training error 0.00000000949277.
Epoch 2402, Loss: 0.00000001742428, Improvement: 0.00000000016288, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2403
Epoch 2403, Loss: 0.00000001340623, Improvement: -0.00000000401805, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2404
Epoch 2404, Loss: 0.00000001431643, Improvement: 0.00000000091020, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2405
Epoch 2405, Loss: 0.00000001449330, Improvement: 0.00000000017688, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2406
Epoch 2406, Loss: 0.00000001858131, Improvement: 0.00000000408800, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2407
Epoch 2407, Loss: 0.00000002757101, Improvement: 0.00000000898970, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2408
Epoch 2408, Loss: 0.00000003142334, Improvement: 0.00000000385233, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2409
Epoch 2409, Loss: 0.00000002544066, Improvement: -0.00000000598268, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2410
Epoch 2410, Loss: 0.00000002419141, Improvement: -0.00000000124925, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2411
Epoch 2411, Loss: 0.00000003995055, Improvement: 0.00000001575914, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2412
Epoch 2412, Loss: 0.00000003085674, Improvement: -0.00000000909381, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2413
Epoch 2413, Loss: 0.00000003519420, Improvement: 0.00000000433747, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2414
Epoch 2414, Loss: 0.00000004101231, Improvement: 0.00000000581811, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2415
Epoch 2415, Loss: 0.00000003496551, Improvement: -0.00000000604680, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2416
Epoch 2416, Loss: 0.00000003076060, Improvement: -0.00000000420491, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2417
Epoch 2417, Loss: 0.00000003222637, Improvement: 0.00000000146577, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2418
Epoch 2418, Loss: 0.00000003996801, Improvement: 0.00000000774163, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2419
Epoch 2419, Loss: 0.00000003661067, Improvement: -0.00000000335734, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2420
Epoch 2420, Loss: 0.00000002903238, Improvement: -0.00000000757829, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2421
Epoch 2421, Loss: 0.00000002530731, Improvement: -0.00000000372506, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2422
Epoch 2422, Loss: 0.00000003267888, Improvement: 0.00000000737157, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2423
Epoch 2423, Loss: 0.00000005570943, Improvement: 0.00000002303055, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2424
Epoch 2424, Loss: 0.00000008861647, Improvement: 0.00000003290704, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2425
Epoch 2425, Loss: 0.00000007860291, Improvement: -0.00000001001356, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2426
Epoch 2426, Loss: 0.00000003324650, Improvement: -0.00000004535641, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2427
Epoch 2427, Loss: 0.00000002587169, Improvement: -0.00000000737481, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2428
Epoch 2428, Loss: 0.00000002512695, Improvement: -0.00000000074474, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2429
Epoch 2429, Loss: 0.00000002050159, Improvement: -0.00000000462537, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2430
Epoch 2430, Loss: 0.00000004013528, Improvement: 0.00000001963369, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2431
Epoch 2431, Loss: 0.00000003343863, Improvement: -0.00000000669665, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2432
Epoch 2432, Loss: 0.00000004693718, Improvement: 0.00000001349856, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2433
Epoch 2433, Loss: 0.00000003065738, Improvement: -0.00000001627981, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2434
Epoch 2434, Loss: 0.00000002545045, Improvement: -0.00000000520692, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2435
Epoch 2435, Loss: 0.00000004550541, Improvement: 0.00000002005496, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2436
Epoch 2436, Loss: 0.00000006327113, Improvement: 0.00000001776572, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2437
Epoch 2437, Loss: 0.00000003404440, Improvement: -0.00000002922673, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2438
Epoch 2438, Loss: 0.00000002746044, Improvement: -0.00000000658396, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2439
Epoch 2439, Loss: 0.00000003409503, Improvement: 0.00000000663460, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2440
Epoch 2440, Loss: 0.00000006604506, Improvement: 0.00000003195003, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2441
Epoch 2441, Loss: 0.00000005496962, Improvement: -0.00000001107544, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2442
Epoch 2442, Loss: 0.00000006859959, Improvement: 0.00000001362997, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2443
Epoch 2443, Loss: 0.00000007550068, Improvement: 0.00000000690108, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2444
Epoch 2444, Loss: 0.00000011263054, Improvement: 0.00000003712987, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2445
Epoch 2445, Loss: 0.00000007227722, Improvement: -0.00000004035332, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2446
Epoch 2446, Loss: 0.00000002367706, Improvement: -0.00000004860016, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2447
Epoch 2447, Loss: 0.00000001668923, Improvement: -0.00000000698784, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2448
Epoch 2448, Loss: 0.00000001673780, Improvement: 0.00000000004858, Best Loss: 0.00000000949277 in Epoch 2402
Epoch 2449
A best model at epoch 2449 has been saved with training error 0.00000000873448.
Epoch 2449, Loss: 0.00000001482585, Improvement: -0.00000000191195, Best Loss: 0.00000000873448 in Epoch 2449
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000001332157, Improvement: -0.00000000150429, Best Loss: 0.00000000873448 in Epoch 2449
Epoch 2451
A best model at epoch 2451 has been saved with training error 0.00000000815219.
Epoch 2451, Loss: 0.00000001359035, Improvement: 0.00000000026878, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2452
Epoch 2452, Loss: 0.00000001275659, Improvement: -0.00000000083375, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2453
Epoch 2453, Loss: 0.00000002018820, Improvement: 0.00000000743160, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2454
Epoch 2454, Loss: 0.00000001796793, Improvement: -0.00000000222026, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2455
Epoch 2455, Loss: 0.00000001788559, Improvement: -0.00000000008234, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2456
Epoch 2456, Loss: 0.00000002005789, Improvement: 0.00000000217230, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2457
Epoch 2457, Loss: 0.00000001692750, Improvement: -0.00000000313039, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2458
Epoch 2458, Loss: 0.00000001800941, Improvement: 0.00000000108191, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2459
Epoch 2459, Loss: 0.00000001391787, Improvement: -0.00000000409154, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2460
Epoch 2460, Loss: 0.00000001428621, Improvement: 0.00000000036834, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2461
Epoch 2461, Loss: 0.00000001403755, Improvement: -0.00000000024866, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2462
Epoch 2462, Loss: 0.00000001439138, Improvement: 0.00000000035383, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2463
Epoch 2463, Loss: 0.00000001753425, Improvement: 0.00000000314287, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2464
Epoch 2464, Loss: 0.00000003459724, Improvement: 0.00000001706299, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2465
Epoch 2465, Loss: 0.00000002999225, Improvement: -0.00000000460499, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2466
Epoch 2466, Loss: 0.00000002665286, Improvement: -0.00000000333938, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2467
Epoch 2467, Loss: 0.00000003381748, Improvement: 0.00000000716461, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2468
Epoch 2468, Loss: 0.00000003411576, Improvement: 0.00000000029829, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2469
Epoch 2469, Loss: 0.00000007785579, Improvement: 0.00000004374003, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2470
Epoch 2470, Loss: 0.00000007276717, Improvement: -0.00000000508862, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2471
Epoch 2471, Loss: 0.00000004976848, Improvement: -0.00000002299869, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2472
Epoch 2472, Loss: 0.00000003133035, Improvement: -0.00000001843812, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2473
Epoch 2473, Loss: 0.00000002516865, Improvement: -0.00000000616171, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2474
Epoch 2474, Loss: 0.00000002130548, Improvement: -0.00000000386317, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2475
Epoch 2475, Loss: 0.00000001516894, Improvement: -0.00000000613654, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2476
Epoch 2476, Loss: 0.00000001636435, Improvement: 0.00000000119541, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2477
Epoch 2477, Loss: 0.00000001676529, Improvement: 0.00000000040094, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2478
Epoch 2478, Loss: 0.00000002282217, Improvement: 0.00000000605688, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2479
Epoch 2479, Loss: 0.00000001926237, Improvement: -0.00000000355980, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2480
Epoch 2480, Loss: 0.00000002187369, Improvement: 0.00000000261132, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2481
Epoch 2481, Loss: 0.00000002679682, Improvement: 0.00000000492313, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2482
Epoch 2482, Loss: 0.00000002227134, Improvement: -0.00000000452549, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2483
Epoch 2483, Loss: 0.00000002196885, Improvement: -0.00000000030249, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2484
Epoch 2484, Loss: 0.00000001997660, Improvement: -0.00000000199225, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2485
Epoch 2485, Loss: 0.00000002721903, Improvement: 0.00000000724243, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2486
Epoch 2486, Loss: 0.00000003374548, Improvement: 0.00000000652644, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2487
Epoch 2487, Loss: 0.00000003434582, Improvement: 0.00000000060034, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2488
Epoch 2488, Loss: 0.00000005483643, Improvement: 0.00000002049061, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2489
Epoch 2489, Loss: 0.00000004430439, Improvement: -0.00000001053204, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2490
Epoch 2490, Loss: 0.00000006241309, Improvement: 0.00000001810870, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2491
Epoch 2491, Loss: 0.00000005554560, Improvement: -0.00000000686749, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2492
Epoch 2492, Loss: 0.00000006096511, Improvement: 0.00000000541951, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2493
Epoch 2493, Loss: 0.00000004799252, Improvement: -0.00000001297259, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2494
Epoch 2494, Loss: 0.00000003338276, Improvement: -0.00000001460977, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2495
Epoch 2495, Loss: 0.00000002471973, Improvement: -0.00000000866302, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2496
Epoch 2496, Loss: 0.00000001993452, Improvement: -0.00000000478521, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2497
Epoch 2497, Loss: 0.00000002137697, Improvement: 0.00000000144245, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2498
Epoch 2498, Loss: 0.00000001667545, Improvement: -0.00000000470152, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2499
Epoch 2499, Loss: 0.00000003588694, Improvement: 0.00000001921148, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000004188011, Improvement: 0.00000000599318, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2501
Epoch 2501, Loss: 0.00000003385114, Improvement: -0.00000000802897, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2502
Epoch 2502, Loss: 0.00000003151813, Improvement: -0.00000000233301, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2503
Epoch 2503, Loss: 0.00000004542124, Improvement: 0.00000001390312, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2504
Epoch 2504, Loss: 0.00000003597327, Improvement: -0.00000000944797, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2505
Epoch 2505, Loss: 0.00000006556807, Improvement: 0.00000002959480, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2506
Epoch 2506, Loss: 0.00000003479116, Improvement: -0.00000003077692, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2507
Epoch 2507, Loss: 0.00000004299079, Improvement: 0.00000000819964, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2508
Epoch 2508, Loss: 0.00000003723209, Improvement: -0.00000000575870, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2509
Epoch 2509, Loss: 0.00000003985158, Improvement: 0.00000000261949, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2510
Epoch 2510, Loss: 0.00000003794604, Improvement: -0.00000000190555, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2511
Epoch 2511, Loss: 0.00000004366526, Improvement: 0.00000000571923, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2512
Epoch 2512, Loss: 0.00000007147245, Improvement: 0.00000002780719, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2513
Epoch 2513, Loss: 0.00000005167456, Improvement: -0.00000001979789, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2514
Epoch 2514, Loss: 0.00000003867764, Improvement: -0.00000001299692, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2515
Epoch 2515, Loss: 0.00000002010849, Improvement: -0.00000001856916, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2516
Epoch 2516, Loss: 0.00000001804742, Improvement: -0.00000000206106, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2517
Epoch 2517, Loss: 0.00000001518094, Improvement: -0.00000000286648, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2518
Epoch 2518, Loss: 0.00000001537022, Improvement: 0.00000000018928, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2519
Epoch 2519, Loss: 0.00000001855802, Improvement: 0.00000000318780, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2520
Epoch 2520, Loss: 0.00000002459526, Improvement: 0.00000000603724, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2521
Epoch 2521, Loss: 0.00000002167927, Improvement: -0.00000000291599, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2522
Epoch 2522, Loss: 0.00000001694663, Improvement: -0.00000000473264, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2523
Epoch 2523, Loss: 0.00000001939060, Improvement: 0.00000000244398, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2524
Epoch 2524, Loss: 0.00000003377517, Improvement: 0.00000001438457, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2525
Epoch 2525, Loss: 0.00000002891926, Improvement: -0.00000000485591, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2526
Epoch 2526, Loss: 0.00000003051040, Improvement: 0.00000000159114, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2527
Epoch 2527, Loss: 0.00000002583086, Improvement: -0.00000000467953, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2528
Epoch 2528, Loss: 0.00000002516077, Improvement: -0.00000000067009, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2529
Epoch 2529, Loss: 0.00000003428235, Improvement: 0.00000000912158, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2530
Epoch 2530, Loss: 0.00000006861151, Improvement: 0.00000003432917, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2531
Epoch 2531, Loss: 0.00000005711623, Improvement: -0.00000001149529, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2532
Epoch 2532, Loss: 0.00000004824618, Improvement: -0.00000000887005, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2533
Epoch 2533, Loss: 0.00000003268644, Improvement: -0.00000001555975, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2534
Epoch 2534, Loss: 0.00000005414725, Improvement: 0.00000002146082, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2535
Epoch 2535, Loss: 0.00000007816815, Improvement: 0.00000002402090, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2536
Epoch 2536, Loss: 0.00000004670468, Improvement: -0.00000003146347, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2537
Epoch 2537, Loss: 0.00000002637249, Improvement: -0.00000002033219, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2538
Epoch 2538, Loss: 0.00000002115965, Improvement: -0.00000000521284, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2539
Epoch 2539, Loss: 0.00000001620124, Improvement: -0.00000000495841, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2540
Epoch 2540, Loss: 0.00000001467121, Improvement: -0.00000000153003, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2541
Epoch 2541, Loss: 0.00000001868250, Improvement: 0.00000000401130, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2542
Epoch 2542, Loss: 0.00000002951197, Improvement: 0.00000001082946, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2543
Epoch 2543, Loss: 0.00000002775806, Improvement: -0.00000000175391, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2544
Epoch 2544, Loss: 0.00000001768732, Improvement: -0.00000001007073, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2545
Epoch 2545, Loss: 0.00000001796147, Improvement: 0.00000000027415, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2546
Epoch 2546, Loss: 0.00000001752003, Improvement: -0.00000000044144, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2547
Epoch 2547, Loss: 0.00000002056141, Improvement: 0.00000000304138, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2548
Epoch 2548, Loss: 0.00000004699431, Improvement: 0.00000002643289, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2549
Epoch 2549, Loss: 0.00000004824499, Improvement: 0.00000000125069, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000003460584, Improvement: -0.00000001363915, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2551
Epoch 2551, Loss: 0.00000003196734, Improvement: -0.00000000263850, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2552
Epoch 2552, Loss: 0.00000003838704, Improvement: 0.00000000641969, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2553
Epoch 2553, Loss: 0.00000003367347, Improvement: -0.00000000471357, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2554
Epoch 2554, Loss: 0.00000005010689, Improvement: 0.00000001643342, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2555
Epoch 2555, Loss: 0.00000006034684, Improvement: 0.00000001023995, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2556
Epoch 2556, Loss: 0.00000003422814, Improvement: -0.00000002611871, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2557
Epoch 2557, Loss: 0.00000001651556, Improvement: -0.00000001771257, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2558
Epoch 2558, Loss: 0.00000001936696, Improvement: 0.00000000285140, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2559
Epoch 2559, Loss: 0.00000003823001, Improvement: 0.00000001886306, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2560
Epoch 2560, Loss: 0.00000003806287, Improvement: -0.00000000016715, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2561
Epoch 2561, Loss: 0.00000003858459, Improvement: 0.00000000052173, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2562
Epoch 2562, Loss: 0.00000004271917, Improvement: 0.00000000413457, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2563
Epoch 2563, Loss: 0.00000004326400, Improvement: 0.00000000054484, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2564
Epoch 2564, Loss: 0.00000003967057, Improvement: -0.00000000359343, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2565
Epoch 2565, Loss: 0.00000002781419, Improvement: -0.00000001185639, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2566
Epoch 2566, Loss: 0.00000002548138, Improvement: -0.00000000233281, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2567
Epoch 2567, Loss: 0.00000002173969, Improvement: -0.00000000374169, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2568
Epoch 2568, Loss: 0.00000001937165, Improvement: -0.00000000236804, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2569
Epoch 2569, Loss: 0.00000002156635, Improvement: 0.00000000219471, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2570
Epoch 2570, Loss: 0.00000002878221, Improvement: 0.00000000721586, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2571
Epoch 2571, Loss: 0.00000002492912, Improvement: -0.00000000385309, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2572
Epoch 2572, Loss: 0.00000003092873, Improvement: 0.00000000599961, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2573
Epoch 2573, Loss: 0.00000002398333, Improvement: -0.00000000694541, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2574
Epoch 2574, Loss: 0.00000003312381, Improvement: 0.00000000914048, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2575
Epoch 2575, Loss: 0.00000002497480, Improvement: -0.00000000814901, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2576
Epoch 2576, Loss: 0.00000003760473, Improvement: 0.00000001262994, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2577
Epoch 2577, Loss: 0.00000002635757, Improvement: -0.00000001124717, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2578
Epoch 2578, Loss: 0.00000007097721, Improvement: 0.00000004461964, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2579
Epoch 2579, Loss: 0.00000005459544, Improvement: -0.00000001638177, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2580
Epoch 2580, Loss: 0.00000006251273, Improvement: 0.00000000791729, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2581
Epoch 2581, Loss: 0.00000002806612, Improvement: -0.00000003444661, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2582
Epoch 2582, Loss: 0.00000001648347, Improvement: -0.00000001158265, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2583
Epoch 2583, Loss: 0.00000001402793, Improvement: -0.00000000245554, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2584
Epoch 2584, Loss: 0.00000001327620, Improvement: -0.00000000075173, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2585
Epoch 2585, Loss: 0.00000001913687, Improvement: 0.00000000586067, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2586
Epoch 2586, Loss: 0.00000001985399, Improvement: 0.00000000071711, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2587
Epoch 2587, Loss: 0.00000001680252, Improvement: -0.00000000305146, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2588
Epoch 2588, Loss: 0.00000001675606, Improvement: -0.00000000004646, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2589
Epoch 2589, Loss: 0.00000001785263, Improvement: 0.00000000109657, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2590
Epoch 2590, Loss: 0.00000001867206, Improvement: 0.00000000081943, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2591
Epoch 2591, Loss: 0.00000002782951, Improvement: 0.00000000915745, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2592
Epoch 2592, Loss: 0.00000003350078, Improvement: 0.00000000567127, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2593
Epoch 2593, Loss: 0.00000002551871, Improvement: -0.00000000798207, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2594
Epoch 2594, Loss: 0.00000002481997, Improvement: -0.00000000069874, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2595
Epoch 2595, Loss: 0.00000003138571, Improvement: 0.00000000656574, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2596
Epoch 2596, Loss: 0.00000004716727, Improvement: 0.00000001578155, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2597
Epoch 2597, Loss: 0.00000004913402, Improvement: 0.00000000196676, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2598
Epoch 2598, Loss: 0.00000003926349, Improvement: -0.00000000987053, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2599
Epoch 2599, Loss: 0.00000002217197, Improvement: -0.00000001709152, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000001832654, Improvement: -0.00000000384543, Best Loss: 0.00000000815219 in Epoch 2451
Epoch 2601
A best model at epoch 2601 has been saved with training error 0.00000000756622.
Epoch 2601, Loss: 0.00000001690688, Improvement: -0.00000000141965, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2602
Epoch 2602, Loss: 0.00000003301165, Improvement: 0.00000001610477, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2603
Epoch 2603, Loss: 0.00000002456014, Improvement: -0.00000000845152, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2604
Epoch 2604, Loss: 0.00000001832862, Improvement: -0.00000000623152, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2605
Epoch 2605, Loss: 0.00000001837756, Improvement: 0.00000000004894, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2606
Epoch 2606, Loss: 0.00000004055126, Improvement: 0.00000002217370, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2607
Epoch 2607, Loss: 0.00000005695664, Improvement: 0.00000001640538, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2608
Epoch 2608, Loss: 0.00000004175538, Improvement: -0.00000001520126, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2609
Epoch 2609, Loss: 0.00000005235845, Improvement: 0.00000001060307, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2610
Epoch 2610, Loss: 0.00000006636389, Improvement: 0.00000001400544, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2611
Epoch 2611, Loss: 0.00000007660956, Improvement: 0.00000001024568, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2612
Epoch 2612, Loss: 0.00000004788292, Improvement: -0.00000002872664, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2613
Epoch 2613, Loss: 0.00000001516673, Improvement: -0.00000003271619, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2614
Epoch 2614, Loss: 0.00000001195394, Improvement: -0.00000000321279, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2615
Epoch 2615, Loss: 0.00000001320290, Improvement: 0.00000000124897, Best Loss: 0.00000000756622 in Epoch 2601
Epoch 2616
A best model at epoch 2616 has been saved with training error 0.00000000655031.
Epoch 2616, Loss: 0.00000001282825, Improvement: -0.00000000037466, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2617
Epoch 2617, Loss: 0.00000001372877, Improvement: 0.00000000090052, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2618
Epoch 2618, Loss: 0.00000002404915, Improvement: 0.00000001032038, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2619
Epoch 2619, Loss: 0.00000003136289, Improvement: 0.00000000731374, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2620
Epoch 2620, Loss: 0.00000002157483, Improvement: -0.00000000978806, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2621
Epoch 2621, Loss: 0.00000002334247, Improvement: 0.00000000176764, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2622
Epoch 2622, Loss: 0.00000003151780, Improvement: 0.00000000817533, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2623
Epoch 2623, Loss: 0.00000002904281, Improvement: -0.00000000247498, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2624
Epoch 2624, Loss: 0.00000002160634, Improvement: -0.00000000743647, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2625
Epoch 2625, Loss: 0.00000002200675, Improvement: 0.00000000040041, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2626
Epoch 2626, Loss: 0.00000001935536, Improvement: -0.00000000265139, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2627
Epoch 2627, Loss: 0.00000001688906, Improvement: -0.00000000246629, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2628
Epoch 2628, Loss: 0.00000001878149, Improvement: 0.00000000189242, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2629
Epoch 2629, Loss: 0.00000003220879, Improvement: 0.00000001342730, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2630
Epoch 2630, Loss: 0.00000003074920, Improvement: -0.00000000145959, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2631
Epoch 2631, Loss: 0.00000002361532, Improvement: -0.00000000713389, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2632
Epoch 2632, Loss: 0.00000002325370, Improvement: -0.00000000036162, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2633
Epoch 2633, Loss: 0.00000003124604, Improvement: 0.00000000799234, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2634
Epoch 2634, Loss: 0.00000004919495, Improvement: 0.00000001794891, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2635
Epoch 2635, Loss: 0.00000003949426, Improvement: -0.00000000970069, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2636
Epoch 2636, Loss: 0.00000003724558, Improvement: -0.00000000224868, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2637
Epoch 2637, Loss: 0.00000004015946, Improvement: 0.00000000291388, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2638
Epoch 2638, Loss: 0.00000004908671, Improvement: 0.00000000892725, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2639
Epoch 2639, Loss: 0.00000003128928, Improvement: -0.00000001779742, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2640
Epoch 2640, Loss: 0.00000004959587, Improvement: 0.00000001830659, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2641
Epoch 2641, Loss: 0.00000006996656, Improvement: 0.00000002037069, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2642
Epoch 2642, Loss: 0.00000008771432, Improvement: 0.00000001774776, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2643
Epoch 2643, Loss: 0.00000007427207, Improvement: -0.00000001344225, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2644
Epoch 2644, Loss: 0.00000003029719, Improvement: -0.00000004397489, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2645
Epoch 2645, Loss: 0.00000001745862, Improvement: -0.00000001283856, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2646
Epoch 2646, Loss: 0.00000001544479, Improvement: -0.00000000201383, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2647
Epoch 2647, Loss: 0.00000001350029, Improvement: -0.00000000194450, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2648
Epoch 2648, Loss: 0.00000001487440, Improvement: 0.00000000137411, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2649
Epoch 2649, Loss: 0.00000001308210, Improvement: -0.00000000179230, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000001388209, Improvement: 0.00000000079999, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2651
Epoch 2651, Loss: 0.00000001113997, Improvement: -0.00000000274212, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2652
Epoch 2652, Loss: 0.00000001127774, Improvement: 0.00000000013777, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2653
Epoch 2653, Loss: 0.00000001401821, Improvement: 0.00000000274047, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2654
Epoch 2654, Loss: 0.00000001352566, Improvement: -0.00000000049255, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2655
Epoch 2655, Loss: 0.00000001688683, Improvement: 0.00000000336117, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2656
Epoch 2656, Loss: 0.00000002948309, Improvement: 0.00000001259626, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2657
Epoch 2657, Loss: 0.00000002053031, Improvement: -0.00000000895279, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2658
Epoch 2658, Loss: 0.00000001815299, Improvement: -0.00000000237732, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2659
Epoch 2659, Loss: 0.00000001375460, Improvement: -0.00000000439839, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2660
Epoch 2660, Loss: 0.00000001500628, Improvement: 0.00000000125168, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2661
Epoch 2661, Loss: 0.00000001508836, Improvement: 0.00000000008208, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2662
Epoch 2662, Loss: 0.00000001583814, Improvement: 0.00000000074977, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2663
Epoch 2663, Loss: 0.00000002531426, Improvement: 0.00000000947612, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2664
Epoch 2664, Loss: 0.00000003331752, Improvement: 0.00000000800326, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2665
Epoch 2665, Loss: 0.00000003591277, Improvement: 0.00000000259525, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2666
Epoch 2666, Loss: 0.00000005170965, Improvement: 0.00000001579687, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2667
Epoch 2667, Loss: 0.00000007516913, Improvement: 0.00000002345949, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2668
Epoch 2668, Loss: 0.00000009145437, Improvement: 0.00000001628524, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2669
Epoch 2669, Loss: 0.00000006278326, Improvement: -0.00000002867111, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2670
Epoch 2670, Loss: 0.00000003876230, Improvement: -0.00000002402097, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2671
Epoch 2671, Loss: 0.00000002366658, Improvement: -0.00000001509572, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2672
Epoch 2672, Loss: 0.00000001661429, Improvement: -0.00000000705228, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2673
Epoch 2673, Loss: 0.00000001406095, Improvement: -0.00000000255335, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2674
Epoch 2674, Loss: 0.00000001252471, Improvement: -0.00000000153624, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2675
Epoch 2675, Loss: 0.00000001405334, Improvement: 0.00000000152863, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2676
Epoch 2676, Loss: 0.00000001384125, Improvement: -0.00000000021209, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2677
Epoch 2677, Loss: 0.00000001144742, Improvement: -0.00000000239383, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2678
Epoch 2678, Loss: 0.00000001144112, Improvement: -0.00000000000629, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2679
Epoch 2679, Loss: 0.00000001491812, Improvement: 0.00000000347700, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2680
Epoch 2680, Loss: 0.00000001793077, Improvement: 0.00000000301264, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2681
Epoch 2681, Loss: 0.00000001734067, Improvement: -0.00000000059009, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2682
Epoch 2682, Loss: 0.00000001992407, Improvement: 0.00000000258339, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2683
Epoch 2683, Loss: 0.00000001573851, Improvement: -0.00000000418556, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2684
Epoch 2684, Loss: 0.00000001604588, Improvement: 0.00000000030737, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2685
Epoch 2685, Loss: 0.00000002376709, Improvement: 0.00000000772121, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2686
Epoch 2686, Loss: 0.00000002568624, Improvement: 0.00000000191916, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2687
Epoch 2687, Loss: 0.00000003150684, Improvement: 0.00000000582060, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2688
Epoch 2688, Loss: 0.00000002758929, Improvement: -0.00000000391755, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2689
Epoch 2689, Loss: 0.00000004688580, Improvement: 0.00000001929651, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2690
Epoch 2690, Loss: 0.00000003148468, Improvement: -0.00000001540112, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2691
Epoch 2691, Loss: 0.00000003021235, Improvement: -0.00000000127233, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2692
Epoch 2692, Loss: 0.00000003569290, Improvement: 0.00000000548055, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2693
Epoch 2693, Loss: 0.00000003274488, Improvement: -0.00000000294802, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2694
Epoch 2694, Loss: 0.00000002507417, Improvement: -0.00000000767071, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2695
Epoch 2695, Loss: 0.00000002249903, Improvement: -0.00000000257515, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2696
Epoch 2696, Loss: 0.00000002944415, Improvement: 0.00000000694512, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2697
Epoch 2697, Loss: 0.00000002959692, Improvement: 0.00000000015277, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2698
Epoch 2698, Loss: 0.00000002163449, Improvement: -0.00000000796243, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2699
Epoch 2699, Loss: 0.00000002065528, Improvement: -0.00000000097921, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000002658188, Improvement: 0.00000000592660, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2701
Epoch 2701, Loss: 0.00000006892788, Improvement: 0.00000004234600, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2702
Epoch 2702, Loss: 0.00000006917567, Improvement: 0.00000000024779, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2703
Epoch 2703, Loss: 0.00000005127613, Improvement: -0.00000001789953, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2704
Epoch 2704, Loss: 0.00000004397310, Improvement: -0.00000000730303, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2705
Epoch 2705, Loss: 0.00000003757416, Improvement: -0.00000000639894, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2706
Epoch 2706, Loss: 0.00000001968185, Improvement: -0.00000001789231, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2707
Epoch 2707, Loss: 0.00000002278264, Improvement: 0.00000000310079, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2708
Epoch 2708, Loss: 0.00000002919517, Improvement: 0.00000000641252, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2709
Epoch 2709, Loss: 0.00000001752631, Improvement: -0.00000001166886, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2710
Epoch 2710, Loss: 0.00000001276623, Improvement: -0.00000000476008, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2711
Epoch 2711, Loss: 0.00000001362616, Improvement: 0.00000000085993, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2712
Epoch 2712, Loss: 0.00000001388655, Improvement: 0.00000000026039, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2713
Epoch 2713, Loss: 0.00000001593994, Improvement: 0.00000000205339, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2714
Epoch 2714, Loss: 0.00000001767901, Improvement: 0.00000000173907, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2715
Epoch 2715, Loss: 0.00000001493901, Improvement: -0.00000000274000, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2716
Epoch 2716, Loss: 0.00000001508927, Improvement: 0.00000000015027, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2717
Epoch 2717, Loss: 0.00000002261456, Improvement: 0.00000000752529, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2718
Epoch 2718, Loss: 0.00000001933099, Improvement: -0.00000000328357, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2719
Epoch 2719, Loss: 0.00000001666187, Improvement: -0.00000000266913, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2720
Epoch 2720, Loss: 0.00000001639428, Improvement: -0.00000000026759, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2721
Epoch 2721, Loss: 0.00000001970368, Improvement: 0.00000000330940, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2722
Epoch 2722, Loss: 0.00000001725830, Improvement: -0.00000000244538, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2723
Epoch 2723, Loss: 0.00000002632564, Improvement: 0.00000000906734, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2724
Epoch 2724, Loss: 0.00000004208581, Improvement: 0.00000001576017, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2725
Epoch 2725, Loss: 0.00000009954726, Improvement: 0.00000005746145, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2726
Epoch 2726, Loss: 0.00000011039887, Improvement: 0.00000001085161, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2727
Epoch 2727, Loss: 0.00000008987922, Improvement: -0.00000002051965, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2728
Epoch 2728, Loss: 0.00000004446290, Improvement: -0.00000004541632, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2729
Epoch 2729, Loss: 0.00000002142004, Improvement: -0.00000002304286, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2730
Epoch 2730, Loss: 0.00000001448450, Improvement: -0.00000000693554, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2731
Epoch 2731, Loss: 0.00000001084727, Improvement: -0.00000000363722, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2732
Epoch 2732, Loss: 0.00000001117787, Improvement: 0.00000000033059, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2733
Epoch 2733, Loss: 0.00000001522864, Improvement: 0.00000000405077, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2734
Epoch 2734, Loss: 0.00000002211231, Improvement: 0.00000000688367, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2735
Epoch 2735, Loss: 0.00000003860573, Improvement: 0.00000001649342, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2736
Epoch 2736, Loss: 0.00000003101596, Improvement: -0.00000000758977, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2737
Epoch 2737, Loss: 0.00000002422459, Improvement: -0.00000000679137, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2738
Epoch 2738, Loss: 0.00000001908638, Improvement: -0.00000000513821, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2739
Epoch 2739, Loss: 0.00000002043526, Improvement: 0.00000000134889, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2740
Epoch 2740, Loss: 0.00000002167618, Improvement: 0.00000000124092, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2741
Epoch 2741, Loss: 0.00000001724306, Improvement: -0.00000000443312, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2742
Epoch 2742, Loss: 0.00000001537411, Improvement: -0.00000000186895, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2743
Epoch 2743, Loss: 0.00000001414726, Improvement: -0.00000000122685, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2744
Epoch 2744, Loss: 0.00000001181825, Improvement: -0.00000000232901, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2745
Epoch 2745, Loss: 0.00000001134907, Improvement: -0.00000000046918, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2746
Epoch 2746, Loss: 0.00000001367007, Improvement: 0.00000000232100, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2747
Epoch 2747, Loss: 0.00000001276923, Improvement: -0.00000000090084, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2748
Epoch 2748, Loss: 0.00000001243091, Improvement: -0.00000000033832, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2749
Epoch 2749, Loss: 0.00000002415682, Improvement: 0.00000001172591, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000003206872, Improvement: 0.00000000791191, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2751
Epoch 2751, Loss: 0.00000003854968, Improvement: 0.00000000648096, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2752
Epoch 2752, Loss: 0.00000005308291, Improvement: 0.00000001453323, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2753
Epoch 2753, Loss: 0.00000003925421, Improvement: -0.00000001382870, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2754
Epoch 2754, Loss: 0.00000002446028, Improvement: -0.00000001479393, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2755
Epoch 2755, Loss: 0.00000002297698, Improvement: -0.00000000148330, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2756
Epoch 2756, Loss: 0.00000002147295, Improvement: -0.00000000150403, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2757
Epoch 2757, Loss: 0.00000007343233, Improvement: 0.00000005195938, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2758
Epoch 2758, Loss: 0.00000004896870, Improvement: -0.00000002446363, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2759
Epoch 2759, Loss: 0.00000004894779, Improvement: -0.00000000002092, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2760
Epoch 2760, Loss: 0.00000004508656, Improvement: -0.00000000386123, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2761
Epoch 2761, Loss: 0.00000003378341, Improvement: -0.00000001130315, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2762
Epoch 2762, Loss: 0.00000001854407, Improvement: -0.00000001523934, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2763
Epoch 2763, Loss: 0.00000002370638, Improvement: 0.00000000516231, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2764
Epoch 2764, Loss: 0.00000001880042, Improvement: -0.00000000490596, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2765
Epoch 2765, Loss: 0.00000001808302, Improvement: -0.00000000071740, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2766
Epoch 2766, Loss: 0.00000001743617, Improvement: -0.00000000064685, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2767
Epoch 2767, Loss: 0.00000001509615, Improvement: -0.00000000234002, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2768
Epoch 2768, Loss: 0.00000001652335, Improvement: 0.00000000142720, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2769
Epoch 2769, Loss: 0.00000001154589, Improvement: -0.00000000497746, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2770
Epoch 2770, Loss: 0.00000001793985, Improvement: 0.00000000639396, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2771
Epoch 2771, Loss: 0.00000001565667, Improvement: -0.00000000228317, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2772
Epoch 2772, Loss: 0.00000001503744, Improvement: -0.00000000061923, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2773
Epoch 2773, Loss: 0.00000002320463, Improvement: 0.00000000816719, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2774
Epoch 2774, Loss: 0.00000002785084, Improvement: 0.00000000464621, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2775
Epoch 2775, Loss: 0.00000001714728, Improvement: -0.00000001070356, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2776
Epoch 2776, Loss: 0.00000003452784, Improvement: 0.00000001738055, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2777
Epoch 2777, Loss: 0.00000003251046, Improvement: -0.00000000201737, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2778
Epoch 2778, Loss: 0.00000005604977, Improvement: 0.00000002353931, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2779
Epoch 2779, Loss: 0.00000005729806, Improvement: 0.00000000124829, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2780
Epoch 2780, Loss: 0.00000002653806, Improvement: -0.00000003076000, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2781
Epoch 2781, Loss: 0.00000003256931, Improvement: 0.00000000603125, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2782
Epoch 2782, Loss: 0.00000004717764, Improvement: 0.00000001460833, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2783
Epoch 2783, Loss: 0.00000006842519, Improvement: 0.00000002124756, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2784
Epoch 2784, Loss: 0.00000005458126, Improvement: -0.00000001384394, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2785
Epoch 2785, Loss: 0.00000003706709, Improvement: -0.00000001751417, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2786
Epoch 2786, Loss: 0.00000002653324, Improvement: -0.00000001053385, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2787
Epoch 2787, Loss: 0.00000002956970, Improvement: 0.00000000303646, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2788
Epoch 2788, Loss: 0.00000002598654, Improvement: -0.00000000358316, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2789
Epoch 2789, Loss: 0.00000001652293, Improvement: -0.00000000946361, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2790
Epoch 2790, Loss: 0.00000001205150, Improvement: -0.00000000447143, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2791
Epoch 2791, Loss: 0.00000001389033, Improvement: 0.00000000183883, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2792
Epoch 2792, Loss: 0.00000001286689, Improvement: -0.00000000102344, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2793
Epoch 2793, Loss: 0.00000001290123, Improvement: 0.00000000003434, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2794
Epoch 2794, Loss: 0.00000001526652, Improvement: 0.00000000236529, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2795
Epoch 2795, Loss: 0.00000001218005, Improvement: -0.00000000308647, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2796
Epoch 2796, Loss: 0.00000001769736, Improvement: 0.00000000551731, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2797
Epoch 2797, Loss: 0.00000002340682, Improvement: 0.00000000570945, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2798
Epoch 2798, Loss: 0.00000005508206, Improvement: 0.00000003167524, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2799
Epoch 2799, Loss: 0.00000007107080, Improvement: 0.00000001598874, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000004636151, Improvement: -0.00000002470929, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2801
Epoch 2801, Loss: 0.00000002463412, Improvement: -0.00000002172738, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2802
Epoch 2802, Loss: 0.00000002157293, Improvement: -0.00000000306119, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2803
Epoch 2803, Loss: 0.00000001263483, Improvement: -0.00000000893810, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2804
Epoch 2804, Loss: 0.00000001235718, Improvement: -0.00000000027765, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2805
Epoch 2805, Loss: 0.00000001144030, Improvement: -0.00000000091689, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2806
Epoch 2806, Loss: 0.00000002660930, Improvement: 0.00000001516901, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2807
Epoch 2807, Loss: 0.00000002455261, Improvement: -0.00000000205670, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2808
Epoch 2808, Loss: 0.00000002766577, Improvement: 0.00000000311316, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2809
Epoch 2809, Loss: 0.00000002565658, Improvement: -0.00000000200919, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2810
Epoch 2810, Loss: 0.00000003577052, Improvement: 0.00000001011394, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2811
Epoch 2811, Loss: 0.00000001932618, Improvement: -0.00000001644434, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2812
Epoch 2812, Loss: 0.00000001588753, Improvement: -0.00000000343865, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2813
Epoch 2813, Loss: 0.00000002255907, Improvement: 0.00000000667154, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2814
Epoch 2814, Loss: 0.00000002140733, Improvement: -0.00000000115174, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2815
Epoch 2815, Loss: 0.00000001992482, Improvement: -0.00000000148251, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2816
Epoch 2816, Loss: 0.00000001890331, Improvement: -0.00000000102151, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2817
Epoch 2817, Loss: 0.00000002325987, Improvement: 0.00000000435656, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2818
Epoch 2818, Loss: 0.00000004533299, Improvement: 0.00000002207312, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2819
Epoch 2819, Loss: 0.00000009074874, Improvement: 0.00000004541575, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2820
Epoch 2820, Loss: 0.00000007015467, Improvement: -0.00000002059407, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2821
Epoch 2821, Loss: 0.00000003743788, Improvement: -0.00000003271678, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2822
Epoch 2822, Loss: 0.00000002173352, Improvement: -0.00000001570437, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2823
Epoch 2823, Loss: 0.00000001245591, Improvement: -0.00000000927760, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2824
Epoch 2824, Loss: 0.00000001240597, Improvement: -0.00000000004994, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2825
Epoch 2825, Loss: 0.00000001523948, Improvement: 0.00000000283351, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2826
Epoch 2826, Loss: 0.00000001691137, Improvement: 0.00000000167190, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2827
Epoch 2827, Loss: 0.00000001477660, Improvement: -0.00000000213477, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2828
Epoch 2828, Loss: 0.00000001282160, Improvement: -0.00000000195501, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2829
Epoch 2829, Loss: 0.00000001326346, Improvement: 0.00000000044186, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2830
Epoch 2830, Loss: 0.00000001849697, Improvement: 0.00000000523350, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2831
Epoch 2831, Loss: 0.00000004095620, Improvement: 0.00000002245924, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2832
Epoch 2832, Loss: 0.00000009771383, Improvement: 0.00000005675763, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2833
Epoch 2833, Loss: 0.00000009064143, Improvement: -0.00000000707241, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2834
Epoch 2834, Loss: 0.00000003493429, Improvement: -0.00000005570714, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2835
Epoch 2835, Loss: 0.00000001625673, Improvement: -0.00000001867756, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2836
Epoch 2836, Loss: 0.00000001249215, Improvement: -0.00000000376457, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2837
Epoch 2837, Loss: 0.00000001099565, Improvement: -0.00000000149651, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2838
Epoch 2838, Loss: 0.00000001021219, Improvement: -0.00000000078345, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2839
Epoch 2839, Loss: 0.00000001001262, Improvement: -0.00000000019957, Best Loss: 0.00000000655031 in Epoch 2616
Epoch 2840
A best model at epoch 2840 has been saved with training error 0.00000000636520.
Epoch 2840, Loss: 0.00000000813190, Improvement: -0.00000000188072, Best Loss: 0.00000000636520 in Epoch 2840
Epoch 2841
A best model at epoch 2841 has been saved with training error 0.00000000599095.
Epoch 2841, Loss: 0.00000000834301, Improvement: 0.00000000021111, Best Loss: 0.00000000599095 in Epoch 2841
Epoch 2842
Epoch 2842, Loss: 0.00000000885116, Improvement: 0.00000000050815, Best Loss: 0.00000000599095 in Epoch 2841
Epoch 2843
Epoch 2843, Loss: 0.00000001038648, Improvement: 0.00000000153532, Best Loss: 0.00000000599095 in Epoch 2841
Epoch 2844
Epoch 2844, Loss: 0.00000001041612, Improvement: 0.00000000002965, Best Loss: 0.00000000599095 in Epoch 2841
Epoch 2845
A best model at epoch 2845 has been saved with training error 0.00000000597969.
Epoch 2845, Loss: 0.00000001414143, Improvement: 0.00000000372530, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2846
Epoch 2846, Loss: 0.00000001163551, Improvement: -0.00000000250591, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2847
Epoch 2847, Loss: 0.00000001328659, Improvement: 0.00000000165108, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2848
Epoch 2848, Loss: 0.00000001478731, Improvement: 0.00000000150072, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2849
Epoch 2849, Loss: 0.00000001222777, Improvement: -0.00000000255954, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000000946237, Improvement: -0.00000000276540, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2851
Epoch 2851, Loss: 0.00000000964981, Improvement: 0.00000000018744, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2852
Epoch 2852, Loss: 0.00000001241562, Improvement: 0.00000000276581, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2853
Epoch 2853, Loss: 0.00000001066455, Improvement: -0.00000000175107, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2854
Epoch 2854, Loss: 0.00000001340277, Improvement: 0.00000000273821, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2855
Epoch 2855, Loss: 0.00000001087738, Improvement: -0.00000000252538, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2856
Epoch 2856, Loss: 0.00000001191388, Improvement: 0.00000000103650, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2857
Epoch 2857, Loss: 0.00000001687358, Improvement: 0.00000000495970, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2858
Epoch 2858, Loss: 0.00000001384756, Improvement: -0.00000000302602, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2859
Epoch 2859, Loss: 0.00000001113803, Improvement: -0.00000000270954, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2860
Epoch 2860, Loss: 0.00000001372432, Improvement: 0.00000000258629, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2861
Epoch 2861, Loss: 0.00000002188224, Improvement: 0.00000000815792, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2862
Epoch 2862, Loss: 0.00000003836751, Improvement: 0.00000001648527, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2863
Epoch 2863, Loss: 0.00000004326080, Improvement: 0.00000000489329, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2864
Epoch 2864, Loss: 0.00000003296940, Improvement: -0.00000001029140, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2865
Epoch 2865, Loss: 0.00000001680824, Improvement: -0.00000001616116, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2866
Epoch 2866, Loss: 0.00000002099721, Improvement: 0.00000000418897, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2867
Epoch 2867, Loss: 0.00000003478808, Improvement: 0.00000001379087, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2868
Epoch 2868, Loss: 0.00000003742984, Improvement: 0.00000000264176, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2869
Epoch 2869, Loss: 0.00000003365626, Improvement: -0.00000000377358, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2870
Epoch 2870, Loss: 0.00000002871293, Improvement: -0.00000000494332, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2871
Epoch 2871, Loss: 0.00000004024465, Improvement: 0.00000001153172, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2872
Epoch 2872, Loss: 0.00000003810289, Improvement: -0.00000000214176, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2873
Epoch 2873, Loss: 0.00000003719577, Improvement: -0.00000000090712, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2874
Epoch 2874, Loss: 0.00000002605003, Improvement: -0.00000001114574, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2875
Epoch 2875, Loss: 0.00000003492488, Improvement: 0.00000000887485, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2876
Epoch 2876, Loss: 0.00000002722015, Improvement: -0.00000000770473, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2877
Epoch 2877, Loss: 0.00000001859334, Improvement: -0.00000000862681, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2878
Epoch 2878, Loss: 0.00000001642895, Improvement: -0.00000000216438, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2879
Epoch 2879, Loss: 0.00000001861948, Improvement: 0.00000000219052, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2880
Epoch 2880, Loss: 0.00000002046639, Improvement: 0.00000000184692, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2881
Epoch 2881, Loss: 0.00000001775573, Improvement: -0.00000000271067, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2882
Epoch 2882, Loss: 0.00000001594382, Improvement: -0.00000000181190, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2883
Epoch 2883, Loss: 0.00000002837214, Improvement: 0.00000001242832, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2884
Epoch 2884, Loss: 0.00000004405981, Improvement: 0.00000001568767, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2885
Epoch 2885, Loss: 0.00000002671875, Improvement: -0.00000001734106, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2886
Epoch 2886, Loss: 0.00000003650494, Improvement: 0.00000000978619, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2887
Epoch 2887, Loss: 0.00000005269619, Improvement: 0.00000001619125, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2888
Epoch 2888, Loss: 0.00000005106903, Improvement: -0.00000000162716, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2889
Epoch 2889, Loss: 0.00000002799851, Improvement: -0.00000002307052, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2890
Epoch 2890, Loss: 0.00000002035907, Improvement: -0.00000000763944, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2891
Epoch 2891, Loss: 0.00000001496341, Improvement: -0.00000000539566, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2892
Epoch 2892, Loss: 0.00000002513971, Improvement: 0.00000001017630, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2893
Epoch 2893, Loss: 0.00000002994230, Improvement: 0.00000000480259, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2894
Epoch 2894, Loss: 0.00000001579329, Improvement: -0.00000001414901, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2895
Epoch 2895, Loss: 0.00000001684104, Improvement: 0.00000000104775, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2896
Epoch 2896, Loss: 0.00000001315898, Improvement: -0.00000000368206, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2897
Epoch 2897, Loss: 0.00000001754691, Improvement: 0.00000000438793, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2898
Epoch 2898, Loss: 0.00000003518890, Improvement: 0.00000001764199, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2899
Epoch 2899, Loss: 0.00000002673302, Improvement: -0.00000000845588, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000003693233, Improvement: 0.00000001019931, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2901
Epoch 2901, Loss: 0.00000003697914, Improvement: 0.00000000004681, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2902
Epoch 2902, Loss: 0.00000004486770, Improvement: 0.00000000788856, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2903
Epoch 2903, Loss: 0.00000002868742, Improvement: -0.00000001618027, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2904
Epoch 2904, Loss: 0.00000001859535, Improvement: -0.00000001009208, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2905
Epoch 2905, Loss: 0.00000001132439, Improvement: -0.00000000727096, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2906
Epoch 2906, Loss: 0.00000001897396, Improvement: 0.00000000764957, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2907
Epoch 2907, Loss: 0.00000002550489, Improvement: 0.00000000653093, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2908
Epoch 2908, Loss: 0.00000005464120, Improvement: 0.00000002913631, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2909
Epoch 2909, Loss: 0.00000004861691, Improvement: -0.00000000602429, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2910
Epoch 2910, Loss: 0.00000002652322, Improvement: -0.00000002209369, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2911
Epoch 2911, Loss: 0.00000001607629, Improvement: -0.00000001044692, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2912
Epoch 2912, Loss: 0.00000001212162, Improvement: -0.00000000395467, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2913
Epoch 2913, Loss: 0.00000001065082, Improvement: -0.00000000147080, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2914
Epoch 2914, Loss: 0.00000001269994, Improvement: 0.00000000204912, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2915
Epoch 2915, Loss: 0.00000001235808, Improvement: -0.00000000034186, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2916
Epoch 2916, Loss: 0.00000001511548, Improvement: 0.00000000275740, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2917
Epoch 2917, Loss: 0.00000002331535, Improvement: 0.00000000819987, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2918
Epoch 2918, Loss: 0.00000002883083, Improvement: 0.00000000551547, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2919
Epoch 2919, Loss: 0.00000003137734, Improvement: 0.00000000254651, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2920
Epoch 2920, Loss: 0.00000001891675, Improvement: -0.00000001246058, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2921
Epoch 2921, Loss: 0.00000001322570, Improvement: -0.00000000569105, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2922
Epoch 2922, Loss: 0.00000001278804, Improvement: -0.00000000043766, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2923
Epoch 2923, Loss: 0.00000001081360, Improvement: -0.00000000197444, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2924
Epoch 2924, Loss: 0.00000001449109, Improvement: 0.00000000367749, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2925
Epoch 2925, Loss: 0.00000001793777, Improvement: 0.00000000344668, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2926
Epoch 2926, Loss: 0.00000002089903, Improvement: 0.00000000296126, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2927
Epoch 2927, Loss: 0.00000002039385, Improvement: -0.00000000050517, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2928
Epoch 2928, Loss: 0.00000001765329, Improvement: -0.00000000274056, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2929
Epoch 2929, Loss: 0.00000002940617, Improvement: 0.00000001175288, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2930
Epoch 2930, Loss: 0.00000006196152, Improvement: 0.00000003255534, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2931
Epoch 2931, Loss: 0.00000005328248, Improvement: -0.00000000867904, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2932
Epoch 2932, Loss: 0.00000004139985, Improvement: -0.00000001188263, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2933
Epoch 2933, Loss: 0.00000004046436, Improvement: -0.00000000093549, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2934
Epoch 2934, Loss: 0.00000003026306, Improvement: -0.00000001020129, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2935
Epoch 2935, Loss: 0.00000003139599, Improvement: 0.00000000113293, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2936
Epoch 2936, Loss: 0.00000003114570, Improvement: -0.00000000025029, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2937
Epoch 2937, Loss: 0.00000001936711, Improvement: -0.00000001177859, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2938
Epoch 2938, Loss: 0.00000002277818, Improvement: 0.00000000341107, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2939
Epoch 2939, Loss: 0.00000001603012, Improvement: -0.00000000674806, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2940
Epoch 2940, Loss: 0.00000001539072, Improvement: -0.00000000063940, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2941
Epoch 2941, Loss: 0.00000001645324, Improvement: 0.00000000106251, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2942
Epoch 2942, Loss: 0.00000003027784, Improvement: 0.00000001382460, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2943
Epoch 2943, Loss: 0.00000002764258, Improvement: -0.00000000263526, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2944
Epoch 2944, Loss: 0.00000001427372, Improvement: -0.00000001336885, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2945
Epoch 2945, Loss: 0.00000001522218, Improvement: 0.00000000094846, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2946
Epoch 2946, Loss: 0.00000001765581, Improvement: 0.00000000243363, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2947
Epoch 2947, Loss: 0.00000001692951, Improvement: -0.00000000072630, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2948
Epoch 2948, Loss: 0.00000001588938, Improvement: -0.00000000104012, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2949
Epoch 2949, Loss: 0.00000001371150, Improvement: -0.00000000217789, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000001944538, Improvement: 0.00000000573389, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2951
Epoch 2951, Loss: 0.00000003380115, Improvement: 0.00000001435577, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2952
Epoch 2952, Loss: 0.00000004134945, Improvement: 0.00000000754830, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2953
Epoch 2953, Loss: 0.00000002189709, Improvement: -0.00000001945236, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2954
Epoch 2954, Loss: 0.00000001869349, Improvement: -0.00000000320359, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2955
Epoch 2955, Loss: 0.00000001531405, Improvement: -0.00000000337944, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2956
Epoch 2956, Loss: 0.00000004022374, Improvement: 0.00000002490969, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2957
Epoch 2957, Loss: 0.00000003905566, Improvement: -0.00000000116807, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2958
Epoch 2958, Loss: 0.00000004509927, Improvement: 0.00000000604360, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2959
Epoch 2959, Loss: 0.00000007088341, Improvement: 0.00000002578415, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2960
Epoch 2960, Loss: 0.00000011148139, Improvement: 0.00000004059797, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2961
Epoch 2961, Loss: 0.00000008084615, Improvement: -0.00000003063524, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2962
Epoch 2962, Loss: 0.00000002956871, Improvement: -0.00000005127744, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2963
Epoch 2963, Loss: 0.00000001297975, Improvement: -0.00000001658896, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2964
Epoch 2964, Loss: 0.00000000900211, Improvement: -0.00000000397765, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2965
Epoch 2965, Loss: 0.00000000917525, Improvement: 0.00000000017315, Best Loss: 0.00000000597969 in Epoch 2845
Epoch 2966
A best model at epoch 2966 has been saved with training error 0.00000000580756.
Epoch 2966, Loss: 0.00000000865390, Improvement: -0.00000000052136, Best Loss: 0.00000000580756 in Epoch 2966
Epoch 2967
Epoch 2967, Loss: 0.00000000801614, Improvement: -0.00000000063775, Best Loss: 0.00000000580756 in Epoch 2966
Epoch 2968
A best model at epoch 2968 has been saved with training error 0.00000000548863.
Epoch 2968, Loss: 0.00000000773580, Improvement: -0.00000000028034, Best Loss: 0.00000000548863 in Epoch 2968
Epoch 2969
A best model at epoch 2969 has been saved with training error 0.00000000507387.
Epoch 2969, Loss: 0.00000000793847, Improvement: 0.00000000020267, Best Loss: 0.00000000507387 in Epoch 2969
Epoch 2970
Epoch 2970, Loss: 0.00000000810545, Improvement: 0.00000000016698, Best Loss: 0.00000000507387 in Epoch 2969
Epoch 2971
Epoch 2971, Loss: 0.00000000774383, Improvement: -0.00000000036162, Best Loss: 0.00000000507387 in Epoch 2969
Epoch 2972
Epoch 2972, Loss: 0.00000000833200, Improvement: 0.00000000058817, Best Loss: 0.00000000507387 in Epoch 2969
Epoch 2973
A best model at epoch 2973 has been saved with training error 0.00000000503928.
Epoch 2973, Loss: 0.00000000753769, Improvement: -0.00000000079431, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2974
Epoch 2974, Loss: 0.00000000867252, Improvement: 0.00000000113483, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2975
Epoch 2975, Loss: 0.00000001009561, Improvement: 0.00000000142308, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2976
Epoch 2976, Loss: 0.00000000943288, Improvement: -0.00000000066272, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2977
Epoch 2977, Loss: 0.00000000864722, Improvement: -0.00000000078566, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2978
Epoch 2978, Loss: 0.00000000791077, Improvement: -0.00000000073645, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2979
Epoch 2979, Loss: 0.00000000846216, Improvement: 0.00000000055139, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2980
Epoch 2980, Loss: 0.00000001047032, Improvement: 0.00000000200816, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2981
Epoch 2981, Loss: 0.00000001777695, Improvement: 0.00000000730663, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2982
Epoch 2982, Loss: 0.00000002182887, Improvement: 0.00000000405191, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2983
Epoch 2983, Loss: 0.00000001389471, Improvement: -0.00000000793415, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2984
Epoch 2984, Loss: 0.00000001327273, Improvement: -0.00000000062198, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2985
Epoch 2985, Loss: 0.00000001221567, Improvement: -0.00000000105707, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2986
Epoch 2986, Loss: 0.00000001267574, Improvement: 0.00000000046008, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2987
Epoch 2987, Loss: 0.00000001881634, Improvement: 0.00000000614060, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2988
Epoch 2988, Loss: 0.00000002488974, Improvement: 0.00000000607340, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2989
Epoch 2989, Loss: 0.00000002121042, Improvement: -0.00000000367933, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2990
Epoch 2990, Loss: 0.00000007473004, Improvement: 0.00000005351963, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2991
Epoch 2991, Loss: 0.00000011112386, Improvement: 0.00000003639382, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2992
Epoch 2992, Loss: 0.00000007044796, Improvement: -0.00000004067590, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2993
Epoch 2993, Loss: 0.00000003674299, Improvement: -0.00000003370497, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2994
Epoch 2994, Loss: 0.00000001637839, Improvement: -0.00000002036460, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2995
Epoch 2995, Loss: 0.00000000968641, Improvement: -0.00000000669198, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2996
Epoch 2996, Loss: 0.00000000882095, Improvement: -0.00000000086545, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2997
Epoch 2997, Loss: 0.00000001297986, Improvement: 0.00000000415890, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2998
Epoch 2998, Loss: 0.00000001911719, Improvement: 0.00000000613733, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 2999
Epoch 2999, Loss: 0.00000001219171, Improvement: -0.00000000692548, Best Loss: 0.00000000503928 in Epoch 2973
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000001074433, Improvement: -0.00000000144738, Best Loss: 0.00000000503928 in Epoch 2973
