The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00026165810414.
A best model at epoch 1 has been saved with training error 0.00006599642802.
Epoch 1, Loss: 0.00189519567903, Improvement: 0.00189519567903, Best Loss: 0.00006599642802 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.00005132595106.
Epoch 2, Loss: 0.00009820389714, Improvement: -0.00179699178188, Best Loss: 0.00005132595106 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.00004889715638.
A best model at epoch 3 has been saved with training error 0.00003966252189.
A best model at epoch 3 has been saved with training error 0.00003484472109.
Epoch 3, Loss: 0.00005959214359, Improvement: -0.00003861175355, Best Loss: 0.00003484472109 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.00003465925693.
Epoch 4, Loss: 0.00005158428394, Improvement: -0.00000800785965, Best Loss: 0.00003465925693 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.00003109969111.
Epoch 5, Loss: 0.00005021787747, Improvement: -0.00000136640647, Best Loss: 0.00003109969111 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.00004969754355, Improvement: -0.00000052033392, Best Loss: 0.00003109969111 in Epoch 5
Epoch 7
A best model at epoch 7 has been saved with training error 0.00002530845813.
Epoch 7, Loss: 0.00004923797042, Improvement: -0.00000045957313, Best Loss: 0.00002530845813 in Epoch 7
Epoch 8
Epoch 8, Loss: 0.00004878120162, Improvement: -0.00000045676879, Best Loss: 0.00002530845813 in Epoch 7
Epoch 9
Epoch 9, Loss: 0.00004830136277, Improvement: -0.00000047983885, Best Loss: 0.00002530845813 in Epoch 7
Epoch 10
Epoch 10, Loss: 0.00004781995940, Improvement: -0.00000048140337, Best Loss: 0.00002530845813 in Epoch 7
Epoch 11
Epoch 11, Loss: 0.00004731671979, Improvement: -0.00000050323961, Best Loss: 0.00002530845813 in Epoch 7
Epoch 12
Epoch 12, Loss: 0.00004677608194, Improvement: -0.00000054063785, Best Loss: 0.00002530845813 in Epoch 7
Epoch 13
Epoch 13, Loss: 0.00004618968760, Improvement: -0.00000058639434, Best Loss: 0.00002530845813 in Epoch 7
Epoch 14
Epoch 14, Loss: 0.00004562250451, Improvement: -0.00000056718309, Best Loss: 0.00002530845813 in Epoch 7
Epoch 15
Epoch 15, Loss: 0.00004496863476, Improvement: -0.00000065386976, Best Loss: 0.00002530845813 in Epoch 7
Epoch 16
Epoch 16, Loss: 0.00004429182591, Improvement: -0.00000067680885, Best Loss: 0.00002530845813 in Epoch 7
Epoch 17
Epoch 17, Loss: 0.00004359203840, Improvement: -0.00000069978751, Best Loss: 0.00002530845813 in Epoch 7
Epoch 18
A best model at epoch 18 has been saved with training error 0.00002302121538.
Epoch 18, Loss: 0.00004282559530, Improvement: -0.00000076644310, Best Loss: 0.00002302121538 in Epoch 18
Epoch 19
Epoch 19, Loss: 0.00004199832920, Improvement: -0.00000082726610, Best Loss: 0.00002302121538 in Epoch 18
Epoch 20
Epoch 20, Loss: 0.00004113143423, Improvement: -0.00000086689497, Best Loss: 0.00002302121538 in Epoch 18
Epoch 21
A best model at epoch 21 has been saved with training error 0.00002071561721.
Epoch 21, Loss: 0.00004019837606, Improvement: -0.00000093305816, Best Loss: 0.00002071561721 in Epoch 21
Epoch 22
Epoch 22, Loss: 0.00003910203013, Improvement: -0.00000109634593, Best Loss: 0.00002071561721 in Epoch 21
Epoch 23
Epoch 23, Loss: 0.00003798599491, Improvement: -0.00000111603522, Best Loss: 0.00002071561721 in Epoch 21
Epoch 24
Epoch 24, Loss: 0.00003676524984, Improvement: -0.00000122074507, Best Loss: 0.00002071561721 in Epoch 21
Epoch 25
Epoch 25, Loss: 0.00003541666638, Improvement: -0.00000134858346, Best Loss: 0.00002071561721 in Epoch 21
Epoch 26
A best model at epoch 26 has been saved with training error 0.00001927792255.
Epoch 26, Loss: 0.00003399893822, Improvement: -0.00000141772816, Best Loss: 0.00001927792255 in Epoch 26
Epoch 27
A best model at epoch 27 has been saved with training error 0.00001712137782.
Epoch 27, Loss: 0.00003242060084, Improvement: -0.00000157833738, Best Loss: 0.00001712137782 in Epoch 27
Epoch 28
Epoch 28, Loss: 0.00003073810913, Improvement: -0.00000168249171, Best Loss: 0.00001712137782 in Epoch 27
Epoch 29
Epoch 29, Loss: 0.00002896329461, Improvement: -0.00000177481452, Best Loss: 0.00001712137782 in Epoch 27
Epoch 30
Epoch 30, Loss: 0.00002705421903, Improvement: -0.00000190907558, Best Loss: 0.00001712137782 in Epoch 27
Epoch 31
A best model at epoch 31 has been saved with training error 0.00001387989232.
Epoch 31, Loss: 0.00002508045732, Improvement: -0.00000197376171, Best Loss: 0.00001387989232 in Epoch 31
Epoch 32
A best model at epoch 32 has been saved with training error 0.00001082923518.
Epoch 32, Loss: 0.00002311086964, Improvement: -0.00000196958767, Best Loss: 0.00001082923518 in Epoch 32
Epoch 33
Epoch 33, Loss: 0.00002110041924, Improvement: -0.00000201045041, Best Loss: 0.00001082923518 in Epoch 32
Epoch 34
Epoch 34, Loss: 0.00001923888117, Improvement: -0.00000186153807, Best Loss: 0.00001082923518 in Epoch 32
Epoch 35
A best model at epoch 35 has been saved with training error 0.00001074880311.
Epoch 35, Loss: 0.00001741797314, Improvement: -0.00000182090803, Best Loss: 0.00001074880311 in Epoch 35
Epoch 36
A best model at epoch 36 has been saved with training error 0.00000913873282.
Epoch 36, Loss: 0.00001571806470, Improvement: -0.00000169990844, Best Loss: 0.00000913873282 in Epoch 36
Epoch 37
Epoch 37, Loss: 0.00001408647022, Improvement: -0.00000163159448, Best Loss: 0.00000913873282 in Epoch 36
Epoch 38
A best model at epoch 38 has been saved with training error 0.00000837876632.
A best model at epoch 38 has been saved with training error 0.00000714629414.
Epoch 38, Loss: 0.00001256647199, Improvement: -0.00000151999823, Best Loss: 0.00000714629414 in Epoch 38
Epoch 39
A best model at epoch 39 has been saved with training error 0.00000699656994.
Epoch 39, Loss: 0.00001106787138, Improvement: -0.00000149860061, Best Loss: 0.00000699656994 in Epoch 39
Epoch 40
A best model at epoch 40 has been saved with training error 0.00000656879547.
Epoch 40, Loss: 0.00000962145837, Improvement: -0.00000144641301, Best Loss: 0.00000656879547 in Epoch 40
Epoch 41
A best model at epoch 41 has been saved with training error 0.00000643844487.
A best model at epoch 41 has been saved with training error 0.00000562242894.
Epoch 41, Loss: 0.00000823877438, Improvement: -0.00000138268399, Best Loss: 0.00000562242894 in Epoch 41
Epoch 42
A best model at epoch 42 has been saved with training error 0.00000431785656.
Epoch 42, Loss: 0.00000695566641, Improvement: -0.00000128310796, Best Loss: 0.00000431785656 in Epoch 42
Epoch 43
A best model at epoch 43 has been saved with training error 0.00000423437723.
A best model at epoch 43 has been saved with training error 0.00000398480142.
Epoch 43, Loss: 0.00000581797417, Improvement: -0.00000113769224, Best Loss: 0.00000398480142 in Epoch 43
Epoch 44
A best model at epoch 44 has been saved with training error 0.00000359172554.
A best model at epoch 44 has been saved with training error 0.00000349877473.
A best model at epoch 44 has been saved with training error 0.00000316641331.
Epoch 44, Loss: 0.00000480948736, Improvement: -0.00000100848681, Best Loss: 0.00000316641331 in Epoch 44
Epoch 45
A best model at epoch 45 has been saved with training error 0.00000253256962.
A best model at epoch 45 has been saved with training error 0.00000219510935.
Epoch 45, Loss: 0.00000397492018, Improvement: -0.00000083456717, Best Loss: 0.00000219510935 in Epoch 45
Epoch 46
Epoch 46, Loss: 0.00000331475250, Improvement: -0.00000066016769, Best Loss: 0.00000219510935 in Epoch 45
Epoch 47
A best model at epoch 47 has been saved with training error 0.00000213853741.
A best model at epoch 47 has been saved with training error 0.00000201858552.
A best model at epoch 47 has been saved with training error 0.00000182861334.
Epoch 47, Loss: 0.00000281116347, Improvement: -0.00000050358903, Best Loss: 0.00000182861334 in Epoch 47
Epoch 48
A best model at epoch 48 has been saved with training error 0.00000140807015.
Epoch 48, Loss: 0.00000242396183, Improvement: -0.00000038720164, Best Loss: 0.00000140807015 in Epoch 48
Epoch 49
Epoch 49, Loss: 0.00000214404073, Improvement: -0.00000027992110, Best Loss: 0.00000140807015 in Epoch 48
Epoch 50
A best model at epoch 50 has been saved with training error 0.00000117978300.
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00000193201217, Improvement: -0.00000021202856, Best Loss: 0.00000117978300 in Epoch 50
Epoch 51
Epoch 51, Loss: 0.00000177409137, Improvement: -0.00000015792081, Best Loss: 0.00000117978300 in Epoch 50
Epoch 52
A best model at epoch 52 has been saved with training error 0.00000114546106.
Epoch 52, Loss: 0.00000165101430, Improvement: -0.00000012307707, Best Loss: 0.00000114546106 in Epoch 52
Epoch 53
Epoch 53, Loss: 0.00000155304307, Improvement: -0.00000009797122, Best Loss: 0.00000114546106 in Epoch 52
Epoch 54
A best model at epoch 54 has been saved with training error 0.00000111662848.
A best model at epoch 54 has been saved with training error 0.00000097911925.
Epoch 54, Loss: 0.00000146629733, Improvement: -0.00000008674574, Best Loss: 0.00000097911925 in Epoch 54
Epoch 55
A best model at epoch 55 has been saved with training error 0.00000097231941.
Epoch 55, Loss: 0.00000139558277, Improvement: -0.00000007071457, Best Loss: 0.00000097231941 in Epoch 55
Epoch 56
A best model at epoch 56 has been saved with training error 0.00000078711713.
Epoch 56, Loss: 0.00000133514495, Improvement: -0.00000006043782, Best Loss: 0.00000078711713 in Epoch 56
Epoch 57
Epoch 57, Loss: 0.00000127661317, Improvement: -0.00000005853178, Best Loss: 0.00000078711713 in Epoch 56
Epoch 58
A best model at epoch 58 has been saved with training error 0.00000076120961.
Epoch 58, Loss: 0.00000123109179, Improvement: -0.00000004552138, Best Loss: 0.00000076120961 in Epoch 58
Epoch 59
Epoch 59, Loss: 0.00000118525511, Improvement: -0.00000004583668, Best Loss: 0.00000076120961 in Epoch 58
Epoch 60
Epoch 60, Loss: 0.00000114171330, Improvement: -0.00000004354181, Best Loss: 0.00000076120961 in Epoch 58
Epoch 61
Epoch 61, Loss: 0.00000110645283, Improvement: -0.00000003526047, Best Loss: 0.00000076120961 in Epoch 58
Epoch 62
Epoch 62, Loss: 0.00000106508218, Improvement: -0.00000004137064, Best Loss: 0.00000076120961 in Epoch 58
Epoch 63
A best model at epoch 63 has been saved with training error 0.00000064177431.
Epoch 63, Loss: 0.00000103022283, Improvement: -0.00000003485935, Best Loss: 0.00000064177431 in Epoch 63
Epoch 64
Epoch 64, Loss: 0.00000100102863, Improvement: -0.00000002919420, Best Loss: 0.00000064177431 in Epoch 63
Epoch 65
Epoch 65, Loss: 0.00000096571912, Improvement: -0.00000003530950, Best Loss: 0.00000064177431 in Epoch 63
Epoch 66
A best model at epoch 66 has been saved with training error 0.00000062736785.
Epoch 66, Loss: 0.00000093580348, Improvement: -0.00000002991564, Best Loss: 0.00000062736785 in Epoch 66
Epoch 67
Epoch 67, Loss: 0.00000090722144, Improvement: -0.00000002858205, Best Loss: 0.00000062736785 in Epoch 66
Epoch 68
Epoch 68, Loss: 0.00000087735754, Improvement: -0.00000002986389, Best Loss: 0.00000062736785 in Epoch 66
Epoch 69
A best model at epoch 69 has been saved with training error 0.00000057824099.
Epoch 69, Loss: 0.00000085368560, Improvement: -0.00000002367195, Best Loss: 0.00000057824099 in Epoch 69
Epoch 70
Epoch 70, Loss: 0.00000082603654, Improvement: -0.00000002764906, Best Loss: 0.00000057824099 in Epoch 69
Epoch 71
A best model at epoch 71 has been saved with training error 0.00000054485866.
Epoch 71, Loss: 0.00000079877773, Improvement: -0.00000002725880, Best Loss: 0.00000054485866 in Epoch 71
Epoch 72
A best model at epoch 72 has been saved with training error 0.00000052931625.
Epoch 72, Loss: 0.00000077319392, Improvement: -0.00000002558382, Best Loss: 0.00000052931625 in Epoch 72
Epoch 73
Epoch 73, Loss: 0.00000074947253, Improvement: -0.00000002372139, Best Loss: 0.00000052931625 in Epoch 72
Epoch 74
Epoch 74, Loss: 0.00000072449267, Improvement: -0.00000002497985, Best Loss: 0.00000052931625 in Epoch 72
Epoch 75
Epoch 75, Loss: 0.00000070312289, Improvement: -0.00000002136978, Best Loss: 0.00000052931625 in Epoch 72
Epoch 76
Epoch 76, Loss: 0.00000068093329, Improvement: -0.00000002218960, Best Loss: 0.00000052931625 in Epoch 72
Epoch 77
A best model at epoch 77 has been saved with training error 0.00000052824265.
A best model at epoch 77 has been saved with training error 0.00000051605178.
Epoch 77, Loss: 0.00000065851477, Improvement: -0.00000002241852, Best Loss: 0.00000051605178 in Epoch 77
Epoch 78
A best model at epoch 78 has been saved with training error 0.00000045447987.
Epoch 78, Loss: 0.00000063949322, Improvement: -0.00000001902155, Best Loss: 0.00000045447987 in Epoch 78
Epoch 79
A best model at epoch 79 has been saved with training error 0.00000045403860.
A best model at epoch 79 has been saved with training error 0.00000042689416.
Epoch 79, Loss: 0.00000060982240, Improvement: -0.00000002967082, Best Loss: 0.00000042689416 in Epoch 79
Epoch 80
A best model at epoch 80 has been saved with training error 0.00000041101836.
Epoch 80, Loss: 0.00000058742365, Improvement: -0.00000002239875, Best Loss: 0.00000041101836 in Epoch 80
Epoch 81
A best model at epoch 81 has been saved with training error 0.00000033902856.
Epoch 81, Loss: 0.00000056589860, Improvement: -0.00000002152505, Best Loss: 0.00000033902856 in Epoch 81
Epoch 82
Epoch 82, Loss: 0.00000054534771, Improvement: -0.00000002055089, Best Loss: 0.00000033902856 in Epoch 81
Epoch 83
Epoch 83, Loss: 0.00000052600071, Improvement: -0.00000001934701, Best Loss: 0.00000033902856 in Epoch 81
Epoch 84
Epoch 84, Loss: 0.00000050448233, Improvement: -0.00000002151838, Best Loss: 0.00000033902856 in Epoch 81
Epoch 85
Epoch 85, Loss: 0.00000048479102, Improvement: -0.00000001969130, Best Loss: 0.00000033902856 in Epoch 81
Epoch 86
Epoch 86, Loss: 0.00000046736219, Improvement: -0.00000001742883, Best Loss: 0.00000033902856 in Epoch 81
Epoch 87
A best model at epoch 87 has been saved with training error 0.00000032331914.
Epoch 87, Loss: 0.00000044882603, Improvement: -0.00000001853617, Best Loss: 0.00000032331914 in Epoch 87
Epoch 88
Epoch 88, Loss: 0.00000043037231, Improvement: -0.00000001845372, Best Loss: 0.00000032331914 in Epoch 87
Epoch 89
A best model at epoch 89 has been saved with training error 0.00000024869320.
Epoch 89, Loss: 0.00000041359262, Improvement: -0.00000001677968, Best Loss: 0.00000024869320 in Epoch 89
Epoch 90
Epoch 90, Loss: 0.00000039791845, Improvement: -0.00000001567417, Best Loss: 0.00000024869320 in Epoch 89
Epoch 91
Epoch 91, Loss: 0.00000038170467, Improvement: -0.00000001621378, Best Loss: 0.00000024869320 in Epoch 89
Epoch 92
Epoch 92, Loss: 0.00000036491852, Improvement: -0.00000001678616, Best Loss: 0.00000024869320 in Epoch 89
Epoch 93
Epoch 93, Loss: 0.00000035113908, Improvement: -0.00000001377943, Best Loss: 0.00000024869320 in Epoch 89
Epoch 94
Epoch 94, Loss: 0.00000033664423, Improvement: -0.00000001449485, Best Loss: 0.00000024869320 in Epoch 89
Epoch 95
A best model at epoch 95 has been saved with training error 0.00000021540150.
Epoch 95, Loss: 0.00000032432404, Improvement: -0.00000001232019, Best Loss: 0.00000021540150 in Epoch 95
Epoch 96
A best model at epoch 96 has been saved with training error 0.00000020551288.
Epoch 96, Loss: 0.00000031260998, Improvement: -0.00000001171406, Best Loss: 0.00000020551288 in Epoch 96
Epoch 97
Epoch 97, Loss: 0.00000030325611, Improvement: -0.00000000935387, Best Loss: 0.00000020551288 in Epoch 96
Epoch 98
Epoch 98, Loss: 0.00000029216769, Improvement: -0.00000001108843, Best Loss: 0.00000020551288 in Epoch 96
Epoch 99
Epoch 99, Loss: 0.00000028219599, Improvement: -0.00000000997170, Best Loss: 0.00000020551288 in Epoch 96
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00000027327430, Improvement: -0.00000000892169, Best Loss: 0.00000020551288 in Epoch 96
Epoch 101
Epoch 101, Loss: 0.00000026657990, Improvement: -0.00000000669440, Best Loss: 0.00000020551288 in Epoch 96
Epoch 102
A best model at epoch 102 has been saved with training error 0.00000019566761.
Epoch 102, Loss: 0.00000025888043, Improvement: -0.00000000769947, Best Loss: 0.00000019566761 in Epoch 102
Epoch 103
Epoch 103, Loss: 0.00000025373053, Improvement: -0.00000000514990, Best Loss: 0.00000019566761 in Epoch 102
Epoch 104
A best model at epoch 104 has been saved with training error 0.00000016039093.
Epoch 104, Loss: 0.00000024790021, Improvement: -0.00000000583032, Best Loss: 0.00000016039093 in Epoch 104
Epoch 105
Epoch 105, Loss: 0.00000024412775, Improvement: -0.00000000377246, Best Loss: 0.00000016039093 in Epoch 104
Epoch 106
Epoch 106, Loss: 0.00000023995833, Improvement: -0.00000000416942, Best Loss: 0.00000016039093 in Epoch 104
Epoch 107
Epoch 107, Loss: 0.00000023548117, Improvement: -0.00000000447716, Best Loss: 0.00000016039093 in Epoch 104
Epoch 108
Epoch 108, Loss: 0.00000023449859, Improvement: -0.00000000098258, Best Loss: 0.00000016039093 in Epoch 104
Epoch 109
A best model at epoch 109 has been saved with training error 0.00000015357070.
Epoch 109, Loss: 0.00000022906206, Improvement: -0.00000000543653, Best Loss: 0.00000015357070 in Epoch 109
Epoch 110
Epoch 110, Loss: 0.00000022616303, Improvement: -0.00000000289903, Best Loss: 0.00000015357070 in Epoch 109
Epoch 111
A best model at epoch 111 has been saved with training error 0.00000013512316.
Epoch 111, Loss: 0.00000022234334, Improvement: -0.00000000381970, Best Loss: 0.00000013512316 in Epoch 111
Epoch 112
Epoch 112, Loss: 0.00000022011903, Improvement: -0.00000000222431, Best Loss: 0.00000013512316 in Epoch 111
Epoch 113
Epoch 113, Loss: 0.00000021861220, Improvement: -0.00000000150683, Best Loss: 0.00000013512316 in Epoch 111
Epoch 114
Epoch 114, Loss: 0.00000021683777, Improvement: -0.00000000177443, Best Loss: 0.00000013512316 in Epoch 111
Epoch 115
Epoch 115, Loss: 0.00000021547733, Improvement: -0.00000000136043, Best Loss: 0.00000013512316 in Epoch 111
Epoch 116
Epoch 116, Loss: 0.00000021503536, Improvement: -0.00000000044197, Best Loss: 0.00000013512316 in Epoch 111
Epoch 117
Epoch 117, Loss: 0.00000021338476, Improvement: -0.00000000165060, Best Loss: 0.00000013512316 in Epoch 111
Epoch 118
Epoch 118, Loss: 0.00000021273661, Improvement: -0.00000000064815, Best Loss: 0.00000013512316 in Epoch 111
Epoch 119
Epoch 119, Loss: 0.00000021025119, Improvement: -0.00000000248542, Best Loss: 0.00000013512316 in Epoch 111
Epoch 120
Epoch 120, Loss: 0.00000020910543, Improvement: -0.00000000114576, Best Loss: 0.00000013512316 in Epoch 111
Epoch 121
Epoch 121, Loss: 0.00000020876727, Improvement: -0.00000000033816, Best Loss: 0.00000013512316 in Epoch 111
Epoch 122
Epoch 122, Loss: 0.00000020983548, Improvement: 0.00000000106821, Best Loss: 0.00000013512316 in Epoch 111
Epoch 123
Epoch 123, Loss: 0.00000020879963, Improvement: -0.00000000103585, Best Loss: 0.00000013512316 in Epoch 111
Epoch 124
Epoch 124, Loss: 0.00000020611612, Improvement: -0.00000000268351, Best Loss: 0.00000013512316 in Epoch 111
Epoch 125
Epoch 125, Loss: 0.00000020727575, Improvement: 0.00000000115963, Best Loss: 0.00000013512316 in Epoch 111
Epoch 126
Epoch 126, Loss: 0.00000020706951, Improvement: -0.00000000020624, Best Loss: 0.00000013512316 in Epoch 111
Epoch 127
Epoch 127, Loss: 0.00000020754337, Improvement: 0.00000000047386, Best Loss: 0.00000013512316 in Epoch 111
Epoch 128
Epoch 128, Loss: 0.00000020435721, Improvement: -0.00000000318616, Best Loss: 0.00000013512316 in Epoch 111
Epoch 129
Epoch 129, Loss: 0.00000020512344, Improvement: 0.00000000076624, Best Loss: 0.00000013512316 in Epoch 111
Epoch 130
Epoch 130, Loss: 0.00000020362170, Improvement: -0.00000000150174, Best Loss: 0.00000013512316 in Epoch 111
Epoch 131
Epoch 131, Loss: 0.00000020339729, Improvement: -0.00000000022441, Best Loss: 0.00000013512316 in Epoch 111
Epoch 132
Epoch 132, Loss: 0.00000020557156, Improvement: 0.00000000217426, Best Loss: 0.00000013512316 in Epoch 111
Epoch 133
A best model at epoch 133 has been saved with training error 0.00000012220741.
Epoch 133, Loss: 0.00000020434959, Improvement: -0.00000000122197, Best Loss: 0.00000012220741 in Epoch 133
Epoch 134
Epoch 134, Loss: 0.00000020221946, Improvement: -0.00000000213012, Best Loss: 0.00000012220741 in Epoch 133
Epoch 135
A best model at epoch 135 has been saved with training error 0.00000012093899.
Epoch 135, Loss: 0.00000020232819, Improvement: 0.00000000010873, Best Loss: 0.00000012093899 in Epoch 135
Epoch 136
Epoch 136, Loss: 0.00000020146417, Improvement: -0.00000000086402, Best Loss: 0.00000012093899 in Epoch 135
Epoch 137
Epoch 137, Loss: 0.00000020010736, Improvement: -0.00000000135681, Best Loss: 0.00000012093899 in Epoch 135
Epoch 138
Epoch 138, Loss: 0.00000020160723, Improvement: 0.00000000149987, Best Loss: 0.00000012093899 in Epoch 135
Epoch 139
Epoch 139, Loss: 0.00000020065620, Improvement: -0.00000000095103, Best Loss: 0.00000012093899 in Epoch 135
Epoch 140
Epoch 140, Loss: 0.00000020211072, Improvement: 0.00000000145452, Best Loss: 0.00000012093899 in Epoch 135
Epoch 141
Epoch 141, Loss: 0.00000020066852, Improvement: -0.00000000144220, Best Loss: 0.00000012093899 in Epoch 135
Epoch 142
Epoch 142, Loss: 0.00000020308692, Improvement: 0.00000000241840, Best Loss: 0.00000012093899 in Epoch 135
Epoch 143
Epoch 143, Loss: 0.00000019905312, Improvement: -0.00000000403381, Best Loss: 0.00000012093899 in Epoch 135
Epoch 144
Epoch 144, Loss: 0.00000019972123, Improvement: 0.00000000066811, Best Loss: 0.00000012093899 in Epoch 135
Epoch 145
Epoch 145, Loss: 0.00000020486525, Improvement: 0.00000000514402, Best Loss: 0.00000012093899 in Epoch 135
Epoch 146
Epoch 146, Loss: 0.00000021454433, Improvement: 0.00000000967908, Best Loss: 0.00000012093899 in Epoch 135
Epoch 147
Epoch 147, Loss: 0.00000020944908, Improvement: -0.00000000509524, Best Loss: 0.00000012093899 in Epoch 135
Epoch 148
Epoch 148, Loss: 0.00000029329372, Improvement: 0.00000008384463, Best Loss: 0.00000012093899 in Epoch 135
Epoch 149
Epoch 149, Loss: 0.00000025186065, Improvement: -0.00000004143307, Best Loss: 0.00000012093899 in Epoch 135
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00000022202501, Improvement: -0.00000002983564, Best Loss: 0.00000012093899 in Epoch 135
Epoch 151
Epoch 151, Loss: 0.00000026563285, Improvement: 0.00000004360784, Best Loss: 0.00000012093899 in Epoch 135
Epoch 152
Epoch 152, Loss: 0.00000026668027, Improvement: 0.00000000104742, Best Loss: 0.00000012093899 in Epoch 135
Epoch 153
Epoch 153, Loss: 0.00000020259280, Improvement: -0.00000006408747, Best Loss: 0.00000012093899 in Epoch 135
Epoch 154
Epoch 154, Loss: 0.00000019826595, Improvement: -0.00000000432684, Best Loss: 0.00000012093899 in Epoch 135
Epoch 155
Epoch 155, Loss: 0.00000029258481, Improvement: 0.00000009431886, Best Loss: 0.00000012093899 in Epoch 135
Epoch 156
Epoch 156, Loss: 0.00000032346819, Improvement: 0.00000003088337, Best Loss: 0.00000012093899 in Epoch 135
Epoch 157
Epoch 157, Loss: 0.00000026709828, Improvement: -0.00000005636991, Best Loss: 0.00000012093899 in Epoch 135
Epoch 158
Epoch 158, Loss: 0.00000035192756, Improvement: 0.00000008482928, Best Loss: 0.00000012093899 in Epoch 135
Epoch 159
Epoch 159, Loss: 0.00000031440105, Improvement: -0.00000003752651, Best Loss: 0.00000012093899 in Epoch 135
Epoch 160
Epoch 160, Loss: 0.00000026302027, Improvement: -0.00000005138078, Best Loss: 0.00000012093899 in Epoch 135
Epoch 161
Epoch 161, Loss: 0.00000029103741, Improvement: 0.00000002801714, Best Loss: 0.00000012093899 in Epoch 135
Epoch 162
Epoch 162, Loss: 0.00000034121288, Improvement: 0.00000005017547, Best Loss: 0.00000012093899 in Epoch 135
Epoch 163
Epoch 163, Loss: 0.00000026470767, Improvement: -0.00000007650521, Best Loss: 0.00000012093899 in Epoch 135
Epoch 164
A best model at epoch 164 has been saved with training error 0.00000011761672.
Epoch 164, Loss: 0.00000035249033, Improvement: 0.00000008778266, Best Loss: 0.00000011761672 in Epoch 164
Epoch 165
Epoch 165, Loss: 0.00000019661211, Improvement: -0.00000015587823, Best Loss: 0.00000011761672 in Epoch 164
Epoch 166
Epoch 166, Loss: 0.00000019608069, Improvement: -0.00000000053142, Best Loss: 0.00000011761672 in Epoch 164
Epoch 167
Epoch 167, Loss: 0.00000019755432, Improvement: 0.00000000147363, Best Loss: 0.00000011761672 in Epoch 164
Epoch 168
Epoch 168, Loss: 0.00000021983959, Improvement: 0.00000002228527, Best Loss: 0.00000011761672 in Epoch 164
Epoch 169
Epoch 169, Loss: 0.00000058845115, Improvement: 0.00000036861155, Best Loss: 0.00000011761672 in Epoch 164
Epoch 170
Epoch 170, Loss: 0.00000028828690, Improvement: -0.00000030016425, Best Loss: 0.00000011761672 in Epoch 164
Epoch 171
Epoch 171, Loss: 0.00000020574751, Improvement: -0.00000008253938, Best Loss: 0.00000011761672 in Epoch 164
Epoch 172
Epoch 172, Loss: 0.00000023159613, Improvement: 0.00000002584862, Best Loss: 0.00000011761672 in Epoch 164
Epoch 173
Epoch 173, Loss: 0.00000022108090, Improvement: -0.00000001051523, Best Loss: 0.00000011761672 in Epoch 164
Epoch 174
Epoch 174, Loss: 0.00000034621251, Improvement: 0.00000012513161, Best Loss: 0.00000011761672 in Epoch 164
Epoch 175
Epoch 175, Loss: 0.00000046446598, Improvement: 0.00000011825347, Best Loss: 0.00000011761672 in Epoch 164
Epoch 176
Epoch 176, Loss: 0.00000069471928, Improvement: 0.00000023025330, Best Loss: 0.00000011761672 in Epoch 164
Epoch 177
Epoch 177, Loss: 0.00000056170228, Improvement: -0.00000013301700, Best Loss: 0.00000011761672 in Epoch 164
Epoch 178
Epoch 178, Loss: 0.00000034469950, Improvement: -0.00000021700278, Best Loss: 0.00000011761672 in Epoch 164
Epoch 179
Epoch 179, Loss: 0.00000029411253, Improvement: -0.00000005058697, Best Loss: 0.00000011761672 in Epoch 164
Epoch 180
Epoch 180, Loss: 0.00000019888256, Improvement: -0.00000009522997, Best Loss: 0.00000011761672 in Epoch 164
Epoch 181
Epoch 181, Loss: 0.00000019282164, Improvement: -0.00000000606092, Best Loss: 0.00000011761672 in Epoch 164
Epoch 182
Epoch 182, Loss: 0.00000022364203, Improvement: 0.00000003082039, Best Loss: 0.00000011761672 in Epoch 164
Epoch 183
Epoch 183, Loss: 0.00000027831503, Improvement: 0.00000005467300, Best Loss: 0.00000011761672 in Epoch 164
Epoch 184
Epoch 184, Loss: 0.00000068694897, Improvement: 0.00000040863394, Best Loss: 0.00000011761672 in Epoch 164
Epoch 185
Epoch 185, Loss: 0.00000034578767, Improvement: -0.00000034116130, Best Loss: 0.00000011761672 in Epoch 164
Epoch 186
Epoch 186, Loss: 0.00000022391701, Improvement: -0.00000012187065, Best Loss: 0.00000011761672 in Epoch 164
Epoch 187
Epoch 187, Loss: 0.00000019117308, Improvement: -0.00000003274393, Best Loss: 0.00000011761672 in Epoch 164
Epoch 188
Epoch 188, Loss: 0.00000038695111, Improvement: 0.00000019577803, Best Loss: 0.00000011761672 in Epoch 164
Epoch 189
Epoch 189, Loss: 0.00000092349894, Improvement: 0.00000053654783, Best Loss: 0.00000011761672 in Epoch 164
Epoch 190
Epoch 190, Loss: 0.00000036834500, Improvement: -0.00000055515394, Best Loss: 0.00000011761672 in Epoch 164
Epoch 191
Epoch 191, Loss: 0.00000021450987, Improvement: -0.00000015383513, Best Loss: 0.00000011761672 in Epoch 164
Epoch 192
Epoch 192, Loss: 0.00000019562147, Improvement: -0.00000001888839, Best Loss: 0.00000011761672 in Epoch 164
Epoch 193
Epoch 193, Loss: 0.00000020030081, Improvement: 0.00000000467934, Best Loss: 0.00000011761672 in Epoch 164
Epoch 194
Epoch 194, Loss: 0.00000019431218, Improvement: -0.00000000598863, Best Loss: 0.00000011761672 in Epoch 164
Epoch 195
Epoch 195, Loss: 0.00000019275558, Improvement: -0.00000000155661, Best Loss: 0.00000011761672 in Epoch 164
Epoch 196
Epoch 196, Loss: 0.00000024132784, Improvement: 0.00000004857227, Best Loss: 0.00000011761672 in Epoch 164
Epoch 197
Epoch 197, Loss: 0.00000092842068, Improvement: 0.00000068709284, Best Loss: 0.00000011761672 in Epoch 164
Epoch 198
Epoch 198, Loss: 0.00000028048936, Improvement: -0.00000064793133, Best Loss: 0.00000011761672 in Epoch 164
Epoch 199
Epoch 199, Loss: 0.00000021132880, Improvement: -0.00000006916056, Best Loss: 0.00000011761672 in Epoch 164
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00000019384405, Improvement: -0.00000001748475, Best Loss: 0.00000011761672 in Epoch 164
Epoch 201
Epoch 201, Loss: 0.00000019190624, Improvement: -0.00000000193781, Best Loss: 0.00000011761672 in Epoch 164
Epoch 202
Epoch 202, Loss: 0.00000018969097, Improvement: -0.00000000221528, Best Loss: 0.00000011761672 in Epoch 164
Epoch 203
Epoch 203, Loss: 0.00000019007954, Improvement: 0.00000000038858, Best Loss: 0.00000011761672 in Epoch 164
Epoch 204
Epoch 204, Loss: 0.00000019177174, Improvement: 0.00000000169220, Best Loss: 0.00000011761672 in Epoch 164
Epoch 205
Epoch 205, Loss: 0.00000028390672, Improvement: 0.00000009213498, Best Loss: 0.00000011761672 in Epoch 164
Epoch 206
Epoch 206, Loss: 0.00000163245767, Improvement: 0.00000134855095, Best Loss: 0.00000011761672 in Epoch 164
Epoch 207
Epoch 207, Loss: 0.00000041125390, Improvement: -0.00000122120378, Best Loss: 0.00000011761672 in Epoch 164
Epoch 208
Epoch 208, Loss: 0.00000024451473, Improvement: -0.00000016673917, Best Loss: 0.00000011761672 in Epoch 164
Epoch 209
Epoch 209, Loss: 0.00000023340850, Improvement: -0.00000001110623, Best Loss: 0.00000011761672 in Epoch 164
Epoch 210
Epoch 210, Loss: 0.00000019075199, Improvement: -0.00000004265651, Best Loss: 0.00000011761672 in Epoch 164
Epoch 211
Epoch 211, Loss: 0.00000019097402, Improvement: 0.00000000022202, Best Loss: 0.00000011761672 in Epoch 164
Epoch 212
Epoch 212, Loss: 0.00000019516036, Improvement: 0.00000000418635, Best Loss: 0.00000011761672 in Epoch 164
Epoch 213
Epoch 213, Loss: 0.00000018932372, Improvement: -0.00000000583665, Best Loss: 0.00000011761672 in Epoch 164
Epoch 214
Epoch 214, Loss: 0.00000018811604, Improvement: -0.00000000120767, Best Loss: 0.00000011761672 in Epoch 164
Epoch 215
Epoch 215, Loss: 0.00000028489730, Improvement: 0.00000009678126, Best Loss: 0.00000011761672 in Epoch 164
Epoch 216
Epoch 216, Loss: 0.00000116604849, Improvement: 0.00000088115118, Best Loss: 0.00000011761672 in Epoch 164
Epoch 217
Epoch 217, Loss: 0.00000034004311, Improvement: -0.00000082600538, Best Loss: 0.00000011761672 in Epoch 164
Epoch 218
Epoch 218, Loss: 0.00000028248569, Improvement: -0.00000005755742, Best Loss: 0.00000011761672 in Epoch 164
Epoch 219
Epoch 219, Loss: 0.00000020190723, Improvement: -0.00000008057846, Best Loss: 0.00000011761672 in Epoch 164
Epoch 220
Epoch 220, Loss: 0.00000018814943, Improvement: -0.00000001375780, Best Loss: 0.00000011761672 in Epoch 164
Epoch 221
Epoch 221, Loss: 0.00000018818922, Improvement: 0.00000000003979, Best Loss: 0.00000011761672 in Epoch 164
Epoch 222
Epoch 222, Loss: 0.00000018914423, Improvement: 0.00000000095502, Best Loss: 0.00000011761672 in Epoch 164
Epoch 223
Epoch 223, Loss: 0.00000019564958, Improvement: 0.00000000650535, Best Loss: 0.00000011761672 in Epoch 164
Epoch 224
Epoch 224, Loss: 0.00000086683326, Improvement: 0.00000067118367, Best Loss: 0.00000011761672 in Epoch 164
Epoch 225
Epoch 225, Loss: 0.00000056641111, Improvement: -0.00000030042214, Best Loss: 0.00000011761672 in Epoch 164
Epoch 226
Epoch 226, Loss: 0.00000132398375, Improvement: 0.00000075757264, Best Loss: 0.00000011761672 in Epoch 164
Epoch 227
Epoch 227, Loss: 0.00000031962877, Improvement: -0.00000100435498, Best Loss: 0.00000011761672 in Epoch 164
Epoch 228
Epoch 228, Loss: 0.00000018985419, Improvement: -0.00000012977458, Best Loss: 0.00000011761672 in Epoch 164
Epoch 229
Epoch 229, Loss: 0.00000018824268, Improvement: -0.00000000161152, Best Loss: 0.00000011761672 in Epoch 164
Epoch 230
Epoch 230, Loss: 0.00000018657830, Improvement: -0.00000000166437, Best Loss: 0.00000011761672 in Epoch 164
Epoch 231
Epoch 231, Loss: 0.00000018777649, Improvement: 0.00000000119819, Best Loss: 0.00000011761672 in Epoch 164
Epoch 232
Epoch 232, Loss: 0.00000018694796, Improvement: -0.00000000082853, Best Loss: 0.00000011761672 in Epoch 164
Epoch 233
Epoch 233, Loss: 0.00000019765263, Improvement: 0.00000001070468, Best Loss: 0.00000011761672 in Epoch 164
Epoch 234
Epoch 234, Loss: 0.00000133547196, Improvement: 0.00000113781933, Best Loss: 0.00000011761672 in Epoch 164
Epoch 235
Epoch 235, Loss: 0.00000145736553, Improvement: 0.00000012189357, Best Loss: 0.00000011761672 in Epoch 164
Epoch 236
Epoch 236, Loss: 0.00000040724715, Improvement: -0.00000105011837, Best Loss: 0.00000011761672 in Epoch 164
Epoch 237
Epoch 237, Loss: 0.00000019482996, Improvement: -0.00000021241719, Best Loss: 0.00000011761672 in Epoch 164
Epoch 238
Epoch 238, Loss: 0.00000018400099, Improvement: -0.00000001082897, Best Loss: 0.00000011761672 in Epoch 164
Epoch 239
Epoch 239, Loss: 0.00000018327839, Improvement: -0.00000000072260, Best Loss: 0.00000011761672 in Epoch 164
Epoch 240
Epoch 240, Loss: 0.00000018260514, Improvement: -0.00000000067325, Best Loss: 0.00000011761672 in Epoch 164
Epoch 241
Epoch 241, Loss: 0.00000018272004, Improvement: 0.00000000011490, Best Loss: 0.00000011761672 in Epoch 164
Epoch 242
A best model at epoch 242 has been saved with training error 0.00000011130104.
Epoch 242, Loss: 0.00000018267250, Improvement: -0.00000000004754, Best Loss: 0.00000011130104 in Epoch 242
Epoch 243
Epoch 243, Loss: 0.00000018471941, Improvement: 0.00000000204691, Best Loss: 0.00000011130104 in Epoch 242
Epoch 244
Epoch 244, Loss: 0.00000018531434, Improvement: 0.00000000059493, Best Loss: 0.00000011130104 in Epoch 242
Epoch 245
Epoch 245, Loss: 0.00000018872772, Improvement: 0.00000000341338, Best Loss: 0.00000011130104 in Epoch 242
Epoch 246
Epoch 246, Loss: 0.00000068176089, Improvement: 0.00000049303317, Best Loss: 0.00000011130104 in Epoch 242
Epoch 247
Epoch 247, Loss: 0.00000154905953, Improvement: 0.00000086729864, Best Loss: 0.00000011130104 in Epoch 242
Epoch 248
Epoch 248, Loss: 0.00000042622993, Improvement: -0.00000112282960, Best Loss: 0.00000011130104 in Epoch 242
Epoch 249
Epoch 249, Loss: 0.00000022355756, Improvement: -0.00000020267237, Best Loss: 0.00000011130104 in Epoch 242
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000019095182, Improvement: -0.00000003260574, Best Loss: 0.00000011130104 in Epoch 242
Epoch 251
Epoch 251, Loss: 0.00000018576673, Improvement: -0.00000000518508, Best Loss: 0.00000011130104 in Epoch 242
Epoch 252
A best model at epoch 252 has been saved with training error 0.00000009738029.
Epoch 252, Loss: 0.00000018177765, Improvement: -0.00000000398908, Best Loss: 0.00000009738029 in Epoch 252
Epoch 253
Epoch 253, Loss: 0.00000018478554, Improvement: 0.00000000300789, Best Loss: 0.00000009738029 in Epoch 252
Epoch 254
Epoch 254, Loss: 0.00000018446765, Improvement: -0.00000000031788, Best Loss: 0.00000009738029 in Epoch 252
Epoch 255
Epoch 255, Loss: 0.00000018353281, Improvement: -0.00000000093484, Best Loss: 0.00000009738029 in Epoch 252
Epoch 256
Epoch 256, Loss: 0.00000018381126, Improvement: 0.00000000027845, Best Loss: 0.00000009738029 in Epoch 252
Epoch 257
Epoch 257, Loss: 0.00000019152092, Improvement: 0.00000000770966, Best Loss: 0.00000009738029 in Epoch 252
Epoch 258
Epoch 258, Loss: 0.00000176319997, Improvement: 0.00000157167905, Best Loss: 0.00000009738029 in Epoch 252
Epoch 259
Epoch 259, Loss: 0.00000055533661, Improvement: -0.00000120786336, Best Loss: 0.00000009738029 in Epoch 252
Epoch 260
Epoch 260, Loss: 0.00000022832301, Improvement: -0.00000032701360, Best Loss: 0.00000009738029 in Epoch 252
Epoch 261
Epoch 261, Loss: 0.00000018506158, Improvement: -0.00000004326142, Best Loss: 0.00000009738029 in Epoch 252
Epoch 262
Epoch 262, Loss: 0.00000018178135, Improvement: -0.00000000328023, Best Loss: 0.00000009738029 in Epoch 252
Epoch 263
Epoch 263, Loss: 0.00000018115609, Improvement: -0.00000000062526, Best Loss: 0.00000009738029 in Epoch 252
Epoch 264
Epoch 264, Loss: 0.00000018033839, Improvement: -0.00000000081770, Best Loss: 0.00000009738029 in Epoch 252
Epoch 265
Epoch 265, Loss: 0.00000018088048, Improvement: 0.00000000054210, Best Loss: 0.00000009738029 in Epoch 252
Epoch 266
Epoch 266, Loss: 0.00000017987351, Improvement: -0.00000000100697, Best Loss: 0.00000009738029 in Epoch 252
Epoch 267
Epoch 267, Loss: 0.00000018098590, Improvement: 0.00000000111239, Best Loss: 0.00000009738029 in Epoch 252
Epoch 268
Epoch 268, Loss: 0.00000017995789, Improvement: -0.00000000102801, Best Loss: 0.00000009738029 in Epoch 252
Epoch 269
Epoch 269, Loss: 0.00000018031643, Improvement: 0.00000000035854, Best Loss: 0.00000009738029 in Epoch 252
Epoch 270
Epoch 270, Loss: 0.00000018176105, Improvement: 0.00000000144462, Best Loss: 0.00000009738029 in Epoch 252
Epoch 271
Epoch 271, Loss: 0.00000018114849, Improvement: -0.00000000061257, Best Loss: 0.00000009738029 in Epoch 252
Epoch 272
Epoch 272, Loss: 0.00000021408203, Improvement: 0.00000003293354, Best Loss: 0.00000009738029 in Epoch 252
Epoch 273
Epoch 273, Loss: 0.00000159938092, Improvement: 0.00000138529889, Best Loss: 0.00000009738029 in Epoch 252
Epoch 274
Epoch 274, Loss: 0.00000044851980, Improvement: -0.00000115086112, Best Loss: 0.00000009738029 in Epoch 252
Epoch 275
Epoch 275, Loss: 0.00000019057279, Improvement: -0.00000025794701, Best Loss: 0.00000009738029 in Epoch 252
Epoch 276
Epoch 276, Loss: 0.00000018336929, Improvement: -0.00000000720349, Best Loss: 0.00000009738029 in Epoch 252
Epoch 277
Epoch 277, Loss: 0.00000018108749, Improvement: -0.00000000228181, Best Loss: 0.00000009738029 in Epoch 252
Epoch 278
Epoch 278, Loss: 0.00000018306248, Improvement: 0.00000000197499, Best Loss: 0.00000009738029 in Epoch 252
Epoch 279
Epoch 279, Loss: 0.00000017925685, Improvement: -0.00000000380562, Best Loss: 0.00000009738029 in Epoch 252
Epoch 280
Epoch 280, Loss: 0.00000018083853, Improvement: 0.00000000158167, Best Loss: 0.00000009738029 in Epoch 252
Epoch 281
Epoch 281, Loss: 0.00000018385039, Improvement: 0.00000000301187, Best Loss: 0.00000009738029 in Epoch 252
Epoch 282
Epoch 282, Loss: 0.00000019409654, Improvement: 0.00000001024614, Best Loss: 0.00000009738029 in Epoch 252
Epoch 283
Epoch 283, Loss: 0.00000095625943, Improvement: 0.00000076216289, Best Loss: 0.00000009738029 in Epoch 252
Epoch 284
Epoch 284, Loss: 0.00000119059865, Improvement: 0.00000023433922, Best Loss: 0.00000009738029 in Epoch 252
Epoch 285
Epoch 285, Loss: 0.00000038941523, Improvement: -0.00000080118342, Best Loss: 0.00000009738029 in Epoch 252
Epoch 286
Epoch 286, Loss: 0.00000024246072, Improvement: -0.00000014695451, Best Loss: 0.00000009738029 in Epoch 252
Epoch 287
Epoch 287, Loss: 0.00000020035768, Improvement: -0.00000004210304, Best Loss: 0.00000009738029 in Epoch 252
Epoch 288
Epoch 288, Loss: 0.00000018128819, Improvement: -0.00000001906949, Best Loss: 0.00000009738029 in Epoch 252
Epoch 289
Epoch 289, Loss: 0.00000018006188, Improvement: -0.00000000122631, Best Loss: 0.00000009738029 in Epoch 252
Epoch 290
Epoch 290, Loss: 0.00000017707799, Improvement: -0.00000000298389, Best Loss: 0.00000009738029 in Epoch 252
Epoch 291
Epoch 291, Loss: 0.00000017690601, Improvement: -0.00000000017198, Best Loss: 0.00000009738029 in Epoch 252
Epoch 292
Epoch 292, Loss: 0.00000017816863, Improvement: 0.00000000126261, Best Loss: 0.00000009738029 in Epoch 252
Epoch 293
Epoch 293, Loss: 0.00000017917418, Improvement: 0.00000000100556, Best Loss: 0.00000009738029 in Epoch 252
Epoch 294
Epoch 294, Loss: 0.00000017971758, Improvement: 0.00000000054339, Best Loss: 0.00000009738029 in Epoch 252
Epoch 295
Epoch 295, Loss: 0.00000017843933, Improvement: -0.00000000127825, Best Loss: 0.00000009738029 in Epoch 252
Epoch 296
Epoch 296, Loss: 0.00000018191530, Improvement: 0.00000000347597, Best Loss: 0.00000009738029 in Epoch 252
Epoch 297
Epoch 297, Loss: 0.00000113617854, Improvement: 0.00000095426325, Best Loss: 0.00000009738029 in Epoch 252
Epoch 298
Epoch 298, Loss: 0.00000035619217, Improvement: -0.00000077998637, Best Loss: 0.00000009738029 in Epoch 252
Epoch 299
Epoch 299, Loss: 0.00000027123100, Improvement: -0.00000008496118, Best Loss: 0.00000009738029 in Epoch 252
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000021967926, Improvement: -0.00000005155174, Best Loss: 0.00000009738029 in Epoch 252
Epoch 301
Epoch 301, Loss: 0.00000018881401, Improvement: -0.00000003086525, Best Loss: 0.00000009738029 in Epoch 252
Epoch 302
Epoch 302, Loss: 0.00000017796896, Improvement: -0.00000001084505, Best Loss: 0.00000009738029 in Epoch 252
Epoch 303
Epoch 303, Loss: 0.00000017623954, Improvement: -0.00000000172942, Best Loss: 0.00000009738029 in Epoch 252
Epoch 304
Epoch 304, Loss: 0.00000017481935, Improvement: -0.00000000142019, Best Loss: 0.00000009738029 in Epoch 252
Epoch 305
Epoch 305, Loss: 0.00000017539892, Improvement: 0.00000000057957, Best Loss: 0.00000009738029 in Epoch 252
Epoch 306
Epoch 306, Loss: 0.00000017491966, Improvement: -0.00000000047926, Best Loss: 0.00000009738029 in Epoch 252
Epoch 307
Epoch 307, Loss: 0.00000017593581, Improvement: 0.00000000101615, Best Loss: 0.00000009738029 in Epoch 252
Epoch 308
Epoch 308, Loss: 0.00000017820470, Improvement: 0.00000000226889, Best Loss: 0.00000009738029 in Epoch 252
Epoch 309
Epoch 309, Loss: 0.00000019410582, Improvement: 0.00000001590112, Best Loss: 0.00000009738029 in Epoch 252
Epoch 310
Epoch 310, Loss: 0.00000027982024, Improvement: 0.00000008571442, Best Loss: 0.00000009738029 in Epoch 252
Epoch 311
Epoch 311, Loss: 0.00000054529068, Improvement: 0.00000026547044, Best Loss: 0.00000009738029 in Epoch 252
Epoch 312
Epoch 312, Loss: 0.00000159754939, Improvement: 0.00000105225871, Best Loss: 0.00000009738029 in Epoch 252
Epoch 313
Epoch 313, Loss: 0.00000093810817, Improvement: -0.00000065944122, Best Loss: 0.00000009738029 in Epoch 252
Epoch 314
Epoch 314, Loss: 0.00000025461076, Improvement: -0.00000068349741, Best Loss: 0.00000009738029 in Epoch 252
Epoch 315
Epoch 315, Loss: 0.00000019074432, Improvement: -0.00000006386643, Best Loss: 0.00000009738029 in Epoch 252
Epoch 316
Epoch 316, Loss: 0.00000017577095, Improvement: -0.00000001497338, Best Loss: 0.00000009738029 in Epoch 252
Epoch 317
Epoch 317, Loss: 0.00000017533900, Improvement: -0.00000000043195, Best Loss: 0.00000009738029 in Epoch 252
Epoch 318
Epoch 318, Loss: 0.00000017387407, Improvement: -0.00000000146493, Best Loss: 0.00000009738029 in Epoch 252
Epoch 319
Epoch 319, Loss: 0.00000017269322, Improvement: -0.00000000118085, Best Loss: 0.00000009738029 in Epoch 252
Epoch 320
Epoch 320, Loss: 0.00000017230468, Improvement: -0.00000000038854, Best Loss: 0.00000009738029 in Epoch 252
Epoch 321
Epoch 321, Loss: 0.00000017284492, Improvement: 0.00000000054025, Best Loss: 0.00000009738029 in Epoch 252
Epoch 322
Epoch 322, Loss: 0.00000017290300, Improvement: 0.00000000005807, Best Loss: 0.00000009738029 in Epoch 252
Epoch 323
Epoch 323, Loss: 0.00000017257310, Improvement: -0.00000000032989, Best Loss: 0.00000009738029 in Epoch 252
Epoch 324
Epoch 324, Loss: 0.00000017312500, Improvement: 0.00000000055189, Best Loss: 0.00000009738029 in Epoch 252
Epoch 325
Epoch 325, Loss: 0.00000017285558, Improvement: -0.00000000026942, Best Loss: 0.00000009738029 in Epoch 252
Epoch 326
Epoch 326, Loss: 0.00000017247855, Improvement: -0.00000000037703, Best Loss: 0.00000009738029 in Epoch 252
Epoch 327
Epoch 327, Loss: 0.00000017215037, Improvement: -0.00000000032818, Best Loss: 0.00000009738029 in Epoch 252
Epoch 328
Epoch 328, Loss: 0.00000017207390, Improvement: -0.00000000007648, Best Loss: 0.00000009738029 in Epoch 252
Epoch 329
Epoch 329, Loss: 0.00000017282316, Improvement: 0.00000000074927, Best Loss: 0.00000009738029 in Epoch 252
Epoch 330
A best model at epoch 330 has been saved with training error 0.00000009207635.
Epoch 330, Loss: 0.00000017228298, Improvement: -0.00000000054018, Best Loss: 0.00000009207635 in Epoch 330
Epoch 331
Epoch 331, Loss: 0.00000017189461, Improvement: -0.00000000038838, Best Loss: 0.00000009207635 in Epoch 330
Epoch 332
Epoch 332, Loss: 0.00000017188200, Improvement: -0.00000000001261, Best Loss: 0.00000009207635 in Epoch 330
Epoch 333
Epoch 333, Loss: 0.00000017111921, Improvement: -0.00000000076279, Best Loss: 0.00000009207635 in Epoch 330
Epoch 334
Epoch 334, Loss: 0.00000017209729, Improvement: 0.00000000097808, Best Loss: 0.00000009207635 in Epoch 330
Epoch 335
Epoch 335, Loss: 0.00000017209170, Improvement: -0.00000000000559, Best Loss: 0.00000009207635 in Epoch 330
Epoch 336
Epoch 336, Loss: 0.00000017139697, Improvement: -0.00000000069473, Best Loss: 0.00000009207635 in Epoch 330
Epoch 337
Epoch 337, Loss: 0.00000017214243, Improvement: 0.00000000074546, Best Loss: 0.00000009207635 in Epoch 330
Epoch 338
Epoch 338, Loss: 0.00000022114653, Improvement: 0.00000004900410, Best Loss: 0.00000009207635 in Epoch 330
Epoch 339
Epoch 339, Loss: 0.00000070869301, Improvement: 0.00000048754648, Best Loss: 0.00000009207635 in Epoch 330
Epoch 340
Epoch 340, Loss: 0.00000099125179, Improvement: 0.00000028255878, Best Loss: 0.00000009207635 in Epoch 330
Epoch 341
Epoch 341, Loss: 0.00000044511793, Improvement: -0.00000054613386, Best Loss: 0.00000009207635 in Epoch 330
Epoch 342
Epoch 342, Loss: 0.00000028889336, Improvement: -0.00000015622457, Best Loss: 0.00000009207635 in Epoch 330
Epoch 343
Epoch 343, Loss: 0.00000018921579, Improvement: -0.00000009967757, Best Loss: 0.00000009207635 in Epoch 330
Epoch 344
Epoch 344, Loss: 0.00000017279244, Improvement: -0.00000001642336, Best Loss: 0.00000009207635 in Epoch 330
Epoch 345
Epoch 345, Loss: 0.00000017190158, Improvement: -0.00000000089085, Best Loss: 0.00000009207635 in Epoch 330
Epoch 346
Epoch 346, Loss: 0.00000017260472, Improvement: 0.00000000070313, Best Loss: 0.00000009207635 in Epoch 330
Epoch 347
Epoch 347, Loss: 0.00000017447585, Improvement: 0.00000000187113, Best Loss: 0.00000009207635 in Epoch 330
Epoch 348
Epoch 348, Loss: 0.00000017330263, Improvement: -0.00000000117322, Best Loss: 0.00000009207635 in Epoch 330
Epoch 349
Epoch 349, Loss: 0.00000017162901, Improvement: -0.00000000167362, Best Loss: 0.00000009207635 in Epoch 330
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000017167209, Improvement: 0.00000000004308, Best Loss: 0.00000009207635 in Epoch 330
Epoch 351
Epoch 351, Loss: 0.00000017203338, Improvement: 0.00000000036129, Best Loss: 0.00000009207635 in Epoch 330
Epoch 352
Epoch 352, Loss: 0.00000017146589, Improvement: -0.00000000056748, Best Loss: 0.00000009207635 in Epoch 330
Epoch 353
Epoch 353, Loss: 0.00000019456277, Improvement: 0.00000002309688, Best Loss: 0.00000009207635 in Epoch 330
Epoch 354
Epoch 354, Loss: 0.00000071217412, Improvement: 0.00000051761135, Best Loss: 0.00000009207635 in Epoch 330
Epoch 355
Epoch 355, Loss: 0.00000062467132, Improvement: -0.00000008750280, Best Loss: 0.00000009207635 in Epoch 330
Epoch 356
Epoch 356, Loss: 0.00000058939387, Improvement: -0.00000003527746, Best Loss: 0.00000009207635 in Epoch 330
Epoch 357
Epoch 357, Loss: 0.00000027807899, Improvement: -0.00000031131488, Best Loss: 0.00000009207635 in Epoch 330
Epoch 358
Epoch 358, Loss: 0.00000019566095, Improvement: -0.00000008241804, Best Loss: 0.00000009207635 in Epoch 330
Epoch 359
Epoch 359, Loss: 0.00000019048814, Improvement: -0.00000000517282, Best Loss: 0.00000009207635 in Epoch 330
Epoch 360
Epoch 360, Loss: 0.00000020949424, Improvement: 0.00000001900610, Best Loss: 0.00000009207635 in Epoch 330
Epoch 361
Epoch 361, Loss: 0.00000018007034, Improvement: -0.00000002942390, Best Loss: 0.00000009207635 in Epoch 330
Epoch 362
Epoch 362, Loss: 0.00000043390993, Improvement: 0.00000025383959, Best Loss: 0.00000009207635 in Epoch 330
Epoch 363
Epoch 363, Loss: 0.00000050149134, Improvement: 0.00000006758141, Best Loss: 0.00000009207635 in Epoch 330
Epoch 364
Epoch 364, Loss: 0.00000026199706, Improvement: -0.00000023949428, Best Loss: 0.00000009207635 in Epoch 330
Epoch 365
Epoch 365, Loss: 0.00000019127692, Improvement: -0.00000007072014, Best Loss: 0.00000009207635 in Epoch 330
Epoch 366
Epoch 366, Loss: 0.00000017547901, Improvement: -0.00000001579790, Best Loss: 0.00000009207635 in Epoch 330
Epoch 367
Epoch 367, Loss: 0.00000020145702, Improvement: 0.00000002597801, Best Loss: 0.00000009207635 in Epoch 330
Epoch 368
Epoch 368, Loss: 0.00000020836987, Improvement: 0.00000000691285, Best Loss: 0.00000009207635 in Epoch 330
Epoch 369
Epoch 369, Loss: 0.00000069568418, Improvement: 0.00000048731431, Best Loss: 0.00000009207635 in Epoch 330
Epoch 370
Epoch 370, Loss: 0.00000038944327, Improvement: -0.00000030624092, Best Loss: 0.00000009207635 in Epoch 330
Epoch 371
Epoch 371, Loss: 0.00000019895765, Improvement: -0.00000019048562, Best Loss: 0.00000009207635 in Epoch 330
Epoch 372
Epoch 372, Loss: 0.00000017362379, Improvement: -0.00000002533386, Best Loss: 0.00000009207635 in Epoch 330
Epoch 373
Epoch 373, Loss: 0.00000017439402, Improvement: 0.00000000077023, Best Loss: 0.00000009207635 in Epoch 330
Epoch 374
Epoch 374, Loss: 0.00000017307570, Improvement: -0.00000000131832, Best Loss: 0.00000009207635 in Epoch 330
Epoch 375
Epoch 375, Loss: 0.00000017043456, Improvement: -0.00000000264114, Best Loss: 0.00000009207635 in Epoch 330
Epoch 376
Epoch 376, Loss: 0.00000017070837, Improvement: 0.00000000027381, Best Loss: 0.00000009207635 in Epoch 330
Epoch 377
Epoch 377, Loss: 0.00000017045147, Improvement: -0.00000000025690, Best Loss: 0.00000009207635 in Epoch 330
Epoch 378
Epoch 378, Loss: 0.00000017149593, Improvement: 0.00000000104447, Best Loss: 0.00000009207635 in Epoch 330
Epoch 379
Epoch 379, Loss: 0.00000017181366, Improvement: 0.00000000031773, Best Loss: 0.00000009207635 in Epoch 330
Epoch 380
Epoch 380, Loss: 0.00000018026905, Improvement: 0.00000000845539, Best Loss: 0.00000009207635 in Epoch 330
Epoch 381
Epoch 381, Loss: 0.00000021070577, Improvement: 0.00000003043672, Best Loss: 0.00000009207635 in Epoch 330
Epoch 382
Epoch 382, Loss: 0.00000018081561, Improvement: -0.00000002989015, Best Loss: 0.00000009207635 in Epoch 330
Epoch 383
Epoch 383, Loss: 0.00000023848192, Improvement: 0.00000005766631, Best Loss: 0.00000009207635 in Epoch 330
Epoch 384
Epoch 384, Loss: 0.00000042713423, Improvement: 0.00000018865231, Best Loss: 0.00000009207635 in Epoch 330
Epoch 385
Epoch 385, Loss: 0.00000111047322, Improvement: 0.00000068333899, Best Loss: 0.00000009207635 in Epoch 330
Epoch 386
Epoch 386, Loss: 0.00000082620985, Improvement: -0.00000028426337, Best Loss: 0.00000009207635 in Epoch 330
Epoch 387
Epoch 387, Loss: 0.00000043174164, Improvement: -0.00000039446821, Best Loss: 0.00000009207635 in Epoch 330
Epoch 388
Epoch 388, Loss: 0.00000020595567, Improvement: -0.00000022578597, Best Loss: 0.00000009207635 in Epoch 330
Epoch 389
Epoch 389, Loss: 0.00000017435635, Improvement: -0.00000003159933, Best Loss: 0.00000009207635 in Epoch 330
Epoch 390
Epoch 390, Loss: 0.00000017830867, Improvement: 0.00000000395233, Best Loss: 0.00000009207635 in Epoch 330
Epoch 391
Epoch 391, Loss: 0.00000017246489, Improvement: -0.00000000584378, Best Loss: 0.00000009207635 in Epoch 330
Epoch 392
Epoch 392, Loss: 0.00000016980812, Improvement: -0.00000000265678, Best Loss: 0.00000009207635 in Epoch 330
Epoch 393
Epoch 393, Loss: 0.00000016934956, Improvement: -0.00000000045856, Best Loss: 0.00000009207635 in Epoch 330
Epoch 394
Epoch 394, Loss: 0.00000017026117, Improvement: 0.00000000091161, Best Loss: 0.00000009207635 in Epoch 330
Epoch 395
Epoch 395, Loss: 0.00000017107997, Improvement: 0.00000000081880, Best Loss: 0.00000009207635 in Epoch 330
Epoch 396
Epoch 396, Loss: 0.00000017206450, Improvement: 0.00000000098453, Best Loss: 0.00000009207635 in Epoch 330
Epoch 397
Epoch 397, Loss: 0.00000016926791, Improvement: -0.00000000279659, Best Loss: 0.00000009207635 in Epoch 330
Epoch 398
Epoch 398, Loss: 0.00000017043541, Improvement: 0.00000000116750, Best Loss: 0.00000009207635 in Epoch 330
Epoch 399
Epoch 399, Loss: 0.00000016948357, Improvement: -0.00000000095184, Best Loss: 0.00000009207635 in Epoch 330
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000016952361, Improvement: 0.00000000004004, Best Loss: 0.00000009207635 in Epoch 330
Epoch 401
Epoch 401, Loss: 0.00000016880735, Improvement: -0.00000000071626, Best Loss: 0.00000009207635 in Epoch 330
Epoch 402
Epoch 402, Loss: 0.00000017465051, Improvement: 0.00000000584316, Best Loss: 0.00000009207635 in Epoch 330
Epoch 403
Epoch 403, Loss: 0.00000017276929, Improvement: -0.00000000188122, Best Loss: 0.00000009207635 in Epoch 330
Epoch 404
Epoch 404, Loss: 0.00000017262324, Improvement: -0.00000000014605, Best Loss: 0.00000009207635 in Epoch 330
Epoch 405
Epoch 405, Loss: 0.00000017245677, Improvement: -0.00000000016648, Best Loss: 0.00000009207635 in Epoch 330
Epoch 406
Epoch 406, Loss: 0.00000017037275, Improvement: -0.00000000208402, Best Loss: 0.00000009207635 in Epoch 330
Epoch 407
Epoch 407, Loss: 0.00000017022817, Improvement: -0.00000000014458, Best Loss: 0.00000009207635 in Epoch 330
Epoch 408
Epoch 408, Loss: 0.00000017066479, Improvement: 0.00000000043662, Best Loss: 0.00000009207635 in Epoch 330
Epoch 409
Epoch 409, Loss: 0.00000017655219, Improvement: 0.00000000588740, Best Loss: 0.00000009207635 in Epoch 330
Epoch 410
Epoch 410, Loss: 0.00000017788059, Improvement: 0.00000000132840, Best Loss: 0.00000009207635 in Epoch 330
Epoch 411
Epoch 411, Loss: 0.00000017181019, Improvement: -0.00000000607040, Best Loss: 0.00000009207635 in Epoch 330
Epoch 412
Epoch 412, Loss: 0.00000017323343, Improvement: 0.00000000142324, Best Loss: 0.00000009207635 in Epoch 330
Epoch 413
Epoch 413, Loss: 0.00000021132663, Improvement: 0.00000003809321, Best Loss: 0.00000009207635 in Epoch 330
Epoch 414
Epoch 414, Loss: 0.00000057960329, Improvement: 0.00000036827665, Best Loss: 0.00000009207635 in Epoch 330
Epoch 415
Epoch 415, Loss: 0.00000039970117, Improvement: -0.00000017990212, Best Loss: 0.00000009207635 in Epoch 330
Epoch 416
Epoch 416, Loss: 0.00000018911580, Improvement: -0.00000021058537, Best Loss: 0.00000009207635 in Epoch 330
Epoch 417
Epoch 417, Loss: 0.00000017746702, Improvement: -0.00000001164878, Best Loss: 0.00000009207635 in Epoch 330
Epoch 418
Epoch 418, Loss: 0.00000017450306, Improvement: -0.00000000296396, Best Loss: 0.00000009207635 in Epoch 330
Epoch 419
Epoch 419, Loss: 0.00000017524938, Improvement: 0.00000000074632, Best Loss: 0.00000009207635 in Epoch 330
Epoch 420
Epoch 420, Loss: 0.00000017275112, Improvement: -0.00000000249827, Best Loss: 0.00000009207635 in Epoch 330
Epoch 421
Epoch 421, Loss: 0.00000017433495, Improvement: 0.00000000158383, Best Loss: 0.00000009207635 in Epoch 330
Epoch 422
Epoch 422, Loss: 0.00000017287192, Improvement: -0.00000000146302, Best Loss: 0.00000009207635 in Epoch 330
Epoch 423
Epoch 423, Loss: 0.00000018508106, Improvement: 0.00000001220914, Best Loss: 0.00000009207635 in Epoch 330
Epoch 424
Epoch 424, Loss: 0.00000021486413, Improvement: 0.00000002978306, Best Loss: 0.00000009207635 in Epoch 330
Epoch 425
Epoch 425, Loss: 0.00000018572660, Improvement: -0.00000002913752, Best Loss: 0.00000009207635 in Epoch 330
Epoch 426
Epoch 426, Loss: 0.00000017881691, Improvement: -0.00000000690969, Best Loss: 0.00000009207635 in Epoch 330
Epoch 427
Epoch 427, Loss: 0.00000045598127, Improvement: 0.00000027716437, Best Loss: 0.00000009207635 in Epoch 330
Epoch 428
Epoch 428, Loss: 0.00000054419773, Improvement: 0.00000008821645, Best Loss: 0.00000009207635 in Epoch 330
Epoch 429
Epoch 429, Loss: 0.00000026468589, Improvement: -0.00000027951184, Best Loss: 0.00000009207635 in Epoch 330
Epoch 430
Epoch 430, Loss: 0.00000020872904, Improvement: -0.00000005595684, Best Loss: 0.00000009207635 in Epoch 330
Epoch 431
Epoch 431, Loss: 0.00000017579037, Improvement: -0.00000003293867, Best Loss: 0.00000009207635 in Epoch 330
Epoch 432
Epoch 432, Loss: 0.00000016944310, Improvement: -0.00000000634727, Best Loss: 0.00000009207635 in Epoch 330
Epoch 433
Epoch 433, Loss: 0.00000016966300, Improvement: 0.00000000021990, Best Loss: 0.00000009207635 in Epoch 330
Epoch 434
Epoch 434, Loss: 0.00000016872729, Improvement: -0.00000000093570, Best Loss: 0.00000009207635 in Epoch 330
Epoch 435
Epoch 435, Loss: 0.00000017114484, Improvement: 0.00000000241754, Best Loss: 0.00000009207635 in Epoch 330
Epoch 436
Epoch 436, Loss: 0.00000016870194, Improvement: -0.00000000244290, Best Loss: 0.00000009207635 in Epoch 330
Epoch 437
Epoch 437, Loss: 0.00000016980049, Improvement: 0.00000000109855, Best Loss: 0.00000009207635 in Epoch 330
Epoch 438
Epoch 438, Loss: 0.00000017421205, Improvement: 0.00000000441156, Best Loss: 0.00000009207635 in Epoch 330
Epoch 439
Epoch 439, Loss: 0.00000017598251, Improvement: 0.00000000177046, Best Loss: 0.00000009207635 in Epoch 330
Epoch 440
Epoch 440, Loss: 0.00000017061059, Improvement: -0.00000000537192, Best Loss: 0.00000009207635 in Epoch 330
Epoch 441
Epoch 441, Loss: 0.00000017081953, Improvement: 0.00000000020895, Best Loss: 0.00000009207635 in Epoch 330
Epoch 442
Epoch 442, Loss: 0.00000016945246, Improvement: -0.00000000136707, Best Loss: 0.00000009207635 in Epoch 330
Epoch 443
Epoch 443, Loss: 0.00000017135496, Improvement: 0.00000000190250, Best Loss: 0.00000009207635 in Epoch 330
Epoch 444
Epoch 444, Loss: 0.00000017799824, Improvement: 0.00000000664328, Best Loss: 0.00000009207635 in Epoch 330
Epoch 445
Epoch 445, Loss: 0.00000018077647, Improvement: 0.00000000277823, Best Loss: 0.00000009207635 in Epoch 330
Epoch 446
Epoch 446, Loss: 0.00000017501604, Improvement: -0.00000000576043, Best Loss: 0.00000009207635 in Epoch 330
Epoch 447
Epoch 447, Loss: 0.00000017273028, Improvement: -0.00000000228576, Best Loss: 0.00000009207635 in Epoch 330
Epoch 448
Epoch 448, Loss: 0.00000017642759, Improvement: 0.00000000369731, Best Loss: 0.00000009207635 in Epoch 330
Epoch 449
Epoch 449, Loss: 0.00000016979278, Improvement: -0.00000000663481, Best Loss: 0.00000009207635 in Epoch 330
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000017163021, Improvement: 0.00000000183744, Best Loss: 0.00000009207635 in Epoch 330
Epoch 451
Epoch 451, Loss: 0.00000017616617, Improvement: 0.00000000453596, Best Loss: 0.00000009207635 in Epoch 330
Epoch 452
Epoch 452, Loss: 0.00000026771611, Improvement: 0.00000009154994, Best Loss: 0.00000009207635 in Epoch 330
Epoch 453
Epoch 453, Loss: 0.00000046114722, Improvement: 0.00000019343111, Best Loss: 0.00000009207635 in Epoch 330
Epoch 454
Epoch 454, Loss: 0.00000021452272, Improvement: -0.00000024662450, Best Loss: 0.00000009207635 in Epoch 330
Epoch 455
Epoch 455, Loss: 0.00000025323533, Improvement: 0.00000003871261, Best Loss: 0.00000009207635 in Epoch 330
Epoch 456
Epoch 456, Loss: 0.00000034640200, Improvement: 0.00000009316667, Best Loss: 0.00000009207635 in Epoch 330
Epoch 457
Epoch 457, Loss: 0.00000034843372, Improvement: 0.00000000203171, Best Loss: 0.00000009207635 in Epoch 330
Epoch 458
Epoch 458, Loss: 0.00000024694855, Improvement: -0.00000010148517, Best Loss: 0.00000009207635 in Epoch 330
Epoch 459
Epoch 459, Loss: 0.00000039268421, Improvement: 0.00000014573566, Best Loss: 0.00000009207635 in Epoch 330
Epoch 460
Epoch 460, Loss: 0.00000026074032, Improvement: -0.00000013194388, Best Loss: 0.00000009207635 in Epoch 330
Epoch 461
Epoch 461, Loss: 0.00000022989841, Improvement: -0.00000003084192, Best Loss: 0.00000009207635 in Epoch 330
Epoch 462
Epoch 462, Loss: 0.00000023255012, Improvement: 0.00000000265171, Best Loss: 0.00000009207635 in Epoch 330
Epoch 463
Epoch 463, Loss: 0.00000020223135, Improvement: -0.00000003031877, Best Loss: 0.00000009207635 in Epoch 330
Epoch 464
Epoch 464, Loss: 0.00000018411215, Improvement: -0.00000001811920, Best Loss: 0.00000009207635 in Epoch 330
Epoch 465
Epoch 465, Loss: 0.00000017998946, Improvement: -0.00000000412269, Best Loss: 0.00000009207635 in Epoch 330
Epoch 466
Epoch 466, Loss: 0.00000017119573, Improvement: -0.00000000879373, Best Loss: 0.00000009207635 in Epoch 330
Epoch 467
Epoch 467, Loss: 0.00000017661388, Improvement: 0.00000000541815, Best Loss: 0.00000009207635 in Epoch 330
Epoch 468
Epoch 468, Loss: 0.00000020928557, Improvement: 0.00000003267168, Best Loss: 0.00000009207635 in Epoch 330
Epoch 469
Epoch 469, Loss: 0.00000018120973, Improvement: -0.00000002807584, Best Loss: 0.00000009207635 in Epoch 330
Epoch 470
Epoch 470, Loss: 0.00000027525716, Improvement: 0.00000009404744, Best Loss: 0.00000009207635 in Epoch 330
Epoch 471
Epoch 471, Loss: 0.00000023249274, Improvement: -0.00000004276443, Best Loss: 0.00000009207635 in Epoch 330
Epoch 472
Epoch 472, Loss: 0.00000030919731, Improvement: 0.00000007670457, Best Loss: 0.00000009207635 in Epoch 330
Epoch 473
Epoch 473, Loss: 0.00000057881227, Improvement: 0.00000026961496, Best Loss: 0.00000009207635 in Epoch 330
Epoch 474
Epoch 474, Loss: 0.00000026864461, Improvement: -0.00000031016766, Best Loss: 0.00000009207635 in Epoch 330
Epoch 475
Epoch 475, Loss: 0.00000020456723, Improvement: -0.00000006407738, Best Loss: 0.00000009207635 in Epoch 330
Epoch 476
Epoch 476, Loss: 0.00000018140078, Improvement: -0.00000002316644, Best Loss: 0.00000009207635 in Epoch 330
Epoch 477
Epoch 477, Loss: 0.00000017927882, Improvement: -0.00000000212197, Best Loss: 0.00000009207635 in Epoch 330
Epoch 478
Epoch 478, Loss: 0.00000017306103, Improvement: -0.00000000621779, Best Loss: 0.00000009207635 in Epoch 330
Epoch 479
Epoch 479, Loss: 0.00000016975527, Improvement: -0.00000000330576, Best Loss: 0.00000009207635 in Epoch 330
Epoch 480
Epoch 480, Loss: 0.00000016841307, Improvement: -0.00000000134220, Best Loss: 0.00000009207635 in Epoch 330
Epoch 481
Epoch 481, Loss: 0.00000016802452, Improvement: -0.00000000038855, Best Loss: 0.00000009207635 in Epoch 330
Epoch 482
Epoch 482, Loss: 0.00000016885315, Improvement: 0.00000000082863, Best Loss: 0.00000009207635 in Epoch 330
Epoch 483
Epoch 483, Loss: 0.00000016984719, Improvement: 0.00000000099404, Best Loss: 0.00000009207635 in Epoch 330
Epoch 484
Epoch 484, Loss: 0.00000017075064, Improvement: 0.00000000090345, Best Loss: 0.00000009207635 in Epoch 330
Epoch 485
Epoch 485, Loss: 0.00000017108442, Improvement: 0.00000000033378, Best Loss: 0.00000009207635 in Epoch 330
Epoch 486
Epoch 486, Loss: 0.00000016937995, Improvement: -0.00000000170448, Best Loss: 0.00000009207635 in Epoch 330
Epoch 487
Epoch 487, Loss: 0.00000016961016, Improvement: 0.00000000023022, Best Loss: 0.00000009207635 in Epoch 330
Epoch 488
Epoch 488, Loss: 0.00000016908395, Improvement: -0.00000000052622, Best Loss: 0.00000009207635 in Epoch 330
Epoch 489
Epoch 489, Loss: 0.00000016921036, Improvement: 0.00000000012641, Best Loss: 0.00000009207635 in Epoch 330
Epoch 490
Epoch 490, Loss: 0.00000017007382, Improvement: 0.00000000086346, Best Loss: 0.00000009207635 in Epoch 330
Epoch 491
Epoch 491, Loss: 0.00000016990520, Improvement: -0.00000000016862, Best Loss: 0.00000009207635 in Epoch 330
Epoch 492
Epoch 492, Loss: 0.00000017014928, Improvement: 0.00000000024409, Best Loss: 0.00000009207635 in Epoch 330
Epoch 493
Epoch 493, Loss: 0.00000016957073, Improvement: -0.00000000057855, Best Loss: 0.00000009207635 in Epoch 330
Epoch 494
Epoch 494, Loss: 0.00000017487554, Improvement: 0.00000000530481, Best Loss: 0.00000009207635 in Epoch 330
Epoch 495
Epoch 495, Loss: 0.00000016911226, Improvement: -0.00000000576328, Best Loss: 0.00000009207635 in Epoch 330
Epoch 496
Epoch 496, Loss: 0.00000017003387, Improvement: 0.00000000092161, Best Loss: 0.00000009207635 in Epoch 330
Epoch 497
Epoch 497, Loss: 0.00000017674183, Improvement: 0.00000000670796, Best Loss: 0.00000009207635 in Epoch 330
Epoch 498
Epoch 498, Loss: 0.00000017651367, Improvement: -0.00000000022816, Best Loss: 0.00000009207635 in Epoch 330
Epoch 499
Epoch 499, Loss: 0.00000017685754, Improvement: 0.00000000034387, Best Loss: 0.00000009207635 in Epoch 330
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000017311115, Improvement: -0.00000000374638, Best Loss: 0.00000009207635 in Epoch 330
Epoch 501
Epoch 501, Loss: 0.00000017347413, Improvement: 0.00000000036298, Best Loss: 0.00000009207635 in Epoch 330
Epoch 502
Epoch 502, Loss: 0.00000017472312, Improvement: 0.00000000124899, Best Loss: 0.00000009207635 in Epoch 330
Epoch 503
Epoch 503, Loss: 0.00000017664015, Improvement: 0.00000000191703, Best Loss: 0.00000009207635 in Epoch 330
Epoch 504
Epoch 504, Loss: 0.00000017739261, Improvement: 0.00000000075246, Best Loss: 0.00000009207635 in Epoch 330
Epoch 505
Epoch 505, Loss: 0.00000036202733, Improvement: 0.00000018463472, Best Loss: 0.00000009207635 in Epoch 330
Epoch 506
Epoch 506, Loss: 0.00000024069231, Improvement: -0.00000012133502, Best Loss: 0.00000009207635 in Epoch 330
Epoch 507
Epoch 507, Loss: 0.00000018522634, Improvement: -0.00000005546597, Best Loss: 0.00000009207635 in Epoch 330
Epoch 508
Epoch 508, Loss: 0.00000018704492, Improvement: 0.00000000181858, Best Loss: 0.00000009207635 in Epoch 330
Epoch 509
Epoch 509, Loss: 0.00000018943313, Improvement: 0.00000000238821, Best Loss: 0.00000009207635 in Epoch 330
Epoch 510
Epoch 510, Loss: 0.00000019217646, Improvement: 0.00000000274333, Best Loss: 0.00000009207635 in Epoch 330
Epoch 511
Epoch 511, Loss: 0.00000019869442, Improvement: 0.00000000651796, Best Loss: 0.00000009207635 in Epoch 330
Epoch 512
Epoch 512, Loss: 0.00000017761464, Improvement: -0.00000002107977, Best Loss: 0.00000009207635 in Epoch 330
Epoch 513
Epoch 513, Loss: 0.00000017296979, Improvement: -0.00000000464485, Best Loss: 0.00000009207635 in Epoch 330
Epoch 514
Epoch 514, Loss: 0.00000017982355, Improvement: 0.00000000685376, Best Loss: 0.00000009207635 in Epoch 330
Epoch 515
Epoch 515, Loss: 0.00000018102124, Improvement: 0.00000000119769, Best Loss: 0.00000009207635 in Epoch 330
Epoch 516
Epoch 516, Loss: 0.00000027462473, Improvement: 0.00000009360348, Best Loss: 0.00000009207635 in Epoch 330
Epoch 517
Epoch 517, Loss: 0.00000027593330, Improvement: 0.00000000130858, Best Loss: 0.00000009207635 in Epoch 330
Epoch 518
Epoch 518, Loss: 0.00000023829335, Improvement: -0.00000003763995, Best Loss: 0.00000009207635 in Epoch 330
Epoch 519
Epoch 519, Loss: 0.00000039765187, Improvement: 0.00000015935852, Best Loss: 0.00000009207635 in Epoch 330
Epoch 520
Epoch 520, Loss: 0.00000032651033, Improvement: -0.00000007114154, Best Loss: 0.00000009207635 in Epoch 330
Epoch 521
Epoch 521, Loss: 0.00000019591049, Improvement: -0.00000013059984, Best Loss: 0.00000009207635 in Epoch 330
Epoch 522
Epoch 522, Loss: 0.00000017845212, Improvement: -0.00000001745837, Best Loss: 0.00000009207635 in Epoch 330
Epoch 523
Epoch 523, Loss: 0.00000018310869, Improvement: 0.00000000465657, Best Loss: 0.00000009207635 in Epoch 330
Epoch 524
Epoch 524, Loss: 0.00000017784631, Improvement: -0.00000000526237, Best Loss: 0.00000009207635 in Epoch 330
Epoch 525
Epoch 525, Loss: 0.00000018963894, Improvement: 0.00000001179263, Best Loss: 0.00000009207635 in Epoch 330
Epoch 526
Epoch 526, Loss: 0.00000017766934, Improvement: -0.00000001196960, Best Loss: 0.00000009207635 in Epoch 330
Epoch 527
Epoch 527, Loss: 0.00000017260346, Improvement: -0.00000000506588, Best Loss: 0.00000009207635 in Epoch 330
Epoch 528
Epoch 528, Loss: 0.00000016985634, Improvement: -0.00000000274712, Best Loss: 0.00000009207635 in Epoch 330
Epoch 529
Epoch 529, Loss: 0.00000017036396, Improvement: 0.00000000050762, Best Loss: 0.00000009207635 in Epoch 330
Epoch 530
Epoch 530, Loss: 0.00000017018735, Improvement: -0.00000000017661, Best Loss: 0.00000009207635 in Epoch 330
Epoch 531
Epoch 531, Loss: 0.00000016874264, Improvement: -0.00000000144471, Best Loss: 0.00000009207635 in Epoch 330
Epoch 532
Epoch 532, Loss: 0.00000017014415, Improvement: 0.00000000140151, Best Loss: 0.00000009207635 in Epoch 330
Epoch 533
Epoch 533, Loss: 0.00000017041188, Improvement: 0.00000000026773, Best Loss: 0.00000009207635 in Epoch 330
Epoch 534
Epoch 534, Loss: 0.00000018236920, Improvement: 0.00000001195732, Best Loss: 0.00000009207635 in Epoch 330
Epoch 535
Epoch 535, Loss: 0.00000018054669, Improvement: -0.00000000182252, Best Loss: 0.00000009207635 in Epoch 330
Epoch 536
Epoch 536, Loss: 0.00000018347240, Improvement: 0.00000000292572, Best Loss: 0.00000009207635 in Epoch 330
Epoch 537
Epoch 537, Loss: 0.00000018351399, Improvement: 0.00000000004158, Best Loss: 0.00000009207635 in Epoch 330
Epoch 538
Epoch 538, Loss: 0.00000027076146, Improvement: 0.00000008724747, Best Loss: 0.00000009207635 in Epoch 330
Epoch 539
Epoch 539, Loss: 0.00000017419416, Improvement: -0.00000009656730, Best Loss: 0.00000009207635 in Epoch 330
Epoch 540
Epoch 540, Loss: 0.00000017312392, Improvement: -0.00000000107024, Best Loss: 0.00000009207635 in Epoch 330
Epoch 541
Epoch 541, Loss: 0.00000017068766, Improvement: -0.00000000243625, Best Loss: 0.00000009207635 in Epoch 330
Epoch 542
Epoch 542, Loss: 0.00000018408203, Improvement: 0.00000001339437, Best Loss: 0.00000009207635 in Epoch 330
Epoch 543
Epoch 543, Loss: 0.00000020705889, Improvement: 0.00000002297685, Best Loss: 0.00000009207635 in Epoch 330
Epoch 544
Epoch 544, Loss: 0.00000031013008, Improvement: 0.00000010307119, Best Loss: 0.00000009207635 in Epoch 330
Epoch 545
Epoch 545, Loss: 0.00000021639811, Improvement: -0.00000009373197, Best Loss: 0.00000009207635 in Epoch 330
Epoch 546
Epoch 546, Loss: 0.00000018140548, Improvement: -0.00000003499263, Best Loss: 0.00000009207635 in Epoch 330
Epoch 547
Epoch 547, Loss: 0.00000017504710, Improvement: -0.00000000635838, Best Loss: 0.00000009207635 in Epoch 330
Epoch 548
Epoch 548, Loss: 0.00000017102855, Improvement: -0.00000000401855, Best Loss: 0.00000009207635 in Epoch 330
Epoch 549
Epoch 549, Loss: 0.00000017421264, Improvement: 0.00000000318409, Best Loss: 0.00000009207635 in Epoch 330
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000017185198, Improvement: -0.00000000236067, Best Loss: 0.00000009207635 in Epoch 330
Epoch 551
Epoch 551, Loss: 0.00000017419797, Improvement: 0.00000000234600, Best Loss: 0.00000009207635 in Epoch 330
Epoch 552
Epoch 552, Loss: 0.00000027826926, Improvement: 0.00000010407128, Best Loss: 0.00000009207635 in Epoch 330
Epoch 553
Epoch 553, Loss: 0.00000028821928, Improvement: 0.00000000995002, Best Loss: 0.00000009207635 in Epoch 330
Epoch 554
Epoch 554, Loss: 0.00000018411406, Improvement: -0.00000010410522, Best Loss: 0.00000009207635 in Epoch 330
Epoch 555
Epoch 555, Loss: 0.00000017697351, Improvement: -0.00000000714055, Best Loss: 0.00000009207635 in Epoch 330
Epoch 556
Epoch 556, Loss: 0.00000017589291, Improvement: -0.00000000108060, Best Loss: 0.00000009207635 in Epoch 330
Epoch 557
Epoch 557, Loss: 0.00000017723337, Improvement: 0.00000000134045, Best Loss: 0.00000009207635 in Epoch 330
Epoch 558
Epoch 558, Loss: 0.00000016952494, Improvement: -0.00000000770843, Best Loss: 0.00000009207635 in Epoch 330
Epoch 559
Epoch 559, Loss: 0.00000016725689, Improvement: -0.00000000226805, Best Loss: 0.00000009207635 in Epoch 330
Epoch 560
Epoch 560, Loss: 0.00000016723968, Improvement: -0.00000000001721, Best Loss: 0.00000009207635 in Epoch 330
Epoch 561
Epoch 561, Loss: 0.00000016668408, Improvement: -0.00000000055560, Best Loss: 0.00000009207635 in Epoch 330
Epoch 562
Epoch 562, Loss: 0.00000017259728, Improvement: 0.00000000591320, Best Loss: 0.00000009207635 in Epoch 330
Epoch 563
Epoch 563, Loss: 0.00000017571150, Improvement: 0.00000000311422, Best Loss: 0.00000009207635 in Epoch 330
Epoch 564
Epoch 564, Loss: 0.00000019926638, Improvement: 0.00000002355488, Best Loss: 0.00000009207635 in Epoch 330
Epoch 565
Epoch 565, Loss: 0.00000032732786, Improvement: 0.00000012806148, Best Loss: 0.00000009207635 in Epoch 330
Epoch 566
Epoch 566, Loss: 0.00000024159893, Improvement: -0.00000008572893, Best Loss: 0.00000009207635 in Epoch 330
Epoch 567
Epoch 567, Loss: 0.00000022472939, Improvement: -0.00000001686954, Best Loss: 0.00000009207635 in Epoch 330
Epoch 568
Epoch 568, Loss: 0.00000021028047, Improvement: -0.00000001444893, Best Loss: 0.00000009207635 in Epoch 330
Epoch 569
Epoch 569, Loss: 0.00000017472072, Improvement: -0.00000003555974, Best Loss: 0.00000009207635 in Epoch 330
Epoch 570
Epoch 570, Loss: 0.00000016757831, Improvement: -0.00000000714242, Best Loss: 0.00000009207635 in Epoch 330
Epoch 571
Epoch 571, Loss: 0.00000016539534, Improvement: -0.00000000218297, Best Loss: 0.00000009207635 in Epoch 330
Epoch 572
Epoch 572, Loss: 0.00000016550950, Improvement: 0.00000000011417, Best Loss: 0.00000009207635 in Epoch 330
Epoch 573
Epoch 573, Loss: 0.00000016908729, Improvement: 0.00000000357779, Best Loss: 0.00000009207635 in Epoch 330
Epoch 574
Epoch 574, Loss: 0.00000016613116, Improvement: -0.00000000295613, Best Loss: 0.00000009207635 in Epoch 330
Epoch 575
Epoch 575, Loss: 0.00000016640225, Improvement: 0.00000000027109, Best Loss: 0.00000009207635 in Epoch 330
Epoch 576
Epoch 576, Loss: 0.00000016499945, Improvement: -0.00000000140280, Best Loss: 0.00000009207635 in Epoch 330
Epoch 577
Epoch 577, Loss: 0.00000016649075, Improvement: 0.00000000149130, Best Loss: 0.00000009207635 in Epoch 330
Epoch 578
Epoch 578, Loss: 0.00000016768873, Improvement: 0.00000000119798, Best Loss: 0.00000009207635 in Epoch 330
Epoch 579
Epoch 579, Loss: 0.00000016557886, Improvement: -0.00000000210987, Best Loss: 0.00000009207635 in Epoch 330
Epoch 580
Epoch 580, Loss: 0.00000016770553, Improvement: 0.00000000212667, Best Loss: 0.00000009207635 in Epoch 330
Epoch 581
Epoch 581, Loss: 0.00000016757588, Improvement: -0.00000000012965, Best Loss: 0.00000009207635 in Epoch 330
Epoch 582
Epoch 582, Loss: 0.00000016839519, Improvement: 0.00000000081931, Best Loss: 0.00000009207635 in Epoch 330
Epoch 583
A best model at epoch 583 has been saved with training error 0.00000008279420.
Epoch 583, Loss: 0.00000017208644, Improvement: 0.00000000369125, Best Loss: 0.00000008279420 in Epoch 583
Epoch 584
Epoch 584, Loss: 0.00000017899608, Improvement: 0.00000000690965, Best Loss: 0.00000008279420 in Epoch 583
Epoch 585
Epoch 585, Loss: 0.00000017028782, Improvement: -0.00000000870826, Best Loss: 0.00000008279420 in Epoch 583
Epoch 586
Epoch 586, Loss: 0.00000017996660, Improvement: 0.00000000967877, Best Loss: 0.00000008279420 in Epoch 583
Epoch 587
Epoch 587, Loss: 0.00000025981839, Improvement: 0.00000007985179, Best Loss: 0.00000008279420 in Epoch 583
Epoch 588
Epoch 588, Loss: 0.00000033774003, Improvement: 0.00000007792164, Best Loss: 0.00000008279420 in Epoch 583
Epoch 589
Epoch 589, Loss: 0.00000020069290, Improvement: -0.00000013704712, Best Loss: 0.00000008279420 in Epoch 583
Epoch 590
Epoch 590, Loss: 0.00000016751026, Improvement: -0.00000003318264, Best Loss: 0.00000008279420 in Epoch 583
Epoch 591
Epoch 591, Loss: 0.00000016503050, Improvement: -0.00000000247976, Best Loss: 0.00000008279420 in Epoch 583
Epoch 592
Epoch 592, Loss: 0.00000016507234, Improvement: 0.00000000004184, Best Loss: 0.00000008279420 in Epoch 583
Epoch 593
Epoch 593, Loss: 0.00000016320636, Improvement: -0.00000000186598, Best Loss: 0.00000008279420 in Epoch 583
Epoch 594
Epoch 594, Loss: 0.00000016526223, Improvement: 0.00000000205587, Best Loss: 0.00000008279420 in Epoch 583
Epoch 595
Epoch 595, Loss: 0.00000016687682, Improvement: 0.00000000161459, Best Loss: 0.00000008279420 in Epoch 583
Epoch 596
Epoch 596, Loss: 0.00000016309054, Improvement: -0.00000000378629, Best Loss: 0.00000008279420 in Epoch 583
Epoch 597
Epoch 597, Loss: 0.00000016270748, Improvement: -0.00000000038306, Best Loss: 0.00000008279420 in Epoch 583
Epoch 598
Epoch 598, Loss: 0.00000016311631, Improvement: 0.00000000040883, Best Loss: 0.00000008279420 in Epoch 583
Epoch 599
Epoch 599, Loss: 0.00000017690588, Improvement: 0.00000001378957, Best Loss: 0.00000008279420 in Epoch 583
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000018174781, Improvement: 0.00000000484193, Best Loss: 0.00000008279420 in Epoch 583
Epoch 601
Epoch 601, Loss: 0.00000028046658, Improvement: 0.00000009871877, Best Loss: 0.00000008279420 in Epoch 583
Epoch 602
Epoch 602, Loss: 0.00000029750675, Improvement: 0.00000001704017, Best Loss: 0.00000008279420 in Epoch 583
Epoch 603
Epoch 603, Loss: 0.00000021862057, Improvement: -0.00000007888618, Best Loss: 0.00000008279420 in Epoch 583
Epoch 604
Epoch 604, Loss: 0.00000018940120, Improvement: -0.00000002921937, Best Loss: 0.00000008279420 in Epoch 583
Epoch 605
Epoch 605, Loss: 0.00000017958102, Improvement: -0.00000000982018, Best Loss: 0.00000008279420 in Epoch 583
Epoch 606
Epoch 606, Loss: 0.00000017328509, Improvement: -0.00000000629593, Best Loss: 0.00000008279420 in Epoch 583
Epoch 607
Epoch 607, Loss: 0.00000017039298, Improvement: -0.00000000289211, Best Loss: 0.00000008279420 in Epoch 583
Epoch 608
Epoch 608, Loss: 0.00000016515010, Improvement: -0.00000000524288, Best Loss: 0.00000008279420 in Epoch 583
Epoch 609
Epoch 609, Loss: 0.00000016292659, Improvement: -0.00000000222350, Best Loss: 0.00000008279420 in Epoch 583
Epoch 610
Epoch 610, Loss: 0.00000016645230, Improvement: 0.00000000352571, Best Loss: 0.00000008279420 in Epoch 583
Epoch 611
Epoch 611, Loss: 0.00000017094862, Improvement: 0.00000000449631, Best Loss: 0.00000008279420 in Epoch 583
Epoch 612
Epoch 612, Loss: 0.00000017071004, Improvement: -0.00000000023857, Best Loss: 0.00000008279420 in Epoch 583
Epoch 613
Epoch 613, Loss: 0.00000016783454, Improvement: -0.00000000287550, Best Loss: 0.00000008279420 in Epoch 583
Epoch 614
Epoch 614, Loss: 0.00000016368111, Improvement: -0.00000000415344, Best Loss: 0.00000008279420 in Epoch 583
Epoch 615
Epoch 615, Loss: 0.00000020061673, Improvement: 0.00000003693563, Best Loss: 0.00000008279420 in Epoch 583
Epoch 616
Epoch 616, Loss: 0.00000019515707, Improvement: -0.00000000545966, Best Loss: 0.00000008279420 in Epoch 583
Epoch 617
Epoch 617, Loss: 0.00000017586150, Improvement: -0.00000001929558, Best Loss: 0.00000008279420 in Epoch 583
Epoch 618
Epoch 618, Loss: 0.00000019014225, Improvement: 0.00000001428076, Best Loss: 0.00000008279420 in Epoch 583
Epoch 619
Epoch 619, Loss: 0.00000016252516, Improvement: -0.00000002761709, Best Loss: 0.00000008279420 in Epoch 583
Epoch 620
Epoch 620, Loss: 0.00000016632865, Improvement: 0.00000000380349, Best Loss: 0.00000008279420 in Epoch 583
Epoch 621
Epoch 621, Loss: 0.00000016660887, Improvement: 0.00000000028021, Best Loss: 0.00000008279420 in Epoch 583
Epoch 622
Epoch 622, Loss: 0.00000024129190, Improvement: 0.00000007468303, Best Loss: 0.00000008279420 in Epoch 583
Epoch 623
Epoch 623, Loss: 0.00000023750373, Improvement: -0.00000000378816, Best Loss: 0.00000008279420 in Epoch 583
Epoch 624
Epoch 624, Loss: 0.00000018279154, Improvement: -0.00000005471219, Best Loss: 0.00000008279420 in Epoch 583
Epoch 625
Epoch 625, Loss: 0.00000017066561, Improvement: -0.00000001212594, Best Loss: 0.00000008279420 in Epoch 583
Epoch 626
Epoch 626, Loss: 0.00000016817134, Improvement: -0.00000000249426, Best Loss: 0.00000008279420 in Epoch 583
Epoch 627
Epoch 627, Loss: 0.00000016589364, Improvement: -0.00000000227770, Best Loss: 0.00000008279420 in Epoch 583
Epoch 628
Epoch 628, Loss: 0.00000016328220, Improvement: -0.00000000261144, Best Loss: 0.00000008279420 in Epoch 583
Epoch 629
Epoch 629, Loss: 0.00000016240601, Improvement: -0.00000000087619, Best Loss: 0.00000008279420 in Epoch 583
Epoch 630
Epoch 630, Loss: 0.00000016865114, Improvement: 0.00000000624513, Best Loss: 0.00000008279420 in Epoch 583
Epoch 631
Epoch 631, Loss: 0.00000017475655, Improvement: 0.00000000610541, Best Loss: 0.00000008279420 in Epoch 583
Epoch 632
Epoch 632, Loss: 0.00000016889224, Improvement: -0.00000000586431, Best Loss: 0.00000008279420 in Epoch 583
Epoch 633
Epoch 633, Loss: 0.00000017451963, Improvement: 0.00000000562739, Best Loss: 0.00000008279420 in Epoch 583
Epoch 634
Epoch 634, Loss: 0.00000019315312, Improvement: 0.00000001863349, Best Loss: 0.00000008279420 in Epoch 583
Epoch 635
Epoch 635, Loss: 0.00000018341118, Improvement: -0.00000000974194, Best Loss: 0.00000008279420 in Epoch 583
Epoch 636
Epoch 636, Loss: 0.00000019233087, Improvement: 0.00000000891969, Best Loss: 0.00000008279420 in Epoch 583
Epoch 637
Epoch 637, Loss: 0.00000019047050, Improvement: -0.00000000186037, Best Loss: 0.00000008279420 in Epoch 583
Epoch 638
Epoch 638, Loss: 0.00000016881324, Improvement: -0.00000002165726, Best Loss: 0.00000008279420 in Epoch 583
Epoch 639
Epoch 639, Loss: 0.00000018429740, Improvement: 0.00000001548417, Best Loss: 0.00000008279420 in Epoch 583
Epoch 640
Epoch 640, Loss: 0.00000020495727, Improvement: 0.00000002065987, Best Loss: 0.00000008279420 in Epoch 583
Epoch 641
Epoch 641, Loss: 0.00000020365685, Improvement: -0.00000000130042, Best Loss: 0.00000008279420 in Epoch 583
Epoch 642
Epoch 642, Loss: 0.00000017860763, Improvement: -0.00000002504923, Best Loss: 0.00000008279420 in Epoch 583
Epoch 643
Epoch 643, Loss: 0.00000018550803, Improvement: 0.00000000690041, Best Loss: 0.00000008279420 in Epoch 583
Epoch 644
Epoch 644, Loss: 0.00000018649554, Improvement: 0.00000000098751, Best Loss: 0.00000008279420 in Epoch 583
Epoch 645
Epoch 645, Loss: 0.00000027078656, Improvement: 0.00000008429102, Best Loss: 0.00000008279420 in Epoch 583
Epoch 646
Epoch 646, Loss: 0.00000018583517, Improvement: -0.00000008495139, Best Loss: 0.00000008279420 in Epoch 583
Epoch 647
Epoch 647, Loss: 0.00000019038426, Improvement: 0.00000000454909, Best Loss: 0.00000008279420 in Epoch 583
Epoch 648
Epoch 648, Loss: 0.00000018027925, Improvement: -0.00000001010501, Best Loss: 0.00000008279420 in Epoch 583
Epoch 649
Epoch 649, Loss: 0.00000016187897, Improvement: -0.00000001840028, Best Loss: 0.00000008279420 in Epoch 583
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000016370743, Improvement: 0.00000000182846, Best Loss: 0.00000008279420 in Epoch 583
Epoch 651
Epoch 651, Loss: 0.00000016182689, Improvement: -0.00000000188055, Best Loss: 0.00000008279420 in Epoch 583
Epoch 652
Epoch 652, Loss: 0.00000015956701, Improvement: -0.00000000225988, Best Loss: 0.00000008279420 in Epoch 583
Epoch 653
Epoch 653, Loss: 0.00000016475081, Improvement: 0.00000000518380, Best Loss: 0.00000008279420 in Epoch 583
Epoch 654
Epoch 654, Loss: 0.00000016076571, Improvement: -0.00000000398510, Best Loss: 0.00000008279420 in Epoch 583
Epoch 655
Epoch 655, Loss: 0.00000016620471, Improvement: 0.00000000543900, Best Loss: 0.00000008279420 in Epoch 583
Epoch 656
Epoch 656, Loss: 0.00000016694673, Improvement: 0.00000000074202, Best Loss: 0.00000008279420 in Epoch 583
Epoch 657
Epoch 657, Loss: 0.00000015977850, Improvement: -0.00000000716824, Best Loss: 0.00000008279420 in Epoch 583
Epoch 658
Epoch 658, Loss: 0.00000015910832, Improvement: -0.00000000067017, Best Loss: 0.00000008279420 in Epoch 583
Epoch 659
Epoch 659, Loss: 0.00000016951289, Improvement: 0.00000001040456, Best Loss: 0.00000008279420 in Epoch 583
Epoch 660
Epoch 660, Loss: 0.00000018235007, Improvement: 0.00000001283718, Best Loss: 0.00000008279420 in Epoch 583
Epoch 661
Epoch 661, Loss: 0.00000017346138, Improvement: -0.00000000888868, Best Loss: 0.00000008279420 in Epoch 583
Epoch 662
Epoch 662, Loss: 0.00000017269682, Improvement: -0.00000000076457, Best Loss: 0.00000008279420 in Epoch 583
Epoch 663
Epoch 663, Loss: 0.00000016565936, Improvement: -0.00000000703745, Best Loss: 0.00000008279420 in Epoch 583
Epoch 664
Epoch 664, Loss: 0.00000016291620, Improvement: -0.00000000274316, Best Loss: 0.00000008279420 in Epoch 583
Epoch 665
Epoch 665, Loss: 0.00000016837680, Improvement: 0.00000000546060, Best Loss: 0.00000008279420 in Epoch 583
Epoch 666
Epoch 666, Loss: 0.00000016448254, Improvement: -0.00000000389426, Best Loss: 0.00000008279420 in Epoch 583
Epoch 667
Epoch 667, Loss: 0.00000017076056, Improvement: 0.00000000627802, Best Loss: 0.00000008279420 in Epoch 583
Epoch 668
Epoch 668, Loss: 0.00000026576731, Improvement: 0.00000009500676, Best Loss: 0.00000008279420 in Epoch 583
Epoch 669
Epoch 669, Loss: 0.00000022279007, Improvement: -0.00000004297724, Best Loss: 0.00000008279420 in Epoch 583
Epoch 670
Epoch 670, Loss: 0.00000020445899, Improvement: -0.00000001833109, Best Loss: 0.00000008279420 in Epoch 583
Epoch 671
Epoch 671, Loss: 0.00000022275766, Improvement: 0.00000001829867, Best Loss: 0.00000008279420 in Epoch 583
Epoch 672
Epoch 672, Loss: 0.00000019885231, Improvement: -0.00000002390535, Best Loss: 0.00000008279420 in Epoch 583
Epoch 673
Epoch 673, Loss: 0.00000021349915, Improvement: 0.00000001464684, Best Loss: 0.00000008279420 in Epoch 583
Epoch 674
Epoch 674, Loss: 0.00000024765370, Improvement: 0.00000003415455, Best Loss: 0.00000008279420 in Epoch 583
Epoch 675
Epoch 675, Loss: 0.00000017659487, Improvement: -0.00000007105883, Best Loss: 0.00000008279420 in Epoch 583
Epoch 676
Epoch 676, Loss: 0.00000016107679, Improvement: -0.00000001551808, Best Loss: 0.00000008279420 in Epoch 583
Epoch 677
Epoch 677, Loss: 0.00000016089707, Improvement: -0.00000000017972, Best Loss: 0.00000008279420 in Epoch 583
Epoch 678
Epoch 678, Loss: 0.00000016034659, Improvement: -0.00000000055049, Best Loss: 0.00000008279420 in Epoch 583
Epoch 679
Epoch 679, Loss: 0.00000015918546, Improvement: -0.00000000116113, Best Loss: 0.00000008279420 in Epoch 583
Epoch 680
Epoch 680, Loss: 0.00000016164217, Improvement: 0.00000000245672, Best Loss: 0.00000008279420 in Epoch 583
Epoch 681
Epoch 681, Loss: 0.00000016074193, Improvement: -0.00000000090025, Best Loss: 0.00000008279420 in Epoch 583
Epoch 682
Epoch 682, Loss: 0.00000016071905, Improvement: -0.00000000002288, Best Loss: 0.00000008279420 in Epoch 583
Epoch 683
Epoch 683, Loss: 0.00000016508523, Improvement: 0.00000000436618, Best Loss: 0.00000008279420 in Epoch 583
Epoch 684
Epoch 684, Loss: 0.00000016649353, Improvement: 0.00000000140830, Best Loss: 0.00000008279420 in Epoch 583
Epoch 685
Epoch 685, Loss: 0.00000017611261, Improvement: 0.00000000961908, Best Loss: 0.00000008279420 in Epoch 583
Epoch 686
Epoch 686, Loss: 0.00000017934000, Improvement: 0.00000000322739, Best Loss: 0.00000008279420 in Epoch 583
Epoch 687
Epoch 687, Loss: 0.00000021014194, Improvement: 0.00000003080194, Best Loss: 0.00000008279420 in Epoch 583
Epoch 688
Epoch 688, Loss: 0.00000022222076, Improvement: 0.00000001207881, Best Loss: 0.00000008279420 in Epoch 583
Epoch 689
Epoch 689, Loss: 0.00000019853358, Improvement: -0.00000002368717, Best Loss: 0.00000008279420 in Epoch 583
Epoch 690
Epoch 690, Loss: 0.00000017764179, Improvement: -0.00000002089179, Best Loss: 0.00000008279420 in Epoch 583
Epoch 691
Epoch 691, Loss: 0.00000018019409, Improvement: 0.00000000255230, Best Loss: 0.00000008279420 in Epoch 583
Epoch 692
Epoch 692, Loss: 0.00000016306470, Improvement: -0.00000001712940, Best Loss: 0.00000008279420 in Epoch 583
Epoch 693
Epoch 693, Loss: 0.00000016393439, Improvement: 0.00000000086969, Best Loss: 0.00000008279420 in Epoch 583
Epoch 694
Epoch 694, Loss: 0.00000016017743, Improvement: -0.00000000375695, Best Loss: 0.00000008279420 in Epoch 583
Epoch 695
Epoch 695, Loss: 0.00000016563197, Improvement: 0.00000000545454, Best Loss: 0.00000008279420 in Epoch 583
Epoch 696
Epoch 696, Loss: 0.00000017140804, Improvement: 0.00000000577607, Best Loss: 0.00000008279420 in Epoch 583
Epoch 697
Epoch 697, Loss: 0.00000017970994, Improvement: 0.00000000830190, Best Loss: 0.00000008279420 in Epoch 583
Epoch 698
Epoch 698, Loss: 0.00000019951845, Improvement: 0.00000001980851, Best Loss: 0.00000008279420 in Epoch 583
Epoch 699
Epoch 699, Loss: 0.00000023592694, Improvement: 0.00000003640849, Best Loss: 0.00000008279420 in Epoch 583
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000017168523, Improvement: -0.00000006424171, Best Loss: 0.00000008279420 in Epoch 583
Epoch 701
Epoch 701, Loss: 0.00000017103565, Improvement: -0.00000000064958, Best Loss: 0.00000008279420 in Epoch 583
Epoch 702
Epoch 702, Loss: 0.00000017469445, Improvement: 0.00000000365879, Best Loss: 0.00000008279420 in Epoch 583
Epoch 703
Epoch 703, Loss: 0.00000017552231, Improvement: 0.00000000082786, Best Loss: 0.00000008279420 in Epoch 583
Epoch 704
Epoch 704, Loss: 0.00000018241506, Improvement: 0.00000000689275, Best Loss: 0.00000008279420 in Epoch 583
Epoch 705
Epoch 705, Loss: 0.00000016696197, Improvement: -0.00000001545309, Best Loss: 0.00000008279420 in Epoch 583
Epoch 706
Epoch 706, Loss: 0.00000016774106, Improvement: 0.00000000077908, Best Loss: 0.00000008279420 in Epoch 583
Epoch 707
Epoch 707, Loss: 0.00000015883740, Improvement: -0.00000000890366, Best Loss: 0.00000008279420 in Epoch 583
Epoch 708
Epoch 708, Loss: 0.00000016148080, Improvement: 0.00000000264340, Best Loss: 0.00000008279420 in Epoch 583
Epoch 709
Epoch 709, Loss: 0.00000017006302, Improvement: 0.00000000858222, Best Loss: 0.00000008279420 in Epoch 583
Epoch 710
Epoch 710, Loss: 0.00000017690843, Improvement: 0.00000000684540, Best Loss: 0.00000008279420 in Epoch 583
Epoch 711
Epoch 711, Loss: 0.00000017132105, Improvement: -0.00000000558738, Best Loss: 0.00000008279420 in Epoch 583
Epoch 712
Epoch 712, Loss: 0.00000017216221, Improvement: 0.00000000084116, Best Loss: 0.00000008279420 in Epoch 583
Epoch 713
Epoch 713, Loss: 0.00000024163632, Improvement: 0.00000006947410, Best Loss: 0.00000008279420 in Epoch 583
Epoch 714
Epoch 714, Loss: 0.00000019534684, Improvement: -0.00000004628948, Best Loss: 0.00000008279420 in Epoch 583
Epoch 715
Epoch 715, Loss: 0.00000019888176, Improvement: 0.00000000353492, Best Loss: 0.00000008279420 in Epoch 583
Epoch 716
Epoch 716, Loss: 0.00000017524436, Improvement: -0.00000002363740, Best Loss: 0.00000008279420 in Epoch 583
Epoch 717
Epoch 717, Loss: 0.00000020310065, Improvement: 0.00000002785630, Best Loss: 0.00000008279420 in Epoch 583
Epoch 718
Epoch 718, Loss: 0.00000018014203, Improvement: -0.00000002295863, Best Loss: 0.00000008279420 in Epoch 583
Epoch 719
Epoch 719, Loss: 0.00000017865917, Improvement: -0.00000000148286, Best Loss: 0.00000008279420 in Epoch 583
Epoch 720
Epoch 720, Loss: 0.00000016881847, Improvement: -0.00000000984070, Best Loss: 0.00000008279420 in Epoch 583
Epoch 721
Epoch 721, Loss: 0.00000016840847, Improvement: -0.00000000041000, Best Loss: 0.00000008279420 in Epoch 583
Epoch 722
Epoch 722, Loss: 0.00000017075486, Improvement: 0.00000000234639, Best Loss: 0.00000008279420 in Epoch 583
Epoch 723
Epoch 723, Loss: 0.00000016010275, Improvement: -0.00000001065211, Best Loss: 0.00000008279420 in Epoch 583
Epoch 724
Epoch 724, Loss: 0.00000015862670, Improvement: -0.00000000147605, Best Loss: 0.00000008279420 in Epoch 583
Epoch 725
Epoch 725, Loss: 0.00000016024944, Improvement: 0.00000000162273, Best Loss: 0.00000008279420 in Epoch 583
Epoch 726
Epoch 726, Loss: 0.00000016055958, Improvement: 0.00000000031014, Best Loss: 0.00000008279420 in Epoch 583
Epoch 727
Epoch 727, Loss: 0.00000017441319, Improvement: 0.00000001385361, Best Loss: 0.00000008279420 in Epoch 583
Epoch 728
Epoch 728, Loss: 0.00000021650824, Improvement: 0.00000004209505, Best Loss: 0.00000008279420 in Epoch 583
Epoch 729
Epoch 729, Loss: 0.00000024102183, Improvement: 0.00000002451359, Best Loss: 0.00000008279420 in Epoch 583
Epoch 730
Epoch 730, Loss: 0.00000020547082, Improvement: -0.00000003555100, Best Loss: 0.00000008279420 in Epoch 583
Epoch 731
Epoch 731, Loss: 0.00000016983923, Improvement: -0.00000003563159, Best Loss: 0.00000008279420 in Epoch 583
Epoch 732
Epoch 732, Loss: 0.00000016151859, Improvement: -0.00000000832064, Best Loss: 0.00000008279420 in Epoch 583
Epoch 733
Epoch 733, Loss: 0.00000015869080, Improvement: -0.00000000282779, Best Loss: 0.00000008279420 in Epoch 583
Epoch 734
A best model at epoch 734 has been saved with training error 0.00000008218485.
Epoch 734, Loss: 0.00000015843403, Improvement: -0.00000000025677, Best Loss: 0.00000008218485 in Epoch 734
Epoch 735
Epoch 735, Loss: 0.00000016085856, Improvement: 0.00000000242453, Best Loss: 0.00000008218485 in Epoch 734
Epoch 736
Epoch 736, Loss: 0.00000015901707, Improvement: -0.00000000184149, Best Loss: 0.00000008218485 in Epoch 734
Epoch 737
Epoch 737, Loss: 0.00000016199585, Improvement: 0.00000000297878, Best Loss: 0.00000008218485 in Epoch 734
Epoch 738
Epoch 738, Loss: 0.00000016085611, Improvement: -0.00000000113975, Best Loss: 0.00000008218485 in Epoch 734
Epoch 739
Epoch 739, Loss: 0.00000015723715, Improvement: -0.00000000361895, Best Loss: 0.00000008218485 in Epoch 734
Epoch 740
Epoch 740, Loss: 0.00000016270442, Improvement: 0.00000000546726, Best Loss: 0.00000008218485 in Epoch 734
Epoch 741
Epoch 741, Loss: 0.00000016255686, Improvement: -0.00000000014756, Best Loss: 0.00000008218485 in Epoch 734
Epoch 742
Epoch 742, Loss: 0.00000018109955, Improvement: 0.00000001854269, Best Loss: 0.00000008218485 in Epoch 734
Epoch 743
Epoch 743, Loss: 0.00000021224715, Improvement: 0.00000003114760, Best Loss: 0.00000008218485 in Epoch 734
Epoch 744
Epoch 744, Loss: 0.00000020693278, Improvement: -0.00000000531436, Best Loss: 0.00000008218485 in Epoch 734
Epoch 745
Epoch 745, Loss: 0.00000016388188, Improvement: -0.00000004305090, Best Loss: 0.00000008218485 in Epoch 734
Epoch 746
Epoch 746, Loss: 0.00000016270411, Improvement: -0.00000000117777, Best Loss: 0.00000008218485 in Epoch 734
Epoch 747
Epoch 747, Loss: 0.00000015938951, Improvement: -0.00000000331461, Best Loss: 0.00000008218485 in Epoch 734
Epoch 748
Epoch 748, Loss: 0.00000015991814, Improvement: 0.00000000052863, Best Loss: 0.00000008218485 in Epoch 734
Epoch 749
Epoch 749, Loss: 0.00000016427278, Improvement: 0.00000000435464, Best Loss: 0.00000008218485 in Epoch 734
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000016678775, Improvement: 0.00000000251497, Best Loss: 0.00000008218485 in Epoch 734
Epoch 751
Epoch 751, Loss: 0.00000016070484, Improvement: -0.00000000608291, Best Loss: 0.00000008218485 in Epoch 734
Epoch 752
Epoch 752, Loss: 0.00000016930280, Improvement: 0.00000000859796, Best Loss: 0.00000008218485 in Epoch 734
Epoch 753
Epoch 753, Loss: 0.00000021926025, Improvement: 0.00000004995745, Best Loss: 0.00000008218485 in Epoch 734
Epoch 754
Epoch 754, Loss: 0.00000029502792, Improvement: 0.00000007576767, Best Loss: 0.00000008218485 in Epoch 734
Epoch 755
Epoch 755, Loss: 0.00000020968864, Improvement: -0.00000008533928, Best Loss: 0.00000008218485 in Epoch 734
Epoch 756
Epoch 756, Loss: 0.00000017511922, Improvement: -0.00000003456942, Best Loss: 0.00000008218485 in Epoch 734
Epoch 757
Epoch 757, Loss: 0.00000016648467, Improvement: -0.00000000863455, Best Loss: 0.00000008218485 in Epoch 734
Epoch 758
Epoch 758, Loss: 0.00000015992639, Improvement: -0.00000000655827, Best Loss: 0.00000008218485 in Epoch 734
Epoch 759
Epoch 759, Loss: 0.00000015809772, Improvement: -0.00000000182868, Best Loss: 0.00000008218485 in Epoch 734
Epoch 760
Epoch 760, Loss: 0.00000015852720, Improvement: 0.00000000042948, Best Loss: 0.00000008218485 in Epoch 734
Epoch 761
Epoch 761, Loss: 0.00000015641698, Improvement: -0.00000000211023, Best Loss: 0.00000008218485 in Epoch 734
Epoch 762
Epoch 762, Loss: 0.00000015731867, Improvement: 0.00000000090169, Best Loss: 0.00000008218485 in Epoch 734
Epoch 763
Epoch 763, Loss: 0.00000016452427, Improvement: 0.00000000720560, Best Loss: 0.00000008218485 in Epoch 734
Epoch 764
Epoch 764, Loss: 0.00000016187393, Improvement: -0.00000000265034, Best Loss: 0.00000008218485 in Epoch 734
Epoch 765
Epoch 765, Loss: 0.00000016196879, Improvement: 0.00000000009486, Best Loss: 0.00000008218485 in Epoch 734
Epoch 766
Epoch 766, Loss: 0.00000015803200, Improvement: -0.00000000393678, Best Loss: 0.00000008218485 in Epoch 734
Epoch 767
Epoch 767, Loss: 0.00000015920988, Improvement: 0.00000000117788, Best Loss: 0.00000008218485 in Epoch 734
Epoch 768
Epoch 768, Loss: 0.00000016208253, Improvement: 0.00000000287265, Best Loss: 0.00000008218485 in Epoch 734
Epoch 769
Epoch 769, Loss: 0.00000016362738, Improvement: 0.00000000154485, Best Loss: 0.00000008218485 in Epoch 734
Epoch 770
Epoch 770, Loss: 0.00000016012076, Improvement: -0.00000000350662, Best Loss: 0.00000008218485 in Epoch 734
Epoch 771
Epoch 771, Loss: 0.00000015990571, Improvement: -0.00000000021505, Best Loss: 0.00000008218485 in Epoch 734
Epoch 772
Epoch 772, Loss: 0.00000017347733, Improvement: 0.00000001357163, Best Loss: 0.00000008218485 in Epoch 734
Epoch 773
Epoch 773, Loss: 0.00000017912577, Improvement: 0.00000000564844, Best Loss: 0.00000008218485 in Epoch 734
Epoch 774
Epoch 774, Loss: 0.00000032994720, Improvement: 0.00000015082143, Best Loss: 0.00000008218485 in Epoch 734
Epoch 775
Epoch 775, Loss: 0.00000019584163, Improvement: -0.00000013410557, Best Loss: 0.00000008218485 in Epoch 734
Epoch 776
Epoch 776, Loss: 0.00000016667915, Improvement: -0.00000002916247, Best Loss: 0.00000008218485 in Epoch 734
Epoch 777
Epoch 777, Loss: 0.00000015942789, Improvement: -0.00000000725126, Best Loss: 0.00000008218485 in Epoch 734
Epoch 778
Epoch 778, Loss: 0.00000015710126, Improvement: -0.00000000232664, Best Loss: 0.00000008218485 in Epoch 734
Epoch 779
Epoch 779, Loss: 0.00000015687522, Improvement: -0.00000000022604, Best Loss: 0.00000008218485 in Epoch 734
Epoch 780
Epoch 780, Loss: 0.00000015598102, Improvement: -0.00000000089420, Best Loss: 0.00000008218485 in Epoch 734
Epoch 781
Epoch 781, Loss: 0.00000015769984, Improvement: 0.00000000171883, Best Loss: 0.00000008218485 in Epoch 734
Epoch 782
Epoch 782, Loss: 0.00000015938139, Improvement: 0.00000000168155, Best Loss: 0.00000008218485 in Epoch 734
Epoch 783
Epoch 783, Loss: 0.00000015620691, Improvement: -0.00000000317448, Best Loss: 0.00000008218485 in Epoch 734
Epoch 784
Epoch 784, Loss: 0.00000015726336, Improvement: 0.00000000105645, Best Loss: 0.00000008218485 in Epoch 734
Epoch 785
Epoch 785, Loss: 0.00000015711946, Improvement: -0.00000000014390, Best Loss: 0.00000008218485 in Epoch 734
Epoch 786
Epoch 786, Loss: 0.00000015574846, Improvement: -0.00000000137100, Best Loss: 0.00000008218485 in Epoch 734
Epoch 787
Epoch 787, Loss: 0.00000015552903, Improvement: -0.00000000021943, Best Loss: 0.00000008218485 in Epoch 734
Epoch 788
Epoch 788, Loss: 0.00000015743349, Improvement: 0.00000000190446, Best Loss: 0.00000008218485 in Epoch 734
Epoch 789
Epoch 789, Loss: 0.00000016168118, Improvement: 0.00000000424769, Best Loss: 0.00000008218485 in Epoch 734
Epoch 790
Epoch 790, Loss: 0.00000015580461, Improvement: -0.00000000587656, Best Loss: 0.00000008218485 in Epoch 734
Epoch 791
Epoch 791, Loss: 0.00000016055704, Improvement: 0.00000000475243, Best Loss: 0.00000008218485 in Epoch 734
Epoch 792
Epoch 792, Loss: 0.00000015604842, Improvement: -0.00000000450862, Best Loss: 0.00000008218485 in Epoch 734
Epoch 793
Epoch 793, Loss: 0.00000015508352, Improvement: -0.00000000096489, Best Loss: 0.00000008218485 in Epoch 734
Epoch 794
Epoch 794, Loss: 0.00000015497677, Improvement: -0.00000000010676, Best Loss: 0.00000008218485 in Epoch 734
Epoch 795
Epoch 795, Loss: 0.00000016497111, Improvement: 0.00000000999435, Best Loss: 0.00000008218485 in Epoch 734
Epoch 796
Epoch 796, Loss: 0.00000016322869, Improvement: -0.00000000174242, Best Loss: 0.00000008218485 in Epoch 734
Epoch 797
Epoch 797, Loss: 0.00000015909387, Improvement: -0.00000000413483, Best Loss: 0.00000008218485 in Epoch 734
Epoch 798
Epoch 798, Loss: 0.00000015874652, Improvement: -0.00000000034734, Best Loss: 0.00000008218485 in Epoch 734
Epoch 799
Epoch 799, Loss: 0.00000015632441, Improvement: -0.00000000242211, Best Loss: 0.00000008218485 in Epoch 734
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000015871083, Improvement: 0.00000000238643, Best Loss: 0.00000008218485 in Epoch 734
Epoch 801
Epoch 801, Loss: 0.00000016700636, Improvement: 0.00000000829553, Best Loss: 0.00000008218485 in Epoch 734
Epoch 802
Epoch 802, Loss: 0.00000018919003, Improvement: 0.00000002218367, Best Loss: 0.00000008218485 in Epoch 734
Epoch 803
Epoch 803, Loss: 0.00000019054773, Improvement: 0.00000000135770, Best Loss: 0.00000008218485 in Epoch 734
Epoch 804
Epoch 804, Loss: 0.00000020596524, Improvement: 0.00000001541751, Best Loss: 0.00000008218485 in Epoch 734
Epoch 805
Epoch 805, Loss: 0.00000029861423, Improvement: 0.00000009264899, Best Loss: 0.00000008218485 in Epoch 734
Epoch 806
Epoch 806, Loss: 0.00000018362754, Improvement: -0.00000011498669, Best Loss: 0.00000008218485 in Epoch 734
Epoch 807
Epoch 807, Loss: 0.00000016963006, Improvement: -0.00000001399747, Best Loss: 0.00000008218485 in Epoch 734
Epoch 808
Epoch 808, Loss: 0.00000016406011, Improvement: -0.00000000556995, Best Loss: 0.00000008218485 in Epoch 734
Epoch 809
Epoch 809, Loss: 0.00000015566924, Improvement: -0.00000000839087, Best Loss: 0.00000008218485 in Epoch 734
Epoch 810
Epoch 810, Loss: 0.00000015616426, Improvement: 0.00000000049502, Best Loss: 0.00000008218485 in Epoch 734
Epoch 811
Epoch 811, Loss: 0.00000015829171, Improvement: 0.00000000212745, Best Loss: 0.00000008218485 in Epoch 734
Epoch 812
Epoch 812, Loss: 0.00000015756528, Improvement: -0.00000000072643, Best Loss: 0.00000008218485 in Epoch 734
Epoch 813
Epoch 813, Loss: 0.00000016225797, Improvement: 0.00000000469269, Best Loss: 0.00000008218485 in Epoch 734
Epoch 814
Epoch 814, Loss: 0.00000016312771, Improvement: 0.00000000086973, Best Loss: 0.00000008218485 in Epoch 734
Epoch 815
Epoch 815, Loss: 0.00000016310383, Improvement: -0.00000000002388, Best Loss: 0.00000008218485 in Epoch 734
Epoch 816
Epoch 816, Loss: 0.00000015471732, Improvement: -0.00000000838650, Best Loss: 0.00000008218485 in Epoch 734
Epoch 817
Epoch 817, Loss: 0.00000015584084, Improvement: 0.00000000112352, Best Loss: 0.00000008218485 in Epoch 734
Epoch 818
Epoch 818, Loss: 0.00000015626863, Improvement: 0.00000000042778, Best Loss: 0.00000008218485 in Epoch 734
Epoch 819
Epoch 819, Loss: 0.00000015744711, Improvement: 0.00000000117848, Best Loss: 0.00000008218485 in Epoch 734
Epoch 820
Epoch 820, Loss: 0.00000016134935, Improvement: 0.00000000390224, Best Loss: 0.00000008218485 in Epoch 734
Epoch 821
Epoch 821, Loss: 0.00000016282167, Improvement: 0.00000000147233, Best Loss: 0.00000008218485 in Epoch 734
Epoch 822
Epoch 822, Loss: 0.00000016977879, Improvement: 0.00000000695711, Best Loss: 0.00000008218485 in Epoch 734
Epoch 823
Epoch 823, Loss: 0.00000017127332, Improvement: 0.00000000149453, Best Loss: 0.00000008218485 in Epoch 734
Epoch 824
Epoch 824, Loss: 0.00000016835392, Improvement: -0.00000000291940, Best Loss: 0.00000008218485 in Epoch 734
Epoch 825
Epoch 825, Loss: 0.00000015987190, Improvement: -0.00000000848201, Best Loss: 0.00000008218485 in Epoch 734
Epoch 826
Epoch 826, Loss: 0.00000015817596, Improvement: -0.00000000169594, Best Loss: 0.00000008218485 in Epoch 734
Epoch 827
Epoch 827, Loss: 0.00000016121429, Improvement: 0.00000000303833, Best Loss: 0.00000008218485 in Epoch 734
Epoch 828
Epoch 828, Loss: 0.00000016174658, Improvement: 0.00000000053228, Best Loss: 0.00000008218485 in Epoch 734
Epoch 829
Epoch 829, Loss: 0.00000016101641, Improvement: -0.00000000073017, Best Loss: 0.00000008218485 in Epoch 734
Epoch 830
Epoch 830, Loss: 0.00000015904236, Improvement: -0.00000000197405, Best Loss: 0.00000008218485 in Epoch 734
Epoch 831
Epoch 831, Loss: 0.00000016681161, Improvement: 0.00000000776925, Best Loss: 0.00000008218485 in Epoch 734
Epoch 832
Epoch 832, Loss: 0.00000020908379, Improvement: 0.00000004227218, Best Loss: 0.00000008218485 in Epoch 734
Epoch 833
Epoch 833, Loss: 0.00000025693602, Improvement: 0.00000004785223, Best Loss: 0.00000008218485 in Epoch 734
Epoch 834
Epoch 834, Loss: 0.00000017612754, Improvement: -0.00000008080848, Best Loss: 0.00000008218485 in Epoch 734
Epoch 835
Epoch 835, Loss: 0.00000016253711, Improvement: -0.00000001359043, Best Loss: 0.00000008218485 in Epoch 734
Epoch 836
Epoch 836, Loss: 0.00000016355024, Improvement: 0.00000000101313, Best Loss: 0.00000008218485 in Epoch 734
Epoch 837
Epoch 837, Loss: 0.00000016651498, Improvement: 0.00000000296475, Best Loss: 0.00000008218485 in Epoch 734
Epoch 838
Epoch 838, Loss: 0.00000016923312, Improvement: 0.00000000271814, Best Loss: 0.00000008218485 in Epoch 734
Epoch 839
Epoch 839, Loss: 0.00000015811539, Improvement: -0.00000001111774, Best Loss: 0.00000008218485 in Epoch 734
Epoch 840
Epoch 840, Loss: 0.00000015413553, Improvement: -0.00000000397986, Best Loss: 0.00000008218485 in Epoch 734
Epoch 841
Epoch 841, Loss: 0.00000015487046, Improvement: 0.00000000073494, Best Loss: 0.00000008218485 in Epoch 734
Epoch 842
Epoch 842, Loss: 0.00000015583490, Improvement: 0.00000000096444, Best Loss: 0.00000008218485 in Epoch 734
Epoch 843
Epoch 843, Loss: 0.00000016083802, Improvement: 0.00000000500312, Best Loss: 0.00000008218485 in Epoch 734
Epoch 844
Epoch 844, Loss: 0.00000016280410, Improvement: 0.00000000196608, Best Loss: 0.00000008218485 in Epoch 734
Epoch 845
Epoch 845, Loss: 0.00000015924941, Improvement: -0.00000000355469, Best Loss: 0.00000008218485 in Epoch 734
Epoch 846
Epoch 846, Loss: 0.00000015832047, Improvement: -0.00000000092894, Best Loss: 0.00000008218485 in Epoch 734
Epoch 847
Epoch 847, Loss: 0.00000015646905, Improvement: -0.00000000185142, Best Loss: 0.00000008218485 in Epoch 734
Epoch 848
Epoch 848, Loss: 0.00000015749416, Improvement: 0.00000000102511, Best Loss: 0.00000008218485 in Epoch 734
Epoch 849
Epoch 849, Loss: 0.00000015942393, Improvement: 0.00000000192977, Best Loss: 0.00000008218485 in Epoch 734
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000016258253, Improvement: 0.00000000315860, Best Loss: 0.00000008218485 in Epoch 734
Epoch 851
Epoch 851, Loss: 0.00000015839467, Improvement: -0.00000000418786, Best Loss: 0.00000008218485 in Epoch 734
Epoch 852
Epoch 852, Loss: 0.00000017055444, Improvement: 0.00000001215978, Best Loss: 0.00000008218485 in Epoch 734
Epoch 853
Epoch 853, Loss: 0.00000015770487, Improvement: -0.00000001284957, Best Loss: 0.00000008218485 in Epoch 734
Epoch 854
Epoch 854, Loss: 0.00000016015915, Improvement: 0.00000000245428, Best Loss: 0.00000008218485 in Epoch 734
Epoch 855
Epoch 855, Loss: 0.00000015931780, Improvement: -0.00000000084134, Best Loss: 0.00000008218485 in Epoch 734
Epoch 856
Epoch 856, Loss: 0.00000017494493, Improvement: 0.00000001562712, Best Loss: 0.00000008218485 in Epoch 734
Epoch 857
Epoch 857, Loss: 0.00000017006613, Improvement: -0.00000000487880, Best Loss: 0.00000008218485 in Epoch 734
Epoch 858
Epoch 858, Loss: 0.00000015945127, Improvement: -0.00000001061486, Best Loss: 0.00000008218485 in Epoch 734
Epoch 859
Epoch 859, Loss: 0.00000025053328, Improvement: 0.00000009108201, Best Loss: 0.00000008218485 in Epoch 734
Epoch 860
Epoch 860, Loss: 0.00000025840135, Improvement: 0.00000000786807, Best Loss: 0.00000008218485 in Epoch 734
Epoch 861
Epoch 861, Loss: 0.00000019391575, Improvement: -0.00000006448560, Best Loss: 0.00000008218485 in Epoch 734
Epoch 862
Epoch 862, Loss: 0.00000016499676, Improvement: -0.00000002891899, Best Loss: 0.00000008218485 in Epoch 734
Epoch 863
Epoch 863, Loss: 0.00000015804773, Improvement: -0.00000000694902, Best Loss: 0.00000008218485 in Epoch 734
Epoch 864
Epoch 864, Loss: 0.00000015882858, Improvement: 0.00000000078085, Best Loss: 0.00000008218485 in Epoch 734
Epoch 865
Epoch 865, Loss: 0.00000015481788, Improvement: -0.00000000401071, Best Loss: 0.00000008218485 in Epoch 734
Epoch 866
Epoch 866, Loss: 0.00000015456202, Improvement: -0.00000000025585, Best Loss: 0.00000008218485 in Epoch 734
Epoch 867
Epoch 867, Loss: 0.00000015731240, Improvement: 0.00000000275038, Best Loss: 0.00000008218485 in Epoch 734
Epoch 868
Epoch 868, Loss: 0.00000015482758, Improvement: -0.00000000248482, Best Loss: 0.00000008218485 in Epoch 734
Epoch 869
Epoch 869, Loss: 0.00000015387386, Improvement: -0.00000000095372, Best Loss: 0.00000008218485 in Epoch 734
Epoch 870
Epoch 870, Loss: 0.00000015419331, Improvement: 0.00000000031945, Best Loss: 0.00000008218485 in Epoch 734
Epoch 871
Epoch 871, Loss: 0.00000015471639, Improvement: 0.00000000052308, Best Loss: 0.00000008218485 in Epoch 734
Epoch 872
Epoch 872, Loss: 0.00000015355511, Improvement: -0.00000000116128, Best Loss: 0.00000008218485 in Epoch 734
Epoch 873
Epoch 873, Loss: 0.00000015886158, Improvement: 0.00000000530647, Best Loss: 0.00000008218485 in Epoch 734
Epoch 874
Epoch 874, Loss: 0.00000016014422, Improvement: 0.00000000128264, Best Loss: 0.00000008218485 in Epoch 734
Epoch 875
Epoch 875, Loss: 0.00000016003860, Improvement: -0.00000000010562, Best Loss: 0.00000008218485 in Epoch 734
Epoch 876
Epoch 876, Loss: 0.00000015632436, Improvement: -0.00000000371424, Best Loss: 0.00000008218485 in Epoch 734
Epoch 877
Epoch 877, Loss: 0.00000015515834, Improvement: -0.00000000116602, Best Loss: 0.00000008218485 in Epoch 734
Epoch 878
Epoch 878, Loss: 0.00000015414176, Improvement: -0.00000000101658, Best Loss: 0.00000008218485 in Epoch 734
Epoch 879
Epoch 879, Loss: 0.00000016502018, Improvement: 0.00000001087843, Best Loss: 0.00000008218485 in Epoch 734
Epoch 880
Epoch 880, Loss: 0.00000015721062, Improvement: -0.00000000780957, Best Loss: 0.00000008218485 in Epoch 734
Epoch 881
Epoch 881, Loss: 0.00000015396906, Improvement: -0.00000000324156, Best Loss: 0.00000008218485 in Epoch 734
Epoch 882
Epoch 882, Loss: 0.00000015349714, Improvement: -0.00000000047192, Best Loss: 0.00000008218485 in Epoch 734
Epoch 883
Epoch 883, Loss: 0.00000016359985, Improvement: 0.00000001010271, Best Loss: 0.00000008218485 in Epoch 734
Epoch 884
Epoch 884, Loss: 0.00000017841642, Improvement: 0.00000001481657, Best Loss: 0.00000008218485 in Epoch 734
Epoch 885
Epoch 885, Loss: 0.00000017724865, Improvement: -0.00000000116777, Best Loss: 0.00000008218485 in Epoch 734
Epoch 886
Epoch 886, Loss: 0.00000017560856, Improvement: -0.00000000164009, Best Loss: 0.00000008218485 in Epoch 734
Epoch 887
Epoch 887, Loss: 0.00000017571223, Improvement: 0.00000000010367, Best Loss: 0.00000008218485 in Epoch 734
Epoch 888
Epoch 888, Loss: 0.00000018683448, Improvement: 0.00000001112225, Best Loss: 0.00000008218485 in Epoch 734
Epoch 889
Epoch 889, Loss: 0.00000020370729, Improvement: 0.00000001687281, Best Loss: 0.00000008218485 in Epoch 734
Epoch 890
Epoch 890, Loss: 0.00000016784608, Improvement: -0.00000003586120, Best Loss: 0.00000008218485 in Epoch 734
Epoch 891
Epoch 891, Loss: 0.00000017894981, Improvement: 0.00000001110373, Best Loss: 0.00000008218485 in Epoch 734
Epoch 892
Epoch 892, Loss: 0.00000018614243, Improvement: 0.00000000719262, Best Loss: 0.00000008218485 in Epoch 734
Epoch 893
Epoch 893, Loss: 0.00000016686760, Improvement: -0.00000001927483, Best Loss: 0.00000008218485 in Epoch 734
Epoch 894
Epoch 894, Loss: 0.00000015456674, Improvement: -0.00000001230086, Best Loss: 0.00000008218485 in Epoch 734
Epoch 895
Epoch 895, Loss: 0.00000022174013, Improvement: 0.00000006717340, Best Loss: 0.00000008218485 in Epoch 734
Epoch 896
Epoch 896, Loss: 0.00000029601983, Improvement: 0.00000007427970, Best Loss: 0.00000008218485 in Epoch 734
Epoch 897
Epoch 897, Loss: 0.00000020286846, Improvement: -0.00000009315137, Best Loss: 0.00000008218485 in Epoch 734
Epoch 898
Epoch 898, Loss: 0.00000018020746, Improvement: -0.00000002266100, Best Loss: 0.00000008218485 in Epoch 734
Epoch 899
Epoch 899, Loss: 0.00000015962931, Improvement: -0.00000002057815, Best Loss: 0.00000008218485 in Epoch 734
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000015474107, Improvement: -0.00000000488825, Best Loss: 0.00000008218485 in Epoch 734
Epoch 901
Epoch 901, Loss: 0.00000015685756, Improvement: 0.00000000211650, Best Loss: 0.00000008218485 in Epoch 734
Epoch 902
Epoch 902, Loss: 0.00000015247123, Improvement: -0.00000000438633, Best Loss: 0.00000008218485 in Epoch 734
Epoch 903
Epoch 903, Loss: 0.00000015128635, Improvement: -0.00000000118489, Best Loss: 0.00000008218485 in Epoch 734
Epoch 904
Epoch 904, Loss: 0.00000015059822, Improvement: -0.00000000068813, Best Loss: 0.00000008218485 in Epoch 734
Epoch 905
Epoch 905, Loss: 0.00000014916572, Improvement: -0.00000000143250, Best Loss: 0.00000008218485 in Epoch 734
Epoch 906
Epoch 906, Loss: 0.00000015189461, Improvement: 0.00000000272889, Best Loss: 0.00000008218485 in Epoch 734
Epoch 907
Epoch 907, Loss: 0.00000014817770, Improvement: -0.00000000371691, Best Loss: 0.00000008218485 in Epoch 734
Epoch 908
A best model at epoch 908 has been saved with training error 0.00000008209528.
Epoch 908, Loss: 0.00000014656545, Improvement: -0.00000000161225, Best Loss: 0.00000008209528 in Epoch 908
Epoch 909
Epoch 909, Loss: 0.00000014534787, Improvement: -0.00000000121758, Best Loss: 0.00000008209528 in Epoch 908
Epoch 910
Epoch 910, Loss: 0.00000014682523, Improvement: 0.00000000147736, Best Loss: 0.00000008209528 in Epoch 908
Epoch 911
Epoch 911, Loss: 0.00000014451650, Improvement: -0.00000000230873, Best Loss: 0.00000008209528 in Epoch 908
Epoch 912
Epoch 912, Loss: 0.00000014456884, Improvement: 0.00000000005235, Best Loss: 0.00000008209528 in Epoch 908
Epoch 913
Epoch 913, Loss: 0.00000014875459, Improvement: 0.00000000418575, Best Loss: 0.00000008209528 in Epoch 908
Epoch 914
Epoch 914, Loss: 0.00000014334206, Improvement: -0.00000000541253, Best Loss: 0.00000008209528 in Epoch 908
Epoch 915
Epoch 915, Loss: 0.00000013865017, Improvement: -0.00000000469189, Best Loss: 0.00000008209528 in Epoch 908
Epoch 916
Epoch 916, Loss: 0.00000015685603, Improvement: 0.00000001820586, Best Loss: 0.00000008209528 in Epoch 908
Epoch 917
Epoch 917, Loss: 0.00000020131002, Improvement: 0.00000004445399, Best Loss: 0.00000008209528 in Epoch 908
Epoch 918
Epoch 918, Loss: 0.00000016447229, Improvement: -0.00000003683773, Best Loss: 0.00000008209528 in Epoch 908
Epoch 919
Epoch 919, Loss: 0.00000014743033, Improvement: -0.00000001704196, Best Loss: 0.00000008209528 in Epoch 908
Epoch 920
Epoch 920, Loss: 0.00000018348131, Improvement: 0.00000003605098, Best Loss: 0.00000008209528 in Epoch 908
Epoch 921
Epoch 921, Loss: 0.00000018796644, Improvement: 0.00000000448513, Best Loss: 0.00000008209528 in Epoch 908
Epoch 922
Epoch 922, Loss: 0.00000014958197, Improvement: -0.00000003838447, Best Loss: 0.00000008209528 in Epoch 908
Epoch 923
Epoch 923, Loss: 0.00000014140747, Improvement: -0.00000000817450, Best Loss: 0.00000008209528 in Epoch 908
Epoch 924
Epoch 924, Loss: 0.00000014631889, Improvement: 0.00000000491143, Best Loss: 0.00000008209528 in Epoch 908
Epoch 925
Epoch 925, Loss: 0.00000014719742, Improvement: 0.00000000087853, Best Loss: 0.00000008209528 in Epoch 908
Epoch 926
Epoch 926, Loss: 0.00000013679785, Improvement: -0.00000001039957, Best Loss: 0.00000008209528 in Epoch 908
Epoch 927
Epoch 927, Loss: 0.00000013557216, Improvement: -0.00000000122570, Best Loss: 0.00000008209528 in Epoch 908
Epoch 928
Epoch 928, Loss: 0.00000013903522, Improvement: 0.00000000346307, Best Loss: 0.00000008209528 in Epoch 908
Epoch 929
Epoch 929, Loss: 0.00000016133085, Improvement: 0.00000002229562, Best Loss: 0.00000008209528 in Epoch 908
Epoch 930
Epoch 930, Loss: 0.00000015572939, Improvement: -0.00000000560146, Best Loss: 0.00000008209528 in Epoch 908
Epoch 931
Epoch 931, Loss: 0.00000020012693, Improvement: 0.00000004439755, Best Loss: 0.00000008209528 in Epoch 908
Epoch 932
Epoch 932, Loss: 0.00000026740144, Improvement: 0.00000006727451, Best Loss: 0.00000008209528 in Epoch 908
Epoch 933
Epoch 933, Loss: 0.00000016643083, Improvement: -0.00000010097061, Best Loss: 0.00000008209528 in Epoch 908
Epoch 934
Epoch 934, Loss: 0.00000013574239, Improvement: -0.00000003068844, Best Loss: 0.00000008209528 in Epoch 908
Epoch 935
Epoch 935, Loss: 0.00000012764964, Improvement: -0.00000000809275, Best Loss: 0.00000008209528 in Epoch 908
Epoch 936
A best model at epoch 936 has been saved with training error 0.00000006562677.
Epoch 936, Loss: 0.00000012589626, Improvement: -0.00000000175338, Best Loss: 0.00000006562677 in Epoch 936
Epoch 937
Epoch 937, Loss: 0.00000012729860, Improvement: 0.00000000140234, Best Loss: 0.00000006562677 in Epoch 936
Epoch 938
Epoch 938, Loss: 0.00000012301161, Improvement: -0.00000000428700, Best Loss: 0.00000006562677 in Epoch 936
Epoch 939
Epoch 939, Loss: 0.00000012422269, Improvement: 0.00000000121108, Best Loss: 0.00000006562677 in Epoch 936
Epoch 940
Epoch 940, Loss: 0.00000013080135, Improvement: 0.00000000657866, Best Loss: 0.00000006562677 in Epoch 936
Epoch 941
Epoch 941, Loss: 0.00000015640973, Improvement: 0.00000002560838, Best Loss: 0.00000006562677 in Epoch 936
Epoch 942
Epoch 942, Loss: 0.00000019995056, Improvement: 0.00000004354083, Best Loss: 0.00000006562677 in Epoch 936
Epoch 943
Epoch 943, Loss: 0.00000021340899, Improvement: 0.00000001345843, Best Loss: 0.00000006562677 in Epoch 936
Epoch 944
Epoch 944, Loss: 0.00000021456340, Improvement: 0.00000000115440, Best Loss: 0.00000006562677 in Epoch 936
Epoch 945
Epoch 945, Loss: 0.00000015745088, Improvement: -0.00000005711251, Best Loss: 0.00000006562677 in Epoch 936
Epoch 946
Epoch 946, Loss: 0.00000013036642, Improvement: -0.00000002708446, Best Loss: 0.00000006562677 in Epoch 936
Epoch 947
Epoch 947, Loss: 0.00000012294100, Improvement: -0.00000000742542, Best Loss: 0.00000006562677 in Epoch 936
Epoch 948
Epoch 948, Loss: 0.00000011926887, Improvement: -0.00000000367214, Best Loss: 0.00000006562677 in Epoch 936
Epoch 949
Epoch 949, Loss: 0.00000011654279, Improvement: -0.00000000272608, Best Loss: 0.00000006562677 in Epoch 936
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000011476884, Improvement: -0.00000000177395, Best Loss: 0.00000006562677 in Epoch 936
Epoch 951
Epoch 951, Loss: 0.00000011424375, Improvement: -0.00000000052509, Best Loss: 0.00000006562677 in Epoch 936
Epoch 952
Epoch 952, Loss: 0.00000012061118, Improvement: 0.00000000636743, Best Loss: 0.00000006562677 in Epoch 936
Epoch 953
Epoch 953, Loss: 0.00000012495723, Improvement: 0.00000000434606, Best Loss: 0.00000006562677 in Epoch 936
Epoch 954
Epoch 954, Loss: 0.00000014438800, Improvement: 0.00000001943077, Best Loss: 0.00000006562677 in Epoch 936
Epoch 955
Epoch 955, Loss: 0.00000020432357, Improvement: 0.00000005993557, Best Loss: 0.00000006562677 in Epoch 936
Epoch 956
Epoch 956, Loss: 0.00000032692411, Improvement: 0.00000012260054, Best Loss: 0.00000006562677 in Epoch 936
Epoch 957
Epoch 957, Loss: 0.00000022141260, Improvement: -0.00000010551151, Best Loss: 0.00000006562677 in Epoch 936
Epoch 958
Epoch 958, Loss: 0.00000014905010, Improvement: -0.00000007236250, Best Loss: 0.00000006562677 in Epoch 936
Epoch 959
Epoch 959, Loss: 0.00000012938911, Improvement: -0.00000001966099, Best Loss: 0.00000006562677 in Epoch 936
Epoch 960
Epoch 960, Loss: 0.00000012310668, Improvement: -0.00000000628243, Best Loss: 0.00000006562677 in Epoch 936
Epoch 961
Epoch 961, Loss: 0.00000011934114, Improvement: -0.00000000376554, Best Loss: 0.00000006562677 in Epoch 936
Epoch 962
Epoch 962, Loss: 0.00000011689917, Improvement: -0.00000000244197, Best Loss: 0.00000006562677 in Epoch 936
Epoch 963
A best model at epoch 963 has been saved with training error 0.00000006193706.
Epoch 963, Loss: 0.00000011464901, Improvement: -0.00000000225016, Best Loss: 0.00000006193706 in Epoch 963
Epoch 964
Epoch 964, Loss: 0.00000011280250, Improvement: -0.00000000184651, Best Loss: 0.00000006193706 in Epoch 963
Epoch 965
Epoch 965, Loss: 0.00000011056050, Improvement: -0.00000000224199, Best Loss: 0.00000006193706 in Epoch 963
Epoch 966
Epoch 966, Loss: 0.00000010916840, Improvement: -0.00000000139210, Best Loss: 0.00000006193706 in Epoch 963
Epoch 967
Epoch 967, Loss: 0.00000010892754, Improvement: -0.00000000024086, Best Loss: 0.00000006193706 in Epoch 963
Epoch 968
Epoch 968, Loss: 0.00000010935047, Improvement: 0.00000000042293, Best Loss: 0.00000006193706 in Epoch 963
Epoch 969
Epoch 969, Loss: 0.00000011758828, Improvement: 0.00000000823781, Best Loss: 0.00000006193706 in Epoch 963
Epoch 970
Epoch 970, Loss: 0.00000011445217, Improvement: -0.00000000313611, Best Loss: 0.00000006193706 in Epoch 963
Epoch 971
Epoch 971, Loss: 0.00000010958169, Improvement: -0.00000000487047, Best Loss: 0.00000006193706 in Epoch 963
Epoch 972
Epoch 972, Loss: 0.00000011919650, Improvement: 0.00000000961480, Best Loss: 0.00000006193706 in Epoch 963
Epoch 973
Epoch 973, Loss: 0.00000011166999, Improvement: -0.00000000752650, Best Loss: 0.00000006193706 in Epoch 963
Epoch 974
Epoch 974, Loss: 0.00000011682940, Improvement: 0.00000000515941, Best Loss: 0.00000006193706 in Epoch 963
Epoch 975
Epoch 975, Loss: 0.00000011757389, Improvement: 0.00000000074449, Best Loss: 0.00000006193706 in Epoch 963
Epoch 976
Epoch 976, Loss: 0.00000014303264, Improvement: 0.00000002545875, Best Loss: 0.00000006193706 in Epoch 963
Epoch 977
Epoch 977, Loss: 0.00000025283575, Improvement: 0.00000010980311, Best Loss: 0.00000006193706 in Epoch 963
Epoch 978
Epoch 978, Loss: 0.00000022628151, Improvement: -0.00000002655424, Best Loss: 0.00000006193706 in Epoch 963
Epoch 979
Epoch 979, Loss: 0.00000013274394, Improvement: -0.00000009353757, Best Loss: 0.00000006193706 in Epoch 963
Epoch 980
Epoch 980, Loss: 0.00000011235765, Improvement: -0.00000002038630, Best Loss: 0.00000006193706 in Epoch 963
Epoch 981
Epoch 981, Loss: 0.00000011177800, Improvement: -0.00000000057965, Best Loss: 0.00000006193706 in Epoch 963
Epoch 982
Epoch 982, Loss: 0.00000011857687, Improvement: 0.00000000679888, Best Loss: 0.00000006193706 in Epoch 963
Epoch 983
Epoch 983, Loss: 0.00000010820247, Improvement: -0.00000001037440, Best Loss: 0.00000006193706 in Epoch 963
Epoch 984
Epoch 984, Loss: 0.00000011053995, Improvement: 0.00000000233748, Best Loss: 0.00000006193706 in Epoch 963
Epoch 985
Epoch 985, Loss: 0.00000011762190, Improvement: 0.00000000708195, Best Loss: 0.00000006193706 in Epoch 963
Epoch 986
Epoch 986, Loss: 0.00000011640173, Improvement: -0.00000000122016, Best Loss: 0.00000006193706 in Epoch 963
Epoch 987
Epoch 987, Loss: 0.00000012802922, Improvement: 0.00000001162749, Best Loss: 0.00000006193706 in Epoch 963
Epoch 988
Epoch 988, Loss: 0.00000010653756, Improvement: -0.00000002149166, Best Loss: 0.00000006193706 in Epoch 963
Epoch 989
Epoch 989, Loss: 0.00000010503355, Improvement: -0.00000000150401, Best Loss: 0.00000006193706 in Epoch 963
Epoch 990
Epoch 990, Loss: 0.00000010065574, Improvement: -0.00000000437781, Best Loss: 0.00000006193706 in Epoch 963
Epoch 991
A best model at epoch 991 has been saved with training error 0.00000006071895.
Epoch 991, Loss: 0.00000010196221, Improvement: 0.00000000130647, Best Loss: 0.00000006071895 in Epoch 991
Epoch 992
Epoch 992, Loss: 0.00000011132968, Improvement: 0.00000000936748, Best Loss: 0.00000006071895 in Epoch 991
Epoch 993
Epoch 993, Loss: 0.00000012497891, Improvement: 0.00000001364923, Best Loss: 0.00000006071895 in Epoch 991
Epoch 994
Epoch 994, Loss: 0.00000016834662, Improvement: 0.00000004336770, Best Loss: 0.00000006071895 in Epoch 991
Epoch 995
Epoch 995, Loss: 0.00000017594756, Improvement: 0.00000000760094, Best Loss: 0.00000006071895 in Epoch 991
Epoch 996
Epoch 996, Loss: 0.00000020978863, Improvement: 0.00000003384107, Best Loss: 0.00000006071895 in Epoch 991
Epoch 997
Epoch 997, Loss: 0.00000016904256, Improvement: -0.00000004074607, Best Loss: 0.00000006071895 in Epoch 991
Epoch 998
Epoch 998, Loss: 0.00000012350620, Improvement: -0.00000004553635, Best Loss: 0.00000006071895 in Epoch 991
Epoch 999
Epoch 999, Loss: 0.00000011386338, Improvement: -0.00000000964282, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000011362441, Improvement: -0.00000000023897, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1001
Epoch 1001, Loss: 0.00000010603513, Improvement: -0.00000000758928, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1002
Epoch 1002, Loss: 0.00000010665575, Improvement: 0.00000000062062, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1003
Epoch 1003, Loss: 0.00000010066300, Improvement: -0.00000000599275, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1004
Epoch 1004, Loss: 0.00000009682750, Improvement: -0.00000000383550, Best Loss: 0.00000006071895 in Epoch 991
Epoch 1005
A best model at epoch 1005 has been saved with training error 0.00000005700998.
Epoch 1005, Loss: 0.00000009524457, Improvement: -0.00000000158293, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1006
Epoch 1006, Loss: 0.00000009840115, Improvement: 0.00000000315658, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1007
Epoch 1007, Loss: 0.00000011494959, Improvement: 0.00000001654845, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1008
Epoch 1008, Loss: 0.00000013134726, Improvement: 0.00000001639766, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1009
Epoch 1009, Loss: 0.00000011220423, Improvement: -0.00000001914303, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1010
Epoch 1010, Loss: 0.00000009995209, Improvement: -0.00000001225214, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1011
Epoch 1011, Loss: 0.00000009981599, Improvement: -0.00000000013610, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1012
Epoch 1012, Loss: 0.00000012030633, Improvement: 0.00000002049034, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1013
Epoch 1013, Loss: 0.00000012951985, Improvement: 0.00000000921352, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1014
Epoch 1014, Loss: 0.00000018480258, Improvement: 0.00000005528273, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1015
Epoch 1015, Loss: 0.00000031069575, Improvement: 0.00000012589317, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1016
Epoch 1016, Loss: 0.00000024827911, Improvement: -0.00000006241664, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1017
Epoch 1017, Loss: 0.00000015410271, Improvement: -0.00000009417640, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1018
Epoch 1018, Loss: 0.00000011093760, Improvement: -0.00000004316511, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1019
Epoch 1019, Loss: 0.00000010467091, Improvement: -0.00000000626669, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1020
Epoch 1020, Loss: 0.00000010084535, Improvement: -0.00000000382556, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1021
Epoch 1021, Loss: 0.00000009883585, Improvement: -0.00000000200950, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1022
Epoch 1022, Loss: 0.00000009872448, Improvement: -0.00000000011137, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1023
Epoch 1023, Loss: 0.00000009616650, Improvement: -0.00000000255798, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1024
Epoch 1024, Loss: 0.00000009512829, Improvement: -0.00000000103820, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1025
Epoch 1025, Loss: 0.00000009491426, Improvement: -0.00000000021403, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1026
Epoch 1026, Loss: 0.00000010278439, Improvement: 0.00000000787013, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1027
Epoch 1027, Loss: 0.00000010086299, Improvement: -0.00000000192141, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1028
Epoch 1028, Loss: 0.00000010234397, Improvement: 0.00000000148098, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1029
Epoch 1029, Loss: 0.00000010219258, Improvement: -0.00000000015139, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1030
Epoch 1030, Loss: 0.00000012127867, Improvement: 0.00000001908609, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1031
Epoch 1031, Loss: 0.00000012091413, Improvement: -0.00000000036454, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1032
Epoch 1032, Loss: 0.00000010832652, Improvement: -0.00000001258760, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1033
Epoch 1033, Loss: 0.00000009882537, Improvement: -0.00000000950115, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1034
Epoch 1034, Loss: 0.00000010438660, Improvement: 0.00000000556123, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1035
Epoch 1035, Loss: 0.00000012537923, Improvement: 0.00000002099262, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1036
Epoch 1036, Loss: 0.00000020294693, Improvement: 0.00000007756770, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1037
Epoch 1037, Loss: 0.00000024113217, Improvement: 0.00000003818524, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1038
Epoch 1038, Loss: 0.00000014090214, Improvement: -0.00000010023003, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1039
Epoch 1039, Loss: 0.00000013729044, Improvement: -0.00000000361170, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1040
Epoch 1040, Loss: 0.00000012847894, Improvement: -0.00000000881150, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1041
Epoch 1041, Loss: 0.00000011652034, Improvement: -0.00000001195860, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1042
Epoch 1042, Loss: 0.00000010696554, Improvement: -0.00000000955480, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1043
Epoch 1043, Loss: 0.00000009660977, Improvement: -0.00000001035576, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1044
Epoch 1044, Loss: 0.00000009351306, Improvement: -0.00000000309671, Best Loss: 0.00000005700998 in Epoch 1005
Epoch 1045
A best model at epoch 1045 has been saved with training error 0.00000005640802.
Epoch 1045, Loss: 0.00000009392151, Improvement: 0.00000000040844, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1046
Epoch 1046, Loss: 0.00000009227565, Improvement: -0.00000000164585, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1047
Epoch 1047, Loss: 0.00000009437922, Improvement: 0.00000000210357, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1048
Epoch 1048, Loss: 0.00000009084461, Improvement: -0.00000000353461, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1049
Epoch 1049, Loss: 0.00000009232368, Improvement: 0.00000000147907, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000009634107, Improvement: 0.00000000401739, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1051
Epoch 1051, Loss: 0.00000009120928, Improvement: -0.00000000513179, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1052
Epoch 1052, Loss: 0.00000009410762, Improvement: 0.00000000289833, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1053
Epoch 1053, Loss: 0.00000010478209, Improvement: 0.00000001067447, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1054
Epoch 1054, Loss: 0.00000010469465, Improvement: -0.00000000008743, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1055
Epoch 1055, Loss: 0.00000011022051, Improvement: 0.00000000552585, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1056
Epoch 1056, Loss: 0.00000015129946, Improvement: 0.00000004107896, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1057
Epoch 1057, Loss: 0.00000011476387, Improvement: -0.00000003653559, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1058
Epoch 1058, Loss: 0.00000010301164, Improvement: -0.00000001175223, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1059
Epoch 1059, Loss: 0.00000011064308, Improvement: 0.00000000763144, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1060
Epoch 1060, Loss: 0.00000009646123, Improvement: -0.00000001418186, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1061
Epoch 1061, Loss: 0.00000009959325, Improvement: 0.00000000313202, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1062
Epoch 1062, Loss: 0.00000011348571, Improvement: 0.00000001389247, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1063
Epoch 1063, Loss: 0.00000011851358, Improvement: 0.00000000502787, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1064
Epoch 1064, Loss: 0.00000012422360, Improvement: 0.00000000571001, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1065
Epoch 1065, Loss: 0.00000012371652, Improvement: -0.00000000050708, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1066
Epoch 1066, Loss: 0.00000010510614, Improvement: -0.00000001861038, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1067
Epoch 1067, Loss: 0.00000009174869, Improvement: -0.00000001335745, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1068
Epoch 1068, Loss: 0.00000008906573, Improvement: -0.00000000268296, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1069
Epoch 1069, Loss: 0.00000009674014, Improvement: 0.00000000767441, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1070
Epoch 1070, Loss: 0.00000012311108, Improvement: 0.00000002637095, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1071
Epoch 1071, Loss: 0.00000019983564, Improvement: 0.00000007672456, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1072
Epoch 1072, Loss: 0.00000024894306, Improvement: 0.00000004910741, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1073
Epoch 1073, Loss: 0.00000027294734, Improvement: 0.00000002400429, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1074
Epoch 1074, Loss: 0.00000015908872, Improvement: -0.00000011385863, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1075
Epoch 1075, Loss: 0.00000010980218, Improvement: -0.00000004928654, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1076
Epoch 1076, Loss: 0.00000009943109, Improvement: -0.00000001037109, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1077
Epoch 1077, Loss: 0.00000009483700, Improvement: -0.00000000459410, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1078
Epoch 1078, Loss: 0.00000009178023, Improvement: -0.00000000305677, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1079
Epoch 1079, Loss: 0.00000009028561, Improvement: -0.00000000149462, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1080
Epoch 1080, Loss: 0.00000008833617, Improvement: -0.00000000194944, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1081
Epoch 1081, Loss: 0.00000008868325, Improvement: 0.00000000034708, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1082
Epoch 1082, Loss: 0.00000009230212, Improvement: 0.00000000361888, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1083
Epoch 1083, Loss: 0.00000009802839, Improvement: 0.00000000572626, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1084
Epoch 1084, Loss: 0.00000009550942, Improvement: -0.00000000251896, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1085
Epoch 1085, Loss: 0.00000008862469, Improvement: -0.00000000688473, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1086
Epoch 1086, Loss: 0.00000008526305, Improvement: -0.00000000336164, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1087
Epoch 1087, Loss: 0.00000008568815, Improvement: 0.00000000042509, Best Loss: 0.00000005640802 in Epoch 1045
Epoch 1088
A best model at epoch 1088 has been saved with training error 0.00000005047894.
Epoch 1088, Loss: 0.00000008492343, Improvement: -0.00000000076472, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1089
Epoch 1089, Loss: 0.00000008718141, Improvement: 0.00000000225799, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1090
Epoch 1090, Loss: 0.00000010033614, Improvement: 0.00000001315472, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1091
Epoch 1091, Loss: 0.00000008974242, Improvement: -0.00000001059372, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1092
Epoch 1092, Loss: 0.00000008637951, Improvement: -0.00000000336291, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1093
Epoch 1093, Loss: 0.00000009208279, Improvement: 0.00000000570329, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1094
Epoch 1094, Loss: 0.00000015537135, Improvement: 0.00000006328856, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1095
Epoch 1095, Loss: 0.00000015959472, Improvement: 0.00000000422337, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1096
Epoch 1096, Loss: 0.00000017677088, Improvement: 0.00000001717616, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1097
Epoch 1097, Loss: 0.00000014754210, Improvement: -0.00000002922878, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1098
Epoch 1098, Loss: 0.00000010847956, Improvement: -0.00000003906254, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1099
Epoch 1099, Loss: 0.00000010524199, Improvement: -0.00000000323757, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000009264093, Improvement: -0.00000001260105, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1101
Epoch 1101, Loss: 0.00000008546263, Improvement: -0.00000000717830, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1102
Epoch 1102, Loss: 0.00000008342984, Improvement: -0.00000000203279, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1103
Epoch 1103, Loss: 0.00000008245662, Improvement: -0.00000000097322, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1104
Epoch 1104, Loss: 0.00000008224937, Improvement: -0.00000000020725, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1105
Epoch 1105, Loss: 0.00000008944464, Improvement: 0.00000000719527, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1106
Epoch 1106, Loss: 0.00000009679292, Improvement: 0.00000000734828, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1107
Epoch 1107, Loss: 0.00000009175638, Improvement: -0.00000000503654, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1108
Epoch 1108, Loss: 0.00000009321305, Improvement: 0.00000000145666, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1109
Epoch 1109, Loss: 0.00000011336429, Improvement: 0.00000002015125, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1110
Epoch 1110, Loss: 0.00000010493904, Improvement: -0.00000000842525, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1111
Epoch 1111, Loss: 0.00000011565336, Improvement: 0.00000001071431, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1112
Epoch 1112, Loss: 0.00000012300080, Improvement: 0.00000000734744, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1113
Epoch 1113, Loss: 0.00000012649173, Improvement: 0.00000000349093, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1114
Epoch 1114, Loss: 0.00000017003308, Improvement: 0.00000004354135, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1115
Epoch 1115, Loss: 0.00000013353161, Improvement: -0.00000003650147, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1116
Epoch 1116, Loss: 0.00000009608605, Improvement: -0.00000003744555, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1117
Epoch 1117, Loss: 0.00000009884586, Improvement: 0.00000000275981, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1118
Epoch 1118, Loss: 0.00000009568109, Improvement: -0.00000000316477, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1119
Epoch 1119, Loss: 0.00000008722365, Improvement: -0.00000000845745, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1120
Epoch 1120, Loss: 0.00000008490616, Improvement: -0.00000000231748, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1121
Epoch 1121, Loss: 0.00000009030569, Improvement: 0.00000000539953, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1122
Epoch 1122, Loss: 0.00000008867285, Improvement: -0.00000000163284, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1123
Epoch 1123, Loss: 0.00000011234592, Improvement: 0.00000002367308, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1124
Epoch 1124, Loss: 0.00000008971344, Improvement: -0.00000002263248, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1125
Epoch 1125, Loss: 0.00000013444516, Improvement: 0.00000004473172, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1126
Epoch 1126, Loss: 0.00000026171127, Improvement: 0.00000012726611, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1127
Epoch 1127, Loss: 0.00000022372685, Improvement: -0.00000003798442, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1128
Epoch 1128, Loss: 0.00000012819319, Improvement: -0.00000009553366, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1129
Epoch 1129, Loss: 0.00000009387807, Improvement: -0.00000003431512, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1130
Epoch 1130, Loss: 0.00000008739278, Improvement: -0.00000000648529, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1131
Epoch 1131, Loss: 0.00000008340245, Improvement: -0.00000000399034, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1132
Epoch 1132, Loss: 0.00000008150429, Improvement: -0.00000000189816, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1133
Epoch 1133, Loss: 0.00000008023979, Improvement: -0.00000000126450, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1134
Epoch 1134, Loss: 0.00000008176927, Improvement: 0.00000000152948, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1135
Epoch 1135, Loss: 0.00000008111655, Improvement: -0.00000000065272, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1136
Epoch 1136, Loss: 0.00000008111326, Improvement: -0.00000000000329, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1137
Epoch 1137, Loss: 0.00000007741363, Improvement: -0.00000000369963, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1138
Epoch 1138, Loss: 0.00000007726045, Improvement: -0.00000000015318, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1139
Epoch 1139, Loss: 0.00000007796456, Improvement: 0.00000000070411, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1140
Epoch 1140, Loss: 0.00000008427319, Improvement: 0.00000000630862, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1141
Epoch 1141, Loss: 0.00000008518150, Improvement: 0.00000000090832, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1142
Epoch 1142, Loss: 0.00000009696915, Improvement: 0.00000001178765, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1143
Epoch 1143, Loss: 0.00000012138959, Improvement: 0.00000002442044, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1144
Epoch 1144, Loss: 0.00000011276548, Improvement: -0.00000000862411, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1145
Epoch 1145, Loss: 0.00000009996347, Improvement: -0.00000001280201, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1146
Epoch 1146, Loss: 0.00000008755394, Improvement: -0.00000001240953, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1147
Epoch 1147, Loss: 0.00000011345912, Improvement: 0.00000002590518, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1148
Epoch 1148, Loss: 0.00000010261589, Improvement: -0.00000001084323, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1149
Epoch 1149, Loss: 0.00000012049610, Improvement: 0.00000001788021, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000016476205, Improvement: 0.00000004426595, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1151
Epoch 1151, Loss: 0.00000013301768, Improvement: -0.00000003174437, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1152
Epoch 1152, Loss: 0.00000014801987, Improvement: 0.00000001500219, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1153
Epoch 1153, Loss: 0.00000013119273, Improvement: -0.00000001682714, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1154
Epoch 1154, Loss: 0.00000011917434, Improvement: -0.00000001201839, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1155
Epoch 1155, Loss: 0.00000010004231, Improvement: -0.00000001913203, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1156
Epoch 1156, Loss: 0.00000008779316, Improvement: -0.00000001224915, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1157
Epoch 1157, Loss: 0.00000008444266, Improvement: -0.00000000335050, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1158
Epoch 1158, Loss: 0.00000008556755, Improvement: 0.00000000112490, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1159
Epoch 1159, Loss: 0.00000007894536, Improvement: -0.00000000662219, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1160
Epoch 1160, Loss: 0.00000007571328, Improvement: -0.00000000323207, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1161
Epoch 1161, Loss: 0.00000007831996, Improvement: 0.00000000260667, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1162
Epoch 1162, Loss: 0.00000030407453, Improvement: 0.00000022575457, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1163
Epoch 1163, Loss: 0.00000031903336, Improvement: 0.00000001495883, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1164
Epoch 1164, Loss: 0.00000014066810, Improvement: -0.00000017836526, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1165
Epoch 1165, Loss: 0.00000009099586, Improvement: -0.00000004967224, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1166
Epoch 1166, Loss: 0.00000008563154, Improvement: -0.00000000536431, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1167
Epoch 1167, Loss: 0.00000008200931, Improvement: -0.00000000362223, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1168
Epoch 1168, Loss: 0.00000008105052, Improvement: -0.00000000095879, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1169
Epoch 1169, Loss: 0.00000007855755, Improvement: -0.00000000249298, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1170
Epoch 1170, Loss: 0.00000007762338, Improvement: -0.00000000093417, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1171
Epoch 1171, Loss: 0.00000008539135, Improvement: 0.00000000776797, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1172
Epoch 1172, Loss: 0.00000007984865, Improvement: -0.00000000554270, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1173
Epoch 1173, Loss: 0.00000007513140, Improvement: -0.00000000471724, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1174
Epoch 1174, Loss: 0.00000007439975, Improvement: -0.00000000073166, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1175
Epoch 1175, Loss: 0.00000007286569, Improvement: -0.00000000153406, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1176
Epoch 1176, Loss: 0.00000007368495, Improvement: 0.00000000081925, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1177
Epoch 1177, Loss: 0.00000007951521, Improvement: 0.00000000583027, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1178
Epoch 1178, Loss: 0.00000007965874, Improvement: 0.00000000014353, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1179
Epoch 1179, Loss: 0.00000007418058, Improvement: -0.00000000547817, Best Loss: 0.00000005047894 in Epoch 1088
Epoch 1180
A best model at epoch 1180 has been saved with training error 0.00000004987230.
Epoch 1180, Loss: 0.00000007388456, Improvement: -0.00000000029602, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1181
Epoch 1181, Loss: 0.00000007357351, Improvement: -0.00000000031104, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1182
Epoch 1182, Loss: 0.00000007491267, Improvement: 0.00000000133916, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1183
Epoch 1183, Loss: 0.00000009052624, Improvement: 0.00000001561357, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1184
Epoch 1184, Loss: 0.00000008459633, Improvement: -0.00000000592991, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1185
Epoch 1185, Loss: 0.00000007555610, Improvement: -0.00000000904023, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1186
Epoch 1186, Loss: 0.00000007375949, Improvement: -0.00000000179662, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1187
Epoch 1187, Loss: 0.00000007352606, Improvement: -0.00000000023343, Best Loss: 0.00000004987230 in Epoch 1180
Epoch 1188
A best model at epoch 1188 has been saved with training error 0.00000004635625.
Epoch 1188, Loss: 0.00000007014435, Improvement: -0.00000000338170, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1189
Epoch 1189, Loss: 0.00000007327168, Improvement: 0.00000000312732, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1190
Epoch 1190, Loss: 0.00000007046067, Improvement: -0.00000000281101, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1191
Epoch 1191, Loss: 0.00000007125950, Improvement: 0.00000000079883, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1192
Epoch 1192, Loss: 0.00000010028254, Improvement: 0.00000002902304, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1193
Epoch 1193, Loss: 0.00000011246728, Improvement: 0.00000001218474, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1194
Epoch 1194, Loss: 0.00000007930437, Improvement: -0.00000003316291, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1195
Epoch 1195, Loss: 0.00000007298160, Improvement: -0.00000000632277, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1196
Epoch 1196, Loss: 0.00000006862190, Improvement: -0.00000000435970, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1197
Epoch 1197, Loss: 0.00000006643045, Improvement: -0.00000000219145, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1198
Epoch 1198, Loss: 0.00000006859645, Improvement: 0.00000000216600, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1199
Epoch 1199, Loss: 0.00000008028707, Improvement: 0.00000001169062, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000010194541, Improvement: 0.00000002165834, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1201
Epoch 1201, Loss: 0.00000024908183, Improvement: 0.00000014713642, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1202
Epoch 1202, Loss: 0.00000013535955, Improvement: -0.00000011372227, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1203
Epoch 1203, Loss: 0.00000008951018, Improvement: -0.00000004584937, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1204
Epoch 1204, Loss: 0.00000007381450, Improvement: -0.00000001569568, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1205
Epoch 1205, Loss: 0.00000006926609, Improvement: -0.00000000454841, Best Loss: 0.00000004635625 in Epoch 1188
Epoch 1206
A best model at epoch 1206 has been saved with training error 0.00000004554815.
Epoch 1206, Loss: 0.00000006828515, Improvement: -0.00000000098094, Best Loss: 0.00000004554815 in Epoch 1206
Epoch 1207
Epoch 1207, Loss: 0.00000006684858, Improvement: -0.00000000143657, Best Loss: 0.00000004554815 in Epoch 1206
Epoch 1208
Epoch 1208, Loss: 0.00000006496348, Improvement: -0.00000000188511, Best Loss: 0.00000004554815 in Epoch 1206
Epoch 1209
Epoch 1209, Loss: 0.00000006430103, Improvement: -0.00000000066245, Best Loss: 0.00000004554815 in Epoch 1206
Epoch 1210
Epoch 1210, Loss: 0.00000006530094, Improvement: 0.00000000099991, Best Loss: 0.00000004554815 in Epoch 1206
Epoch 1211
A best model at epoch 1211 has been saved with training error 0.00000004287702.
Epoch 1211, Loss: 0.00000006228328, Improvement: -0.00000000301765, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1212
Epoch 1212, Loss: 0.00000006026339, Improvement: -0.00000000201989, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1213
Epoch 1213, Loss: 0.00000006231346, Improvement: 0.00000000205007, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1214
Epoch 1214, Loss: 0.00000010688492, Improvement: 0.00000004457146, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1215
Epoch 1215, Loss: 0.00000012157656, Improvement: 0.00000001469164, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1216
Epoch 1216, Loss: 0.00000009567083, Improvement: -0.00000002590573, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1217
Epoch 1217, Loss: 0.00000010344663, Improvement: 0.00000000777580, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1218
Epoch 1218, Loss: 0.00000013011774, Improvement: 0.00000002667110, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1219
Epoch 1219, Loss: 0.00000008131727, Improvement: -0.00000004880046, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1220
Epoch 1220, Loss: 0.00000006536549, Improvement: -0.00000001595178, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1221
Epoch 1221, Loss: 0.00000006092791, Improvement: -0.00000000443758, Best Loss: 0.00000004287702 in Epoch 1211
Epoch 1222
A best model at epoch 1222 has been saved with training error 0.00000004118922.
A best model at epoch 1222 has been saved with training error 0.00000004028352.
Epoch 1222, Loss: 0.00000005927385, Improvement: -0.00000000165406, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1223
Epoch 1223, Loss: 0.00000005788329, Improvement: -0.00000000139056, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1224
Epoch 1224, Loss: 0.00000005876042, Improvement: 0.00000000087713, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1225
Epoch 1225, Loss: 0.00000006160933, Improvement: 0.00000000284891, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1226
Epoch 1226, Loss: 0.00000005787161, Improvement: -0.00000000373772, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1227
Epoch 1227, Loss: 0.00000005907672, Improvement: 0.00000000120511, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1228
Epoch 1228, Loss: 0.00000010161574, Improvement: 0.00000004253903, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1229
Epoch 1229, Loss: 0.00000022536968, Improvement: 0.00000012375394, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1230
Epoch 1230, Loss: 0.00000028791921, Improvement: 0.00000006254953, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1231
Epoch 1231, Loss: 0.00000021955835, Improvement: -0.00000006836086, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1232
Epoch 1232, Loss: 0.00000012838196, Improvement: -0.00000009117639, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1233
Epoch 1233, Loss: 0.00000010809582, Improvement: -0.00000002028613, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1234
Epoch 1234, Loss: 0.00000007458486, Improvement: -0.00000003351097, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1235
Epoch 1235, Loss: 0.00000007453007, Improvement: -0.00000000005478, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1236
Epoch 1236, Loss: 0.00000006391616, Improvement: -0.00000001061392, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1237
Epoch 1237, Loss: 0.00000005883437, Improvement: -0.00000000508179, Best Loss: 0.00000004028352 in Epoch 1222
Epoch 1238
A best model at epoch 1238 has been saved with training error 0.00000003655026.
Epoch 1238, Loss: 0.00000005769183, Improvement: -0.00000000114254, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1239
Epoch 1239, Loss: 0.00000005707529, Improvement: -0.00000000061653, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1240
Epoch 1240, Loss: 0.00000005802080, Improvement: 0.00000000094551, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1241
Epoch 1241, Loss: 0.00000005527417, Improvement: -0.00000000274663, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1242
Epoch 1242, Loss: 0.00000005629548, Improvement: 0.00000000102132, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1243
Epoch 1243, Loss: 0.00000006266836, Improvement: 0.00000000637288, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1244
Epoch 1244, Loss: 0.00000010052810, Improvement: 0.00000003785974, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1245
Epoch 1245, Loss: 0.00000014210924, Improvement: 0.00000004158114, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1246
Epoch 1246, Loss: 0.00000010376659, Improvement: -0.00000003834265, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1247
Epoch 1247, Loss: 0.00000006999999, Improvement: -0.00000003376659, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1248
Epoch 1248, Loss: 0.00000005849363, Improvement: -0.00000001150636, Best Loss: 0.00000003655026 in Epoch 1238
Epoch 1249
A best model at epoch 1249 has been saved with training error 0.00000003635089.
Epoch 1249, Loss: 0.00000005278523, Improvement: -0.00000000570841, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000005094091, Improvement: -0.00000000184431, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1251
Epoch 1251, Loss: 0.00000005026507, Improvement: -0.00000000067584, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1252
Epoch 1252, Loss: 0.00000005723890, Improvement: 0.00000000697382, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1253
Epoch 1253, Loss: 0.00000007231537, Improvement: 0.00000001507647, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1254
Epoch 1254, Loss: 0.00000010420554, Improvement: 0.00000003189017, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1255
Epoch 1255, Loss: 0.00000033902304, Improvement: 0.00000023481750, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1256
Epoch 1256, Loss: 0.00000017052062, Improvement: -0.00000016850242, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1257
Epoch 1257, Loss: 0.00000009090534, Improvement: -0.00000007961528, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1258
Epoch 1258, Loss: 0.00000007074236, Improvement: -0.00000002016299, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1259
Epoch 1259, Loss: 0.00000006048174, Improvement: -0.00000001026062, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1260
Epoch 1260, Loss: 0.00000005699906, Improvement: -0.00000000348268, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1261
Epoch 1261, Loss: 0.00000005406353, Improvement: -0.00000000293553, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1262
Epoch 1262, Loss: 0.00000005213969, Improvement: -0.00000000192385, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1263
Epoch 1263, Loss: 0.00000005339979, Improvement: 0.00000000126010, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1264
Epoch 1264, Loss: 0.00000005162079, Improvement: -0.00000000177900, Best Loss: 0.00000003635089 in Epoch 1249
Epoch 1265
A best model at epoch 1265 has been saved with training error 0.00000003562925.
Epoch 1265, Loss: 0.00000005040282, Improvement: -0.00000000121797, Best Loss: 0.00000003562925 in Epoch 1265
Epoch 1266
Epoch 1266, Loss: 0.00000004940045, Improvement: -0.00000000100237, Best Loss: 0.00000003562925 in Epoch 1265
Epoch 1267
Epoch 1267, Loss: 0.00000004573426, Improvement: -0.00000000366619, Best Loss: 0.00000003562925 in Epoch 1265
Epoch 1268
A best model at epoch 1268 has been saved with training error 0.00000003524026.
Epoch 1268, Loss: 0.00000004515363, Improvement: -0.00000000058063, Best Loss: 0.00000003524026 in Epoch 1268
Epoch 1269
A best model at epoch 1269 has been saved with training error 0.00000003509523.
A best model at epoch 1269 has been saved with training error 0.00000003506043.
Epoch 1269, Loss: 0.00000004724682, Improvement: 0.00000000209319, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1270
Epoch 1270, Loss: 0.00000004958002, Improvement: 0.00000000233321, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1271
Epoch 1271, Loss: 0.00000005894014, Improvement: 0.00000000936012, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1272
Epoch 1272, Loss: 0.00000007759400, Improvement: 0.00000001865385, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1273
Epoch 1273, Loss: 0.00000007250001, Improvement: -0.00000000509398, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1274
Epoch 1274, Loss: 0.00000011301725, Improvement: 0.00000004051724, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1275
Epoch 1275, Loss: 0.00000008725408, Improvement: -0.00000002576318, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1276
Epoch 1276, Loss: 0.00000007708977, Improvement: -0.00000001016431, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1277
Epoch 1277, Loss: 0.00000010787852, Improvement: 0.00000003078875, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1278
Epoch 1278, Loss: 0.00000008932933, Improvement: -0.00000001854919, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1279
Epoch 1279, Loss: 0.00000010365090, Improvement: 0.00000001432156, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1280
Epoch 1280, Loss: 0.00000008154960, Improvement: -0.00000002210130, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1281
Epoch 1281, Loss: 0.00000007437509, Improvement: -0.00000000717451, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1282
Epoch 1282, Loss: 0.00000006506184, Improvement: -0.00000000931325, Best Loss: 0.00000003506043 in Epoch 1269
Epoch 1283
A best model at epoch 1283 has been saved with training error 0.00000003476986.
Epoch 1283, Loss: 0.00000005028312, Improvement: -0.00000001477872, Best Loss: 0.00000003476986 in Epoch 1283
Epoch 1284
Epoch 1284, Loss: 0.00000004555739, Improvement: -0.00000000472573, Best Loss: 0.00000003476986 in Epoch 1283
Epoch 1285
Epoch 1285, Loss: 0.00000004379440, Improvement: -0.00000000176298, Best Loss: 0.00000003476986 in Epoch 1283
Epoch 1286
A best model at epoch 1286 has been saved with training error 0.00000003415268.
Epoch 1286, Loss: 0.00000005364251, Improvement: 0.00000000984811, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1287
Epoch 1287, Loss: 0.00000006232162, Improvement: 0.00000000867911, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1288
Epoch 1288, Loss: 0.00000005525234, Improvement: -0.00000000706927, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1289
Epoch 1289, Loss: 0.00000009235656, Improvement: 0.00000003710422, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1290
Epoch 1290, Loss: 0.00000011320552, Improvement: 0.00000002084895, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1291
Epoch 1291, Loss: 0.00000009969788, Improvement: -0.00000001350763, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1292
Epoch 1292, Loss: 0.00000006876482, Improvement: -0.00000003093307, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1293
Epoch 1293, Loss: 0.00000006094107, Improvement: -0.00000000782374, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1294
Epoch 1294, Loss: 0.00000005480783, Improvement: -0.00000000613325, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1295
Epoch 1295, Loss: 0.00000004468351, Improvement: -0.00000001012432, Best Loss: 0.00000003415268 in Epoch 1286
Epoch 1296
A best model at epoch 1296 has been saved with training error 0.00000002972967.
Epoch 1296, Loss: 0.00000005936649, Improvement: 0.00000001468298, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1297
Epoch 1297, Loss: 0.00000007143033, Improvement: 0.00000001206384, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1298
Epoch 1298, Loss: 0.00000005085509, Improvement: -0.00000002057525, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1299
Epoch 1299, Loss: 0.00000005284744, Improvement: 0.00000000199236, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000005101746, Improvement: -0.00000000182998, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1301
Epoch 1301, Loss: 0.00000004463515, Improvement: -0.00000000638232, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1302
Epoch 1302, Loss: 0.00000004380378, Improvement: -0.00000000083136, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1303
Epoch 1303, Loss: 0.00000005695841, Improvement: 0.00000001315462, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1304
Epoch 1304, Loss: 0.00000006577689, Improvement: 0.00000000881848, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1305
Epoch 1305, Loss: 0.00000012351650, Improvement: 0.00000005773961, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1306
Epoch 1306, Loss: 0.00000006029586, Improvement: -0.00000006322064, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1307
Epoch 1307, Loss: 0.00000007364002, Improvement: 0.00000001334416, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1308
Epoch 1308, Loss: 0.00000006285022, Improvement: -0.00000001078980, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1309
Epoch 1309, Loss: 0.00000004519263, Improvement: -0.00000001765759, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1310
Epoch 1310, Loss: 0.00000005039968, Improvement: 0.00000000520705, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1311
Epoch 1311, Loss: 0.00000006123486, Improvement: 0.00000001083518, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1312
Epoch 1312, Loss: 0.00000006389261, Improvement: 0.00000000265775, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1313
Epoch 1313, Loss: 0.00000006218368, Improvement: -0.00000000170893, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1314
Epoch 1314, Loss: 0.00000005805296, Improvement: -0.00000000413073, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1315
Epoch 1315, Loss: 0.00000006918233, Improvement: 0.00000001112937, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1316
Epoch 1316, Loss: 0.00000006847604, Improvement: -0.00000000070629, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1317
Epoch 1317, Loss: 0.00000010826070, Improvement: 0.00000003978466, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1318
Epoch 1318, Loss: 0.00000010687748, Improvement: -0.00000000138322, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1319
Epoch 1319, Loss: 0.00000006667157, Improvement: -0.00000004020592, Best Loss: 0.00000002972967 in Epoch 1296
Epoch 1320
A best model at epoch 1320 has been saved with training error 0.00000002756402.
Epoch 1320, Loss: 0.00000004231645, Improvement: -0.00000002435511, Best Loss: 0.00000002756402 in Epoch 1320
Epoch 1321
A best model at epoch 1321 has been saved with training error 0.00000002719246.
Epoch 1321, Loss: 0.00000003717759, Improvement: -0.00000000513887, Best Loss: 0.00000002719246 in Epoch 1321
Epoch 1322
A best model at epoch 1322 has been saved with training error 0.00000002718878.
Epoch 1322, Loss: 0.00000003620351, Improvement: -0.00000000097408, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1323
Epoch 1323, Loss: 0.00000003823050, Improvement: 0.00000000202699, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1324
Epoch 1324, Loss: 0.00000004476344, Improvement: 0.00000000653294, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1325
Epoch 1325, Loss: 0.00000006168578, Improvement: 0.00000001692234, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1326
Epoch 1326, Loss: 0.00000005139043, Improvement: -0.00000001029535, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1327
Epoch 1327, Loss: 0.00000014655435, Improvement: 0.00000009516392, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1328
Epoch 1328, Loss: 0.00000017741512, Improvement: 0.00000003086077, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1329
Epoch 1329, Loss: 0.00000017073258, Improvement: -0.00000000668254, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1330
Epoch 1330, Loss: 0.00000012998174, Improvement: -0.00000004075084, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1331
Epoch 1331, Loss: 0.00000007621422, Improvement: -0.00000005376752, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1332
Epoch 1332, Loss: 0.00000005219679, Improvement: -0.00000002401743, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1333
Epoch 1333, Loss: 0.00000004409547, Improvement: -0.00000000810133, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1334
Epoch 1334, Loss: 0.00000003858447, Improvement: -0.00000000551099, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1335
Epoch 1335, Loss: 0.00000003658349, Improvement: -0.00000000200099, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1336
Epoch 1336, Loss: 0.00000004030946, Improvement: 0.00000000372598, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1337
Epoch 1337, Loss: 0.00000006987532, Improvement: 0.00000002956585, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1338
Epoch 1338, Loss: 0.00000006977110, Improvement: -0.00000000010422, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1339
Epoch 1339, Loss: 0.00000004356042, Improvement: -0.00000002621067, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1340
Epoch 1340, Loss: 0.00000003729318, Improvement: -0.00000000626725, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1341
Epoch 1341, Loss: 0.00000004053720, Improvement: 0.00000000324403, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1342
Epoch 1342, Loss: 0.00000006197850, Improvement: 0.00000002144130, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1343
Epoch 1343, Loss: 0.00000006997364, Improvement: 0.00000000799514, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1344
Epoch 1344, Loss: 0.00000006748463, Improvement: -0.00000000248901, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1345
Epoch 1345, Loss: 0.00000007933342, Improvement: 0.00000001184879, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1346
Epoch 1346, Loss: 0.00000006226780, Improvement: -0.00000001706563, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1347
Epoch 1347, Loss: 0.00000030674895, Improvement: 0.00000024448116, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1348
Epoch 1348, Loss: 0.00000018589107, Improvement: -0.00000012085788, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1349
Epoch 1349, Loss: 0.00000007515474, Improvement: -0.00000011073633, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000005351364, Improvement: -0.00000002164111, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1351
Epoch 1351, Loss: 0.00000004507593, Improvement: -0.00000000843771, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1352
Epoch 1352, Loss: 0.00000004003081, Improvement: -0.00000000504512, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1353
Epoch 1353, Loss: 0.00000003815115, Improvement: -0.00000000187966, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1354
Epoch 1354, Loss: 0.00000003635009, Improvement: -0.00000000180106, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1355
Epoch 1355, Loss: 0.00000003525719, Improvement: -0.00000000109289, Best Loss: 0.00000002718878 in Epoch 1322
Epoch 1356
A best model at epoch 1356 has been saved with training error 0.00000002702018.
A best model at epoch 1356 has been saved with training error 0.00000002595510.
Epoch 1356, Loss: 0.00000003340975, Improvement: -0.00000000184744, Best Loss: 0.00000002595510 in Epoch 1356
Epoch 1357
Epoch 1357, Loss: 0.00000003354401, Improvement: 0.00000000013426, Best Loss: 0.00000002595510 in Epoch 1356
Epoch 1358
Epoch 1358, Loss: 0.00000003281925, Improvement: -0.00000000072477, Best Loss: 0.00000002595510 in Epoch 1356
Epoch 1359
A best model at epoch 1359 has been saved with training error 0.00000002371117.
Epoch 1359, Loss: 0.00000003250026, Improvement: -0.00000000031899, Best Loss: 0.00000002371117 in Epoch 1359
Epoch 1360
Epoch 1360, Loss: 0.00000003145010, Improvement: -0.00000000105016, Best Loss: 0.00000002371117 in Epoch 1359
Epoch 1361
A best model at epoch 1361 has been saved with training error 0.00000002253190.
Epoch 1361, Loss: 0.00000003325711, Improvement: 0.00000000180701, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1362
Epoch 1362, Loss: 0.00000003595386, Improvement: 0.00000000269676, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1363
Epoch 1363, Loss: 0.00000004302722, Improvement: 0.00000000707336, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1364
Epoch 1364, Loss: 0.00000004337852, Improvement: 0.00000000035130, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1365
Epoch 1365, Loss: 0.00000005057443, Improvement: 0.00000000719591, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1366
Epoch 1366, Loss: 0.00000005603980, Improvement: 0.00000000546537, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1367
Epoch 1367, Loss: 0.00000007033187, Improvement: 0.00000001429208, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1368
Epoch 1368, Loss: 0.00000004971410, Improvement: -0.00000002061778, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1369
Epoch 1369, Loss: 0.00000004802670, Improvement: -0.00000000168740, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1370
Epoch 1370, Loss: 0.00000004227412, Improvement: -0.00000000575258, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1371
Epoch 1371, Loss: 0.00000004059487, Improvement: -0.00000000167925, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1372
Epoch 1372, Loss: 0.00000004813602, Improvement: 0.00000000754116, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1373
Epoch 1373, Loss: 0.00000004860864, Improvement: 0.00000000047262, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1374
Epoch 1374, Loss: 0.00000004569983, Improvement: -0.00000000290881, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1375
Epoch 1375, Loss: 0.00000004037282, Improvement: -0.00000000532701, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1376
Epoch 1376, Loss: 0.00000003537630, Improvement: -0.00000000499652, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1377
Epoch 1377, Loss: 0.00000003143297, Improvement: -0.00000000394334, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1378
Epoch 1378, Loss: 0.00000007046023, Improvement: 0.00000003902726, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1379
Epoch 1379, Loss: 0.00000006813959, Improvement: -0.00000000232064, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1380
Epoch 1380, Loss: 0.00000005198602, Improvement: -0.00000001615356, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1381
Epoch 1381, Loss: 0.00000004898189, Improvement: -0.00000000300413, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1382
Epoch 1382, Loss: 0.00000008361249, Improvement: 0.00000003463060, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1383
Epoch 1383, Loss: 0.00000016283898, Improvement: 0.00000007922650, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1384
Epoch 1384, Loss: 0.00000015891312, Improvement: -0.00000000392586, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1385
Epoch 1385, Loss: 0.00000018023404, Improvement: 0.00000002132092, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1386
Epoch 1386, Loss: 0.00000020937378, Improvement: 0.00000002913974, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1387
Epoch 1387, Loss: 0.00000010144134, Improvement: -0.00000010793244, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1388
Epoch 1388, Loss: 0.00000005324122, Improvement: -0.00000004820013, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1389
Epoch 1389, Loss: 0.00000004552453, Improvement: -0.00000000771669, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1390
Epoch 1390, Loss: 0.00000004021443, Improvement: -0.00000000531010, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1391
Epoch 1391, Loss: 0.00000003668540, Improvement: -0.00000000352903, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1392
Epoch 1392, Loss: 0.00000003730203, Improvement: 0.00000000061663, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1393
Epoch 1393, Loss: 0.00000003876812, Improvement: 0.00000000146609, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1394
Epoch 1394, Loss: 0.00000003476756, Improvement: -0.00000000400055, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1395
Epoch 1395, Loss: 0.00000003753594, Improvement: 0.00000000276838, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1396
Epoch 1396, Loss: 0.00000005202923, Improvement: 0.00000001449329, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1397
Epoch 1397, Loss: 0.00000015304157, Improvement: 0.00000010101234, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1398
Epoch 1398, Loss: 0.00000015027200, Improvement: -0.00000000276958, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1399
Epoch 1399, Loss: 0.00000008794039, Improvement: -0.00000006233161, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000004316203, Improvement: -0.00000004477836, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1401
Epoch 1401, Loss: 0.00000003690216, Improvement: -0.00000000625987, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1402
Epoch 1402, Loss: 0.00000003343363, Improvement: -0.00000000346852, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1403
Epoch 1403, Loss: 0.00000003230844, Improvement: -0.00000000112519, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1404
Epoch 1404, Loss: 0.00000003104474, Improvement: -0.00000000126370, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1405
Epoch 1405, Loss: 0.00000003060995, Improvement: -0.00000000043479, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1406
Epoch 1406, Loss: 0.00000003507287, Improvement: 0.00000000446292, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1407
Epoch 1407, Loss: 0.00000004461527, Improvement: 0.00000000954240, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1408
Epoch 1408, Loss: 0.00000005019817, Improvement: 0.00000000558290, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1409
Epoch 1409, Loss: 0.00000005151210, Improvement: 0.00000000131393, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1410
Epoch 1410, Loss: 0.00000007200610, Improvement: 0.00000002049400, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1411
Epoch 1411, Loss: 0.00000012474025, Improvement: 0.00000005273415, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1412
Epoch 1412, Loss: 0.00000010300873, Improvement: -0.00000002173152, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1413
Epoch 1413, Loss: 0.00000005414201, Improvement: -0.00000004886672, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1414
Epoch 1414, Loss: 0.00000003931190, Improvement: -0.00000001483011, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1415
Epoch 1415, Loss: 0.00000003379625, Improvement: -0.00000000551566, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1416
Epoch 1416, Loss: 0.00000003255317, Improvement: -0.00000000124308, Best Loss: 0.00000002253190 in Epoch 1361
Epoch 1417
A best model at epoch 1417 has been saved with training error 0.00000002114881.
Epoch 1417, Loss: 0.00000003086104, Improvement: -0.00000000169213, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1418
Epoch 1418, Loss: 0.00000003152891, Improvement: 0.00000000066787, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1419
Epoch 1419, Loss: 0.00000005213317, Improvement: 0.00000002060426, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1420
Epoch 1420, Loss: 0.00000013658425, Improvement: 0.00000008445108, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1421
Epoch 1421, Loss: 0.00000010899226, Improvement: -0.00000002759199, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1422
Epoch 1422, Loss: 0.00000006465214, Improvement: -0.00000004434013, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1423
Epoch 1423, Loss: 0.00000005766146, Improvement: -0.00000000699068, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1424
Epoch 1424, Loss: 0.00000004636239, Improvement: -0.00000001129907, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1425
Epoch 1425, Loss: 0.00000004738093, Improvement: 0.00000000101854, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1426
Epoch 1426, Loss: 0.00000007573199, Improvement: 0.00000002835106, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1427
Epoch 1427, Loss: 0.00000008704162, Improvement: 0.00000001130963, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1428
Epoch 1428, Loss: 0.00000022280280, Improvement: 0.00000013576118, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1429
Epoch 1429, Loss: 0.00000018217264, Improvement: -0.00000004063017, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1430
Epoch 1430, Loss: 0.00000018315342, Improvement: 0.00000000098078, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1431
Epoch 1431, Loss: 0.00000008992594, Improvement: -0.00000009322748, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1432
Epoch 1432, Loss: 0.00000005194938, Improvement: -0.00000003797657, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1433
Epoch 1433, Loss: 0.00000004065291, Improvement: -0.00000001129647, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1434
Epoch 1434, Loss: 0.00000003671713, Improvement: -0.00000000393578, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1435
Epoch 1435, Loss: 0.00000003460504, Improvement: -0.00000000211209, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1436
Epoch 1436, Loss: 0.00000003375820, Improvement: -0.00000000084684, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1437
Epoch 1437, Loss: 0.00000003232631, Improvement: -0.00000000143189, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1438
Epoch 1438, Loss: 0.00000003274375, Improvement: 0.00000000041744, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1439
Epoch 1439, Loss: 0.00000003517754, Improvement: 0.00000000243379, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1440
Epoch 1440, Loss: 0.00000003711625, Improvement: 0.00000000193870, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1441
Epoch 1441, Loss: 0.00000003887036, Improvement: 0.00000000175412, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1442
Epoch 1442, Loss: 0.00000003900140, Improvement: 0.00000000013103, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1443
Epoch 1443, Loss: 0.00000004100482, Improvement: 0.00000000200342, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1444
Epoch 1444, Loss: 0.00000005675565, Improvement: 0.00000001575083, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1445
Epoch 1445, Loss: 0.00000014604235, Improvement: 0.00000008928670, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1446
Epoch 1446, Loss: 0.00000009923661, Improvement: -0.00000004680574, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1447
Epoch 1447, Loss: 0.00000006741789, Improvement: -0.00000003181872, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1448
Epoch 1448, Loss: 0.00000005477439, Improvement: -0.00000001264350, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1449
Epoch 1449, Loss: 0.00000003467190, Improvement: -0.00000002010249, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000003307878, Improvement: -0.00000000159312, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1451
Epoch 1451, Loss: 0.00000003353079, Improvement: 0.00000000045202, Best Loss: 0.00000002114881 in Epoch 1417
Epoch 1452
A best model at epoch 1452 has been saved with training error 0.00000002053062.
Epoch 1452, Loss: 0.00000002959324, Improvement: -0.00000000393755, Best Loss: 0.00000002053062 in Epoch 1452
Epoch 1453
A best model at epoch 1453 has been saved with training error 0.00000002045017.
Epoch 1453, Loss: 0.00000002919525, Improvement: -0.00000000039799, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1454
Epoch 1454, Loss: 0.00000002886709, Improvement: -0.00000000032816, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1455
Epoch 1455, Loss: 0.00000004622877, Improvement: 0.00000001736168, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1456
Epoch 1456, Loss: 0.00000004933192, Improvement: 0.00000000310316, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1457
Epoch 1457, Loss: 0.00000006496139, Improvement: 0.00000001562947, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1458
Epoch 1458, Loss: 0.00000004447666, Improvement: -0.00000002048474, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1459
Epoch 1459, Loss: 0.00000004355602, Improvement: -0.00000000092064, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1460
Epoch 1460, Loss: 0.00000004434489, Improvement: 0.00000000078887, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1461
Epoch 1461, Loss: 0.00000006279925, Improvement: 0.00000001845436, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1462
Epoch 1462, Loss: 0.00000006610224, Improvement: 0.00000000330299, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1463
Epoch 1463, Loss: 0.00000005437175, Improvement: -0.00000001173049, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1464
Epoch 1464, Loss: 0.00000006828925, Improvement: 0.00000001391750, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1465
Epoch 1465, Loss: 0.00000005377083, Improvement: -0.00000001451842, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1466
Epoch 1466, Loss: 0.00000006912684, Improvement: 0.00000001535601, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1467
Epoch 1467, Loss: 0.00000006375666, Improvement: -0.00000000537019, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1468
Epoch 1468, Loss: 0.00000005385443, Improvement: -0.00000000990222, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1469
Epoch 1469, Loss: 0.00000005095388, Improvement: -0.00000000290055, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1470
Epoch 1470, Loss: 0.00000006267966, Improvement: 0.00000001172578, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1471
Epoch 1471, Loss: 0.00000008360567, Improvement: 0.00000002092601, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1472
Epoch 1472, Loss: 0.00000006611797, Improvement: -0.00000001748769, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1473
Epoch 1473, Loss: 0.00000004658229, Improvement: -0.00000001953568, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1474
Epoch 1474, Loss: 0.00000006406625, Improvement: 0.00000001748396, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1475
Epoch 1475, Loss: 0.00000007803144, Improvement: 0.00000001396518, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1476
Epoch 1476, Loss: 0.00000013558590, Improvement: 0.00000005755446, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1477
Epoch 1477, Loss: 0.00000013586174, Improvement: 0.00000000027584, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1478
Epoch 1478, Loss: 0.00000013109626, Improvement: -0.00000000476548, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1479
Epoch 1479, Loss: 0.00000005065330, Improvement: -0.00000008044297, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1480
Epoch 1480, Loss: 0.00000004243328, Improvement: -0.00000000822001, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1481
Epoch 1481, Loss: 0.00000004754799, Improvement: 0.00000000511471, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1482
Epoch 1482, Loss: 0.00000003581199, Improvement: -0.00000001173599, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1483
Epoch 1483, Loss: 0.00000003409027, Improvement: -0.00000000172172, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1484
Epoch 1484, Loss: 0.00000003377236, Improvement: -0.00000000031791, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1485
Epoch 1485, Loss: 0.00000003013802, Improvement: -0.00000000363434, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1486
Epoch 1486, Loss: 0.00000003043600, Improvement: 0.00000000029798, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1487
Epoch 1487, Loss: 0.00000003844143, Improvement: 0.00000000800543, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1488
Epoch 1488, Loss: 0.00000004907222, Improvement: 0.00000001063079, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1489
Epoch 1489, Loss: 0.00000003732949, Improvement: -0.00000001174273, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1490
Epoch 1490, Loss: 0.00000002695463, Improvement: -0.00000001037486, Best Loss: 0.00000002045017 in Epoch 1453
Epoch 1491
A best model at epoch 1491 has been saved with training error 0.00000001761754.
Epoch 1491, Loss: 0.00000002769477, Improvement: 0.00000000074014, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1492
Epoch 1492, Loss: 0.00000002788200, Improvement: 0.00000000018723, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1493
Epoch 1493, Loss: 0.00000003255746, Improvement: 0.00000000467546, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1494
Epoch 1494, Loss: 0.00000003801487, Improvement: 0.00000000545741, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1495
Epoch 1495, Loss: 0.00000006379149, Improvement: 0.00000002577663, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1496
Epoch 1496, Loss: 0.00000012534910, Improvement: 0.00000006155760, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1497
Epoch 1497, Loss: 0.00000008903274, Improvement: -0.00000003631636, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1498
Epoch 1498, Loss: 0.00000004227073, Improvement: -0.00000004676201, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1499
Epoch 1499, Loss: 0.00000004696500, Improvement: 0.00000000469427, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000005152259, Improvement: 0.00000000455759, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1501
Epoch 1501, Loss: 0.00000004036489, Improvement: -0.00000001115770, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1502
Epoch 1502, Loss: 0.00000003253853, Improvement: -0.00000000782635, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1503
Epoch 1503, Loss: 0.00000003442909, Improvement: 0.00000000189056, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1504
Epoch 1504, Loss: 0.00000002711045, Improvement: -0.00000000731864, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1505
Epoch 1505, Loss: 0.00000002536881, Improvement: -0.00000000174164, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1506
Epoch 1506, Loss: 0.00000002569954, Improvement: 0.00000000033073, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1507
Epoch 1507, Loss: 0.00000002865672, Improvement: 0.00000000295718, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1508
Epoch 1508, Loss: 0.00000003410205, Improvement: 0.00000000544534, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1509
Epoch 1509, Loss: 0.00000003863825, Improvement: 0.00000000453619, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1510
Epoch 1510, Loss: 0.00000004150145, Improvement: 0.00000000286320, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1511
Epoch 1511, Loss: 0.00000004288910, Improvement: 0.00000000138765, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1512
Epoch 1512, Loss: 0.00000003129648, Improvement: -0.00000001159261, Best Loss: 0.00000001761754 in Epoch 1491
Epoch 1513
A best model at epoch 1513 has been saved with training error 0.00000001442893.
Epoch 1513, Loss: 0.00000002423358, Improvement: -0.00000000706290, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1514
Epoch 1514, Loss: 0.00000005505804, Improvement: 0.00000003082446, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1515
Epoch 1515, Loss: 0.00000007822009, Improvement: 0.00000002316206, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1516
Epoch 1516, Loss: 0.00000012169513, Improvement: 0.00000004347504, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1517
Epoch 1517, Loss: 0.00000008475864, Improvement: -0.00000003693650, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1518
Epoch 1518, Loss: 0.00000008002112, Improvement: -0.00000000473751, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1519
Epoch 1519, Loss: 0.00000004979144, Improvement: -0.00000003022968, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1520
Epoch 1520, Loss: 0.00000003150563, Improvement: -0.00000001828580, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1521
Epoch 1521, Loss: 0.00000002915307, Improvement: -0.00000000235256, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1522
Epoch 1522, Loss: 0.00000002753860, Improvement: -0.00000000161447, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1523
Epoch 1523, Loss: 0.00000003306486, Improvement: 0.00000000552626, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1524
Epoch 1524, Loss: 0.00000010973925, Improvement: 0.00000007667439, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1525
Epoch 1525, Loss: 0.00000016113203, Improvement: 0.00000005139278, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1526
Epoch 1526, Loss: 0.00000019860402, Improvement: 0.00000003747199, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1527
Epoch 1527, Loss: 0.00000009753459, Improvement: -0.00000010106943, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1528
Epoch 1528, Loss: 0.00000005654781, Improvement: -0.00000004098679, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1529
Epoch 1529, Loss: 0.00000003416232, Improvement: -0.00000002238549, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1530
Epoch 1530, Loss: 0.00000002879489, Improvement: -0.00000000536742, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1531
Epoch 1531, Loss: 0.00000002697561, Improvement: -0.00000000181928, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1532
Epoch 1532, Loss: 0.00000002559748, Improvement: -0.00000000137814, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1533
Epoch 1533, Loss: 0.00000002597457, Improvement: 0.00000000037710, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1534
Epoch 1534, Loss: 0.00000002464639, Improvement: -0.00000000132818, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1535
Epoch 1535, Loss: 0.00000002694293, Improvement: 0.00000000229654, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1536
Epoch 1536, Loss: 0.00000002445745, Improvement: -0.00000000248548, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1537
Epoch 1537, Loss: 0.00000002488274, Improvement: 0.00000000042529, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1538
Epoch 1538, Loss: 0.00000002587308, Improvement: 0.00000000099034, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1539
Epoch 1539, Loss: 0.00000002867788, Improvement: 0.00000000280480, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1540
Epoch 1540, Loss: 0.00000002930823, Improvement: 0.00000000063035, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1541
Epoch 1541, Loss: 0.00000003489583, Improvement: 0.00000000558759, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1542
Epoch 1542, Loss: 0.00000003926202, Improvement: 0.00000000436619, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1543
Epoch 1543, Loss: 0.00000003932613, Improvement: 0.00000000006411, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1544
Epoch 1544, Loss: 0.00000003882791, Improvement: -0.00000000049823, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1545
Epoch 1545, Loss: 0.00000003212089, Improvement: -0.00000000670702, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1546
Epoch 1546, Loss: 0.00000006251702, Improvement: 0.00000003039613, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1547
Epoch 1547, Loss: 0.00000006760209, Improvement: 0.00000000508508, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1548
Epoch 1548, Loss: 0.00000005415520, Improvement: -0.00000001344689, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1549
Epoch 1549, Loss: 0.00000005219670, Improvement: -0.00000000195850, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000004125746, Improvement: -0.00000001093924, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1551
Epoch 1551, Loss: 0.00000008243172, Improvement: 0.00000004117427, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1552
Epoch 1552, Loss: 0.00000003279891, Improvement: -0.00000004963281, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1553
Epoch 1553, Loss: 0.00000002487338, Improvement: -0.00000000792553, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1554
Epoch 1554, Loss: 0.00000003021163, Improvement: 0.00000000533825, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1555
Epoch 1555, Loss: 0.00000002579901, Improvement: -0.00000000441262, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1556
Epoch 1556, Loss: 0.00000002747515, Improvement: 0.00000000167614, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1557
Epoch 1557, Loss: 0.00000003855966, Improvement: 0.00000001108450, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1558
Epoch 1558, Loss: 0.00000010503265, Improvement: 0.00000006647299, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1559
Epoch 1559, Loss: 0.00000021292868, Improvement: 0.00000010789603, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1560
Epoch 1560, Loss: 0.00000013766547, Improvement: -0.00000007526321, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1561
Epoch 1561, Loss: 0.00000011326191, Improvement: -0.00000002440356, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1562
Epoch 1562, Loss: 0.00000007910529, Improvement: -0.00000003415662, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1563
Epoch 1563, Loss: 0.00000004883916, Improvement: -0.00000003026613, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1564
Epoch 1564, Loss: 0.00000003318984, Improvement: -0.00000001564932, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1565
Epoch 1565, Loss: 0.00000002736782, Improvement: -0.00000000582203, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1566
Epoch 1566, Loss: 0.00000002571383, Improvement: -0.00000000165399, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1567
Epoch 1567, Loss: 0.00000002949751, Improvement: 0.00000000378368, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1568
Epoch 1568, Loss: 0.00000002728642, Improvement: -0.00000000221109, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1569
Epoch 1569, Loss: 0.00000002270149, Improvement: -0.00000000458493, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1570
Epoch 1570, Loss: 0.00000002329051, Improvement: 0.00000000058902, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1571
Epoch 1571, Loss: 0.00000002369350, Improvement: 0.00000000040299, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1572
Epoch 1572, Loss: 0.00000002228277, Improvement: -0.00000000141073, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1573
Epoch 1573, Loss: 0.00000002313987, Improvement: 0.00000000085710, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1574
Epoch 1574, Loss: 0.00000002675984, Improvement: 0.00000000361997, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1575
Epoch 1575, Loss: 0.00000004460646, Improvement: 0.00000001784661, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1576
Epoch 1576, Loss: 0.00000006290605, Improvement: 0.00000001829960, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1577
Epoch 1577, Loss: 0.00000004618998, Improvement: -0.00000001671607, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1578
Epoch 1578, Loss: 0.00000004284098, Improvement: -0.00000000334901, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1579
Epoch 1579, Loss: 0.00000003539155, Improvement: -0.00000000744943, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1580
Epoch 1580, Loss: 0.00000004236244, Improvement: 0.00000000697090, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1581
Epoch 1581, Loss: 0.00000002767968, Improvement: -0.00000001468276, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1582
Epoch 1582, Loss: 0.00000003084779, Improvement: 0.00000000316811, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1583
Epoch 1583, Loss: 0.00000002999876, Improvement: -0.00000000084903, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1584
Epoch 1584, Loss: 0.00000004469015, Improvement: 0.00000001469139, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1585
Epoch 1585, Loss: 0.00000004191049, Improvement: -0.00000000277966, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1586
Epoch 1586, Loss: 0.00000003003460, Improvement: -0.00000001187589, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1587
Epoch 1587, Loss: 0.00000003410236, Improvement: 0.00000000406776, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1588
Epoch 1588, Loss: 0.00000007541998, Improvement: 0.00000004131761, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1589
Epoch 1589, Loss: 0.00000015138634, Improvement: 0.00000007596636, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1590
Epoch 1590, Loss: 0.00000006472867, Improvement: -0.00000008665767, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1591
Epoch 1591, Loss: 0.00000005809155, Improvement: -0.00000000663712, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1592
Epoch 1592, Loss: 0.00000004174169, Improvement: -0.00000001634986, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1593
Epoch 1593, Loss: 0.00000006380163, Improvement: 0.00000002205994, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1594
Epoch 1594, Loss: 0.00000004129098, Improvement: -0.00000002251064, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1595
Epoch 1595, Loss: 0.00000002858682, Improvement: -0.00000001270416, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1596
Epoch 1596, Loss: 0.00000002704040, Improvement: -0.00000000154642, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1597
Epoch 1597, Loss: 0.00000002337405, Improvement: -0.00000000366635, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1598
Epoch 1598, Loss: 0.00000002361021, Improvement: 0.00000000023615, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1599
Epoch 1599, Loss: 0.00000002484246, Improvement: 0.00000000123225, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000002788729, Improvement: 0.00000000304483, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1601
Epoch 1601, Loss: 0.00000003691705, Improvement: 0.00000000902976, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1602
Epoch 1602, Loss: 0.00000002605706, Improvement: -0.00000001085999, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1603
Epoch 1603, Loss: 0.00000002311122, Improvement: -0.00000000294583, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1604
Epoch 1604, Loss: 0.00000002499475, Improvement: 0.00000000188352, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1605
Epoch 1605, Loss: 0.00000002063746, Improvement: -0.00000000435728, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1606
Epoch 1606, Loss: 0.00000002094131, Improvement: 0.00000000030385, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1607
Epoch 1607, Loss: 0.00000001996086, Improvement: -0.00000000098045, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1608
Epoch 1608, Loss: 0.00000002274362, Improvement: 0.00000000278276, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1609
Epoch 1609, Loss: 0.00000004554722, Improvement: 0.00000002280360, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1610
Epoch 1610, Loss: 0.00000011524506, Improvement: 0.00000006969784, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1611
Epoch 1611, Loss: 0.00000008354055, Improvement: -0.00000003170451, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1612
Epoch 1612, Loss: 0.00000005373955, Improvement: -0.00000002980100, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1613
Epoch 1613, Loss: 0.00000003734185, Improvement: -0.00000001639770, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1614
Epoch 1614, Loss: 0.00000003192283, Improvement: -0.00000000541902, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1615
Epoch 1615, Loss: 0.00000003522002, Improvement: 0.00000000329718, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1616
Epoch 1616, Loss: 0.00000002649776, Improvement: -0.00000000872225, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1617
Epoch 1617, Loss: 0.00000002520547, Improvement: -0.00000000129229, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1618
Epoch 1618, Loss: 0.00000002638411, Improvement: 0.00000000117864, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1619
Epoch 1619, Loss: 0.00000004349532, Improvement: 0.00000001711121, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1620
Epoch 1620, Loss: 0.00000005295635, Improvement: 0.00000000946103, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1621
Epoch 1621, Loss: 0.00000004306924, Improvement: -0.00000000988711, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1622
Epoch 1622, Loss: 0.00000002608704, Improvement: -0.00000001698220, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1623
Epoch 1623, Loss: 0.00000002637046, Improvement: 0.00000000028342, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1624
Epoch 1624, Loss: 0.00000003509958, Improvement: 0.00000000872912, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1625
Epoch 1625, Loss: 0.00000008444440, Improvement: 0.00000004934482, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1626
Epoch 1626, Loss: 0.00000003947853, Improvement: -0.00000004496588, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1627
Epoch 1627, Loss: 0.00000003406408, Improvement: -0.00000000541445, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1628
Epoch 1628, Loss: 0.00000002785179, Improvement: -0.00000000621229, Best Loss: 0.00000001442893 in Epoch 1513
Epoch 1629
A best model at epoch 1629 has been saved with training error 0.00000001421470.
Epoch 1629, Loss: 0.00000002108861, Improvement: -0.00000000676318, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1630
Epoch 1630, Loss: 0.00000002021433, Improvement: -0.00000000087428, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1631
Epoch 1631, Loss: 0.00000003347537, Improvement: 0.00000001326104, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1632
Epoch 1632, Loss: 0.00000004019196, Improvement: 0.00000000671658, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1633
Epoch 1633, Loss: 0.00000007725944, Improvement: 0.00000003706748, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1634
Epoch 1634, Loss: 0.00000008695376, Improvement: 0.00000000969432, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1635
Epoch 1635, Loss: 0.00000006007760, Improvement: -0.00000002687616, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1636
Epoch 1636, Loss: 0.00000003739839, Improvement: -0.00000002267921, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1637
Epoch 1637, Loss: 0.00000002829993, Improvement: -0.00000000909846, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1638
Epoch 1638, Loss: 0.00000005889761, Improvement: 0.00000003059768, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1639
Epoch 1639, Loss: 0.00000004945377, Improvement: -0.00000000944384, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1640
Epoch 1640, Loss: 0.00000003694944, Improvement: -0.00000001250433, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1641
Epoch 1641, Loss: 0.00000006935757, Improvement: 0.00000003240813, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1642
Epoch 1642, Loss: 0.00000007136994, Improvement: 0.00000000201236, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1643
Epoch 1643, Loss: 0.00000004696264, Improvement: -0.00000002440729, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1644
Epoch 1644, Loss: 0.00000006064546, Improvement: 0.00000001368282, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1645
Epoch 1645, Loss: 0.00000006933763, Improvement: 0.00000000869216, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1646
Epoch 1646, Loss: 0.00000013956844, Improvement: 0.00000007023081, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1647
Epoch 1647, Loss: 0.00000013896933, Improvement: -0.00000000059911, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1648
Epoch 1648, Loss: 0.00000004629766, Improvement: -0.00000009267167, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1649
Epoch 1649, Loss: 0.00000002827142, Improvement: -0.00000001802625, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000002702871, Improvement: -0.00000000124270, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1651
Epoch 1651, Loss: 0.00000002496093, Improvement: -0.00000000206778, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1652
Epoch 1652, Loss: 0.00000002661259, Improvement: 0.00000000165166, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1653
Epoch 1653, Loss: 0.00000002968601, Improvement: 0.00000000307342, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1654
Epoch 1654, Loss: 0.00000002183012, Improvement: -0.00000000785589, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1655
Epoch 1655, Loss: 0.00000002532791, Improvement: 0.00000000349779, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1656
Epoch 1656, Loss: 0.00000002209455, Improvement: -0.00000000323335, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1657
Epoch 1657, Loss: 0.00000002476055, Improvement: 0.00000000266600, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1658
Epoch 1658, Loss: 0.00000005405498, Improvement: 0.00000002929443, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1659
Epoch 1659, Loss: 0.00000012216762, Improvement: 0.00000006811264, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1660
Epoch 1660, Loss: 0.00000020679715, Improvement: 0.00000008462953, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1661
Epoch 1661, Loss: 0.00000008457246, Improvement: -0.00000012222468, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1662
Epoch 1662, Loss: 0.00000007489330, Improvement: -0.00000000967916, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1663
Epoch 1663, Loss: 0.00000003396857, Improvement: -0.00000004092473, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1664
Epoch 1664, Loss: 0.00000002520646, Improvement: -0.00000000876211, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1665
Epoch 1665, Loss: 0.00000002467156, Improvement: -0.00000000053489, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1666
Epoch 1666, Loss: 0.00000002184232, Improvement: -0.00000000282924, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1667
Epoch 1667, Loss: 0.00000002210352, Improvement: 0.00000000026120, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1668
Epoch 1668, Loss: 0.00000002849488, Improvement: 0.00000000639136, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1669
Epoch 1669, Loss: 0.00000003122834, Improvement: 0.00000000273345, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1670
Epoch 1670, Loss: 0.00000002487038, Improvement: -0.00000000635796, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1671
Epoch 1671, Loss: 0.00000001927509, Improvement: -0.00000000559529, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1672
Epoch 1672, Loss: 0.00000001935986, Improvement: 0.00000000008478, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1673
Epoch 1673, Loss: 0.00000002100958, Improvement: 0.00000000164971, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1674
Epoch 1674, Loss: 0.00000003015493, Improvement: 0.00000000914535, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1675
Epoch 1675, Loss: 0.00000002972839, Improvement: -0.00000000042654, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1676
Epoch 1676, Loss: 0.00000002407011, Improvement: -0.00000000565828, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1677
Epoch 1677, Loss: 0.00000002250894, Improvement: -0.00000000156117, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1678
Epoch 1678, Loss: 0.00000002513157, Improvement: 0.00000000262263, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1679
Epoch 1679, Loss: 0.00000004101423, Improvement: 0.00000001588266, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1680
Epoch 1680, Loss: 0.00000004658529, Improvement: 0.00000000557106, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1681
Epoch 1681, Loss: 0.00000002315676, Improvement: -0.00000002342853, Best Loss: 0.00000001421470 in Epoch 1629
Epoch 1682
A best model at epoch 1682 has been saved with training error 0.00000001421303.
Epoch 1682, Loss: 0.00000002040721, Improvement: -0.00000000274955, Best Loss: 0.00000001421303 in Epoch 1682
Epoch 1683
A best model at epoch 1683 has been saved with training error 0.00000001139834.
Epoch 1683, Loss: 0.00000002042349, Improvement: 0.00000000001628, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1684
Epoch 1684, Loss: 0.00000002103742, Improvement: 0.00000000061393, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1685
Epoch 1685, Loss: 0.00000002886314, Improvement: 0.00000000782573, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1686
Epoch 1686, Loss: 0.00000004123656, Improvement: 0.00000001237342, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1687
Epoch 1687, Loss: 0.00000004238759, Improvement: 0.00000000115103, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1688
Epoch 1688, Loss: 0.00000009019790, Improvement: 0.00000004781031, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1689
Epoch 1689, Loss: 0.00000007889104, Improvement: -0.00000001130686, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1690
Epoch 1690, Loss: 0.00000005887708, Improvement: -0.00000002001396, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1691
Epoch 1691, Loss: 0.00000005759988, Improvement: -0.00000000127721, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1692
Epoch 1692, Loss: 0.00000003558169, Improvement: -0.00000002201818, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1693
Epoch 1693, Loss: 0.00000003762731, Improvement: 0.00000000204562, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1694
Epoch 1694, Loss: 0.00000003921702, Improvement: 0.00000000158970, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1695
Epoch 1695, Loss: 0.00000003437263, Improvement: -0.00000000484439, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1696
Epoch 1696, Loss: 0.00000002620285, Improvement: -0.00000000816977, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1697
Epoch 1697, Loss: 0.00000002067269, Improvement: -0.00000000553016, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1698
Epoch 1698, Loss: 0.00000002122331, Improvement: 0.00000000055062, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1699
Epoch 1699, Loss: 0.00000004282881, Improvement: 0.00000002160550, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000008397038, Improvement: 0.00000004114156, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1701
Epoch 1701, Loss: 0.00000007095528, Improvement: -0.00000001301510, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1702
Epoch 1702, Loss: 0.00000005483473, Improvement: -0.00000001612055, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1703
Epoch 1703, Loss: 0.00000003422699, Improvement: -0.00000002060775, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1704
Epoch 1704, Loss: 0.00000002366083, Improvement: -0.00000001056616, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1705
Epoch 1705, Loss: 0.00000001892737, Improvement: -0.00000000473346, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1706
Epoch 1706, Loss: 0.00000001847820, Improvement: -0.00000000044916, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1707
Epoch 1707, Loss: 0.00000001971572, Improvement: 0.00000000123752, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1708
Epoch 1708, Loss: 0.00000001996679, Improvement: 0.00000000025107, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1709
Epoch 1709, Loss: 0.00000002575791, Improvement: 0.00000000579112, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1710
Epoch 1710, Loss: 0.00000002168015, Improvement: -0.00000000407775, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1711
Epoch 1711, Loss: 0.00000002518586, Improvement: 0.00000000350571, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1712
Epoch 1712, Loss: 0.00000002958493, Improvement: 0.00000000439906, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1713
Epoch 1713, Loss: 0.00000004074065, Improvement: 0.00000001115572, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1714
Epoch 1714, Loss: 0.00000004801782, Improvement: 0.00000000727717, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1715
Epoch 1715, Loss: 0.00000003804930, Improvement: -0.00000000996852, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1716
Epoch 1716, Loss: 0.00000002674875, Improvement: -0.00000001130055, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1717
Epoch 1717, Loss: 0.00000002748250, Improvement: 0.00000000073375, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1718
Epoch 1718, Loss: 0.00000002665703, Improvement: -0.00000000082547, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1719
Epoch 1719, Loss: 0.00000004051008, Improvement: 0.00000001385305, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1720
Epoch 1720, Loss: 0.00000004806351, Improvement: 0.00000000755342, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1721
Epoch 1721, Loss: 0.00000004434671, Improvement: -0.00000000371680, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1722
Epoch 1722, Loss: 0.00000003856742, Improvement: -0.00000000577929, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1723
Epoch 1723, Loss: 0.00000002223884, Improvement: -0.00000001632858, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1724
Epoch 1724, Loss: 0.00000002629091, Improvement: 0.00000000405207, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1725
Epoch 1725, Loss: 0.00000004480692, Improvement: 0.00000001851601, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1726
Epoch 1726, Loss: 0.00000010409081, Improvement: 0.00000005928389, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1727
Epoch 1727, Loss: 0.00000006670872, Improvement: -0.00000003738209, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1728
Epoch 1728, Loss: 0.00000005665344, Improvement: -0.00000001005528, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1729
Epoch 1729, Loss: 0.00000005913354, Improvement: 0.00000000248010, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1730
Epoch 1730, Loss: 0.00000004766414, Improvement: -0.00000001146941, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1731
Epoch 1731, Loss: 0.00000003304837, Improvement: -0.00000001461576, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1732
Epoch 1732, Loss: 0.00000002409582, Improvement: -0.00000000895255, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1733
Epoch 1733, Loss: 0.00000001900181, Improvement: -0.00000000509401, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1734
Epoch 1734, Loss: 0.00000002258219, Improvement: 0.00000000358039, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1735
Epoch 1735, Loss: 0.00000001937537, Improvement: -0.00000000320682, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1736
Epoch 1736, Loss: 0.00000001690386, Improvement: -0.00000000247152, Best Loss: 0.00000001139834 in Epoch 1683
Epoch 1737
A best model at epoch 1737 has been saved with training error 0.00000001090605.
Epoch 1737, Loss: 0.00000001584926, Improvement: -0.00000000105459, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1738
Epoch 1738, Loss: 0.00000001709505, Improvement: 0.00000000124579, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1739
Epoch 1739, Loss: 0.00000001672047, Improvement: -0.00000000037458, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1740
Epoch 1740, Loss: 0.00000001787548, Improvement: 0.00000000115500, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1741
Epoch 1741, Loss: 0.00000001769289, Improvement: -0.00000000018259, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1742
Epoch 1742, Loss: 0.00000003418656, Improvement: 0.00000001649367, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1743
Epoch 1743, Loss: 0.00000005516983, Improvement: 0.00000002098327, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1744
Epoch 1744, Loss: 0.00000003011839, Improvement: -0.00000002505144, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1745
Epoch 1745, Loss: 0.00000002629019, Improvement: -0.00000000382821, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1746
Epoch 1746, Loss: 0.00000002601220, Improvement: -0.00000000027798, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1747
Epoch 1747, Loss: 0.00000002761625, Improvement: 0.00000000160405, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1748
Epoch 1748, Loss: 0.00000003293254, Improvement: 0.00000000531629, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1749
Epoch 1749, Loss: 0.00000004750854, Improvement: 0.00000001457600, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000009079176, Improvement: 0.00000004328322, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1751
Epoch 1751, Loss: 0.00000006539405, Improvement: -0.00000002539771, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1752
Epoch 1752, Loss: 0.00000003869997, Improvement: -0.00000002669408, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1753
Epoch 1753, Loss: 0.00000002168229, Improvement: -0.00000001701768, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1754
Epoch 1754, Loss: 0.00000001812242, Improvement: -0.00000000355987, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1755
Epoch 1755, Loss: 0.00000002135259, Improvement: 0.00000000323017, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1756
Epoch 1756, Loss: 0.00000002195246, Improvement: 0.00000000059987, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1757
Epoch 1757, Loss: 0.00000001798675, Improvement: -0.00000000396571, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1758
Epoch 1758, Loss: 0.00000001498922, Improvement: -0.00000000299753, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1759
Epoch 1759, Loss: 0.00000001686567, Improvement: 0.00000000187645, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1760
Epoch 1760, Loss: 0.00000002238436, Improvement: 0.00000000551870, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1761
Epoch 1761, Loss: 0.00000002382038, Improvement: 0.00000000143602, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1762
Epoch 1762, Loss: 0.00000003450333, Improvement: 0.00000001068295, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1763
Epoch 1763, Loss: 0.00000003844982, Improvement: 0.00000000394650, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1764
Epoch 1764, Loss: 0.00000004149037, Improvement: 0.00000000304055, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1765
Epoch 1765, Loss: 0.00000003234912, Improvement: -0.00000000914125, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1766
Epoch 1766, Loss: 0.00000003184485, Improvement: -0.00000000050428, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1767
Epoch 1767, Loss: 0.00000004958281, Improvement: 0.00000001773796, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1768
Epoch 1768, Loss: 0.00000004915058, Improvement: -0.00000000043222, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1769
Epoch 1769, Loss: 0.00000003261025, Improvement: -0.00000001654033, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1770
Epoch 1770, Loss: 0.00000005866267, Improvement: 0.00000002605242, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1771
Epoch 1771, Loss: 0.00000003626994, Improvement: -0.00000002239273, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1772
Epoch 1772, Loss: 0.00000002667669, Improvement: -0.00000000959325, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1773
Epoch 1773, Loss: 0.00000003663921, Improvement: 0.00000000996252, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1774
Epoch 1774, Loss: 0.00000002865097, Improvement: -0.00000000798824, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1775
Epoch 1775, Loss: 0.00000002523245, Improvement: -0.00000000341852, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1776
Epoch 1776, Loss: 0.00000002428126, Improvement: -0.00000000095119, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1777
Epoch 1777, Loss: 0.00000004024819, Improvement: 0.00000001596694, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1778
Epoch 1778, Loss: 0.00000002806879, Improvement: -0.00000001217941, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1779
Epoch 1779, Loss: 0.00000003234228, Improvement: 0.00000000427349, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1780
Epoch 1780, Loss: 0.00000004796239, Improvement: 0.00000001562010, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1781
Epoch 1781, Loss: 0.00000008168866, Improvement: 0.00000003372627, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1782
Epoch 1782, Loss: 0.00000005119220, Improvement: -0.00000003049646, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1783
Epoch 1783, Loss: 0.00000004475103, Improvement: -0.00000000644117, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1784
Epoch 1784, Loss: 0.00000006549270, Improvement: 0.00000002074167, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1785
Epoch 1785, Loss: 0.00000005669879, Improvement: -0.00000000879391, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1786
Epoch 1786, Loss: 0.00000006908586, Improvement: 0.00000001238707, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1787
Epoch 1787, Loss: 0.00000003594060, Improvement: -0.00000003314526, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1788
Epoch 1788, Loss: 0.00000002175295, Improvement: -0.00000001418764, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1789
Epoch 1789, Loss: 0.00000001915063, Improvement: -0.00000000260232, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1790
Epoch 1790, Loss: 0.00000002785436, Improvement: 0.00000000870373, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1791
Epoch 1791, Loss: 0.00000010727364, Improvement: 0.00000007941928, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1792
Epoch 1792, Loss: 0.00000004591353, Improvement: -0.00000006136011, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1793
Epoch 1793, Loss: 0.00000002077599, Improvement: -0.00000002513754, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1794
Epoch 1794, Loss: 0.00000001735058, Improvement: -0.00000000342541, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1795
Epoch 1795, Loss: 0.00000001842173, Improvement: 0.00000000107115, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1796
Epoch 1796, Loss: 0.00000001962814, Improvement: 0.00000000120641, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1797
Epoch 1797, Loss: 0.00000001747453, Improvement: -0.00000000215360, Best Loss: 0.00000001090605 in Epoch 1737
Epoch 1798
A best model at epoch 1798 has been saved with training error 0.00000000926523.
Epoch 1798, Loss: 0.00000001706334, Improvement: -0.00000000041119, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1799
Epoch 1799, Loss: 0.00000001982693, Improvement: 0.00000000276359, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000003838453, Improvement: 0.00000001855761, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1801
Epoch 1801, Loss: 0.00000003720051, Improvement: -0.00000000118402, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1802
Epoch 1802, Loss: 0.00000003426155, Improvement: -0.00000000293896, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1803
Epoch 1803, Loss: 0.00000002491607, Improvement: -0.00000000934548, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1804
Epoch 1804, Loss: 0.00000004329100, Improvement: 0.00000001837493, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1805
Epoch 1805, Loss: 0.00000004184513, Improvement: -0.00000000144587, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1806
Epoch 1806, Loss: 0.00000004071737, Improvement: -0.00000000112775, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1807
Epoch 1807, Loss: 0.00000008845647, Improvement: 0.00000004773910, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1808
Epoch 1808, Loss: 0.00000011491611, Improvement: 0.00000002645963, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1809
Epoch 1809, Loss: 0.00000010026840, Improvement: -0.00000001464771, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1810
Epoch 1810, Loss: 0.00000004715432, Improvement: -0.00000005311408, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1811
Epoch 1811, Loss: 0.00000002294965, Improvement: -0.00000002420466, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1812
Epoch 1812, Loss: 0.00000001913394, Improvement: -0.00000000381571, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1813
Epoch 1813, Loss: 0.00000001757108, Improvement: -0.00000000156287, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1814
Epoch 1814, Loss: 0.00000001648121, Improvement: -0.00000000108987, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1815
Epoch 1815, Loss: 0.00000001526618, Improvement: -0.00000000121503, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1816
Epoch 1816, Loss: 0.00000001640664, Improvement: 0.00000000114046, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1817
Epoch 1817, Loss: 0.00000001667104, Improvement: 0.00000000026440, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1818
Epoch 1818, Loss: 0.00000001958597, Improvement: 0.00000000291493, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1819
Epoch 1819, Loss: 0.00000001708277, Improvement: -0.00000000250320, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1820
Epoch 1820, Loss: 0.00000001607080, Improvement: -0.00000000101197, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1821
Epoch 1821, Loss: 0.00000001652046, Improvement: 0.00000000044966, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1822
Epoch 1822, Loss: 0.00000001767480, Improvement: 0.00000000115434, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1823
Epoch 1823, Loss: 0.00000001584729, Improvement: -0.00000000182751, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1824
Epoch 1824, Loss: 0.00000001812333, Improvement: 0.00000000227604, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1825
Epoch 1825, Loss: 0.00000002533673, Improvement: 0.00000000721340, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1826
Epoch 1826, Loss: 0.00000001762732, Improvement: -0.00000000770941, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1827
Epoch 1827, Loss: 0.00000001928403, Improvement: 0.00000000165671, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1828
Epoch 1828, Loss: 0.00000002678991, Improvement: 0.00000000750588, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1829
Epoch 1829, Loss: 0.00000002855636, Improvement: 0.00000000176645, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1830
Epoch 1830, Loss: 0.00000002394265, Improvement: -0.00000000461371, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1831
Epoch 1831, Loss: 0.00000002250533, Improvement: -0.00000000143732, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1832
Epoch 1832, Loss: 0.00000002572867, Improvement: 0.00000000322334, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1833
Epoch 1833, Loss: 0.00000006144515, Improvement: 0.00000003571647, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1834
Epoch 1834, Loss: 0.00000004796140, Improvement: -0.00000001348375, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1835
Epoch 1835, Loss: 0.00000003919875, Improvement: -0.00000000876264, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1836
Epoch 1836, Loss: 0.00000002196388, Improvement: -0.00000001723487, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1837
Epoch 1837, Loss: 0.00000004063353, Improvement: 0.00000001866965, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1838
Epoch 1838, Loss: 0.00000009209249, Improvement: 0.00000005145896, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1839
Epoch 1839, Loss: 0.00000014530241, Improvement: 0.00000005320992, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1840
Epoch 1840, Loss: 0.00000009050683, Improvement: -0.00000005479558, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1841
Epoch 1841, Loss: 0.00000005155835, Improvement: -0.00000003894848, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1842
Epoch 1842, Loss: 0.00000003152405, Improvement: -0.00000002003430, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1843
Epoch 1843, Loss: 0.00000001870248, Improvement: -0.00000001282157, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1844
Epoch 1844, Loss: 0.00000001549692, Improvement: -0.00000000320556, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1845
Epoch 1845, Loss: 0.00000001511103, Improvement: -0.00000000038589, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1846
Epoch 1846, Loss: 0.00000001581179, Improvement: 0.00000000070076, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1847
Epoch 1847, Loss: 0.00000001575051, Improvement: -0.00000000006128, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1848
Epoch 1848, Loss: 0.00000001713935, Improvement: 0.00000000138884, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1849
Epoch 1849, Loss: 0.00000001864444, Improvement: 0.00000000150510, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1850
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000001433310, Improvement: -0.00000000431135, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1851
Epoch 1851, Loss: 0.00000001422898, Improvement: -0.00000000010412, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1852
Epoch 1852, Loss: 0.00000001516620, Improvement: 0.00000000093722, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1853
Epoch 1853, Loss: 0.00000001617601, Improvement: 0.00000000100981, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1854
Epoch 1854, Loss: 0.00000001468841, Improvement: -0.00000000148759, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1855
Epoch 1855, Loss: 0.00000001385731, Improvement: -0.00000000083110, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1856
Epoch 1856, Loss: 0.00000001781441, Improvement: 0.00000000395710, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1857
Epoch 1857, Loss: 0.00000003954321, Improvement: 0.00000002172880, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1858
Epoch 1858, Loss: 0.00000002687356, Improvement: -0.00000001266965, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1859
Epoch 1859, Loss: 0.00000002307430, Improvement: -0.00000000379926, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1860
Epoch 1860, Loss: 0.00000001574888, Improvement: -0.00000000732542, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1861
Epoch 1861, Loss: 0.00000001535589, Improvement: -0.00000000039299, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1862
Epoch 1862, Loss: 0.00000001712684, Improvement: 0.00000000177095, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1863
Epoch 1863, Loss: 0.00000002952773, Improvement: 0.00000001240089, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1864
Epoch 1864, Loss: 0.00000003227151, Improvement: 0.00000000274378, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1865
Epoch 1865, Loss: 0.00000004511129, Improvement: 0.00000001283979, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1866
Epoch 1866, Loss: 0.00000003881574, Improvement: -0.00000000629555, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1867
Epoch 1867, Loss: 0.00000003778718, Improvement: -0.00000000102856, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1868
Epoch 1868, Loss: 0.00000002857989, Improvement: -0.00000000920728, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1869
Epoch 1869, Loss: 0.00000002144089, Improvement: -0.00000000713900, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1870
Epoch 1870, Loss: 0.00000001887241, Improvement: -0.00000000256848, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1871
Epoch 1871, Loss: 0.00000002121706, Improvement: 0.00000000234465, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1872
Epoch 1872, Loss: 0.00000002638589, Improvement: 0.00000000516883, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1873
Epoch 1873, Loss: 0.00000007850800, Improvement: 0.00000005212211, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1874
Epoch 1874, Loss: 0.00000005224002, Improvement: -0.00000002626798, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1875
Epoch 1875, Loss: 0.00000003097357, Improvement: -0.00000002126645, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1876
Epoch 1876, Loss: 0.00000003621816, Improvement: 0.00000000524458, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1877
Epoch 1877, Loss: 0.00000003609839, Improvement: -0.00000000011977, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1878
Epoch 1878, Loss: 0.00000002297005, Improvement: -0.00000001312834, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1879
Epoch 1879, Loss: 0.00000002232155, Improvement: -0.00000000064850, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1880
Epoch 1880, Loss: 0.00000002220387, Improvement: -0.00000000011769, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1881
Epoch 1881, Loss: 0.00000003707646, Improvement: 0.00000001487259, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1882
Epoch 1882, Loss: 0.00000004544153, Improvement: 0.00000000836508, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1883
Epoch 1883, Loss: 0.00000014067915, Improvement: 0.00000009523762, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1884
Epoch 1884, Loss: 0.00000011288225, Improvement: -0.00000002779690, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1885
Epoch 1885, Loss: 0.00000004876561, Improvement: -0.00000006411664, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1886
Epoch 1886, Loss: 0.00000001897339, Improvement: -0.00000002979223, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1887
Epoch 1887, Loss: 0.00000001584210, Improvement: -0.00000000313129, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1888
Epoch 1888, Loss: 0.00000001769145, Improvement: 0.00000000184936, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1889
Epoch 1889, Loss: 0.00000001766540, Improvement: -0.00000000002605, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1890
Epoch 1890, Loss: 0.00000001448953, Improvement: -0.00000000317587, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1891
Epoch 1891, Loss: 0.00000001352312, Improvement: -0.00000000096641, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1892
Epoch 1892, Loss: 0.00000001472010, Improvement: 0.00000000119699, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1893
Epoch 1893, Loss: 0.00000001289176, Improvement: -0.00000000182835, Best Loss: 0.00000000926523 in Epoch 1798
Epoch 1894
A best model at epoch 1894 has been saved with training error 0.00000000907938.
Epoch 1894, Loss: 0.00000001420312, Improvement: 0.00000000131136, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1895
Epoch 1895, Loss: 0.00000001736402, Improvement: 0.00000000316090, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1896
Epoch 1896, Loss: 0.00000001680410, Improvement: -0.00000000055992, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1897
Epoch 1897, Loss: 0.00000001440965, Improvement: -0.00000000239446, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1898
Epoch 1898, Loss: 0.00000001377105, Improvement: -0.00000000063859, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1899
Epoch 1899, Loss: 0.00000001402321, Improvement: 0.00000000025215, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000001479376, Improvement: 0.00000000077056, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1901
Epoch 1901, Loss: 0.00000001575313, Improvement: 0.00000000095937, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1902
Epoch 1902, Loss: 0.00000001523161, Improvement: -0.00000000052152, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1903
Epoch 1903, Loss: 0.00000002417491, Improvement: 0.00000000894330, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1904
Epoch 1904, Loss: 0.00000004498583, Improvement: 0.00000002081092, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1905
Epoch 1905, Loss: 0.00000004583085, Improvement: 0.00000000084502, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1906
Epoch 1906, Loss: 0.00000003946438, Improvement: -0.00000000636647, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1907
Epoch 1907, Loss: 0.00000007869585, Improvement: 0.00000003923148, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1908
Epoch 1908, Loss: 0.00000009900699, Improvement: 0.00000002031113, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1909
Epoch 1909, Loss: 0.00000008577700, Improvement: -0.00000001322999, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1910
Epoch 1910, Loss: 0.00000006331001, Improvement: -0.00000002246699, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1911
Epoch 1911, Loss: 0.00000002807570, Improvement: -0.00000003523430, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1912
Epoch 1912, Loss: 0.00000002266219, Improvement: -0.00000000541351, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1913
Epoch 1913, Loss: 0.00000002463225, Improvement: 0.00000000197006, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1914
Epoch 1914, Loss: 0.00000001897947, Improvement: -0.00000000565278, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1915
Epoch 1915, Loss: 0.00000002028332, Improvement: 0.00000000130385, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1916
Epoch 1916, Loss: 0.00000001561755, Improvement: -0.00000000466577, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1917
Epoch 1917, Loss: 0.00000001359621, Improvement: -0.00000000202134, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1918
Epoch 1918, Loss: 0.00000001352765, Improvement: -0.00000000006856, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1919
Epoch 1919, Loss: 0.00000001341554, Improvement: -0.00000000011211, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1920
Epoch 1920, Loss: 0.00000001409959, Improvement: 0.00000000068405, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1921
Epoch 1921, Loss: 0.00000001401427, Improvement: -0.00000000008532, Best Loss: 0.00000000907938 in Epoch 1894
Epoch 1922
A best model at epoch 1922 has been saved with training error 0.00000000903386.
Epoch 1922, Loss: 0.00000001368303, Improvement: -0.00000000033124, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1923
Epoch 1923, Loss: 0.00000001840117, Improvement: 0.00000000471814, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1924
Epoch 1924, Loss: 0.00000004311958, Improvement: 0.00000002471841, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1925
Epoch 1925, Loss: 0.00000007569987, Improvement: 0.00000003258029, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1926
Epoch 1926, Loss: 0.00000009609576, Improvement: 0.00000002039589, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1927
Epoch 1927, Loss: 0.00000004076728, Improvement: -0.00000005532848, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1928
Epoch 1928, Loss: 0.00000003195305, Improvement: -0.00000000881423, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1929
Epoch 1929, Loss: 0.00000004049002, Improvement: 0.00000000853697, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1930
Epoch 1930, Loss: 0.00000003607439, Improvement: -0.00000000441562, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1931
Epoch 1931, Loss: 0.00000006088577, Improvement: 0.00000002481138, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1932
Epoch 1932, Loss: 0.00000006580586, Improvement: 0.00000000492009, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1933
Epoch 1933, Loss: 0.00000003891750, Improvement: -0.00000002688836, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1934
Epoch 1934, Loss: 0.00000004277754, Improvement: 0.00000000386003, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1935
Epoch 1935, Loss: 0.00000002893316, Improvement: -0.00000001384437, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1936
Epoch 1936, Loss: 0.00000002453287, Improvement: -0.00000000440030, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1937
Epoch 1937, Loss: 0.00000002352215, Improvement: -0.00000000101072, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1938
Epoch 1938, Loss: 0.00000002061105, Improvement: -0.00000000291110, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1939
Epoch 1939, Loss: 0.00000002109077, Improvement: 0.00000000047972, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1940
Epoch 1940, Loss: 0.00000003658393, Improvement: 0.00000001549315, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1941
Epoch 1941, Loss: 0.00000002924829, Improvement: -0.00000000733564, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1942
Epoch 1942, Loss: 0.00000002247763, Improvement: -0.00000000677066, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1943
Epoch 1943, Loss: 0.00000002293717, Improvement: 0.00000000045954, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1944
Epoch 1944, Loss: 0.00000002151674, Improvement: -0.00000000142043, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1945
Epoch 1945, Loss: 0.00000001636195, Improvement: -0.00000000515479, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1946
Epoch 1946, Loss: 0.00000001387687, Improvement: -0.00000000248508, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1947
Epoch 1947, Loss: 0.00000001349525, Improvement: -0.00000000038162, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1948
Epoch 1948, Loss: 0.00000001843215, Improvement: 0.00000000493690, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1949
Epoch 1949, Loss: 0.00000003766470, Improvement: 0.00000001923255, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000004215775, Improvement: 0.00000000449306, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1951
Epoch 1951, Loss: 0.00000007755118, Improvement: 0.00000003539342, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1952
Epoch 1952, Loss: 0.00000007493685, Improvement: -0.00000000261432, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1953
Epoch 1953, Loss: 0.00000006017234, Improvement: -0.00000001476451, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1954
Epoch 1954, Loss: 0.00000003321132, Improvement: -0.00000002696102, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1955
Epoch 1955, Loss: 0.00000001592301, Improvement: -0.00000001728832, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1956
Epoch 1956, Loss: 0.00000001347704, Improvement: -0.00000000244597, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1957
Epoch 1957, Loss: 0.00000001416600, Improvement: 0.00000000068897, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1958
Epoch 1958, Loss: 0.00000001758014, Improvement: 0.00000000341414, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1959
Epoch 1959, Loss: 0.00000001895149, Improvement: 0.00000000137135, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1960
Epoch 1960, Loss: 0.00000003029798, Improvement: 0.00000001134649, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1961
Epoch 1961, Loss: 0.00000002536489, Improvement: -0.00000000493309, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1962
Epoch 1962, Loss: 0.00000001927857, Improvement: -0.00000000608631, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1963
Epoch 1963, Loss: 0.00000001598860, Improvement: -0.00000000328997, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1964
Epoch 1964, Loss: 0.00000002015126, Improvement: 0.00000000416265, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1965
Epoch 1965, Loss: 0.00000002114260, Improvement: 0.00000000099135, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1966
Epoch 1966, Loss: 0.00000006454742, Improvement: 0.00000004340482, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1967
Epoch 1967, Loss: 0.00000014500395, Improvement: 0.00000008045653, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1968
Epoch 1968, Loss: 0.00000011155198, Improvement: -0.00000003345198, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1969
Epoch 1969, Loss: 0.00000004058378, Improvement: -0.00000007096820, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1970
Epoch 1970, Loss: 0.00000002679292, Improvement: -0.00000001379085, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1971
Epoch 1971, Loss: 0.00000002368155, Improvement: -0.00000000311138, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1972
Epoch 1972, Loss: 0.00000001479982, Improvement: -0.00000000888173, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1973
Epoch 1973, Loss: 0.00000001370496, Improvement: -0.00000000109486, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1974
Epoch 1974, Loss: 0.00000001276819, Improvement: -0.00000000093677, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1975
Epoch 1975, Loss: 0.00000001277306, Improvement: 0.00000000000487, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1976
Epoch 1976, Loss: 0.00000001354900, Improvement: 0.00000000077595, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1977
Epoch 1977, Loss: 0.00000001253797, Improvement: -0.00000000101103, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1978
Epoch 1978, Loss: 0.00000001254434, Improvement: 0.00000000000637, Best Loss: 0.00000000903386 in Epoch 1922
Epoch 1979
A best model at epoch 1979 has been saved with training error 0.00000000848212.
Epoch 1979, Loss: 0.00000001220855, Improvement: -0.00000000033579, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1980
Epoch 1980, Loss: 0.00000001424870, Improvement: 0.00000000204015, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1981
Epoch 1981, Loss: 0.00000001787916, Improvement: 0.00000000363046, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1982
Epoch 1982, Loss: 0.00000001319492, Improvement: -0.00000000468424, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1983
Epoch 1983, Loss: 0.00000001409077, Improvement: 0.00000000089585, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1984
Epoch 1984, Loss: 0.00000001983012, Improvement: 0.00000000573934, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1985
Epoch 1985, Loss: 0.00000003928580, Improvement: 0.00000001945569, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1986
Epoch 1986, Loss: 0.00000002574797, Improvement: -0.00000001353783, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1987
Epoch 1987, Loss: 0.00000001601386, Improvement: -0.00000000973411, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1988
Epoch 1988, Loss: 0.00000001534628, Improvement: -0.00000000066758, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1989
Epoch 1989, Loss: 0.00000001441745, Improvement: -0.00000000092883, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1990
Epoch 1990, Loss: 0.00000001483593, Improvement: 0.00000000041848, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1991
Epoch 1991, Loss: 0.00000002105769, Improvement: 0.00000000622177, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1992
Epoch 1992, Loss: 0.00000004510444, Improvement: 0.00000002404675, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1993
Epoch 1993, Loss: 0.00000004785402, Improvement: 0.00000000274957, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1994
Epoch 1994, Loss: 0.00000002041691, Improvement: -0.00000002743711, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1995
Epoch 1995, Loss: 0.00000001479686, Improvement: -0.00000000562005, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1996
Epoch 1996, Loss: 0.00000001315048, Improvement: -0.00000000164638, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1997
Epoch 1997, Loss: 0.00000001274727, Improvement: -0.00000000040321, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1998
Epoch 1998, Loss: 0.00000001179683, Improvement: -0.00000000095044, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 1999
Epoch 1999, Loss: 0.00000001406752, Improvement: 0.00000000227069, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000001314657, Improvement: -0.00000000092095, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2001
Epoch 2001, Loss: 0.00000001484268, Improvement: 0.00000000169612, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2002
Epoch 2002, Loss: 0.00000001596048, Improvement: 0.00000000111780, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2003
Epoch 2003, Loss: 0.00000001469990, Improvement: -0.00000000126058, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2004
Epoch 2004, Loss: 0.00000002400597, Improvement: 0.00000000930607, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2005
Epoch 2005, Loss: 0.00000003239900, Improvement: 0.00000000839302, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2006
Epoch 2006, Loss: 0.00000004379913, Improvement: 0.00000001140013, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2007
Epoch 2007, Loss: 0.00000005368306, Improvement: 0.00000000988393, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2008
Epoch 2008, Loss: 0.00000002633904, Improvement: -0.00000002734402, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2009
Epoch 2009, Loss: 0.00000007411842, Improvement: 0.00000004777938, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2010
Epoch 2010, Loss: 0.00000004883649, Improvement: -0.00000002528193, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2011
Epoch 2011, Loss: 0.00000005193823, Improvement: 0.00000000310174, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2012
Epoch 2012, Loss: 0.00000003090798, Improvement: -0.00000002103024, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2013
Epoch 2013, Loss: 0.00000002095780, Improvement: -0.00000000995018, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2014
Epoch 2014, Loss: 0.00000002293414, Improvement: 0.00000000197634, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2015
Epoch 2015, Loss: 0.00000001993804, Improvement: -0.00000000299610, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2016
Epoch 2016, Loss: 0.00000001508382, Improvement: -0.00000000485422, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2017
Epoch 2017, Loss: 0.00000001388435, Improvement: -0.00000000119947, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2018
Epoch 2018, Loss: 0.00000001295046, Improvement: -0.00000000093389, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2019
Epoch 2019, Loss: 0.00000001167251, Improvement: -0.00000000127796, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2020
Epoch 2020, Loss: 0.00000001321455, Improvement: 0.00000000154205, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2021
Epoch 2021, Loss: 0.00000001948405, Improvement: 0.00000000626949, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2022
Epoch 2022, Loss: 0.00000002131376, Improvement: 0.00000000182971, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2023
Epoch 2023, Loss: 0.00000002191561, Improvement: 0.00000000060184, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2024
Epoch 2024, Loss: 0.00000002575368, Improvement: 0.00000000383807, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2025
Epoch 2025, Loss: 0.00000003432644, Improvement: 0.00000000857276, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2026
Epoch 2026, Loss: 0.00000003028176, Improvement: -0.00000000404468, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2027
Epoch 2027, Loss: 0.00000002144206, Improvement: -0.00000000883970, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2028
Epoch 2028, Loss: 0.00000002599852, Improvement: 0.00000000455646, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2029
Epoch 2029, Loss: 0.00000006037785, Improvement: 0.00000003437933, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2030
Epoch 2030, Loss: 0.00000013089437, Improvement: 0.00000007051652, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2031
Epoch 2031, Loss: 0.00000009727247, Improvement: -0.00000003362190, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2032
Epoch 2032, Loss: 0.00000006982158, Improvement: -0.00000002745089, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2033
Epoch 2033, Loss: 0.00000002603669, Improvement: -0.00000004378490, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2034
Epoch 2034, Loss: 0.00000001891079, Improvement: -0.00000000712590, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2035
Epoch 2035, Loss: 0.00000001346046, Improvement: -0.00000000545033, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2036
Epoch 2036, Loss: 0.00000001188493, Improvement: -0.00000000157552, Best Loss: 0.00000000848212 in Epoch 1979
Epoch 2037
A best model at epoch 2037 has been saved with training error 0.00000000824245.
Epoch 2037, Loss: 0.00000001171815, Improvement: -0.00000000016678, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2038
Epoch 2038, Loss: 0.00000001161057, Improvement: -0.00000000010758, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2039
Epoch 2039, Loss: 0.00000001150439, Improvement: -0.00000000010618, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2040
Epoch 2040, Loss: 0.00000001242917, Improvement: 0.00000000092477, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2041
Epoch 2041, Loss: 0.00000001185048, Improvement: -0.00000000057869, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2042
Epoch 2042, Loss: 0.00000001214886, Improvement: 0.00000000029838, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2043
Epoch 2043, Loss: 0.00000001140811, Improvement: -0.00000000074075, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2044
Epoch 2044, Loss: 0.00000001172493, Improvement: 0.00000000031682, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2045
Epoch 2045, Loss: 0.00000001339516, Improvement: 0.00000000167023, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2046
Epoch 2046, Loss: 0.00000001619127, Improvement: 0.00000000279610, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2047
Epoch 2047, Loss: 0.00000002141443, Improvement: 0.00000000522316, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2048
Epoch 2048, Loss: 0.00000001994435, Improvement: -0.00000000147008, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2049
Epoch 2049, Loss: 0.00000001712501, Improvement: -0.00000000281935, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000001488188, Improvement: -0.00000000224313, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2051
Epoch 2051, Loss: 0.00000001359691, Improvement: -0.00000000128497, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2052
Epoch 2052, Loss: 0.00000001511895, Improvement: 0.00000000152204, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2053
Epoch 2053, Loss: 0.00000001240082, Improvement: -0.00000000271813, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2054
Epoch 2054, Loss: 0.00000002096759, Improvement: 0.00000000856678, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2055
Epoch 2055, Loss: 0.00000001989281, Improvement: -0.00000000107478, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2056
Epoch 2056, Loss: 0.00000002071793, Improvement: 0.00000000082512, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2057
Epoch 2057, Loss: 0.00000002040686, Improvement: -0.00000000031107, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2058
Epoch 2058, Loss: 0.00000001829605, Improvement: -0.00000000211082, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2059
Epoch 2059, Loss: 0.00000001511564, Improvement: -0.00000000318040, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2060
Epoch 2060, Loss: 0.00000001463402, Improvement: -0.00000000048162, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2061
Epoch 2061, Loss: 0.00000001464007, Improvement: 0.00000000000605, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2062
Epoch 2062, Loss: 0.00000001468576, Improvement: 0.00000000004569, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2063
Epoch 2063, Loss: 0.00000001472839, Improvement: 0.00000000004263, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2064
Epoch 2064, Loss: 0.00000001603523, Improvement: 0.00000000130685, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2065
Epoch 2065, Loss: 0.00000002206155, Improvement: 0.00000000602631, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2066
Epoch 2066, Loss: 0.00000002581142, Improvement: 0.00000000374987, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2067
Epoch 2067, Loss: 0.00000002674817, Improvement: 0.00000000093676, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2068
Epoch 2068, Loss: 0.00000002957597, Improvement: 0.00000000282779, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2069
Epoch 2069, Loss: 0.00000005563047, Improvement: 0.00000002605451, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2070
Epoch 2070, Loss: 0.00000004571410, Improvement: -0.00000000991637, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2071
Epoch 2071, Loss: 0.00000004740199, Improvement: 0.00000000168790, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2072
Epoch 2072, Loss: 0.00000005679502, Improvement: 0.00000000939303, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2073
Epoch 2073, Loss: 0.00000004186858, Improvement: -0.00000001492644, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2074
Epoch 2074, Loss: 0.00000001929044, Improvement: -0.00000002257814, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2075
Epoch 2075, Loss: 0.00000001599004, Improvement: -0.00000000330040, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2076
Epoch 2076, Loss: 0.00000001715239, Improvement: 0.00000000116235, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2077
Epoch 2077, Loss: 0.00000001447691, Improvement: -0.00000000267549, Best Loss: 0.00000000824245 in Epoch 2037
Epoch 2078
A best model at epoch 2078 has been saved with training error 0.00000000806993.
Epoch 2078, Loss: 0.00000001281839, Improvement: -0.00000000165852, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2079
Epoch 2079, Loss: 0.00000001249172, Improvement: -0.00000000032666, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2080
Epoch 2080, Loss: 0.00000001604230, Improvement: 0.00000000355058, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2081
Epoch 2081, Loss: 0.00000003145623, Improvement: 0.00000001541393, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2082
Epoch 2082, Loss: 0.00000003689663, Improvement: 0.00000000544040, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2083
Epoch 2083, Loss: 0.00000006359793, Improvement: 0.00000002670130, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2084
Epoch 2084, Loss: 0.00000012523386, Improvement: 0.00000006163593, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2085
Epoch 2085, Loss: 0.00000008694056, Improvement: -0.00000003829330, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2086
Epoch 2086, Loss: 0.00000007199112, Improvement: -0.00000001494944, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2087
Epoch 2087, Loss: 0.00000003096761, Improvement: -0.00000004102352, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2088
Epoch 2088, Loss: 0.00000001598269, Improvement: -0.00000001498492, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2089
Epoch 2089, Loss: 0.00000001220070, Improvement: -0.00000000378198, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2090
Epoch 2090, Loss: 0.00000001201126, Improvement: -0.00000000018944, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2091
Epoch 2091, Loss: 0.00000001215521, Improvement: 0.00000000014395, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2092
Epoch 2092, Loss: 0.00000001120077, Improvement: -0.00000000095445, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2093
Epoch 2093, Loss: 0.00000001099634, Improvement: -0.00000000020443, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2094
Epoch 2094, Loss: 0.00000001107763, Improvement: 0.00000000008129, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2095
Epoch 2095, Loss: 0.00000001263121, Improvement: 0.00000000155358, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2096
Epoch 2096, Loss: 0.00000001312890, Improvement: 0.00000000049769, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2097
Epoch 2097, Loss: 0.00000001746649, Improvement: 0.00000000433758, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2098
Epoch 2098, Loss: 0.00000001439342, Improvement: -0.00000000307306, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2099
Epoch 2099, Loss: 0.00000001281964, Improvement: -0.00000000157378, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000001360083, Improvement: 0.00000000078119, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2101
Epoch 2101, Loss: 0.00000002089263, Improvement: 0.00000000729180, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2102
Epoch 2102, Loss: 0.00000001710950, Improvement: -0.00000000378312, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2103
Epoch 2103, Loss: 0.00000001604427, Improvement: -0.00000000106524, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2104
Epoch 2104, Loss: 0.00000001205480, Improvement: -0.00000000398947, Best Loss: 0.00000000806993 in Epoch 2078
Epoch 2105
A best model at epoch 2105 has been saved with training error 0.00000000753109.
Epoch 2105, Loss: 0.00000001025817, Improvement: -0.00000000179663, Best Loss: 0.00000000753109 in Epoch 2105
Epoch 2106
Epoch 2106, Loss: 0.00000001015134, Improvement: -0.00000000010683, Best Loss: 0.00000000753109 in Epoch 2105
Epoch 2107
Epoch 2107, Loss: 0.00000001073128, Improvement: 0.00000000057995, Best Loss: 0.00000000753109 in Epoch 2105
Epoch 2108
Epoch 2108, Loss: 0.00000001009405, Improvement: -0.00000000063724, Best Loss: 0.00000000753109 in Epoch 2105
Epoch 2109
A best model at epoch 2109 has been saved with training error 0.00000000749786.
Epoch 2109, Loss: 0.00000001087149, Improvement: 0.00000000077745, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2110
Epoch 2110, Loss: 0.00000002489898, Improvement: 0.00000001402749, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2111
Epoch 2111, Loss: 0.00000001782647, Improvement: -0.00000000707251, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2112
Epoch 2112, Loss: 0.00000001302381, Improvement: -0.00000000480266, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2113
Epoch 2113, Loss: 0.00000001448917, Improvement: 0.00000000146536, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2114
Epoch 2114, Loss: 0.00000002006381, Improvement: 0.00000000557464, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2115
Epoch 2115, Loss: 0.00000004534374, Improvement: 0.00000002527993, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2116
Epoch 2116, Loss: 0.00000004888877, Improvement: 0.00000000354504, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2117
Epoch 2117, Loss: 0.00000004060208, Improvement: -0.00000000828670, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2118
Epoch 2118, Loss: 0.00000002445148, Improvement: -0.00000001615060, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2119
Epoch 2119, Loss: 0.00000001425096, Improvement: -0.00000001020052, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2120
Epoch 2120, Loss: 0.00000001207797, Improvement: -0.00000000217299, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2121
Epoch 2121, Loss: 0.00000001421580, Improvement: 0.00000000213782, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2122
Epoch 2122, Loss: 0.00000001580409, Improvement: 0.00000000158829, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2123
Epoch 2123, Loss: 0.00000001361542, Improvement: -0.00000000218867, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2124
Epoch 2124, Loss: 0.00000001765703, Improvement: 0.00000000404161, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2125
Epoch 2125, Loss: 0.00000001609305, Improvement: -0.00000000156398, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2126
Epoch 2126, Loss: 0.00000002187119, Improvement: 0.00000000577814, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2127
Epoch 2127, Loss: 0.00000001766645, Improvement: -0.00000000420474, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2128
Epoch 2128, Loss: 0.00000001265190, Improvement: -0.00000000501455, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2129
Epoch 2129, Loss: 0.00000001206807, Improvement: -0.00000000058383, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2130
Epoch 2130, Loss: 0.00000001428746, Improvement: 0.00000000221939, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2131
Epoch 2131, Loss: 0.00000002074180, Improvement: 0.00000000645434, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2132
Epoch 2132, Loss: 0.00000004783978, Improvement: 0.00000002709798, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2133
Epoch 2133, Loss: 0.00000008644357, Improvement: 0.00000003860379, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2134
Epoch 2134, Loss: 0.00000003477706, Improvement: -0.00000005166651, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2135
Epoch 2135, Loss: 0.00000001736402, Improvement: -0.00000001741304, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2136
Epoch 2136, Loss: 0.00000001560169, Improvement: -0.00000000176233, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2137
Epoch 2137, Loss: 0.00000001268978, Improvement: -0.00000000291191, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2138
Epoch 2138, Loss: 0.00000001308532, Improvement: 0.00000000039554, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2139
Epoch 2139, Loss: 0.00000001299721, Improvement: -0.00000000008811, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2140
Epoch 2140, Loss: 0.00000001101690, Improvement: -0.00000000198031, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2141
Epoch 2141, Loss: 0.00000001152538, Improvement: 0.00000000050848, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2142
Epoch 2142, Loss: 0.00000001796789, Improvement: 0.00000000644251, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2143
Epoch 2143, Loss: 0.00000001635348, Improvement: -0.00000000161441, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2144
Epoch 2144, Loss: 0.00000001560193, Improvement: -0.00000000075155, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2145
Epoch 2145, Loss: 0.00000001304995, Improvement: -0.00000000255198, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2146
Epoch 2146, Loss: 0.00000001435489, Improvement: 0.00000000130494, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2147
Epoch 2147, Loss: 0.00000001564537, Improvement: 0.00000000129048, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2148
Epoch 2148, Loss: 0.00000001227018, Improvement: -0.00000000337518, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2149
Epoch 2149, Loss: 0.00000001236843, Improvement: 0.00000000009824, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000001560324, Improvement: 0.00000000323481, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2151
Epoch 2151, Loss: 0.00000003566503, Improvement: 0.00000002006179, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2152
Epoch 2152, Loss: 0.00000001932834, Improvement: -0.00000001633668, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2153
Epoch 2153, Loss: 0.00000006998582, Improvement: 0.00000005065748, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2154
Epoch 2154, Loss: 0.00000005570505, Improvement: -0.00000001428077, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2155
Epoch 2155, Loss: 0.00000003530862, Improvement: -0.00000002039643, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2156
Epoch 2156, Loss: 0.00000003032766, Improvement: -0.00000000498096, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2157
Epoch 2157, Loss: 0.00000001642061, Improvement: -0.00000001390705, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2158
Epoch 2158, Loss: 0.00000001772897, Improvement: 0.00000000130836, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2159
Epoch 2159, Loss: 0.00000001631872, Improvement: -0.00000000141025, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2160
Epoch 2160, Loss: 0.00000001956465, Improvement: 0.00000000324593, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2161
Epoch 2161, Loss: 0.00000003709177, Improvement: 0.00000001752712, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2162
Epoch 2162, Loss: 0.00000005258317, Improvement: 0.00000001549140, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2163
Epoch 2163, Loss: 0.00000002742813, Improvement: -0.00000002515503, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2164
Epoch 2164, Loss: 0.00000001545715, Improvement: -0.00000001197099, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2165
Epoch 2165, Loss: 0.00000001273950, Improvement: -0.00000000271765, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2166
Epoch 2166, Loss: 0.00000001583364, Improvement: 0.00000000309414, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2167
Epoch 2167, Loss: 0.00000001777443, Improvement: 0.00000000194079, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2168
Epoch 2168, Loss: 0.00000001835548, Improvement: 0.00000000058106, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2169
Epoch 2169, Loss: 0.00000001370191, Improvement: -0.00000000465358, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2170
Epoch 2170, Loss: 0.00000001726238, Improvement: 0.00000000356047, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2171
Epoch 2171, Loss: 0.00000002253550, Improvement: 0.00000000527312, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2172
Epoch 2172, Loss: 0.00000002446397, Improvement: 0.00000000192847, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2173
Epoch 2173, Loss: 0.00000003149170, Improvement: 0.00000000702773, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2174
Epoch 2174, Loss: 0.00000002534171, Improvement: -0.00000000614999, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2175
Epoch 2175, Loss: 0.00000002209727, Improvement: -0.00000000324444, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2176
Epoch 2176, Loss: 0.00000001763896, Improvement: -0.00000000445832, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2177
Epoch 2177, Loss: 0.00000003976777, Improvement: 0.00000002212882, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2178
Epoch 2178, Loss: 0.00000006148037, Improvement: 0.00000002171260, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2179
Epoch 2179, Loss: 0.00000006507187, Improvement: 0.00000000359150, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2180
Epoch 2180, Loss: 0.00000007112447, Improvement: 0.00000000605260, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2181
Epoch 2181, Loss: 0.00000007116389, Improvement: 0.00000000003942, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2182
Epoch 2182, Loss: 0.00000003143341, Improvement: -0.00000003973048, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2183
Epoch 2183, Loss: 0.00000001402991, Improvement: -0.00000001740350, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2184
Epoch 2184, Loss: 0.00000001139711, Improvement: -0.00000000263280, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2185
Epoch 2185, Loss: 0.00000001064505, Improvement: -0.00000000075206, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2186
Epoch 2186, Loss: 0.00000001074886, Improvement: 0.00000000010380, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2187
Epoch 2187, Loss: 0.00000001179005, Improvement: 0.00000000104119, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2188
Epoch 2188, Loss: 0.00000001099036, Improvement: -0.00000000079968, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2189
Epoch 2189, Loss: 0.00000001231837, Improvement: 0.00000000132801, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2190
Epoch 2190, Loss: 0.00000001740231, Improvement: 0.00000000508394, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2191
Epoch 2191, Loss: 0.00000002756988, Improvement: 0.00000001016757, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2192
Epoch 2192, Loss: 0.00000001788128, Improvement: -0.00000000968860, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2193
Epoch 2193, Loss: 0.00000001562134, Improvement: -0.00000000225994, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2194
Epoch 2194, Loss: 0.00000002100249, Improvement: 0.00000000538114, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2195
Epoch 2195, Loss: 0.00000002027139, Improvement: -0.00000000073110, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2196
Epoch 2196, Loss: 0.00000001457635, Improvement: -0.00000000569504, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2197
Epoch 2197, Loss: 0.00000001817075, Improvement: 0.00000000359441, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2198
Epoch 2198, Loss: 0.00000001516437, Improvement: -0.00000000300638, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2199
Epoch 2199, Loss: 0.00000001563003, Improvement: 0.00000000046565, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000001779937, Improvement: 0.00000000216934, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2201
Epoch 2201, Loss: 0.00000004046849, Improvement: 0.00000002266913, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2202
Epoch 2202, Loss: 0.00000002294472, Improvement: -0.00000001752378, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2203
Epoch 2203, Loss: 0.00000002356694, Improvement: 0.00000000062223, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2204
Epoch 2204, Loss: 0.00000006669435, Improvement: 0.00000004312741, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2205
Epoch 2205, Loss: 0.00000009483581, Improvement: 0.00000002814145, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2206
Epoch 2206, Loss: 0.00000012859111, Improvement: 0.00000003375530, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2207
Epoch 2207, Loss: 0.00000011217134, Improvement: -0.00000001641977, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2208
Epoch 2208, Loss: 0.00000007211803, Improvement: -0.00000004005331, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2209
Epoch 2209, Loss: 0.00000004520006, Improvement: -0.00000002691797, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2210
Epoch 2210, Loss: 0.00000001797653, Improvement: -0.00000002722353, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2211
Epoch 2211, Loss: 0.00000001379155, Improvement: -0.00000000418498, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2212
Epoch 2212, Loss: 0.00000001307827, Improvement: -0.00000000071328, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2213
Epoch 2213, Loss: 0.00000001248003, Improvement: -0.00000000059825, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2214
Epoch 2214, Loss: 0.00000001241660, Improvement: -0.00000000006343, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2215
Epoch 2215, Loss: 0.00000001108900, Improvement: -0.00000000132760, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2216
Epoch 2216, Loss: 0.00000001056083, Improvement: -0.00000000052817, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2217
Epoch 2217, Loss: 0.00000001038394, Improvement: -0.00000000017689, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2218
Epoch 2218, Loss: 0.00000000997257, Improvement: -0.00000000041137, Best Loss: 0.00000000749786 in Epoch 2109
Epoch 2219
A best model at epoch 2219 has been saved with training error 0.00000000734366.
Epoch 2219, Loss: 0.00000000997220, Improvement: -0.00000000000037, Best Loss: 0.00000000734366 in Epoch 2219
Epoch 2220
A best model at epoch 2220 has been saved with training error 0.00000000685042.
Epoch 2220, Loss: 0.00000000983767, Improvement: -0.00000000013453, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2221
Epoch 2221, Loss: 0.00000001044957, Improvement: 0.00000000061191, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2222
Epoch 2222, Loss: 0.00000001246107, Improvement: 0.00000000201150, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2223
Epoch 2223, Loss: 0.00000001530941, Improvement: 0.00000000284834, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2224
Epoch 2224, Loss: 0.00000001522804, Improvement: -0.00000000008137, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2225
Epoch 2225, Loss: 0.00000001391888, Improvement: -0.00000000130917, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2226
Epoch 2226, Loss: 0.00000001437648, Improvement: 0.00000000045760, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2227
Epoch 2227, Loss: 0.00000001293219, Improvement: -0.00000000144429, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2228
Epoch 2228, Loss: 0.00000001250623, Improvement: -0.00000000042597, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2229
Epoch 2229, Loss: 0.00000001362850, Improvement: 0.00000000112227, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2230
Epoch 2230, Loss: 0.00000001268996, Improvement: -0.00000000093854, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2231
Epoch 2231, Loss: 0.00000001306889, Improvement: 0.00000000037892, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2232
Epoch 2232, Loss: 0.00000001231135, Improvement: -0.00000000075753, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2233
Epoch 2233, Loss: 0.00000001109074, Improvement: -0.00000000122061, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2234
Epoch 2234, Loss: 0.00000001148873, Improvement: 0.00000000039799, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2235
Epoch 2235, Loss: 0.00000001016176, Improvement: -0.00000000132698, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2236
Epoch 2236, Loss: 0.00000001497892, Improvement: 0.00000000481717, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2237
Epoch 2237, Loss: 0.00000004148527, Improvement: 0.00000002650634, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2238
Epoch 2238, Loss: 0.00000005615487, Improvement: 0.00000001466960, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2239
Epoch 2239, Loss: 0.00000005894793, Improvement: 0.00000000279307, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2240
Epoch 2240, Loss: 0.00000003766700, Improvement: -0.00000002128094, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2241
Epoch 2241, Loss: 0.00000003669960, Improvement: -0.00000000096740, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2242
Epoch 2242, Loss: 0.00000002931533, Improvement: -0.00000000738427, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2243
Epoch 2243, Loss: 0.00000002288835, Improvement: -0.00000000642698, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2244
Epoch 2244, Loss: 0.00000001992022, Improvement: -0.00000000296813, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2245
Epoch 2245, Loss: 0.00000001862422, Improvement: -0.00000000129600, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2246
Epoch 2246, Loss: 0.00000006901065, Improvement: 0.00000005038643, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2247
Epoch 2247, Loss: 0.00000003490740, Improvement: -0.00000003410325, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2248
Epoch 2248, Loss: 0.00000001631931, Improvement: -0.00000001858810, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2249
Epoch 2249, Loss: 0.00000001563891, Improvement: -0.00000000068040, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000001344376, Improvement: -0.00000000219515, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2251
Epoch 2251, Loss: 0.00000001133801, Improvement: -0.00000000210575, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2252
Epoch 2252, Loss: 0.00000001300106, Improvement: 0.00000000166306, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2253
Epoch 2253, Loss: 0.00000001409118, Improvement: 0.00000000109012, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2254
Epoch 2254, Loss: 0.00000001378208, Improvement: -0.00000000030910, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2255
Epoch 2255, Loss: 0.00000001047764, Improvement: -0.00000000330444, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2256
Epoch 2256, Loss: 0.00000001428300, Improvement: 0.00000000380536, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2257
Epoch 2257, Loss: 0.00000001201956, Improvement: -0.00000000226344, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2258
Epoch 2258, Loss: 0.00000001459146, Improvement: 0.00000000257190, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2259
Epoch 2259, Loss: 0.00000001948358, Improvement: 0.00000000489212, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2260
Epoch 2260, Loss: 0.00000002203476, Improvement: 0.00000000255118, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2261
Epoch 2261, Loss: 0.00000001623060, Improvement: -0.00000000580416, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2262
Epoch 2262, Loss: 0.00000002474142, Improvement: 0.00000000851082, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2263
Epoch 2263, Loss: 0.00000002920591, Improvement: 0.00000000446449, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2264
Epoch 2264, Loss: 0.00000003411298, Improvement: 0.00000000490708, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2265
Epoch 2265, Loss: 0.00000004815179, Improvement: 0.00000001403880, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2266
Epoch 2266, Loss: 0.00000002967695, Improvement: -0.00000001847484, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2267
Epoch 2267, Loss: 0.00000002614765, Improvement: -0.00000000352930, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2268
Epoch 2268, Loss: 0.00000002675409, Improvement: 0.00000000060643, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2269
Epoch 2269, Loss: 0.00000001571643, Improvement: -0.00000001103766, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2270
Epoch 2270, Loss: 0.00000001117501, Improvement: -0.00000000454142, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2271
Epoch 2271, Loss: 0.00000001065030, Improvement: -0.00000000052471, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2272
Epoch 2272, Loss: 0.00000001454382, Improvement: 0.00000000389352, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2273
Epoch 2273, Loss: 0.00000001606226, Improvement: 0.00000000151845, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2274
Epoch 2274, Loss: 0.00000001352476, Improvement: -0.00000000253750, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2275
Epoch 2275, Loss: 0.00000001693668, Improvement: 0.00000000341191, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2276
Epoch 2276, Loss: 0.00000002718361, Improvement: 0.00000001024694, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2277
Epoch 2277, Loss: 0.00000005139993, Improvement: 0.00000002421631, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2278
Epoch 2278, Loss: 0.00000010579130, Improvement: 0.00000005439137, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2279
Epoch 2279, Loss: 0.00000004905469, Improvement: -0.00000005673660, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2280
Epoch 2280, Loss: 0.00000002350542, Improvement: -0.00000002554928, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2281
Epoch 2281, Loss: 0.00000001373675, Improvement: -0.00000000976867, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2282
Epoch 2282, Loss: 0.00000001501991, Improvement: 0.00000000128316, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2283
Epoch 2283, Loss: 0.00000002334793, Improvement: 0.00000000832802, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2284
Epoch 2284, Loss: 0.00000003138978, Improvement: 0.00000000804186, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2285
Epoch 2285, Loss: 0.00000001813073, Improvement: -0.00000001325905, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2286
Epoch 2286, Loss: 0.00000001203848, Improvement: -0.00000000609225, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2287
Epoch 2287, Loss: 0.00000000987530, Improvement: -0.00000000216318, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2288
Epoch 2288, Loss: 0.00000001760479, Improvement: 0.00000000772950, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2289
Epoch 2289, Loss: 0.00000004049702, Improvement: 0.00000002289223, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2290
Epoch 2290, Loss: 0.00000002848138, Improvement: -0.00000001201564, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2291
Epoch 2291, Loss: 0.00000002913610, Improvement: 0.00000000065472, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2292
Epoch 2292, Loss: 0.00000002092925, Improvement: -0.00000000820685, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2293
Epoch 2293, Loss: 0.00000001530064, Improvement: -0.00000000562860, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2294
Epoch 2294, Loss: 0.00000001134494, Improvement: -0.00000000395570, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2295
Epoch 2295, Loss: 0.00000001175116, Improvement: 0.00000000040622, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2296
Epoch 2296, Loss: 0.00000001021409, Improvement: -0.00000000153707, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2297
Epoch 2297, Loss: 0.00000001043712, Improvement: 0.00000000022303, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2298
Epoch 2298, Loss: 0.00000001159800, Improvement: 0.00000000116089, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2299
Epoch 2299, Loss: 0.00000001763726, Improvement: 0.00000000603926, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000001729702, Improvement: -0.00000000034025, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2301
Epoch 2301, Loss: 0.00000001263851, Improvement: -0.00000000465851, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2302
Epoch 2302, Loss: 0.00000001208010, Improvement: -0.00000000055840, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2303
Epoch 2303, Loss: 0.00000001069475, Improvement: -0.00000000138536, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2304
Epoch 2304, Loss: 0.00000001256207, Improvement: 0.00000000186732, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2305
Epoch 2305, Loss: 0.00000001347936, Improvement: 0.00000000091729, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2306
Epoch 2306, Loss: 0.00000001859329, Improvement: 0.00000000511393, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2307
Epoch 2307, Loss: 0.00000006027331, Improvement: 0.00000004168002, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2308
Epoch 2308, Loss: 0.00000005520417, Improvement: -0.00000000506913, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2309
Epoch 2309, Loss: 0.00000005408740, Improvement: -0.00000000111678, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2310
Epoch 2310, Loss: 0.00000004333062, Improvement: -0.00000001075678, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2311
Epoch 2311, Loss: 0.00000002005846, Improvement: -0.00000002327216, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2312
Epoch 2312, Loss: 0.00000001702429, Improvement: -0.00000000303417, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2313
Epoch 2313, Loss: 0.00000001260644, Improvement: -0.00000000441785, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2314
Epoch 2314, Loss: 0.00000000999464, Improvement: -0.00000000261180, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2315
Epoch 2315, Loss: 0.00000001000713, Improvement: 0.00000000001248, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2316
Epoch 2316, Loss: 0.00000001373671, Improvement: 0.00000000372958, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2317
Epoch 2317, Loss: 0.00000001188881, Improvement: -0.00000000184790, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2318
Epoch 2318, Loss: 0.00000001104851, Improvement: -0.00000000084030, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2319
Epoch 2319, Loss: 0.00000000997719, Improvement: -0.00000000107132, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2320
Epoch 2320, Loss: 0.00000001225371, Improvement: 0.00000000227652, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2321
Epoch 2321, Loss: 0.00000001694722, Improvement: 0.00000000469351, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2322
Epoch 2322, Loss: 0.00000001303212, Improvement: -0.00000000391511, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2323
Epoch 2323, Loss: 0.00000001865589, Improvement: 0.00000000562377, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2324
Epoch 2324, Loss: 0.00000001547956, Improvement: -0.00000000317633, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2325
Epoch 2325, Loss: 0.00000002062686, Improvement: 0.00000000514730, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2326
Epoch 2326, Loss: 0.00000001611267, Improvement: -0.00000000451419, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2327
Epoch 2327, Loss: 0.00000001524096, Improvement: -0.00000000087171, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2328
Epoch 2328, Loss: 0.00000001538364, Improvement: 0.00000000014267, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2329
Epoch 2329, Loss: 0.00000002432640, Improvement: 0.00000000894276, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2330
Epoch 2330, Loss: 0.00000002443168, Improvement: 0.00000000010528, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2331
Epoch 2331, Loss: 0.00000001932367, Improvement: -0.00000000510801, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2332
Epoch 2332, Loss: 0.00000002405375, Improvement: 0.00000000473008, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2333
Epoch 2333, Loss: 0.00000003170114, Improvement: 0.00000000764740, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2334
Epoch 2334, Loss: 0.00000006707631, Improvement: 0.00000003537516, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2335
Epoch 2335, Loss: 0.00000006482394, Improvement: -0.00000000225237, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2336
Epoch 2336, Loss: 0.00000007900344, Improvement: 0.00000001417951, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2337
Epoch 2337, Loss: 0.00000005530244, Improvement: -0.00000002370100, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2338
Epoch 2338, Loss: 0.00000004252366, Improvement: -0.00000001277878, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2339
Epoch 2339, Loss: 0.00000002770864, Improvement: -0.00000001481502, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2340
Epoch 2340, Loss: 0.00000001532625, Improvement: -0.00000001238239, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2341
Epoch 2341, Loss: 0.00000001263949, Improvement: -0.00000000268676, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2342
Epoch 2342, Loss: 0.00000001161799, Improvement: -0.00000000102150, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2343
Epoch 2343, Loss: 0.00000000958656, Improvement: -0.00000000203143, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2344
Epoch 2344, Loss: 0.00000001023905, Improvement: 0.00000000065249, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2345
Epoch 2345, Loss: 0.00000001169421, Improvement: 0.00000000145516, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2346
Epoch 2346, Loss: 0.00000001222060, Improvement: 0.00000000052639, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2347
Epoch 2347, Loss: 0.00000001003488, Improvement: -0.00000000218571, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2348
Epoch 2348, Loss: 0.00000000972568, Improvement: -0.00000000030921, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2349
Epoch 2349, Loss: 0.00000001069943, Improvement: 0.00000000097376, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000001120538, Improvement: 0.00000000050595, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2351
Epoch 2351, Loss: 0.00000001291636, Improvement: 0.00000000171098, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2352
Epoch 2352, Loss: 0.00000001222208, Improvement: -0.00000000069428, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2353
Epoch 2353, Loss: 0.00000001245302, Improvement: 0.00000000023094, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2354
Epoch 2354, Loss: 0.00000001155953, Improvement: -0.00000000089349, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2355
Epoch 2355, Loss: 0.00000001841178, Improvement: 0.00000000685225, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2356
Epoch 2356, Loss: 0.00000001538374, Improvement: -0.00000000302804, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2357
Epoch 2357, Loss: 0.00000005311849, Improvement: 0.00000003773475, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2358
Epoch 2358, Loss: 0.00000004340051, Improvement: -0.00000000971798, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2359
Epoch 2359, Loss: 0.00000003035911, Improvement: -0.00000001304140, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2360
Epoch 2360, Loss: 0.00000002562067, Improvement: -0.00000000473843, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2361
Epoch 2361, Loss: 0.00000001304709, Improvement: -0.00000001257359, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2362
Epoch 2362, Loss: 0.00000000975619, Improvement: -0.00000000329090, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2363
Epoch 2363, Loss: 0.00000000965979, Improvement: -0.00000000009640, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2364
Epoch 2364, Loss: 0.00000000942346, Improvement: -0.00000000023633, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2365
Epoch 2365, Loss: 0.00000000961089, Improvement: 0.00000000018743, Best Loss: 0.00000000685042 in Epoch 2220
Epoch 2366
A best model at epoch 2366 has been saved with training error 0.00000000633113.
Epoch 2366, Loss: 0.00000001040924, Improvement: 0.00000000079835, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2367
Epoch 2367, Loss: 0.00000001250377, Improvement: 0.00000000209453, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2368
Epoch 2368, Loss: 0.00000001351527, Improvement: 0.00000000101150, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2369
Epoch 2369, Loss: 0.00000001587961, Improvement: 0.00000000236434, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2370
Epoch 2370, Loss: 0.00000001811462, Improvement: 0.00000000223502, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2371
Epoch 2371, Loss: 0.00000001732992, Improvement: -0.00000000078470, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2372
Epoch 2372, Loss: 0.00000001946381, Improvement: 0.00000000213389, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2373
Epoch 2373, Loss: 0.00000003049167, Improvement: 0.00000001102786, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2374
Epoch 2374, Loss: 0.00000002261788, Improvement: -0.00000000787378, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2375
Epoch 2375, Loss: 0.00000001954456, Improvement: -0.00000000307333, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2376
Epoch 2376, Loss: 0.00000001922568, Improvement: -0.00000000031888, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2377
Epoch 2377, Loss: 0.00000002197529, Improvement: 0.00000000274961, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2378
Epoch 2378, Loss: 0.00000002280073, Improvement: 0.00000000082544, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2379
Epoch 2379, Loss: 0.00000003198903, Improvement: 0.00000000918830, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2380
Epoch 2380, Loss: 0.00000002910487, Improvement: -0.00000000288417, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2381
Epoch 2381, Loss: 0.00000004759493, Improvement: 0.00000001849007, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2382
Epoch 2382, Loss: 0.00000005649433, Improvement: 0.00000000889940, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2383
Epoch 2383, Loss: 0.00000005965809, Improvement: 0.00000000316376, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2384
Epoch 2384, Loss: 0.00000002544674, Improvement: -0.00000003421135, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2385
Epoch 2385, Loss: 0.00000002081018, Improvement: -0.00000000463656, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2386
Epoch 2386, Loss: 0.00000001740698, Improvement: -0.00000000340320, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2387
Epoch 2387, Loss: 0.00000001397832, Improvement: -0.00000000342865, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2388
Epoch 2388, Loss: 0.00000001068630, Improvement: -0.00000000329203, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2389
Epoch 2389, Loss: 0.00000001051359, Improvement: -0.00000000017271, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2390
Epoch 2390, Loss: 0.00000001213360, Improvement: 0.00000000162002, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2391
Epoch 2391, Loss: 0.00000001067148, Improvement: -0.00000000146212, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2392
Epoch 2392, Loss: 0.00000001030402, Improvement: -0.00000000036746, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2393
Epoch 2393, Loss: 0.00000001409761, Improvement: 0.00000000379359, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2394
Epoch 2394, Loss: 0.00000001064572, Improvement: -0.00000000345188, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2395
Epoch 2395, Loss: 0.00000000970311, Improvement: -0.00000000094261, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2396
Epoch 2396, Loss: 0.00000001026437, Improvement: 0.00000000056126, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2397
Epoch 2397, Loss: 0.00000001306008, Improvement: 0.00000000279571, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2398
Epoch 2398, Loss: 0.00000001585463, Improvement: 0.00000000279455, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2399
Epoch 2399, Loss: 0.00000001788654, Improvement: 0.00000000203191, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000001183689, Improvement: -0.00000000604965, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2401
Epoch 2401, Loss: 0.00000001132716, Improvement: -0.00000000050973, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2402
Epoch 2402, Loss: 0.00000001079347, Improvement: -0.00000000053369, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2403
Epoch 2403, Loss: 0.00000001140547, Improvement: 0.00000000061201, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2404
Epoch 2404, Loss: 0.00000001452267, Improvement: 0.00000000311719, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2405
Epoch 2405, Loss: 0.00000001145962, Improvement: -0.00000000306305, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2406
Epoch 2406, Loss: 0.00000001799183, Improvement: 0.00000000653221, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2407
Epoch 2407, Loss: 0.00000001942670, Improvement: 0.00000000143487, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2408
Epoch 2408, Loss: 0.00000002906006, Improvement: 0.00000000963336, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2409
Epoch 2409, Loss: 0.00000008182814, Improvement: 0.00000005276808, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2410
Epoch 2410, Loss: 0.00000004917214, Improvement: -0.00000003265600, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2411
Epoch 2411, Loss: 0.00000002450358, Improvement: -0.00000002466856, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2412
Epoch 2412, Loss: 0.00000001950870, Improvement: -0.00000000499488, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2413
Epoch 2413, Loss: 0.00000001472641, Improvement: -0.00000000478229, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2414
Epoch 2414, Loss: 0.00000001217868, Improvement: -0.00000000254772, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2415
Epoch 2415, Loss: 0.00000000985643, Improvement: -0.00000000232225, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2416
Epoch 2416, Loss: 0.00000000903732, Improvement: -0.00000000081912, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2417
Epoch 2417, Loss: 0.00000000945240, Improvement: 0.00000000041508, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2418
Epoch 2418, Loss: 0.00000001096076, Improvement: 0.00000000150836, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2419
Epoch 2419, Loss: 0.00000001677998, Improvement: 0.00000000581923, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2420
Epoch 2420, Loss: 0.00000003436734, Improvement: 0.00000001758736, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2421
Epoch 2421, Loss: 0.00000001908133, Improvement: -0.00000001528601, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2422
Epoch 2422, Loss: 0.00000002182426, Improvement: 0.00000000274293, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2423
Epoch 2423, Loss: 0.00000002014460, Improvement: -0.00000000167966, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2424
Epoch 2424, Loss: 0.00000002542238, Improvement: 0.00000000527778, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2425
Epoch 2425, Loss: 0.00000003793757, Improvement: 0.00000001251519, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2426
Epoch 2426, Loss: 0.00000002399739, Improvement: -0.00000001394018, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2427
Epoch 2427, Loss: 0.00000002658374, Improvement: 0.00000000258635, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2428
Epoch 2428, Loss: 0.00000001826548, Improvement: -0.00000000831826, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2429
Epoch 2429, Loss: 0.00000001680242, Improvement: -0.00000000146306, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2430
Epoch 2430, Loss: 0.00000001480171, Improvement: -0.00000000200071, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2431
Epoch 2431, Loss: 0.00000001119809, Improvement: -0.00000000360362, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2432
Epoch 2432, Loss: 0.00000001109888, Improvement: -0.00000000009921, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2433
Epoch 2433, Loss: 0.00000001529941, Improvement: 0.00000000420053, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2434
Epoch 2434, Loss: 0.00000002823973, Improvement: 0.00000001294032, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2435
Epoch 2435, Loss: 0.00000003741187, Improvement: 0.00000000917213, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2436
Epoch 2436, Loss: 0.00000005630229, Improvement: 0.00000001889043, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2437
Epoch 2437, Loss: 0.00000005195842, Improvement: -0.00000000434388, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2438
Epoch 2438, Loss: 0.00000003322994, Improvement: -0.00000001872848, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2439
Epoch 2439, Loss: 0.00000001674906, Improvement: -0.00000001648088, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2440
Epoch 2440, Loss: 0.00000001245707, Improvement: -0.00000000429199, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2441
Epoch 2441, Loss: 0.00000001165073, Improvement: -0.00000000080634, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2442
Epoch 2442, Loss: 0.00000001075695, Improvement: -0.00000000089378, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2443
Epoch 2443, Loss: 0.00000001135017, Improvement: 0.00000000059321, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2444
Epoch 2444, Loss: 0.00000001011434, Improvement: -0.00000000123583, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2445
Epoch 2445, Loss: 0.00000000940444, Improvement: -0.00000000070990, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2446
Epoch 2446, Loss: 0.00000000926949, Improvement: -0.00000000013495, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2447
Epoch 2447, Loss: 0.00000000912644, Improvement: -0.00000000014305, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2448
Epoch 2448, Loss: 0.00000001002987, Improvement: 0.00000000090342, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2449
Epoch 2449, Loss: 0.00000001013957, Improvement: 0.00000000010971, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000002332421, Improvement: 0.00000001318464, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2451
Epoch 2451, Loss: 0.00000004051894, Improvement: 0.00000001719473, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2452
Epoch 2452, Loss: 0.00000002950567, Improvement: -0.00000001101327, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2453
Epoch 2453, Loss: 0.00000002236239, Improvement: -0.00000000714328, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2454
Epoch 2454, Loss: 0.00000001676537, Improvement: -0.00000000559702, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2455
Epoch 2455, Loss: 0.00000001492517, Improvement: -0.00000000184020, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2456
Epoch 2456, Loss: 0.00000001179402, Improvement: -0.00000000313116, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2457
Epoch 2457, Loss: 0.00000002531226, Improvement: 0.00000001351825, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2458
Epoch 2458, Loss: 0.00000006845314, Improvement: 0.00000004314088, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2459
Epoch 2459, Loss: 0.00000005175999, Improvement: -0.00000001669316, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2460
Epoch 2460, Loss: 0.00000002628112, Improvement: -0.00000002547887, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2461
Epoch 2461, Loss: 0.00000001495524, Improvement: -0.00000001132588, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2462
Epoch 2462, Loss: 0.00000001179199, Improvement: -0.00000000316325, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2463
Epoch 2463, Loss: 0.00000000971394, Improvement: -0.00000000207805, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2464
Epoch 2464, Loss: 0.00000001004682, Improvement: 0.00000000033288, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2465
Epoch 2465, Loss: 0.00000000911580, Improvement: -0.00000000093102, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2466
Epoch 2466, Loss: 0.00000000965774, Improvement: 0.00000000054194, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2467
Epoch 2467, Loss: 0.00000001027464, Improvement: 0.00000000061690, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2468
Epoch 2468, Loss: 0.00000001048708, Improvement: 0.00000000021245, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2469
Epoch 2469, Loss: 0.00000001054341, Improvement: 0.00000000005632, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2470
Epoch 2470, Loss: 0.00000001349705, Improvement: 0.00000000295364, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2471
Epoch 2471, Loss: 0.00000001310064, Improvement: -0.00000000039642, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2472
Epoch 2472, Loss: 0.00000001656285, Improvement: 0.00000000346221, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2473
Epoch 2473, Loss: 0.00000001916253, Improvement: 0.00000000259969, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2474
Epoch 2474, Loss: 0.00000002719352, Improvement: 0.00000000803099, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2475
Epoch 2475, Loss: 0.00000002746807, Improvement: 0.00000000027455, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2476
Epoch 2476, Loss: 0.00000003735483, Improvement: 0.00000000988676, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2477
Epoch 2477, Loss: 0.00000002793731, Improvement: -0.00000000941753, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2478
Epoch 2478, Loss: 0.00000001710467, Improvement: -0.00000001083264, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2479
Epoch 2479, Loss: 0.00000001607348, Improvement: -0.00000000103119, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2480
Epoch 2480, Loss: 0.00000001522203, Improvement: -0.00000000085145, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2481
Epoch 2481, Loss: 0.00000001338888, Improvement: -0.00000000183316, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2482
Epoch 2482, Loss: 0.00000001471316, Improvement: 0.00000000132428, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2483
Epoch 2483, Loss: 0.00000001766484, Improvement: 0.00000000295168, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2484
Epoch 2484, Loss: 0.00000003801578, Improvement: 0.00000002035094, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2485
Epoch 2485, Loss: 0.00000005778325, Improvement: 0.00000001976747, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2486
Epoch 2486, Loss: 0.00000002133401, Improvement: -0.00000003644924, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2487
Epoch 2487, Loss: 0.00000001365872, Improvement: -0.00000000767529, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2488
Epoch 2488, Loss: 0.00000001079154, Improvement: -0.00000000286719, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2489
Epoch 2489, Loss: 0.00000001187269, Improvement: 0.00000000108116, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2490
Epoch 2490, Loss: 0.00000001138316, Improvement: -0.00000000048953, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2491
Epoch 2491, Loss: 0.00000001387083, Improvement: 0.00000000248767, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2492
Epoch 2492, Loss: 0.00000001546334, Improvement: 0.00000000159251, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2493
Epoch 2493, Loss: 0.00000001283998, Improvement: -0.00000000262336, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2494
Epoch 2494, Loss: 0.00000000995377, Improvement: -0.00000000288621, Best Loss: 0.00000000633113 in Epoch 2366
Epoch 2495
A best model at epoch 2495 has been saved with training error 0.00000000612936.
Epoch 2495, Loss: 0.00000000957671, Improvement: -0.00000000037707, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2496
Epoch 2496, Loss: 0.00000000975994, Improvement: 0.00000000018323, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2497
Epoch 2497, Loss: 0.00000000964247, Improvement: -0.00000000011747, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2498
Epoch 2498, Loss: 0.00000000996756, Improvement: 0.00000000032509, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2499
Epoch 2499, Loss: 0.00000001111483, Improvement: 0.00000000114727, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000001648810, Improvement: 0.00000000537328, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2501
Epoch 2501, Loss: 0.00000002610123, Improvement: 0.00000000961312, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2502
Epoch 2502, Loss: 0.00000002239373, Improvement: -0.00000000370749, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2503
Epoch 2503, Loss: 0.00000001818787, Improvement: -0.00000000420586, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2504
Epoch 2504, Loss: 0.00000001457736, Improvement: -0.00000000361051, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2505
Epoch 2505, Loss: 0.00000002881270, Improvement: 0.00000001423534, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2506
Epoch 2506, Loss: 0.00000009779222, Improvement: 0.00000006897952, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2507
Epoch 2507, Loss: 0.00000007315618, Improvement: -0.00000002463603, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2508
Epoch 2508, Loss: 0.00000002026227, Improvement: -0.00000005289392, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2509
Epoch 2509, Loss: 0.00000001467415, Improvement: -0.00000000558812, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2510
Epoch 2510, Loss: 0.00000001597650, Improvement: 0.00000000130235, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2511
Epoch 2511, Loss: 0.00000001416757, Improvement: -0.00000000180893, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2512
Epoch 2512, Loss: 0.00000001485001, Improvement: 0.00000000068245, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2513
Epoch 2513, Loss: 0.00000001128558, Improvement: -0.00000000356443, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2514
Epoch 2514, Loss: 0.00000000896719, Improvement: -0.00000000231839, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2515
Epoch 2515, Loss: 0.00000000961519, Improvement: 0.00000000064799, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2516
Epoch 2516, Loss: 0.00000000952375, Improvement: -0.00000000009143, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2517
Epoch 2517, Loss: 0.00000000930408, Improvement: -0.00000000021968, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2518
Epoch 2518, Loss: 0.00000000955169, Improvement: 0.00000000024761, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2519
Epoch 2519, Loss: 0.00000000888083, Improvement: -0.00000000067086, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2520
Epoch 2520, Loss: 0.00000000849843, Improvement: -0.00000000038240, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2521
Epoch 2521, Loss: 0.00000000850489, Improvement: 0.00000000000646, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2522
Epoch 2522, Loss: 0.00000000841909, Improvement: -0.00000000008580, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2523
Epoch 2523, Loss: 0.00000000872541, Improvement: 0.00000000030632, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2524
Epoch 2524, Loss: 0.00000001539611, Improvement: 0.00000000667070, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2525
Epoch 2525, Loss: 0.00000001758048, Improvement: 0.00000000218438, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2526
Epoch 2526, Loss: 0.00000001634921, Improvement: -0.00000000123127, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2527
Epoch 2527, Loss: 0.00000001220972, Improvement: -0.00000000413950, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2528
Epoch 2528, Loss: 0.00000001361420, Improvement: 0.00000000140449, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2529
Epoch 2529, Loss: 0.00000001177853, Improvement: -0.00000000183567, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2530
Epoch 2530, Loss: 0.00000001259591, Improvement: 0.00000000081738, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2531
Epoch 2531, Loss: 0.00000001669254, Improvement: 0.00000000409664, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2532
Epoch 2532, Loss: 0.00000001284611, Improvement: -0.00000000384643, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2533
Epoch 2533, Loss: 0.00000001381260, Improvement: 0.00000000096649, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2534
Epoch 2534, Loss: 0.00000002758245, Improvement: 0.00000001376984, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2535
Epoch 2535, Loss: 0.00000003317850, Improvement: 0.00000000559605, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2536
Epoch 2536, Loss: 0.00000002872911, Improvement: -0.00000000444938, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2537
Epoch 2537, Loss: 0.00000004606954, Improvement: 0.00000001734043, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2538
Epoch 2538, Loss: 0.00000004356816, Improvement: -0.00000000250138, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2539
Epoch 2539, Loss: 0.00000002967365, Improvement: -0.00000001389451, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2540
Epoch 2540, Loss: 0.00000002278459, Improvement: -0.00000000688907, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2541
Epoch 2541, Loss: 0.00000002171902, Improvement: -0.00000000106556, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2542
Epoch 2542, Loss: 0.00000002549165, Improvement: 0.00000000377263, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2543
Epoch 2543, Loss: 0.00000003181886, Improvement: 0.00000000632720, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2544
Epoch 2544, Loss: 0.00000002691171, Improvement: -0.00000000490714, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2545
Epoch 2545, Loss: 0.00000001669396, Improvement: -0.00000001021776, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2546
Epoch 2546, Loss: 0.00000001041359, Improvement: -0.00000000628036, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2547
Epoch 2547, Loss: 0.00000000957578, Improvement: -0.00000000083781, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2548
Epoch 2548, Loss: 0.00000000873336, Improvement: -0.00000000084242, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2549
Epoch 2549, Loss: 0.00000001016790, Improvement: 0.00000000143455, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000001125666, Improvement: 0.00000000108876, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2551
Epoch 2551, Loss: 0.00000000932490, Improvement: -0.00000000193176, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2552
Epoch 2552, Loss: 0.00000001203897, Improvement: 0.00000000271407, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2553
Epoch 2553, Loss: 0.00000001106669, Improvement: -0.00000000097228, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2554
Epoch 2554, Loss: 0.00000001105838, Improvement: -0.00000000000831, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2555
Epoch 2555, Loss: 0.00000000992743, Improvement: -0.00000000113095, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2556
Epoch 2556, Loss: 0.00000001951667, Improvement: 0.00000000958924, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2557
Epoch 2557, Loss: 0.00000002479825, Improvement: 0.00000000528158, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2558
Epoch 2558, Loss: 0.00000003432696, Improvement: 0.00000000952870, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2559
Epoch 2559, Loss: 0.00000004401844, Improvement: 0.00000000969148, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2560
Epoch 2560, Loss: 0.00000007276159, Improvement: 0.00000002874315, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2561
Epoch 2561, Loss: 0.00000002829436, Improvement: -0.00000004446722, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2562
Epoch 2562, Loss: 0.00000001239919, Improvement: -0.00000001589517, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2563
Epoch 2563, Loss: 0.00000001108763, Improvement: -0.00000000131157, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2564
Epoch 2564, Loss: 0.00000001064096, Improvement: -0.00000000044667, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2565
Epoch 2565, Loss: 0.00000000882184, Improvement: -0.00000000181912, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2566
Epoch 2566, Loss: 0.00000001041119, Improvement: 0.00000000158935, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2567
Epoch 2567, Loss: 0.00000001720273, Improvement: 0.00000000679154, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2568
Epoch 2568, Loss: 0.00000002014632, Improvement: 0.00000000294359, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2569
Epoch 2569, Loss: 0.00000002892371, Improvement: 0.00000000877739, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2570
Epoch 2570, Loss: 0.00000001942186, Improvement: -0.00000000950186, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2571
Epoch 2571, Loss: 0.00000003461628, Improvement: 0.00000001519443, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2572
Epoch 2572, Loss: 0.00000012176557, Improvement: 0.00000008714928, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2573
Epoch 2573, Loss: 0.00000006019759, Improvement: -0.00000006156798, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2574
Epoch 2574, Loss: 0.00000002650469, Improvement: -0.00000003369290, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2575
Epoch 2575, Loss: 0.00000001742308, Improvement: -0.00000000908161, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2576
Epoch 2576, Loss: 0.00000001256725, Improvement: -0.00000000485583, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2577
Epoch 2577, Loss: 0.00000001036956, Improvement: -0.00000000219768, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2578
Epoch 2578, Loss: 0.00000000877020, Improvement: -0.00000000159937, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2579
Epoch 2579, Loss: 0.00000000894681, Improvement: 0.00000000017662, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2580
Epoch 2580, Loss: 0.00000000902379, Improvement: 0.00000000007698, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2581
Epoch 2581, Loss: 0.00000000867552, Improvement: -0.00000000034827, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2582
Epoch 2582, Loss: 0.00000000884625, Improvement: 0.00000000017072, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2583
Epoch 2583, Loss: 0.00000000889735, Improvement: 0.00000000005110, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2584
Epoch 2584, Loss: 0.00000000866904, Improvement: -0.00000000022831, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2585
Epoch 2585, Loss: 0.00000000887854, Improvement: 0.00000000020950, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2586
Epoch 2586, Loss: 0.00000000838600, Improvement: -0.00000000049253, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2587
Epoch 2587, Loss: 0.00000000824155, Improvement: -0.00000000014446, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2588
Epoch 2588, Loss: 0.00000000893218, Improvement: 0.00000000069063, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2589
Epoch 2589, Loss: 0.00000000926600, Improvement: 0.00000000033382, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2590
Epoch 2590, Loss: 0.00000000971554, Improvement: 0.00000000044954, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2591
Epoch 2591, Loss: 0.00000000845779, Improvement: -0.00000000125775, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2592
Epoch 2592, Loss: 0.00000000801810, Improvement: -0.00000000043970, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2593
Epoch 2593, Loss: 0.00000000850118, Improvement: 0.00000000048308, Best Loss: 0.00000000612936 in Epoch 2495
Epoch 2594
A best model at epoch 2594 has been saved with training error 0.00000000560778.
Epoch 2594, Loss: 0.00000000849731, Improvement: -0.00000000000387, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2595
Epoch 2595, Loss: 0.00000000892383, Improvement: 0.00000000042652, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2596
Epoch 2596, Loss: 0.00000000956689, Improvement: 0.00000000064306, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2597
Epoch 2597, Loss: 0.00000001038461, Improvement: 0.00000000081772, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2598
Epoch 2598, Loss: 0.00000000883601, Improvement: -0.00000000154860, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2599
Epoch 2599, Loss: 0.00000000853688, Improvement: -0.00000000029913, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000000939096, Improvement: 0.00000000085408, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2601
Epoch 2601, Loss: 0.00000001180029, Improvement: 0.00000000240933, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2602
Epoch 2602, Loss: 0.00000001819146, Improvement: 0.00000000639117, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2603
Epoch 2603, Loss: 0.00000001515382, Improvement: -0.00000000303764, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2604
Epoch 2604, Loss: 0.00000001327517, Improvement: -0.00000000187865, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2605
Epoch 2605, Loss: 0.00000001373431, Improvement: 0.00000000045915, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2606
Epoch 2606, Loss: 0.00000003115165, Improvement: 0.00000001741733, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2607
Epoch 2607, Loss: 0.00000003690914, Improvement: 0.00000000575749, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2608
Epoch 2608, Loss: 0.00000002252605, Improvement: -0.00000001438309, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2609
Epoch 2609, Loss: 0.00000004616297, Improvement: 0.00000002363692, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2610
Epoch 2610, Loss: 0.00000002567964, Improvement: -0.00000002048333, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2611
Epoch 2611, Loss: 0.00000001477609, Improvement: -0.00000001090355, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2612
Epoch 2612, Loss: 0.00000001137102, Improvement: -0.00000000340507, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2613
Epoch 2613, Loss: 0.00000001070751, Improvement: -0.00000000066350, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2614
Epoch 2614, Loss: 0.00000001165122, Improvement: 0.00000000094370, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2615
Epoch 2615, Loss: 0.00000001325568, Improvement: 0.00000000160446, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2616
Epoch 2616, Loss: 0.00000001222889, Improvement: -0.00000000102678, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2617
Epoch 2617, Loss: 0.00000001108128, Improvement: -0.00000000114761, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2618
Epoch 2618, Loss: 0.00000001926484, Improvement: 0.00000000818356, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2619
Epoch 2619, Loss: 0.00000001344194, Improvement: -0.00000000582290, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2620
Epoch 2620, Loss: 0.00000001726322, Improvement: 0.00000000382128, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2621
Epoch 2621, Loss: 0.00000001850639, Improvement: 0.00000000124317, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2622
Epoch 2622, Loss: 0.00000001090506, Improvement: -0.00000000760133, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2623
Epoch 2623, Loss: 0.00000001212768, Improvement: 0.00000000122262, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2624
Epoch 2624, Loss: 0.00000001400570, Improvement: 0.00000000187801, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2625
Epoch 2625, Loss: 0.00000001769239, Improvement: 0.00000000368670, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2626
Epoch 2626, Loss: 0.00000001464814, Improvement: -0.00000000304425, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2627
Epoch 2627, Loss: 0.00000001239282, Improvement: -0.00000000225533, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2628
Epoch 2628, Loss: 0.00000001432146, Improvement: 0.00000000192864, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2629
Epoch 2629, Loss: 0.00000001644296, Improvement: 0.00000000212151, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2630
Epoch 2630, Loss: 0.00000003726461, Improvement: 0.00000002082164, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2631
Epoch 2631, Loss: 0.00000006049625, Improvement: 0.00000002323165, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2632
Epoch 2632, Loss: 0.00000004291710, Improvement: -0.00000001757915, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2633
Epoch 2633, Loss: 0.00000001620986, Improvement: -0.00000002670724, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2634
Epoch 2634, Loss: 0.00000001034263, Improvement: -0.00000000586723, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2635
Epoch 2635, Loss: 0.00000000865518, Improvement: -0.00000000168745, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2636
Epoch 2636, Loss: 0.00000001047411, Improvement: 0.00000000181893, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2637
Epoch 2637, Loss: 0.00000001705858, Improvement: 0.00000000658447, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2638
Epoch 2638, Loss: 0.00000001697003, Improvement: -0.00000000008856, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2639
Epoch 2639, Loss: 0.00000002114707, Improvement: 0.00000000417704, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2640
Epoch 2640, Loss: 0.00000001696929, Improvement: -0.00000000417777, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2641
Epoch 2641, Loss: 0.00000002561458, Improvement: 0.00000000864528, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2642
Epoch 2642, Loss: 0.00000003093324, Improvement: 0.00000000531866, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2643
Epoch 2643, Loss: 0.00000002611172, Improvement: -0.00000000482152, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2644
Epoch 2644, Loss: 0.00000002577661, Improvement: -0.00000000033510, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2645
Epoch 2645, Loss: 0.00000002176455, Improvement: -0.00000000401206, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2646
Epoch 2646, Loss: 0.00000003029528, Improvement: 0.00000000853073, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2647
Epoch 2647, Loss: 0.00000002836526, Improvement: -0.00000000193002, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2648
Epoch 2648, Loss: 0.00000001963759, Improvement: -0.00000000872768, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2649
Epoch 2649, Loss: 0.00000001765573, Improvement: -0.00000000198186, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000002470921, Improvement: 0.00000000705348, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2651
Epoch 2651, Loss: 0.00000002921530, Improvement: 0.00000000450609, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2652
Epoch 2652, Loss: 0.00000003318142, Improvement: 0.00000000396612, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2653
Epoch 2653, Loss: 0.00000001468577, Improvement: -0.00000001849564, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2654
Epoch 2654, Loss: 0.00000001092422, Improvement: -0.00000000376155, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2655
Epoch 2655, Loss: 0.00000001277143, Improvement: 0.00000000184721, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2656
Epoch 2656, Loss: 0.00000001362252, Improvement: 0.00000000085108, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2657
Epoch 2657, Loss: 0.00000001133090, Improvement: -0.00000000229162, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2658
Epoch 2658, Loss: 0.00000001212776, Improvement: 0.00000000079686, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2659
Epoch 2659, Loss: 0.00000001635844, Improvement: 0.00000000423068, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2660
Epoch 2660, Loss: 0.00000001239730, Improvement: -0.00000000396114, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2661
Epoch 2661, Loss: 0.00000005144163, Improvement: 0.00000003904433, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2662
Epoch 2662, Loss: 0.00000012341359, Improvement: 0.00000007197195, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2663
Epoch 2663, Loss: 0.00000011599874, Improvement: -0.00000000741485, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2664
Epoch 2664, Loss: 0.00000004966109, Improvement: -0.00000006633764, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2665
Epoch 2665, Loss: 0.00000003042990, Improvement: -0.00000001923120, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2666
Epoch 2666, Loss: 0.00000002207339, Improvement: -0.00000000835650, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2667
Epoch 2667, Loss: 0.00000001470340, Improvement: -0.00000000737000, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2668
Epoch 2668, Loss: 0.00000001109538, Improvement: -0.00000000360802, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2669
Epoch 2669, Loss: 0.00000001070099, Improvement: -0.00000000039439, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2670
Epoch 2670, Loss: 0.00000000929711, Improvement: -0.00000000140388, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2671
Epoch 2671, Loss: 0.00000000841166, Improvement: -0.00000000088545, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2672
Epoch 2672, Loss: 0.00000000868054, Improvement: 0.00000000026889, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2673
Epoch 2673, Loss: 0.00000000853291, Improvement: -0.00000000014763, Best Loss: 0.00000000560778 in Epoch 2594
Epoch 2674
A best model at epoch 2674 has been saved with training error 0.00000000559593.
Epoch 2674, Loss: 0.00000000858289, Improvement: 0.00000000004998, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2675
Epoch 2675, Loss: 0.00000000835621, Improvement: -0.00000000022668, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2676
Epoch 2676, Loss: 0.00000000809066, Improvement: -0.00000000026555, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2677
Epoch 2677, Loss: 0.00000000827864, Improvement: 0.00000000018797, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2678
Epoch 2678, Loss: 0.00000000994421, Improvement: 0.00000000166557, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2679
Epoch 2679, Loss: 0.00000000789468, Improvement: -0.00000000204953, Best Loss: 0.00000000559593 in Epoch 2674
Epoch 2680
A best model at epoch 2680 has been saved with training error 0.00000000558406.
Epoch 2680, Loss: 0.00000000875696, Improvement: 0.00000000086228, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2681
Epoch 2681, Loss: 0.00000001026287, Improvement: 0.00000000150591, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2682
Epoch 2682, Loss: 0.00000001086104, Improvement: 0.00000000059817, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2683
Epoch 2683, Loss: 0.00000000902542, Improvement: -0.00000000183562, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2684
Epoch 2684, Loss: 0.00000000815089, Improvement: -0.00000000087453, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2685
Epoch 2685, Loss: 0.00000000804857, Improvement: -0.00000000010232, Best Loss: 0.00000000558406 in Epoch 2680
Epoch 2686
A best model at epoch 2686 has been saved with training error 0.00000000533253.
Epoch 2686, Loss: 0.00000000775128, Improvement: -0.00000000029729, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2687
Epoch 2687, Loss: 0.00000001019061, Improvement: 0.00000000243933, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2688
Epoch 2688, Loss: 0.00000001074672, Improvement: 0.00000000055612, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2689
Epoch 2689, Loss: 0.00000001075138, Improvement: 0.00000000000466, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2690
Epoch 2690, Loss: 0.00000000894797, Improvement: -0.00000000180342, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2691
Epoch 2691, Loss: 0.00000000879372, Improvement: -0.00000000015424, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2692
Epoch 2692, Loss: 0.00000000798609, Improvement: -0.00000000080764, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2693
Epoch 2693, Loss: 0.00000000977250, Improvement: 0.00000000178641, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2694
Epoch 2694, Loss: 0.00000001482464, Improvement: 0.00000000505215, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2695
Epoch 2695, Loss: 0.00000001955044, Improvement: 0.00000000472580, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2696
Epoch 2696, Loss: 0.00000001402024, Improvement: -0.00000000553021, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2697
Epoch 2697, Loss: 0.00000001737117, Improvement: 0.00000000335094, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2698
Epoch 2698, Loss: 0.00000002222924, Improvement: 0.00000000485807, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2699
Epoch 2699, Loss: 0.00000002061225, Improvement: -0.00000000161700, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000004469058, Improvement: 0.00000002407833, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2701
Epoch 2701, Loss: 0.00000003970643, Improvement: -0.00000000498415, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2702
Epoch 2702, Loss: 0.00000001606670, Improvement: -0.00000002363973, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2703
Epoch 2703, Loss: 0.00000000963069, Improvement: -0.00000000643600, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2704
Epoch 2704, Loss: 0.00000000892656, Improvement: -0.00000000070413, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2705
Epoch 2705, Loss: 0.00000000952692, Improvement: 0.00000000060036, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2706
Epoch 2706, Loss: 0.00000000936440, Improvement: -0.00000000016252, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2707
Epoch 2707, Loss: 0.00000001217396, Improvement: 0.00000000280957, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2708
Epoch 2708, Loss: 0.00000001320354, Improvement: 0.00000000102958, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2709
Epoch 2709, Loss: 0.00000001575687, Improvement: 0.00000000255333, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2710
Epoch 2710, Loss: 0.00000003168433, Improvement: 0.00000001592746, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2711
Epoch 2711, Loss: 0.00000008834496, Improvement: 0.00000005666063, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2712
Epoch 2712, Loss: 0.00000007076895, Improvement: -0.00000001757602, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2713
Epoch 2713, Loss: 0.00000004051784, Improvement: -0.00000003025110, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2714
Epoch 2714, Loss: 0.00000001931514, Improvement: -0.00000002120270, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2715
Epoch 2715, Loss: 0.00000001017395, Improvement: -0.00000000914119, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2716
Epoch 2716, Loss: 0.00000000871553, Improvement: -0.00000000145842, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2717
Epoch 2717, Loss: 0.00000000960850, Improvement: 0.00000000089297, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2718
Epoch 2718, Loss: 0.00000000867054, Improvement: -0.00000000093796, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2719
Epoch 2719, Loss: 0.00000000834457, Improvement: -0.00000000032597, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2720
Epoch 2720, Loss: 0.00000000785330, Improvement: -0.00000000049127, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2721
Epoch 2721, Loss: 0.00000000788508, Improvement: 0.00000000003178, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2722
Epoch 2722, Loss: 0.00000000789242, Improvement: 0.00000000000734, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2723
Epoch 2723, Loss: 0.00000000875104, Improvement: 0.00000000085862, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2724
Epoch 2724, Loss: 0.00000000885152, Improvement: 0.00000000010048, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2725
Epoch 2725, Loss: 0.00000000870871, Improvement: -0.00000000014281, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2726
Epoch 2726, Loss: 0.00000000903030, Improvement: 0.00000000032159, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2727
Epoch 2727, Loss: 0.00000000990425, Improvement: 0.00000000087395, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2728
Epoch 2728, Loss: 0.00000001739351, Improvement: 0.00000000748926, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2729
Epoch 2729, Loss: 0.00000002182068, Improvement: 0.00000000442717, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2730
Epoch 2730, Loss: 0.00000001848035, Improvement: -0.00000000334033, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2731
Epoch 2731, Loss: 0.00000001316737, Improvement: -0.00000000531299, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2732
Epoch 2732, Loss: 0.00000000965425, Improvement: -0.00000000351312, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2733
Epoch 2733, Loss: 0.00000000887829, Improvement: -0.00000000077596, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2734
Epoch 2734, Loss: 0.00000000933499, Improvement: 0.00000000045670, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2735
Epoch 2735, Loss: 0.00000000947364, Improvement: 0.00000000013864, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2736
Epoch 2736, Loss: 0.00000001126382, Improvement: 0.00000000179018, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2737
Epoch 2737, Loss: 0.00000001286244, Improvement: 0.00000000159863, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2738
Epoch 2738, Loss: 0.00000002753266, Improvement: 0.00000001467022, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2739
Epoch 2739, Loss: 0.00000003213869, Improvement: 0.00000000460602, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2740
Epoch 2740, Loss: 0.00000002093868, Improvement: -0.00000001120001, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2741
Epoch 2741, Loss: 0.00000001419986, Improvement: -0.00000000673882, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2742
Epoch 2742, Loss: 0.00000001109781, Improvement: -0.00000000310205, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2743
Epoch 2743, Loss: 0.00000001530672, Improvement: 0.00000000420891, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2744
Epoch 2744, Loss: 0.00000001607224, Improvement: 0.00000000076552, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2745
Epoch 2745, Loss: 0.00000001991748, Improvement: 0.00000000384524, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2746
Epoch 2746, Loss: 0.00000001563513, Improvement: -0.00000000428235, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2747
Epoch 2747, Loss: 0.00000000890819, Improvement: -0.00000000672695, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2748
Epoch 2748, Loss: 0.00000001159736, Improvement: 0.00000000268918, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2749
Epoch 2749, Loss: 0.00000002237581, Improvement: 0.00000001077845, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000002037952, Improvement: -0.00000000199629, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2751
Epoch 2751, Loss: 0.00000001937033, Improvement: -0.00000000100919, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2752
Epoch 2752, Loss: 0.00000001254638, Improvement: -0.00000000682395, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2753
Epoch 2753, Loss: 0.00000001375284, Improvement: 0.00000000120646, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2754
Epoch 2754, Loss: 0.00000001176265, Improvement: -0.00000000199019, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2755
Epoch 2755, Loss: 0.00000001148122, Improvement: -0.00000000028143, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2756
Epoch 2756, Loss: 0.00000001126657, Improvement: -0.00000000021465, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2757
Epoch 2757, Loss: 0.00000001149067, Improvement: 0.00000000022411, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2758
Epoch 2758, Loss: 0.00000005173127, Improvement: 0.00000004024060, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2759
Epoch 2759, Loss: 0.00000005484149, Improvement: 0.00000000311022, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2760
Epoch 2760, Loss: 0.00000003159639, Improvement: -0.00000002324510, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2761
Epoch 2761, Loss: 0.00000001599449, Improvement: -0.00000001560190, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2762
Epoch 2762, Loss: 0.00000001128371, Improvement: -0.00000000471079, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2763
Epoch 2763, Loss: 0.00000001026960, Improvement: -0.00000000101411, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2764
Epoch 2764, Loss: 0.00000000881007, Improvement: -0.00000000145953, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2765
Epoch 2765, Loss: 0.00000000991079, Improvement: 0.00000000110073, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2766
Epoch 2766, Loss: 0.00000000912726, Improvement: -0.00000000078353, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2767
Epoch 2767, Loss: 0.00000000809805, Improvement: -0.00000000102922, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2768
Epoch 2768, Loss: 0.00000000784714, Improvement: -0.00000000025091, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2769
Epoch 2769, Loss: 0.00000000904286, Improvement: 0.00000000119572, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2770
Epoch 2770, Loss: 0.00000000948519, Improvement: 0.00000000044233, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2771
Epoch 2771, Loss: 0.00000000885536, Improvement: -0.00000000062983, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2772
Epoch 2772, Loss: 0.00000001166235, Improvement: 0.00000000280699, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2773
Epoch 2773, Loss: 0.00000001734202, Improvement: 0.00000000567966, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2774
Epoch 2774, Loss: 0.00000001167501, Improvement: -0.00000000566701, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2775
Epoch 2775, Loss: 0.00000001596008, Improvement: 0.00000000428507, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2776
Epoch 2776, Loss: 0.00000003514081, Improvement: 0.00000001918073, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2777
Epoch 2777, Loss: 0.00000004927695, Improvement: 0.00000001413614, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2778
Epoch 2778, Loss: 0.00000002397658, Improvement: -0.00000002530037, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2779
Epoch 2779, Loss: 0.00000001587343, Improvement: -0.00000000810315, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2780
Epoch 2780, Loss: 0.00000001135569, Improvement: -0.00000000451774, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2781
Epoch 2781, Loss: 0.00000001258700, Improvement: 0.00000000123130, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2782
Epoch 2782, Loss: 0.00000001110564, Improvement: -0.00000000148136, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2783
Epoch 2783, Loss: 0.00000000925865, Improvement: -0.00000000184699, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2784
Epoch 2784, Loss: 0.00000000805952, Improvement: -0.00000000119913, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2785
Epoch 2785, Loss: 0.00000000993801, Improvement: 0.00000000187848, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2786
Epoch 2786, Loss: 0.00000000987337, Improvement: -0.00000000006463, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2787
Epoch 2787, Loss: 0.00000000906656, Improvement: -0.00000000080682, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2788
Epoch 2788, Loss: 0.00000001007274, Improvement: 0.00000000100619, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2789
Epoch 2789, Loss: 0.00000001130024, Improvement: 0.00000000122749, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2790
Epoch 2790, Loss: 0.00000001537512, Improvement: 0.00000000407489, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2791
Epoch 2791, Loss: 0.00000001674936, Improvement: 0.00000000137424, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2792
Epoch 2792, Loss: 0.00000002385972, Improvement: 0.00000000711035, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2793
Epoch 2793, Loss: 0.00000002393772, Improvement: 0.00000000007801, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2794
Epoch 2794, Loss: 0.00000002259823, Improvement: -0.00000000133950, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2795
Epoch 2795, Loss: 0.00000001907889, Improvement: -0.00000000351933, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2796
Epoch 2796, Loss: 0.00000001934884, Improvement: 0.00000000026994, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2797
Epoch 2797, Loss: 0.00000001338559, Improvement: -0.00000000596324, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2798
Epoch 2798, Loss: 0.00000001165308, Improvement: -0.00000000173251, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2799
Epoch 2799, Loss: 0.00000001397148, Improvement: 0.00000000231840, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000001593197, Improvement: 0.00000000196049, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2801
Epoch 2801, Loss: 0.00000001351834, Improvement: -0.00000000241363, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2802
Epoch 2802, Loss: 0.00000001968372, Improvement: 0.00000000616538, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2803
Epoch 2803, Loss: 0.00000006012518, Improvement: 0.00000004044146, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2804
Epoch 2804, Loss: 0.00000008052539, Improvement: 0.00000002040022, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2805
Epoch 2805, Loss: 0.00000002118100, Improvement: -0.00000005934439, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2806
Epoch 2806, Loss: 0.00000001492491, Improvement: -0.00000000625609, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2807
Epoch 2807, Loss: 0.00000001438048, Improvement: -0.00000000054443, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2808
Epoch 2808, Loss: 0.00000001257112, Improvement: -0.00000000180936, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2809
Epoch 2809, Loss: 0.00000000935353, Improvement: -0.00000000321759, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2810
Epoch 2810, Loss: 0.00000000931458, Improvement: -0.00000000003894, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2811
Epoch 2811, Loss: 0.00000001202705, Improvement: 0.00000000271247, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2812
Epoch 2812, Loss: 0.00000001529699, Improvement: 0.00000000326994, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2813
Epoch 2813, Loss: 0.00000001379051, Improvement: -0.00000000150649, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2814
Epoch 2814, Loss: 0.00000001011315, Improvement: -0.00000000367736, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2815
Epoch 2815, Loss: 0.00000000867364, Improvement: -0.00000000143951, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2816
Epoch 2816, Loss: 0.00000000866283, Improvement: -0.00000000001081, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2817
Epoch 2817, Loss: 0.00000000945829, Improvement: 0.00000000079547, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2818
Epoch 2818, Loss: 0.00000000940856, Improvement: -0.00000000004973, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2819
Epoch 2819, Loss: 0.00000001404443, Improvement: 0.00000000463587, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2820
Epoch 2820, Loss: 0.00000001510284, Improvement: 0.00000000105841, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2821
Epoch 2821, Loss: 0.00000001309321, Improvement: -0.00000000200963, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2822
Epoch 2822, Loss: 0.00000001154625, Improvement: -0.00000000154696, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2823
Epoch 2823, Loss: 0.00000001235217, Improvement: 0.00000000080593, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2824
Epoch 2824, Loss: 0.00000001246963, Improvement: 0.00000000011746, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2825
Epoch 2825, Loss: 0.00000001187741, Improvement: -0.00000000059223, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2826
Epoch 2826, Loss: 0.00000000966357, Improvement: -0.00000000221383, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2827
Epoch 2827, Loss: 0.00000000943531, Improvement: -0.00000000022826, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2828
Epoch 2828, Loss: 0.00000000979825, Improvement: 0.00000000036294, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2829
Epoch 2829, Loss: 0.00000001806935, Improvement: 0.00000000827110, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2830
Epoch 2830, Loss: 0.00000002500445, Improvement: 0.00000000693510, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2831
Epoch 2831, Loss: 0.00000005007100, Improvement: 0.00000002506655, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2832
Epoch 2832, Loss: 0.00000002983707, Improvement: -0.00000002023393, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2833
Epoch 2833, Loss: 0.00000003156371, Improvement: 0.00000000172664, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2834
Epoch 2834, Loss: 0.00000003305913, Improvement: 0.00000000149542, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2835
Epoch 2835, Loss: 0.00000004568648, Improvement: 0.00000001262735, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2836
Epoch 2836, Loss: 0.00000005802603, Improvement: 0.00000001233954, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2837
Epoch 2837, Loss: 0.00000004907035, Improvement: -0.00000000895567, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2838
Epoch 2838, Loss: 0.00000003328318, Improvement: -0.00000001578718, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2839
Epoch 2839, Loss: 0.00000002149332, Improvement: -0.00000001178985, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2840
Epoch 2840, Loss: 0.00000001068693, Improvement: -0.00000001080640, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2841
Epoch 2841, Loss: 0.00000000960790, Improvement: -0.00000000107903, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2842
Epoch 2842, Loss: 0.00000001388340, Improvement: 0.00000000427550, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2843
Epoch 2843, Loss: 0.00000001682385, Improvement: 0.00000000294045, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2844
Epoch 2844, Loss: 0.00000001944967, Improvement: 0.00000000262581, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2845
Epoch 2845, Loss: 0.00000001238154, Improvement: -0.00000000706812, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2846
Epoch 2846, Loss: 0.00000001212625, Improvement: -0.00000000025529, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2847
Epoch 2847, Loss: 0.00000000900290, Improvement: -0.00000000312335, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2848
Epoch 2848, Loss: 0.00000000915519, Improvement: 0.00000000015228, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2849
Epoch 2849, Loss: 0.00000000878229, Improvement: -0.00000000037290, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000000858599, Improvement: -0.00000000019630, Best Loss: 0.00000000533253 in Epoch 2686
Epoch 2851
A best model at epoch 2851 has been saved with training error 0.00000000501183.
Epoch 2851, Loss: 0.00000000843279, Improvement: -0.00000000015320, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2852
Epoch 2852, Loss: 0.00000000969360, Improvement: 0.00000000126082, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2853
Epoch 2853, Loss: 0.00000001137132, Improvement: 0.00000000167772, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2854
Epoch 2854, Loss: 0.00000001260964, Improvement: 0.00000000123832, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2855
Epoch 2855, Loss: 0.00000001605776, Improvement: 0.00000000344813, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2856
Epoch 2856, Loss: 0.00000001213412, Improvement: -0.00000000392364, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2857
Epoch 2857, Loss: 0.00000002381563, Improvement: 0.00000001168151, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2858
Epoch 2858, Loss: 0.00000002054563, Improvement: -0.00000000327000, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2859
Epoch 2859, Loss: 0.00000001865954, Improvement: -0.00000000188609, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2860
Epoch 2860, Loss: 0.00000003209678, Improvement: 0.00000001343724, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2861
Epoch 2861, Loss: 0.00000004277774, Improvement: 0.00000001068096, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2862
Epoch 2862, Loss: 0.00000003138355, Improvement: -0.00000001139419, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2863
Epoch 2863, Loss: 0.00000002314328, Improvement: -0.00000000824026, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2864
Epoch 2864, Loss: 0.00000002470077, Improvement: 0.00000000155749, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2865
Epoch 2865, Loss: 0.00000001516734, Improvement: -0.00000000953344, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2866
Epoch 2866, Loss: 0.00000001640599, Improvement: 0.00000000123865, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2867
Epoch 2867, Loss: 0.00000001268633, Improvement: -0.00000000371966, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2868
Epoch 2868, Loss: 0.00000001013271, Improvement: -0.00000000255362, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2869
Epoch 2869, Loss: 0.00000001028497, Improvement: 0.00000000015226, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2870
Epoch 2870, Loss: 0.00000001385478, Improvement: 0.00000000356981, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2871
Epoch 2871, Loss: 0.00000001981103, Improvement: 0.00000000595625, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2872
Epoch 2872, Loss: 0.00000003616917, Improvement: 0.00000001635814, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2873
Epoch 2873, Loss: 0.00000002877008, Improvement: -0.00000000739909, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2874
Epoch 2874, Loss: 0.00000002548886, Improvement: -0.00000000328121, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2875
Epoch 2875, Loss: 0.00000001855272, Improvement: -0.00000000693614, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2876
Epoch 2876, Loss: 0.00000001227486, Improvement: -0.00000000627786, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2877
Epoch 2877, Loss: 0.00000001742191, Improvement: 0.00000000514705, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2878
Epoch 2878, Loss: 0.00000001277450, Improvement: -0.00000000464742, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2879
Epoch 2879, Loss: 0.00000001061631, Improvement: -0.00000000215818, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2880
Epoch 2880, Loss: 0.00000001397642, Improvement: 0.00000000336010, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2881
Epoch 2881, Loss: 0.00000002134001, Improvement: 0.00000000736359, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2882
Epoch 2882, Loss: 0.00000001368788, Improvement: -0.00000000765213, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2883
Epoch 2883, Loss: 0.00000001511795, Improvement: 0.00000000143007, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2884
Epoch 2884, Loss: 0.00000002044532, Improvement: 0.00000000532737, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2885
Epoch 2885, Loss: 0.00000002940115, Improvement: 0.00000000895583, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2886
Epoch 2886, Loss: 0.00000003157985, Improvement: 0.00000000217870, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2887
Epoch 2887, Loss: 0.00000002016531, Improvement: -0.00000001141454, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2888
Epoch 2888, Loss: 0.00000003792122, Improvement: 0.00000001775591, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2889
Epoch 2889, Loss: 0.00000002059883, Improvement: -0.00000001732239, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2890
Epoch 2890, Loss: 0.00000001925910, Improvement: -0.00000000133972, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2891
Epoch 2891, Loss: 0.00000003917643, Improvement: 0.00000001991733, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2892
Epoch 2892, Loss: 0.00000002326685, Improvement: -0.00000001590958, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2893
Epoch 2893, Loss: 0.00000002994395, Improvement: 0.00000000667710, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2894
Epoch 2894, Loss: 0.00000002652003, Improvement: -0.00000000342392, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2895
Epoch 2895, Loss: 0.00000001786311, Improvement: -0.00000000865693, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2896
Epoch 2896, Loss: 0.00000001105362, Improvement: -0.00000000680949, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2897
Epoch 2897, Loss: 0.00000001485507, Improvement: 0.00000000380146, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2898
Epoch 2898, Loss: 0.00000001188055, Improvement: -0.00000000297452, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2899
Epoch 2899, Loss: 0.00000001057846, Improvement: -0.00000000130209, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000001162540, Improvement: 0.00000000104694, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2901
Epoch 2901, Loss: 0.00000001005528, Improvement: -0.00000000157011, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2902
Epoch 2902, Loss: 0.00000001019162, Improvement: 0.00000000013634, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2903
Epoch 2903, Loss: 0.00000000947258, Improvement: -0.00000000071904, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2904
Epoch 2904, Loss: 0.00000001016879, Improvement: 0.00000000069621, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2905
Epoch 2905, Loss: 0.00000001666589, Improvement: 0.00000000649710, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2906
Epoch 2906, Loss: 0.00000002404307, Improvement: 0.00000000737719, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2907
Epoch 2907, Loss: 0.00000003729756, Improvement: 0.00000001325449, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2908
Epoch 2908, Loss: 0.00000002457792, Improvement: -0.00000001271964, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2909
Epoch 2909, Loss: 0.00000002691043, Improvement: 0.00000000233251, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2910
Epoch 2910, Loss: 0.00000004718836, Improvement: 0.00000002027793, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2911
Epoch 2911, Loss: 0.00000003910052, Improvement: -0.00000000808784, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2912
Epoch 2912, Loss: 0.00000002349801, Improvement: -0.00000001560251, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2913
Epoch 2913, Loss: 0.00000002306339, Improvement: -0.00000000043463, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2914
Epoch 2914, Loss: 0.00000002643209, Improvement: 0.00000000336870, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2915
Epoch 2915, Loss: 0.00000001597214, Improvement: -0.00000001045995, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2916
Epoch 2916, Loss: 0.00000001122098, Improvement: -0.00000000475116, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2917
Epoch 2917, Loss: 0.00000001012300, Improvement: -0.00000000109799, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2918
Epoch 2918, Loss: 0.00000000947752, Improvement: -0.00000000064548, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2919
Epoch 2919, Loss: 0.00000001314609, Improvement: 0.00000000366857, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2920
Epoch 2920, Loss: 0.00000004465149, Improvement: 0.00000003150540, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2921
Epoch 2921, Loss: 0.00000002395403, Improvement: -0.00000002069746, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2922
Epoch 2922, Loss: 0.00000001325130, Improvement: -0.00000001070272, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2923
Epoch 2923, Loss: 0.00000001216482, Improvement: -0.00000000108648, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2924
Epoch 2924, Loss: 0.00000001098422, Improvement: -0.00000000118060, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2925
Epoch 2925, Loss: 0.00000000906525, Improvement: -0.00000000191897, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2926
Epoch 2926, Loss: 0.00000000826770, Improvement: -0.00000000079755, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2927
Epoch 2927, Loss: 0.00000000834329, Improvement: 0.00000000007559, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2928
Epoch 2928, Loss: 0.00000000817575, Improvement: -0.00000000016754, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2929
Epoch 2929, Loss: 0.00000000763123, Improvement: -0.00000000054451, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2930
Epoch 2930, Loss: 0.00000000841111, Improvement: 0.00000000077988, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2931
Epoch 2931, Loss: 0.00000001056698, Improvement: 0.00000000215587, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2932
Epoch 2932, Loss: 0.00000001564012, Improvement: 0.00000000507314, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2933
Epoch 2933, Loss: 0.00000001260514, Improvement: -0.00000000303498, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2934
Epoch 2934, Loss: 0.00000000934152, Improvement: -0.00000000326362, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2935
Epoch 2935, Loss: 0.00000000960172, Improvement: 0.00000000026019, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2936
Epoch 2936, Loss: 0.00000001032617, Improvement: 0.00000000072446, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2937
Epoch 2937, Loss: 0.00000001077363, Improvement: 0.00000000044746, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2938
Epoch 2938, Loss: 0.00000001085467, Improvement: 0.00000000008104, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2939
Epoch 2939, Loss: 0.00000001217921, Improvement: 0.00000000132455, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2940
Epoch 2940, Loss: 0.00000001595090, Improvement: 0.00000000377169, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2941
Epoch 2941, Loss: 0.00000002159500, Improvement: 0.00000000564410, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2942
Epoch 2942, Loss: 0.00000002576949, Improvement: 0.00000000417449, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2943
Epoch 2943, Loss: 0.00000006329575, Improvement: 0.00000003752626, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2944
Epoch 2944, Loss: 0.00000003374806, Improvement: -0.00000002954769, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2945
Epoch 2945, Loss: 0.00000002352730, Improvement: -0.00000001022076, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2946
Epoch 2946, Loss: 0.00000001142113, Improvement: -0.00000001210617, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2947
Epoch 2947, Loss: 0.00000000893715, Improvement: -0.00000000248398, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2948
Epoch 2948, Loss: 0.00000000888514, Improvement: -0.00000000005201, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2949
Epoch 2949, Loss: 0.00000000991564, Improvement: 0.00000000103050, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000000997567, Improvement: 0.00000000006004, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2951
Epoch 2951, Loss: 0.00000001255851, Improvement: 0.00000000258284, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2952
Epoch 2952, Loss: 0.00000001730892, Improvement: 0.00000000475041, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2953
Epoch 2953, Loss: 0.00000001152359, Improvement: -0.00000000578533, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2954
Epoch 2954, Loss: 0.00000001330516, Improvement: 0.00000000178157, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2955
Epoch 2955, Loss: 0.00000001526151, Improvement: 0.00000000195635, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2956
Epoch 2956, Loss: 0.00000002531175, Improvement: 0.00000001005024, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2957
Epoch 2957, Loss: 0.00000003142047, Improvement: 0.00000000610872, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2958
Epoch 2958, Loss: 0.00000003046482, Improvement: -0.00000000095565, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2959
Epoch 2959, Loss: 0.00000002193612, Improvement: -0.00000000852870, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2960
Epoch 2960, Loss: 0.00000003468896, Improvement: 0.00000001275285, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2961
Epoch 2961, Loss: 0.00000003584074, Improvement: 0.00000000115178, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2962
Epoch 2962, Loss: 0.00000001602896, Improvement: -0.00000001981178, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2963
Epoch 2963, Loss: 0.00000001207628, Improvement: -0.00000000395267, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2964
Epoch 2964, Loss: 0.00000001066104, Improvement: -0.00000000141524, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2965
Epoch 2965, Loss: 0.00000001099859, Improvement: 0.00000000033755, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2966
Epoch 2966, Loss: 0.00000001040960, Improvement: -0.00000000058900, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2967
Epoch 2967, Loss: 0.00000001146479, Improvement: 0.00000000105519, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2968
Epoch 2968, Loss: 0.00000001525195, Improvement: 0.00000000378716, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2969
Epoch 2969, Loss: 0.00000001175241, Improvement: -0.00000000349954, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2970
Epoch 2970, Loss: 0.00000001091151, Improvement: -0.00000000084090, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2971
Epoch 2971, Loss: 0.00000001555758, Improvement: 0.00000000464607, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2972
Epoch 2972, Loss: 0.00000001875446, Improvement: 0.00000000319689, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2973
Epoch 2973, Loss: 0.00000002549837, Improvement: 0.00000000674391, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2974
Epoch 2974, Loss: 0.00000003975640, Improvement: 0.00000001425803, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2975
Epoch 2975, Loss: 0.00000003103321, Improvement: -0.00000000872320, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2976
Epoch 2976, Loss: 0.00000003241175, Improvement: 0.00000000137854, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2977
Epoch 2977, Loss: 0.00000001717079, Improvement: -0.00000001524096, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2978
Epoch 2978, Loss: 0.00000001901649, Improvement: 0.00000000184570, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2979
Epoch 2979, Loss: 0.00000001967184, Improvement: 0.00000000065535, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2980
Epoch 2980, Loss: 0.00000001241036, Improvement: -0.00000000726148, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2981
Epoch 2981, Loss: 0.00000000882582, Improvement: -0.00000000358454, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2982
Epoch 2982, Loss: 0.00000000879346, Improvement: -0.00000000003236, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2983
Epoch 2983, Loss: 0.00000000832241, Improvement: -0.00000000047105, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2984
Epoch 2984, Loss: 0.00000000892054, Improvement: 0.00000000059812, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2985
Epoch 2985, Loss: 0.00000000977613, Improvement: 0.00000000085559, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2986
Epoch 2986, Loss: 0.00000000940891, Improvement: -0.00000000036721, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2987
Epoch 2987, Loss: 0.00000000881825, Improvement: -0.00000000059066, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2988
Epoch 2988, Loss: 0.00000001061155, Improvement: 0.00000000179330, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2989
Epoch 2989, Loss: 0.00000000935305, Improvement: -0.00000000125851, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2990
Epoch 2990, Loss: 0.00000001341257, Improvement: 0.00000000405953, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2991
Epoch 2991, Loss: 0.00000002293382, Improvement: 0.00000000952125, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2992
Epoch 2992, Loss: 0.00000003141957, Improvement: 0.00000000848574, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2993
Epoch 2993, Loss: 0.00000003819433, Improvement: 0.00000000677477, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2994
Epoch 2994, Loss: 0.00000004570614, Improvement: 0.00000000751180, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2995
Epoch 2995, Loss: 0.00000004939583, Improvement: 0.00000000368970, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2996
Epoch 2996, Loss: 0.00000002597090, Improvement: -0.00000002342494, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2997
Epoch 2997, Loss: 0.00000002132957, Improvement: -0.00000000464133, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2998
Epoch 2998, Loss: 0.00000001761819, Improvement: -0.00000000371137, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 2999
Epoch 2999, Loss: 0.00000001465843, Improvement: -0.00000000295976, Best Loss: 0.00000000501183 in Epoch 2851
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000001281965, Improvement: -0.00000000183879, Best Loss: 0.00000000501183 in Epoch 2851
