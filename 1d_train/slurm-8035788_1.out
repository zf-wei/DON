The dimension of y_tensor is torch.Size([10201, 2]).
The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.
The dimensions of the initial conditions are: (500, 101)
The dimensions of the solutions are: (500, 101, 101)
The dimension of u_tensor is torch.Size([500, 101]).
The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.
The loaded solution dataset has dimension (500, 101, 101),
	 while the arranged linearized dataset has dimension (500, 10201).
The dimension of s_tensor is torch.Size([500, 10201]).
The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.
Epoch 1
A best model at epoch 1 has been saved with training error 0.00011346256360.
A best model at epoch 1 has been saved with training error 0.00009242042870.
A best model at epoch 1 has been saved with training error 0.00006886563642.
Epoch 1, Loss: 0.00076360503554, Improvement: 0.00076360503554, Best Loss: 0.00006886563642 in Epoch 1
Epoch 2
A best model at epoch 2 has been saved with training error 0.00005959221016.
A best model at epoch 2 has been saved with training error 0.00004211981286.
A best model at epoch 2 has been saved with training error 0.00003957655281.
Epoch 2, Loss: 0.00009838431688, Improvement: -0.00066522071866, Best Loss: 0.00003957655281 in Epoch 2
Epoch 3
A best model at epoch 3 has been saved with training error 0.00003899433068.
Epoch 3, Loss: 0.00005732045774, Improvement: -0.00004106385913, Best Loss: 0.00003899433068 in Epoch 3
Epoch 4
A best model at epoch 4 has been saved with training error 0.00003842127990.
A best model at epoch 4 has been saved with training error 0.00003393583393.
Epoch 4, Loss: 0.00005192595163, Improvement: -0.00000539450612, Best Loss: 0.00003393583393 in Epoch 4
Epoch 5
A best model at epoch 5 has been saved with training error 0.00002873485573.
Epoch 5, Loss: 0.00005118569061, Improvement: -0.00000074026102, Best Loss: 0.00002873485573 in Epoch 5
Epoch 6
Epoch 6, Loss: 0.00005088038706, Improvement: -0.00000030530355, Best Loss: 0.00002873485573 in Epoch 5
Epoch 7
Epoch 7, Loss: 0.00005052228171, Improvement: -0.00000035810535, Best Loss: 0.00002873485573 in Epoch 5
Epoch 8
Epoch 8, Loss: 0.00005007263808, Improvement: -0.00000044964363, Best Loss: 0.00002873485573 in Epoch 5
Epoch 9
Epoch 9, Loss: 0.00004952893869, Improvement: -0.00000054369939, Best Loss: 0.00002873485573 in Epoch 5
Epoch 10
Epoch 10, Loss: 0.00004890058663, Improvement: -0.00000062835206, Best Loss: 0.00002873485573 in Epoch 5
Epoch 11
Epoch 11, Loss: 0.00004804538530, Improvement: -0.00000085520132, Best Loss: 0.00002873485573 in Epoch 5
Epoch 12
A best model at epoch 12 has been saved with training error 0.00002608301111.
Epoch 12, Loss: 0.00004709377345, Improvement: -0.00000095161186, Best Loss: 0.00002608301111 in Epoch 12
Epoch 13
Epoch 13, Loss: 0.00004579992474, Improvement: -0.00000129384871, Best Loss: 0.00002608301111 in Epoch 12
Epoch 14
A best model at epoch 14 has been saved with training error 0.00002386059714.
Epoch 14, Loss: 0.00004420572768, Improvement: -0.00000159419706, Best Loss: 0.00002386059714 in Epoch 14
Epoch 15
Epoch 15, Loss: 0.00004236385394, Improvement: -0.00000184187375, Best Loss: 0.00002386059714 in Epoch 14
Epoch 16
Epoch 16, Loss: 0.00004034825570, Improvement: -0.00000201559824, Best Loss: 0.00002386059714 in Epoch 14
Epoch 17
Epoch 17, Loss: 0.00003856734811, Improvement: -0.00000178090759, Best Loss: 0.00002386059714 in Epoch 14
Epoch 18
A best model at epoch 18 has been saved with training error 0.00002328280789.
A best model at epoch 18 has been saved with training error 0.00002025357935.
Epoch 18, Loss: 0.00003748864092, Improvement: -0.00000107870719, Best Loss: 0.00002025357935 in Epoch 18
Epoch 19
A best model at epoch 19 has been saved with training error 0.00001746391172.
Epoch 19, Loss: 0.00003705439558, Improvement: -0.00000043424534, Best Loss: 0.00001746391172 in Epoch 19
Epoch 20
Epoch 20, Loss: 0.00003697308321, Improvement: -0.00000008131237, Best Loss: 0.00001746391172 in Epoch 19
Epoch 21
Epoch 21, Loss: 0.00003695614596, Improvement: -0.00000001693725, Best Loss: 0.00001746391172 in Epoch 19
Epoch 22
A best model at epoch 22 has been saved with training error 0.00001600524956.
Epoch 22, Loss: 0.00003694782445, Improvement: -0.00000000832151, Best Loss: 0.00001600524956 in Epoch 22
Epoch 23
Epoch 23, Loss: 0.00003694071702, Improvement: -0.00000000710743, Best Loss: 0.00001600524956 in Epoch 22
Epoch 24
Epoch 24, Loss: 0.00003693109711, Improvement: -0.00000000961991, Best Loss: 0.00001600524956 in Epoch 22
Epoch 25
Epoch 25, Loss: 0.00003692395157, Improvement: -0.00000000714554, Best Loss: 0.00001600524956 in Epoch 22
Epoch 26
Epoch 26, Loss: 0.00003691946331, Improvement: -0.00000000448827, Best Loss: 0.00001600524956 in Epoch 22
Epoch 27
Epoch 27, Loss: 0.00003692288283, Improvement: 0.00000000341952, Best Loss: 0.00001600524956 in Epoch 22
Epoch 28
Epoch 28, Loss: 0.00003691028896, Improvement: -0.00000001259386, Best Loss: 0.00001600524956 in Epoch 22
Epoch 29
Epoch 29, Loss: 0.00003690442081, Improvement: -0.00000000586815, Best Loss: 0.00001600524956 in Epoch 22
Epoch 30
Epoch 30, Loss: 0.00003690099893, Improvement: -0.00000000342188, Best Loss: 0.00001600524956 in Epoch 22
Epoch 31
Epoch 31, Loss: 0.00003690861804, Improvement: 0.00000000761911, Best Loss: 0.00001600524956 in Epoch 22
Epoch 32
Epoch 32, Loss: 0.00003689582700, Improvement: -0.00000001279104, Best Loss: 0.00001600524956 in Epoch 22
Epoch 33
Epoch 33, Loss: 0.00003688301167, Improvement: -0.00000001281533, Best Loss: 0.00001600524956 in Epoch 22
Epoch 34
Epoch 34, Loss: 0.00003687914050, Improvement: -0.00000000387117, Best Loss: 0.00001600524956 in Epoch 22
Epoch 35
Epoch 35, Loss: 0.00003687922808, Improvement: 0.00000000008758, Best Loss: 0.00001600524956 in Epoch 22
Epoch 36
Epoch 36, Loss: 0.00003689068290, Improvement: 0.00000001145481, Best Loss: 0.00001600524956 in Epoch 22
Epoch 37
Epoch 37, Loss: 0.00003686319806, Improvement: -0.00000002748484, Best Loss: 0.00001600524956 in Epoch 22
Epoch 38
Epoch 38, Loss: 0.00003685627980, Improvement: -0.00000000691825, Best Loss: 0.00001600524956 in Epoch 22
Epoch 39
Epoch 39, Loss: 0.00003684749363, Improvement: -0.00000000878617, Best Loss: 0.00001600524956 in Epoch 22
Epoch 40
Epoch 40, Loss: 0.00003684368194, Improvement: -0.00000000381169, Best Loss: 0.00001600524956 in Epoch 22
Epoch 41
Epoch 41, Loss: 0.00003685054544, Improvement: 0.00000000686350, Best Loss: 0.00001600524956 in Epoch 22
Epoch 42
Epoch 42, Loss: 0.00003683364394, Improvement: -0.00000001690150, Best Loss: 0.00001600524956 in Epoch 22
Epoch 43
Epoch 43, Loss: 0.00003682574779, Improvement: -0.00000000789614, Best Loss: 0.00001600524956 in Epoch 22
Epoch 44
Epoch 44, Loss: 0.00003681263761, Improvement: -0.00000001311018, Best Loss: 0.00001600524956 in Epoch 22
Epoch 45
Epoch 45, Loss: 0.00003681453381, Improvement: 0.00000000189621, Best Loss: 0.00001600524956 in Epoch 22
Epoch 46
Epoch 46, Loss: 0.00003681661083, Improvement: 0.00000000207701, Best Loss: 0.00001600524956 in Epoch 22
Epoch 47
Epoch 47, Loss: 0.00003678884286, Improvement: -0.00000002776796, Best Loss: 0.00001600524956 in Epoch 22
Epoch 48
Epoch 48, Loss: 0.00003678764151, Improvement: -0.00000000120135, Best Loss: 0.00001600524956 in Epoch 22
Epoch 49
Epoch 49, Loss: 0.00003677084969, Improvement: -0.00000001679182, Best Loss: 0.00001600524956 in Epoch 22
Epoch 50
Model saving checkpoint: the model trained after epoch 50 has been saved with the training errors.
Epoch 50, Loss: 0.00003677874065, Improvement: 0.00000000789096, Best Loss: 0.00001600524956 in Epoch 22
Epoch 51
Epoch 51, Loss: 0.00003675736752, Improvement: -0.00000002137313, Best Loss: 0.00001600524956 in Epoch 22
Epoch 52
Epoch 52, Loss: 0.00003676145307, Improvement: 0.00000000408554, Best Loss: 0.00001600524956 in Epoch 22
Epoch 53
Epoch 53, Loss: 0.00003673992951, Improvement: -0.00000002152356, Best Loss: 0.00001600524956 in Epoch 22
Epoch 54
Epoch 54, Loss: 0.00003672641196, Improvement: -0.00000001351755, Best Loss: 0.00001600524956 in Epoch 22
Epoch 55
Epoch 55, Loss: 0.00003671623526, Improvement: -0.00000001017670, Best Loss: 0.00001600524956 in Epoch 22
Epoch 56
Epoch 56, Loss: 0.00003670167189, Improvement: -0.00000001456337, Best Loss: 0.00001600524956 in Epoch 22
Epoch 57
Epoch 57, Loss: 0.00003669023918, Improvement: -0.00000001143271, Best Loss: 0.00001600524956 in Epoch 22
Epoch 58
Epoch 58, Loss: 0.00003669858106, Improvement: 0.00000000834189, Best Loss: 0.00001600524956 in Epoch 22
Epoch 59
Epoch 59, Loss: 0.00003667431592, Improvement: -0.00000002426514, Best Loss: 0.00001600524956 in Epoch 22
Epoch 60
Epoch 60, Loss: 0.00003665441282, Improvement: -0.00000001990311, Best Loss: 0.00001600524956 in Epoch 22
Epoch 61
Epoch 61, Loss: 0.00003663982670, Improvement: -0.00000001458611, Best Loss: 0.00001600524956 in Epoch 22
Epoch 62
Epoch 62, Loss: 0.00003664536534, Improvement: 0.00000000553864, Best Loss: 0.00001600524956 in Epoch 22
Epoch 63
Epoch 63, Loss: 0.00003664748865, Improvement: 0.00000000212331, Best Loss: 0.00001600524956 in Epoch 22
Epoch 64
Epoch 64, Loss: 0.00003661813826, Improvement: -0.00000002935039, Best Loss: 0.00001600524956 in Epoch 22
Epoch 65
Epoch 65, Loss: 0.00003658009045, Improvement: -0.00000003804780, Best Loss: 0.00001600524956 in Epoch 22
Epoch 66
Epoch 66, Loss: 0.00003655506443, Improvement: -0.00000002502602, Best Loss: 0.00001600524956 in Epoch 22
Epoch 67
Epoch 67, Loss: 0.00003653008189, Improvement: -0.00000002498255, Best Loss: 0.00001600524956 in Epoch 22
Epoch 68
Epoch 68, Loss: 0.00003652494634, Improvement: -0.00000000513555, Best Loss: 0.00001600524956 in Epoch 22
Epoch 69
Epoch 69, Loss: 0.00003650212047, Improvement: -0.00000002282586, Best Loss: 0.00001600524956 in Epoch 22
Epoch 70
Epoch 70, Loss: 0.00003646984851, Improvement: -0.00000003227196, Best Loss: 0.00001600524956 in Epoch 22
Epoch 71
Epoch 71, Loss: 0.00003642965567, Improvement: -0.00000004019284, Best Loss: 0.00001600524956 in Epoch 22
Epoch 72
Epoch 72, Loss: 0.00003639911201, Improvement: -0.00000003054365, Best Loss: 0.00001600524956 in Epoch 22
Epoch 73
Epoch 73, Loss: 0.00003637601758, Improvement: -0.00000002309444, Best Loss: 0.00001600524956 in Epoch 22
Epoch 74
Epoch 74, Loss: 0.00003632216512, Improvement: -0.00000005385245, Best Loss: 0.00001600524956 in Epoch 22
Epoch 75
Epoch 75, Loss: 0.00003628801423, Improvement: -0.00000003415089, Best Loss: 0.00001600524956 in Epoch 22
Epoch 76
Epoch 76, Loss: 0.00003623574494, Improvement: -0.00000005226930, Best Loss: 0.00001600524956 in Epoch 22
Epoch 77
Epoch 77, Loss: 0.00003619665113, Improvement: -0.00000003909381, Best Loss: 0.00001600524956 in Epoch 22
Epoch 78
Epoch 78, Loss: 0.00003613556146, Improvement: -0.00000006108967, Best Loss: 0.00001600524956 in Epoch 22
Epoch 79
A best model at epoch 79 has been saved with training error 0.00001532726310.
Epoch 79, Loss: 0.00003606650389, Improvement: -0.00000006905757, Best Loss: 0.00001532726310 in Epoch 79
Epoch 80
Epoch 80, Loss: 0.00003598344338, Improvement: -0.00000008306051, Best Loss: 0.00001532726310 in Epoch 79
Epoch 81
Epoch 81, Loss: 0.00003590255037, Improvement: -0.00000008089301, Best Loss: 0.00001532726310 in Epoch 79
Epoch 82
Epoch 82, Loss: 0.00003579392587, Improvement: -0.00000010862450, Best Loss: 0.00001532726310 in Epoch 79
Epoch 83
Epoch 83, Loss: 0.00003566220421, Improvement: -0.00000013172166, Best Loss: 0.00001532726310 in Epoch 79
Epoch 84
Epoch 84, Loss: 0.00003552119906, Improvement: -0.00000014100515, Best Loss: 0.00001532726310 in Epoch 79
Epoch 85
Epoch 85, Loss: 0.00003533817926, Improvement: -0.00000018301980, Best Loss: 0.00001532726310 in Epoch 79
Epoch 86
Epoch 86, Loss: 0.00003512058493, Improvement: -0.00000021759433, Best Loss: 0.00001532726310 in Epoch 79
Epoch 87
Epoch 87, Loss: 0.00003483232986, Improvement: -0.00000028825507, Best Loss: 0.00001532726310 in Epoch 79
Epoch 88
Epoch 88, Loss: 0.00003450321346, Improvement: -0.00000032911639, Best Loss: 0.00001532726310 in Epoch 79
Epoch 89
Epoch 89, Loss: 0.00003402275152, Improvement: -0.00000048046195, Best Loss: 0.00001532726310 in Epoch 79
Epoch 90
Epoch 90, Loss: 0.00003341591773, Improvement: -0.00000060683378, Best Loss: 0.00001532726310 in Epoch 79
Epoch 91
Epoch 91, Loss: 0.00003244832978, Improvement: -0.00000096758795, Best Loss: 0.00001532726310 in Epoch 79
Epoch 92
Epoch 92, Loss: 0.00003113308530, Improvement: -0.00000131524448, Best Loss: 0.00001532726310 in Epoch 79
Epoch 93
Epoch 93, Loss: 0.00002903785871, Improvement: -0.00000209522659, Best Loss: 0.00001532726310 in Epoch 79
Epoch 94
Epoch 94, Loss: 0.00002595609858, Improvement: -0.00000308176013, Best Loss: 0.00001532726310 in Epoch 79
Epoch 95
A best model at epoch 95 has been saved with training error 0.00001278758646.
Epoch 95, Loss: 0.00002112089196, Improvement: -0.00000483520662, Best Loss: 0.00001278758646 in Epoch 95
Epoch 96
A best model at epoch 96 has been saved with training error 0.00001062804131.
A best model at epoch 96 has been saved with training error 0.00000773375814.
Epoch 96, Loss: 0.00001605785901, Improvement: -0.00000506303295, Best Loss: 0.00000773375814 in Epoch 96
Epoch 97
Epoch 97, Loss: 0.00001366274328, Improvement: -0.00000239511573, Best Loss: 0.00000773375814 in Epoch 96
Epoch 98
Epoch 98, Loss: 0.00001338191278, Improvement: -0.00000028083050, Best Loss: 0.00000773375814 in Epoch 96
Epoch 99
Epoch 99, Loss: 0.00001428473479, Improvement: 0.00000090282201, Best Loss: 0.00000773375814 in Epoch 96
Epoch 100
Model saving checkpoint: the model trained after epoch 100 has been saved with the training errors.
Epoch 100, Loss: 0.00001344077727, Improvement: -0.00000084395751, Best Loss: 0.00000773375814 in Epoch 96
Epoch 101
Epoch 101, Loss: 0.00001285324361, Improvement: -0.00000058753367, Best Loss: 0.00000773375814 in Epoch 96
Epoch 102
Epoch 102, Loss: 0.00001387430252, Improvement: 0.00000102105892, Best Loss: 0.00000773375814 in Epoch 96
Epoch 103
Epoch 103, Loss: 0.00001524072727, Improvement: 0.00000136642475, Best Loss: 0.00000773375814 in Epoch 96
Epoch 104
A best model at epoch 104 has been saved with training error 0.00000765379991.
Epoch 104, Loss: 0.00001322203821, Improvement: -0.00000201868907, Best Loss: 0.00000765379991 in Epoch 104
Epoch 105
Epoch 105, Loss: 0.00001266392701, Improvement: -0.00000055811120, Best Loss: 0.00000765379991 in Epoch 104
Epoch 106
Epoch 106, Loss: 0.00001245467993, Improvement: -0.00000020924708, Best Loss: 0.00000765379991 in Epoch 104
Epoch 107
A best model at epoch 107 has been saved with training error 0.00000686943986.
Epoch 107, Loss: 0.00001245840749, Improvement: 0.00000000372756, Best Loss: 0.00000686943986 in Epoch 107
Epoch 108
Epoch 108, Loss: 0.00001259884921, Improvement: 0.00000014044172, Best Loss: 0.00000686943986 in Epoch 107
Epoch 109
Epoch 109, Loss: 0.00001308168530, Improvement: 0.00000048283609, Best Loss: 0.00000686943986 in Epoch 107
Epoch 110
Epoch 110, Loss: 0.00001247081655, Improvement: -0.00000061086876, Best Loss: 0.00000686943986 in Epoch 107
Epoch 111
A best model at epoch 111 has been saved with training error 0.00000501889099.
Epoch 111, Loss: 0.00001224972593, Improvement: -0.00000022109061, Best Loss: 0.00000501889099 in Epoch 111
Epoch 112
Epoch 112, Loss: 0.00001220420597, Improvement: -0.00000004551996, Best Loss: 0.00000501889099 in Epoch 111
Epoch 113
Epoch 113, Loss: 0.00001214739841, Improvement: -0.00000005680756, Best Loss: 0.00000501889099 in Epoch 111
Epoch 114
Epoch 114, Loss: 0.00001208650135, Improvement: -0.00000006089706, Best Loss: 0.00000501889099 in Epoch 111
Epoch 115
Epoch 115, Loss: 0.00001206708000, Improvement: -0.00000001942135, Best Loss: 0.00000501889099 in Epoch 111
Epoch 116
A best model at epoch 116 has been saved with training error 0.00000461734226.
Epoch 116, Loss: 0.00001206069931, Improvement: -0.00000000638070, Best Loss: 0.00000461734226 in Epoch 116
Epoch 117
Epoch 117, Loss: 0.00001201917703, Improvement: -0.00000004152228, Best Loss: 0.00000461734226 in Epoch 116
Epoch 118
Epoch 118, Loss: 0.00001200626534, Improvement: -0.00000001291169, Best Loss: 0.00000461734226 in Epoch 116
Epoch 119
Epoch 119, Loss: 0.00001199416504, Improvement: -0.00000001210030, Best Loss: 0.00000461734226 in Epoch 116
Epoch 120
Epoch 120, Loss: 0.00001191870879, Improvement: -0.00000007545625, Best Loss: 0.00000461734226 in Epoch 116
Epoch 121
Epoch 121, Loss: 0.00001205265855, Improvement: 0.00000013394977, Best Loss: 0.00000461734226 in Epoch 116
Epoch 122
Epoch 122, Loss: 0.00001291939066, Improvement: 0.00000086673210, Best Loss: 0.00000461734226 in Epoch 116
Epoch 123
Epoch 123, Loss: 0.00001691527273, Improvement: 0.00000399588207, Best Loss: 0.00000461734226 in Epoch 116
Epoch 124
Epoch 124, Loss: 0.00001326354752, Improvement: -0.00000365172521, Best Loss: 0.00000461734226 in Epoch 116
Epoch 125
Epoch 125, Loss: 0.00001204802875, Improvement: -0.00000121551877, Best Loss: 0.00000461734226 in Epoch 116
Epoch 126
Epoch 126, Loss: 0.00001175732168, Improvement: -0.00000029070707, Best Loss: 0.00000461734226 in Epoch 116
Epoch 127
Epoch 127, Loss: 0.00001172155908, Improvement: -0.00000003576260, Best Loss: 0.00000461734226 in Epoch 116
Epoch 128
Epoch 128, Loss: 0.00001169207194, Improvement: -0.00000002948714, Best Loss: 0.00000461734226 in Epoch 116
Epoch 129
Epoch 129, Loss: 0.00001166626341, Improvement: -0.00000002580853, Best Loss: 0.00000461734226 in Epoch 116
Epoch 130
Epoch 130, Loss: 0.00001164271928, Improvement: -0.00000002354413, Best Loss: 0.00000461734226 in Epoch 116
Epoch 131
Epoch 131, Loss: 0.00001162803019, Improvement: -0.00000001468909, Best Loss: 0.00000461734226 in Epoch 116
Epoch 132
Epoch 132, Loss: 0.00001161487532, Improvement: -0.00000001315486, Best Loss: 0.00000461734226 in Epoch 116
Epoch 133
Epoch 133, Loss: 0.00001159144197, Improvement: -0.00000002343336, Best Loss: 0.00000461734226 in Epoch 116
Epoch 134
Epoch 134, Loss: 0.00001159825674, Improvement: 0.00000000681478, Best Loss: 0.00000461734226 in Epoch 116
Epoch 135
Epoch 135, Loss: 0.00001154981564, Improvement: -0.00000004844110, Best Loss: 0.00000461734226 in Epoch 116
Epoch 136
Epoch 136, Loss: 0.00001151647029, Improvement: -0.00000003334535, Best Loss: 0.00000461734226 in Epoch 116
Epoch 137
Epoch 137, Loss: 0.00001146163393, Improvement: -0.00000005483637, Best Loss: 0.00000461734226 in Epoch 116
Epoch 138
Epoch 138, Loss: 0.00001143429381, Improvement: -0.00000002734012, Best Loss: 0.00000461734226 in Epoch 116
Epoch 139
Epoch 139, Loss: 0.00001140048153, Improvement: -0.00000003381228, Best Loss: 0.00000461734226 in Epoch 116
Epoch 140
Epoch 140, Loss: 0.00001139247142, Improvement: -0.00000000801010, Best Loss: 0.00000461734226 in Epoch 116
Epoch 141
Epoch 141, Loss: 0.00001136233980, Improvement: -0.00000003013163, Best Loss: 0.00000461734226 in Epoch 116
Epoch 142
A best model at epoch 142 has been saved with training error 0.00000423959909.
Epoch 142, Loss: 0.00001131000297, Improvement: -0.00000005233683, Best Loss: 0.00000423959909 in Epoch 142
Epoch 143
Epoch 143, Loss: 0.00001128395547, Improvement: -0.00000002604750, Best Loss: 0.00000423959909 in Epoch 142
Epoch 144
Epoch 144, Loss: 0.00001124718979, Improvement: -0.00000003676569, Best Loss: 0.00000423959909 in Epoch 142
Epoch 145
Epoch 145, Loss: 0.00001118646514, Improvement: -0.00000006072464, Best Loss: 0.00000423959909 in Epoch 142
Epoch 146
Epoch 146, Loss: 0.00001115430875, Improvement: -0.00000003215639, Best Loss: 0.00000423959909 in Epoch 142
Epoch 147
Epoch 147, Loss: 0.00001109942630, Improvement: -0.00000005488246, Best Loss: 0.00000423959909 in Epoch 142
Epoch 148
Epoch 148, Loss: 0.00001107133380, Improvement: -0.00000002809250, Best Loss: 0.00000423959909 in Epoch 142
Epoch 149
Epoch 149, Loss: 0.00001101953785, Improvement: -0.00000005179595, Best Loss: 0.00000423959909 in Epoch 142
Epoch 150
Model saving checkpoint: the model trained after epoch 150 has been saved with the training errors.
Epoch 150, Loss: 0.00001112493021, Improvement: 0.00000010539236, Best Loss: 0.00000423959909 in Epoch 142
Epoch 151
Epoch 151, Loss: 0.00001099022675, Improvement: -0.00000013470346, Best Loss: 0.00000423959909 in Epoch 142
Epoch 152
Epoch 152, Loss: 0.00001094619852, Improvement: -0.00000004402823, Best Loss: 0.00000423959909 in Epoch 142
Epoch 153
Epoch 153, Loss: 0.00001269701679, Improvement: 0.00000175081827, Best Loss: 0.00000423959909 in Epoch 142
Epoch 154
Epoch 154, Loss: 0.00001141854609, Improvement: -0.00000127847070, Best Loss: 0.00000423959909 in Epoch 142
Epoch 155
Epoch 155, Loss: 0.00001103669517, Improvement: -0.00000038185092, Best Loss: 0.00000423959909 in Epoch 142
Epoch 156
Epoch 156, Loss: 0.00001058096061, Improvement: -0.00000045573456, Best Loss: 0.00000423959909 in Epoch 142
Epoch 157
Epoch 157, Loss: 0.00001026352295, Improvement: -0.00000031743766, Best Loss: 0.00000423959909 in Epoch 142
Epoch 158
Epoch 158, Loss: 0.00000992401508, Improvement: -0.00000033950787, Best Loss: 0.00000423959909 in Epoch 142
Epoch 159
Epoch 159, Loss: 0.00000964820772, Improvement: -0.00000027580736, Best Loss: 0.00000423959909 in Epoch 142
Epoch 160
Epoch 160, Loss: 0.00000923537352, Improvement: -0.00000041283420, Best Loss: 0.00000423959909 in Epoch 142
Epoch 161
Epoch 161, Loss: 0.00000866620617, Improvement: -0.00000056916736, Best Loss: 0.00000423959909 in Epoch 142
Epoch 162
A best model at epoch 162 has been saved with training error 0.00000400851877.
Epoch 162, Loss: 0.00000791031962, Improvement: -0.00000075588655, Best Loss: 0.00000400851877 in Epoch 162
Epoch 163
A best model at epoch 163 has been saved with training error 0.00000294799770.
Epoch 163, Loss: 0.00000670297410, Improvement: -0.00000120734552, Best Loss: 0.00000294799770 in Epoch 163
Epoch 164
Epoch 164, Loss: 0.00000511805729, Improvement: -0.00000158491681, Best Loss: 0.00000294799770 in Epoch 163
Epoch 165
A best model at epoch 165 has been saved with training error 0.00000294749930.
Epoch 165, Loss: 0.00001190091874, Improvement: 0.00000678286145, Best Loss: 0.00000294749930 in Epoch 165
Epoch 166
Epoch 166, Loss: 0.00001827967837, Improvement: 0.00000637875963, Best Loss: 0.00000294749930 in Epoch 165
Epoch 167
Epoch 167, Loss: 0.00000613615541, Improvement: -0.00001214352296, Best Loss: 0.00000294749930 in Epoch 165
Epoch 168
Epoch 168, Loss: 0.00000520317254, Improvement: -0.00000093298287, Best Loss: 0.00000294749930 in Epoch 165
Epoch 169
A best model at epoch 169 has been saved with training error 0.00000224045743.
A best model at epoch 169 has been saved with training error 0.00000223998131.
Epoch 169, Loss: 0.00000325481012, Improvement: -0.00000194836242, Best Loss: 0.00000223998131 in Epoch 169
Epoch 170
Epoch 170, Loss: 0.00000391754257, Improvement: 0.00000066273245, Best Loss: 0.00000223998131 in Epoch 169
Epoch 171
A best model at epoch 171 has been saved with training error 0.00000197269969.
A best model at epoch 171 has been saved with training error 0.00000191379718.
Epoch 171, Loss: 0.00000311540739, Improvement: -0.00000080213518, Best Loss: 0.00000191379718 in Epoch 171
Epoch 172
A best model at epoch 172 has been saved with training error 0.00000149833875.
A best model at epoch 172 has been saved with training error 0.00000145289084.
Epoch 172, Loss: 0.00000225104434, Improvement: -0.00000086436305, Best Loss: 0.00000145289084 in Epoch 172
Epoch 173
Epoch 173, Loss: 0.00000786047943, Improvement: 0.00000560943509, Best Loss: 0.00000145289084 in Epoch 172
Epoch 174
Epoch 174, Loss: 0.00001095114446, Improvement: 0.00000309066503, Best Loss: 0.00000145289084 in Epoch 172
Epoch 175
Epoch 175, Loss: 0.00000788356192, Improvement: -0.00000306758254, Best Loss: 0.00000145289084 in Epoch 172
Epoch 176
Epoch 176, Loss: 0.00000313054039, Improvement: -0.00000475302153, Best Loss: 0.00000145289084 in Epoch 172
Epoch 177
Epoch 177, Loss: 0.00000250211279, Improvement: -0.00000062842760, Best Loss: 0.00000145289084 in Epoch 172
Epoch 178
Epoch 178, Loss: 0.00000215893710, Improvement: -0.00000034317569, Best Loss: 0.00000145289084 in Epoch 172
Epoch 179
A best model at epoch 179 has been saved with training error 0.00000121886478.
Epoch 179, Loss: 0.00000178589758, Improvement: -0.00000037303952, Best Loss: 0.00000121886478 in Epoch 179
Epoch 180
Epoch 180, Loss: 0.00000183857669, Improvement: 0.00000005267910, Best Loss: 0.00000121886478 in Epoch 179
Epoch 181
Epoch 181, Loss: 0.00000599279423, Improvement: 0.00000415421755, Best Loss: 0.00000121886478 in Epoch 179
Epoch 182
Epoch 182, Loss: 0.00000355951646, Improvement: -0.00000243327777, Best Loss: 0.00000121886478 in Epoch 179
Epoch 183
Epoch 183, Loss: 0.00000202382774, Improvement: -0.00000153568872, Best Loss: 0.00000121886478 in Epoch 179
Epoch 184
A best model at epoch 184 has been saved with training error 0.00000120814195.
Epoch 184, Loss: 0.00000168157700, Improvement: -0.00000034225075, Best Loss: 0.00000120814195 in Epoch 184
Epoch 185
Epoch 185, Loss: 0.00000175923820, Improvement: 0.00000007766120, Best Loss: 0.00000120814195 in Epoch 184
Epoch 186
A best model at epoch 186 has been saved with training error 0.00000116972285.
Epoch 186, Loss: 0.00000169805504, Improvement: -0.00000006118316, Best Loss: 0.00000116972285 in Epoch 186
Epoch 187
Epoch 187, Loss: 0.00000160668723, Improvement: -0.00000009136780, Best Loss: 0.00000116972285 in Epoch 186
Epoch 188
A best model at epoch 188 has been saved with training error 0.00000110729093.
A best model at epoch 188 has been saved with training error 0.00000105002812.
Epoch 188, Loss: 0.00000148989311, Improvement: -0.00000011679413, Best Loss: 0.00000105002812 in Epoch 188
Epoch 189
Epoch 189, Loss: 0.00000148310810, Improvement: -0.00000000678500, Best Loss: 0.00000105002812 in Epoch 188
Epoch 190
A best model at epoch 190 has been saved with training error 0.00000103807179.
Epoch 190, Loss: 0.00000144531516, Improvement: -0.00000003779294, Best Loss: 0.00000103807179 in Epoch 190
Epoch 191
A best model at epoch 191 has been saved with training error 0.00000081241330.
Epoch 191, Loss: 0.00000149635218, Improvement: 0.00000005103702, Best Loss: 0.00000081241330 in Epoch 191
Epoch 192
Epoch 192, Loss: 0.00000144149137, Improvement: -0.00000005486081, Best Loss: 0.00000081241330 in Epoch 191
Epoch 193
Epoch 193, Loss: 0.00000143608802, Improvement: -0.00000000540335, Best Loss: 0.00000081241330 in Epoch 191
Epoch 194
Epoch 194, Loss: 0.00000150109499, Improvement: 0.00000006500697, Best Loss: 0.00000081241330 in Epoch 191
Epoch 195
Epoch 195, Loss: 0.00000204475891, Improvement: 0.00000054366393, Best Loss: 0.00000081241330 in Epoch 191
Epoch 196
Epoch 196, Loss: 0.00000173254949, Improvement: -0.00000031220943, Best Loss: 0.00000081241330 in Epoch 191
Epoch 197
Epoch 197, Loss: 0.00000192849491, Improvement: 0.00000019594543, Best Loss: 0.00000081241330 in Epoch 191
Epoch 198
Epoch 198, Loss: 0.00000677296636, Improvement: 0.00000484447144, Best Loss: 0.00000081241330 in Epoch 191
Epoch 199
Epoch 199, Loss: 0.00000717631683, Improvement: 0.00000040335048, Best Loss: 0.00000081241330 in Epoch 191
Epoch 200
Model saving checkpoint: the model trained after epoch 200 has been saved with the training errors.
Epoch 200, Loss: 0.00000291947650, Improvement: -0.00000425684033, Best Loss: 0.00000081241330 in Epoch 191
Epoch 201
Epoch 201, Loss: 0.00000188194542, Improvement: -0.00000103753109, Best Loss: 0.00000081241330 in Epoch 191
Epoch 202
Epoch 202, Loss: 0.00000161405862, Improvement: -0.00000026788680, Best Loss: 0.00000081241330 in Epoch 191
Epoch 203
Epoch 203, Loss: 0.00000142032201, Improvement: -0.00000019373661, Best Loss: 0.00000081241330 in Epoch 191
Epoch 204
Epoch 204, Loss: 0.00000142273981, Improvement: 0.00000000241780, Best Loss: 0.00000081241330 in Epoch 191
Epoch 205
Epoch 205, Loss: 0.00000183529494, Improvement: 0.00000041255513, Best Loss: 0.00000081241330 in Epoch 191
Epoch 206
Epoch 206, Loss: 0.00000196937492, Improvement: 0.00000013407997, Best Loss: 0.00000081241330 in Epoch 191
Epoch 207
Epoch 207, Loss: 0.00000186019418, Improvement: -0.00000010918073, Best Loss: 0.00000081241330 in Epoch 191
Epoch 208
Epoch 208, Loss: 0.00000201472683, Improvement: 0.00000015453265, Best Loss: 0.00000081241330 in Epoch 191
Epoch 209
Epoch 209, Loss: 0.00000324723418, Improvement: 0.00000123250735, Best Loss: 0.00000081241330 in Epoch 191
Epoch 210
Epoch 210, Loss: 0.00000239834312, Improvement: -0.00000084889106, Best Loss: 0.00000081241330 in Epoch 191
Epoch 211
Epoch 211, Loss: 0.00000307483058, Improvement: 0.00000067648746, Best Loss: 0.00000081241330 in Epoch 191
Epoch 212
Epoch 212, Loss: 0.00000306216716, Improvement: -0.00000001266343, Best Loss: 0.00000081241330 in Epoch 191
Epoch 213
Epoch 213, Loss: 0.00000290538607, Improvement: -0.00000015678108, Best Loss: 0.00000081241330 in Epoch 191
Epoch 214
Epoch 214, Loss: 0.00000294181537, Improvement: 0.00000003642930, Best Loss: 0.00000081241330 in Epoch 191
Epoch 215
Epoch 215, Loss: 0.00000186586228, Improvement: -0.00000107595309, Best Loss: 0.00000081241330 in Epoch 191
Epoch 216
Epoch 216, Loss: 0.00000147679561, Improvement: -0.00000038906667, Best Loss: 0.00000081241330 in Epoch 191
Epoch 217
Epoch 217, Loss: 0.00000161433372, Improvement: 0.00000013753811, Best Loss: 0.00000081241330 in Epoch 191
Epoch 218
Epoch 218, Loss: 0.00000145718526, Improvement: -0.00000015714846, Best Loss: 0.00000081241330 in Epoch 191
Epoch 219
Epoch 219, Loss: 0.00000139916711, Improvement: -0.00000005801815, Best Loss: 0.00000081241330 in Epoch 191
Epoch 220
A best model at epoch 220 has been saved with training error 0.00000072974467.
Epoch 220, Loss: 0.00000126509559, Improvement: -0.00000013407152, Best Loss: 0.00000072974467 in Epoch 220
Epoch 221
Epoch 221, Loss: 0.00000119379446, Improvement: -0.00000007130112, Best Loss: 0.00000072974467 in Epoch 220
Epoch 222
Epoch 222, Loss: 0.00000138063577, Improvement: 0.00000018684131, Best Loss: 0.00000072974467 in Epoch 220
Epoch 223
Epoch 223, Loss: 0.00000143815629, Improvement: 0.00000005752053, Best Loss: 0.00000072974467 in Epoch 220
Epoch 224
Epoch 224, Loss: 0.00000198788724, Improvement: 0.00000054973095, Best Loss: 0.00000072974467 in Epoch 220
Epoch 225
Epoch 225, Loss: 0.00000392627151, Improvement: 0.00000193838427, Best Loss: 0.00000072974467 in Epoch 220
Epoch 226
Epoch 226, Loss: 0.00000217078936, Improvement: -0.00000175548215, Best Loss: 0.00000072974467 in Epoch 220
Epoch 227
Epoch 227, Loss: 0.00000137002004, Improvement: -0.00000080076932, Best Loss: 0.00000072974467 in Epoch 220
Epoch 228
Epoch 228, Loss: 0.00000229145640, Improvement: 0.00000092143636, Best Loss: 0.00000072974467 in Epoch 220
Epoch 229
Epoch 229, Loss: 0.00000365114309, Improvement: 0.00000135968669, Best Loss: 0.00000072974467 in Epoch 220
Epoch 230
Epoch 230, Loss: 0.00000395585511, Improvement: 0.00000030471201, Best Loss: 0.00000072974467 in Epoch 220
Epoch 231
Epoch 231, Loss: 0.00000293766057, Improvement: -0.00000101819454, Best Loss: 0.00000072974467 in Epoch 220
Epoch 232
Epoch 232, Loss: 0.00000459675616, Improvement: 0.00000165909560, Best Loss: 0.00000072974467 in Epoch 220
Epoch 233
Epoch 233, Loss: 0.00000288292654, Improvement: -0.00000171382962, Best Loss: 0.00000072974467 in Epoch 220
Epoch 234
Epoch 234, Loss: 0.00000217790226, Improvement: -0.00000070502428, Best Loss: 0.00000072974467 in Epoch 220
Epoch 235
Epoch 235, Loss: 0.00000140846757, Improvement: -0.00000076943469, Best Loss: 0.00000072974467 in Epoch 220
Epoch 236
Epoch 236, Loss: 0.00000130156028, Improvement: -0.00000010690729, Best Loss: 0.00000072974467 in Epoch 220
Epoch 237
Epoch 237, Loss: 0.00000146548956, Improvement: 0.00000016392928, Best Loss: 0.00000072974467 in Epoch 220
Epoch 238
Epoch 238, Loss: 0.00000134415997, Improvement: -0.00000012132959, Best Loss: 0.00000072974467 in Epoch 220
Epoch 239
Epoch 239, Loss: 0.00000161608502, Improvement: 0.00000027192505, Best Loss: 0.00000072974467 in Epoch 220
Epoch 240
Epoch 240, Loss: 0.00000180325493, Improvement: 0.00000018716991, Best Loss: 0.00000072974467 in Epoch 220
Epoch 241
Epoch 241, Loss: 0.00000157721864, Improvement: -0.00000022603629, Best Loss: 0.00000072974467 in Epoch 220
Epoch 242
Epoch 242, Loss: 0.00000233425355, Improvement: 0.00000075703491, Best Loss: 0.00000072974467 in Epoch 220
Epoch 243
Epoch 243, Loss: 0.00000220283239, Improvement: -0.00000013142115, Best Loss: 0.00000072974467 in Epoch 220
Epoch 244
Epoch 244, Loss: 0.00000247816618, Improvement: 0.00000027533379, Best Loss: 0.00000072974467 in Epoch 220
Epoch 245
Epoch 245, Loss: 0.00000289024292, Improvement: 0.00000041207674, Best Loss: 0.00000072974467 in Epoch 220
Epoch 246
Epoch 246, Loss: 0.00000389572766, Improvement: 0.00000100548475, Best Loss: 0.00000072974467 in Epoch 220
Epoch 247
Epoch 247, Loss: 0.00000225796987, Improvement: -0.00000163775779, Best Loss: 0.00000072974467 in Epoch 220
Epoch 248
Epoch 248, Loss: 0.00000242515449, Improvement: 0.00000016718462, Best Loss: 0.00000072974467 in Epoch 220
Epoch 249
Epoch 249, Loss: 0.00000158420607, Improvement: -0.00000084094841, Best Loss: 0.00000072974467 in Epoch 220
Epoch 250
Model saving checkpoint: the model trained after epoch 250 has been saved with the training errors.
Epoch 250, Loss: 0.00000149652233, Improvement: -0.00000008768375, Best Loss: 0.00000072974467 in Epoch 220
Epoch 251
Epoch 251, Loss: 0.00000150344281, Improvement: 0.00000000692049, Best Loss: 0.00000072974467 in Epoch 220
Epoch 252
Epoch 252, Loss: 0.00000124010572, Improvement: -0.00000026333709, Best Loss: 0.00000072974467 in Epoch 220
Epoch 253
Epoch 253, Loss: 0.00000118599812, Improvement: -0.00000005410760, Best Loss: 0.00000072974467 in Epoch 220
Epoch 254
Epoch 254, Loss: 0.00000281516965, Improvement: 0.00000162917152, Best Loss: 0.00000072974467 in Epoch 220
Epoch 255
Epoch 255, Loss: 0.00000326051511, Improvement: 0.00000044534546, Best Loss: 0.00000072974467 in Epoch 220
Epoch 256
Epoch 256, Loss: 0.00000166071661, Improvement: -0.00000159979850, Best Loss: 0.00000072974467 in Epoch 220
Epoch 257
A best model at epoch 257 has been saved with training error 0.00000070084280.
A best model at epoch 257 has been saved with training error 0.00000069396566.
Epoch 257, Loss: 0.00000109554359, Improvement: -0.00000056517301, Best Loss: 0.00000069396566 in Epoch 257
Epoch 258
Epoch 258, Loss: 0.00000106934435, Improvement: -0.00000002619924, Best Loss: 0.00000069396566 in Epoch 257
Epoch 259
Epoch 259, Loss: 0.00000243234404, Improvement: 0.00000136299969, Best Loss: 0.00000069396566 in Epoch 257
Epoch 260
Epoch 260, Loss: 0.00000311317953, Improvement: 0.00000068083548, Best Loss: 0.00000069396566 in Epoch 257
Epoch 261
Epoch 261, Loss: 0.00000116896107, Improvement: -0.00000194421845, Best Loss: 0.00000069396566 in Epoch 257
Epoch 262
Epoch 262, Loss: 0.00000118563509, Improvement: 0.00000001667402, Best Loss: 0.00000069396566 in Epoch 257
Epoch 263
A best model at epoch 263 has been saved with training error 0.00000067759788.
Epoch 263, Loss: 0.00000123930852, Improvement: 0.00000005367343, Best Loss: 0.00000067759788 in Epoch 263
Epoch 264
Epoch 264, Loss: 0.00000103836190, Improvement: -0.00000020094661, Best Loss: 0.00000067759788 in Epoch 263
Epoch 265
Epoch 265, Loss: 0.00000292245964, Improvement: 0.00000188409774, Best Loss: 0.00000067759788 in Epoch 263
Epoch 266
Epoch 266, Loss: 0.00000639617434, Improvement: 0.00000347371470, Best Loss: 0.00000067759788 in Epoch 263
Epoch 267
Epoch 267, Loss: 0.00000508004865, Improvement: -0.00000131612569, Best Loss: 0.00000067759788 in Epoch 263
Epoch 268
Epoch 268, Loss: 0.00000233288551, Improvement: -0.00000274716314, Best Loss: 0.00000067759788 in Epoch 263
Epoch 269
Epoch 269, Loss: 0.00000138431612, Improvement: -0.00000094856939, Best Loss: 0.00000067759788 in Epoch 263
Epoch 270
Epoch 270, Loss: 0.00000106966776, Improvement: -0.00000031464836, Best Loss: 0.00000067759788 in Epoch 263
Epoch 271
Epoch 271, Loss: 0.00000105411783, Improvement: -0.00000001554993, Best Loss: 0.00000067759788 in Epoch 263
Epoch 272
A best model at epoch 272 has been saved with training error 0.00000062740571.
Epoch 272, Loss: 0.00000095229170, Improvement: -0.00000010182613, Best Loss: 0.00000062740571 in Epoch 272
Epoch 273
A best model at epoch 273 has been saved with training error 0.00000053559700.
Epoch 273, Loss: 0.00000093304129, Improvement: -0.00000001925041, Best Loss: 0.00000053559700 in Epoch 273
Epoch 274
Epoch 274, Loss: 0.00000094702060, Improvement: 0.00000001397932, Best Loss: 0.00000053559700 in Epoch 273
Epoch 275
Epoch 275, Loss: 0.00000092153855, Improvement: -0.00000002548206, Best Loss: 0.00000053559700 in Epoch 273
Epoch 276
Epoch 276, Loss: 0.00000096290450, Improvement: 0.00000004136595, Best Loss: 0.00000053559700 in Epoch 273
Epoch 277
A best model at epoch 277 has been saved with training error 0.00000045043123.
Epoch 277, Loss: 0.00000088979517, Improvement: -0.00000007310932, Best Loss: 0.00000045043123 in Epoch 277
Epoch 278
Epoch 278, Loss: 0.00000084576131, Improvement: -0.00000004403386, Best Loss: 0.00000045043123 in Epoch 277
Epoch 279
Epoch 279, Loss: 0.00000081126321, Improvement: -0.00000003449811, Best Loss: 0.00000045043123 in Epoch 277
Epoch 280
A best model at epoch 280 has been saved with training error 0.00000037149943.
Epoch 280, Loss: 0.00000075433161, Improvement: -0.00000005693160, Best Loss: 0.00000037149943 in Epoch 280
Epoch 281
Epoch 281, Loss: 0.00000072705647, Improvement: -0.00000002727513, Best Loss: 0.00000037149943 in Epoch 280
Epoch 282
Epoch 282, Loss: 0.00000071964788, Improvement: -0.00000000740859, Best Loss: 0.00000037149943 in Epoch 280
Epoch 283
Epoch 283, Loss: 0.00000083280056, Improvement: 0.00000011315268, Best Loss: 0.00000037149943 in Epoch 280
Epoch 284
Epoch 284, Loss: 0.00000093677714, Improvement: 0.00000010397658, Best Loss: 0.00000037149943 in Epoch 280
Epoch 285
Epoch 285, Loss: 0.00000077724619, Improvement: -0.00000015953095, Best Loss: 0.00000037149943 in Epoch 280
Epoch 286
Epoch 286, Loss: 0.00000095128617, Improvement: 0.00000017403998, Best Loss: 0.00000037149943 in Epoch 280
Epoch 287
Epoch 287, Loss: 0.00000179352450, Improvement: 0.00000084223833, Best Loss: 0.00000037149943 in Epoch 280
Epoch 288
Epoch 288, Loss: 0.00000270631806, Improvement: 0.00000091279356, Best Loss: 0.00000037149943 in Epoch 280
Epoch 289
Epoch 289, Loss: 0.00000109510567, Improvement: -0.00000161121239, Best Loss: 0.00000037149943 in Epoch 280
Epoch 290
Epoch 290, Loss: 0.00000084829053, Improvement: -0.00000024681514, Best Loss: 0.00000037149943 in Epoch 280
Epoch 291
Epoch 291, Loss: 0.00000140996624, Improvement: 0.00000056167571, Best Loss: 0.00000037149943 in Epoch 280
Epoch 292
Epoch 292, Loss: 0.00000278342695, Improvement: 0.00000137346071, Best Loss: 0.00000037149943 in Epoch 280
Epoch 293
Epoch 293, Loss: 0.00000393042873, Improvement: 0.00000114700179, Best Loss: 0.00000037149943 in Epoch 280
Epoch 294
Epoch 294, Loss: 0.00000413534398, Improvement: 0.00000020491525, Best Loss: 0.00000037149943 in Epoch 280
Epoch 295
Epoch 295, Loss: 0.00000232836031, Improvement: -0.00000180698368, Best Loss: 0.00000037149943 in Epoch 280
Epoch 296
Epoch 296, Loss: 0.00000138835106, Improvement: -0.00000094000925, Best Loss: 0.00000037149943 in Epoch 280
Epoch 297
Epoch 297, Loss: 0.00000097166759, Improvement: -0.00000041668347, Best Loss: 0.00000037149943 in Epoch 280
Epoch 298
Epoch 298, Loss: 0.00000092749696, Improvement: -0.00000004417063, Best Loss: 0.00000037149943 in Epoch 280
Epoch 299
Epoch 299, Loss: 0.00000109533697, Improvement: 0.00000016784002, Best Loss: 0.00000037149943 in Epoch 280
Epoch 300
Model saving checkpoint: the model trained after epoch 300 has been saved with the training errors.
Epoch 300, Loss: 0.00000205879623, Improvement: 0.00000096345925, Best Loss: 0.00000037149943 in Epoch 280
Epoch 301
Epoch 301, Loss: 0.00000128061531, Improvement: -0.00000077818091, Best Loss: 0.00000037149943 in Epoch 280
Epoch 302
Epoch 302, Loss: 0.00000075407672, Improvement: -0.00000052653859, Best Loss: 0.00000037149943 in Epoch 280
Epoch 303
Epoch 303, Loss: 0.00000070956890, Improvement: -0.00000004450783, Best Loss: 0.00000037149943 in Epoch 280
Epoch 304
Epoch 304, Loss: 0.00000067628588, Improvement: -0.00000003328301, Best Loss: 0.00000037149943 in Epoch 280
Epoch 305
A best model at epoch 305 has been saved with training error 0.00000035600561.
Epoch 305, Loss: 0.00000064792951, Improvement: -0.00000002835638, Best Loss: 0.00000035600561 in Epoch 305
Epoch 306
Epoch 306, Loss: 0.00000069331961, Improvement: 0.00000004539010, Best Loss: 0.00000035600561 in Epoch 305
Epoch 307
Epoch 307, Loss: 0.00000071346469, Improvement: 0.00000002014508, Best Loss: 0.00000035600561 in Epoch 305
Epoch 308
Epoch 308, Loss: 0.00000066549050, Improvement: -0.00000004797419, Best Loss: 0.00000035600561 in Epoch 305
Epoch 309
Epoch 309, Loss: 0.00000070549087, Improvement: 0.00000004000037, Best Loss: 0.00000035600561 in Epoch 305
Epoch 310
Epoch 310, Loss: 0.00000140994834, Improvement: 0.00000070445747, Best Loss: 0.00000035600561 in Epoch 305
Epoch 311
Epoch 311, Loss: 0.00000396330036, Improvement: 0.00000255335202, Best Loss: 0.00000035600561 in Epoch 305
Epoch 312
Epoch 312, Loss: 0.00000241998183, Improvement: -0.00000154331854, Best Loss: 0.00000035600561 in Epoch 305
Epoch 313
Epoch 313, Loss: 0.00000117633263, Improvement: -0.00000124364920, Best Loss: 0.00000035600561 in Epoch 305
Epoch 314
Epoch 314, Loss: 0.00000081804608, Improvement: -0.00000035828654, Best Loss: 0.00000035600561 in Epoch 305
Epoch 315
Epoch 315, Loss: 0.00000080548868, Improvement: -0.00000001255741, Best Loss: 0.00000035600561 in Epoch 305
Epoch 316
Epoch 316, Loss: 0.00000068024321, Improvement: -0.00000012524547, Best Loss: 0.00000035600561 in Epoch 305
Epoch 317
Epoch 317, Loss: 0.00000073304047, Improvement: 0.00000005279725, Best Loss: 0.00000035600561 in Epoch 305
Epoch 318
Epoch 318, Loss: 0.00000074417509, Improvement: 0.00000001113462, Best Loss: 0.00000035600561 in Epoch 305
Epoch 319
Epoch 319, Loss: 0.00000063952053, Improvement: -0.00000010465456, Best Loss: 0.00000035600561 in Epoch 305
Epoch 320
Epoch 320, Loss: 0.00000057942086, Improvement: -0.00000006009967, Best Loss: 0.00000035600561 in Epoch 305
Epoch 321
Epoch 321, Loss: 0.00000056973966, Improvement: -0.00000000968119, Best Loss: 0.00000035600561 in Epoch 305
Epoch 322
Epoch 322, Loss: 0.00000055725246, Improvement: -0.00000001248720, Best Loss: 0.00000035600561 in Epoch 305
Epoch 323
Epoch 323, Loss: 0.00000055786866, Improvement: 0.00000000061620, Best Loss: 0.00000035600561 in Epoch 305
Epoch 324
Epoch 324, Loss: 0.00000062558028, Improvement: 0.00000006771161, Best Loss: 0.00000035600561 in Epoch 305
Epoch 325
Epoch 325, Loss: 0.00000082923038, Improvement: 0.00000020365010, Best Loss: 0.00000035600561 in Epoch 305
Epoch 326
Epoch 326, Loss: 0.00000111998916, Improvement: 0.00000029075878, Best Loss: 0.00000035600561 in Epoch 305
Epoch 327
Epoch 327, Loss: 0.00000150810504, Improvement: 0.00000038811588, Best Loss: 0.00000035600561 in Epoch 305
Epoch 328
Epoch 328, Loss: 0.00000101876933, Improvement: -0.00000048933571, Best Loss: 0.00000035600561 in Epoch 305
Epoch 329
Epoch 329, Loss: 0.00000075169441, Improvement: -0.00000026707492, Best Loss: 0.00000035600561 in Epoch 305
Epoch 330
Epoch 330, Loss: 0.00000067250462, Improvement: -0.00000007918979, Best Loss: 0.00000035600561 in Epoch 305
Epoch 331
Epoch 331, Loss: 0.00000096813823, Improvement: 0.00000029563360, Best Loss: 0.00000035600561 in Epoch 305
Epoch 332
Epoch 332, Loss: 0.00000074753625, Improvement: -0.00000022060197, Best Loss: 0.00000035600561 in Epoch 305
Epoch 333
Epoch 333, Loss: 0.00000076807497, Improvement: 0.00000002053871, Best Loss: 0.00000035600561 in Epoch 305
Epoch 334
Epoch 334, Loss: 0.00000102025172, Improvement: 0.00000025217676, Best Loss: 0.00000035600561 in Epoch 305
Epoch 335
Epoch 335, Loss: 0.00000098741321, Improvement: -0.00000003283851, Best Loss: 0.00000035600561 in Epoch 305
Epoch 336
Epoch 336, Loss: 0.00000184985955, Improvement: 0.00000086244634, Best Loss: 0.00000035600561 in Epoch 305
Epoch 337
Epoch 337, Loss: 0.00000325513851, Improvement: 0.00000140527896, Best Loss: 0.00000035600561 in Epoch 305
Epoch 338
Epoch 338, Loss: 0.00000111373022, Improvement: -0.00000214140829, Best Loss: 0.00000035600561 in Epoch 305
Epoch 339
Epoch 339, Loss: 0.00000078260941, Improvement: -0.00000033112081, Best Loss: 0.00000035600561 in Epoch 305
Epoch 340
Epoch 340, Loss: 0.00000071797912, Improvement: -0.00000006463030, Best Loss: 0.00000035600561 in Epoch 305
Epoch 341
Epoch 341, Loss: 0.00000063697711, Improvement: -0.00000008100200, Best Loss: 0.00000035600561 in Epoch 305
Epoch 342
A best model at epoch 342 has been saved with training error 0.00000034734813.
Epoch 342, Loss: 0.00000061792831, Improvement: -0.00000001904880, Best Loss: 0.00000034734813 in Epoch 342
Epoch 343
Epoch 343, Loss: 0.00000096203465, Improvement: 0.00000034410634, Best Loss: 0.00000034734813 in Epoch 342
Epoch 344
Epoch 344, Loss: 0.00000075264196, Improvement: -0.00000020939270, Best Loss: 0.00000034734813 in Epoch 342
Epoch 345
Epoch 345, Loss: 0.00000063807496, Improvement: -0.00000011456700, Best Loss: 0.00000034734813 in Epoch 342
Epoch 346
Epoch 346, Loss: 0.00000064591833, Improvement: 0.00000000784336, Best Loss: 0.00000034734813 in Epoch 342
Epoch 347
Epoch 347, Loss: 0.00000074339168, Improvement: 0.00000009747335, Best Loss: 0.00000034734813 in Epoch 342
Epoch 348
Epoch 348, Loss: 0.00000244667438, Improvement: 0.00000170328271, Best Loss: 0.00000034734813 in Epoch 342
Epoch 349
Epoch 349, Loss: 0.00000144530932, Improvement: -0.00000100136506, Best Loss: 0.00000034734813 in Epoch 342
Epoch 350
Model saving checkpoint: the model trained after epoch 350 has been saved with the training errors.
Epoch 350, Loss: 0.00000145690144, Improvement: 0.00000001159212, Best Loss: 0.00000034734813 in Epoch 342
Epoch 351
Epoch 351, Loss: 0.00000151852593, Improvement: 0.00000006162449, Best Loss: 0.00000034734813 in Epoch 342
Epoch 352
Epoch 352, Loss: 0.00000082166756, Improvement: -0.00000069685837, Best Loss: 0.00000034734813 in Epoch 342
Epoch 353
Epoch 353, Loss: 0.00000072908538, Improvement: -0.00000009258218, Best Loss: 0.00000034734813 in Epoch 342
Epoch 354
Epoch 354, Loss: 0.00000063107174, Improvement: -0.00000009801364, Best Loss: 0.00000034734813 in Epoch 342
Epoch 355
Epoch 355, Loss: 0.00000062413259, Improvement: -0.00000000693915, Best Loss: 0.00000034734813 in Epoch 342
Epoch 356
Epoch 356, Loss: 0.00000056843798, Improvement: -0.00000005569462, Best Loss: 0.00000034734813 in Epoch 342
Epoch 357
Epoch 357, Loss: 0.00000080321734, Improvement: 0.00000023477936, Best Loss: 0.00000034734813 in Epoch 342
Epoch 358
Epoch 358, Loss: 0.00000080099588, Improvement: -0.00000000222146, Best Loss: 0.00000034734813 in Epoch 342
Epoch 359
Epoch 359, Loss: 0.00000054636706, Improvement: -0.00000025462882, Best Loss: 0.00000034734813 in Epoch 342
Epoch 360
Epoch 360, Loss: 0.00000051366388, Improvement: -0.00000003270318, Best Loss: 0.00000034734813 in Epoch 342
Epoch 361
Epoch 361, Loss: 0.00000067912189, Improvement: 0.00000016545801, Best Loss: 0.00000034734813 in Epoch 342
Epoch 362
Epoch 362, Loss: 0.00000080776442, Improvement: 0.00000012864253, Best Loss: 0.00000034734813 in Epoch 342
Epoch 363
Epoch 363, Loss: 0.00000100600229, Improvement: 0.00000019823787, Best Loss: 0.00000034734813 in Epoch 342
Epoch 364
Epoch 364, Loss: 0.00000111968370, Improvement: 0.00000011368140, Best Loss: 0.00000034734813 in Epoch 342
Epoch 365
Epoch 365, Loss: 0.00000067699706, Improvement: -0.00000044268664, Best Loss: 0.00000034734813 in Epoch 342
Epoch 366
Epoch 366, Loss: 0.00000118640402, Improvement: 0.00000050940697, Best Loss: 0.00000034734813 in Epoch 342
Epoch 367
Epoch 367, Loss: 0.00000228023864, Improvement: 0.00000109383461, Best Loss: 0.00000034734813 in Epoch 342
Epoch 368
Epoch 368, Loss: 0.00000182520660, Improvement: -0.00000045503203, Best Loss: 0.00000034734813 in Epoch 342
Epoch 369
Epoch 369, Loss: 0.00000167607832, Improvement: -0.00000014912828, Best Loss: 0.00000034734813 in Epoch 342
Epoch 370
Epoch 370, Loss: 0.00000218072802, Improvement: 0.00000050464970, Best Loss: 0.00000034734813 in Epoch 342
Epoch 371
Epoch 371, Loss: 0.00000114303225, Improvement: -0.00000103769577, Best Loss: 0.00000034734813 in Epoch 342
Epoch 372
Epoch 372, Loss: 0.00000080242239, Improvement: -0.00000034060985, Best Loss: 0.00000034734813 in Epoch 342
Epoch 373
Epoch 373, Loss: 0.00000058926402, Improvement: -0.00000021315837, Best Loss: 0.00000034734813 in Epoch 342
Epoch 374
Epoch 374, Loss: 0.00000054803770, Improvement: -0.00000004122632, Best Loss: 0.00000034734813 in Epoch 342
Epoch 375
Epoch 375, Loss: 0.00000057483403, Improvement: 0.00000002679633, Best Loss: 0.00000034734813 in Epoch 342
Epoch 376
Epoch 376, Loss: 0.00000055211894, Improvement: -0.00000002271509, Best Loss: 0.00000034734813 in Epoch 342
Epoch 377
Epoch 377, Loss: 0.00000051338773, Improvement: -0.00000003873120, Best Loss: 0.00000034734813 in Epoch 342
Epoch 378
Epoch 378, Loss: 0.00000052993066, Improvement: 0.00000001654292, Best Loss: 0.00000034734813 in Epoch 342
Epoch 379
Epoch 379, Loss: 0.00000059374957, Improvement: 0.00000006381891, Best Loss: 0.00000034734813 in Epoch 342
Epoch 380
Epoch 380, Loss: 0.00000057212789, Improvement: -0.00000002162168, Best Loss: 0.00000034734813 in Epoch 342
Epoch 381
Epoch 381, Loss: 0.00000065531646, Improvement: 0.00000008318857, Best Loss: 0.00000034734813 in Epoch 342
Epoch 382
Epoch 382, Loss: 0.00000058696830, Improvement: -0.00000006834816, Best Loss: 0.00000034734813 in Epoch 342
Epoch 383
Epoch 383, Loss: 0.00000062402151, Improvement: 0.00000003705322, Best Loss: 0.00000034734813 in Epoch 342
Epoch 384
Epoch 384, Loss: 0.00000078813358, Improvement: 0.00000016411207, Best Loss: 0.00000034734813 in Epoch 342
Epoch 385
A best model at epoch 385 has been saved with training error 0.00000033102808.
Epoch 385, Loss: 0.00000064428410, Improvement: -0.00000014384948, Best Loss: 0.00000033102808 in Epoch 385
Epoch 386
Epoch 386, Loss: 0.00000048191072, Improvement: -0.00000016237338, Best Loss: 0.00000033102808 in Epoch 385
Epoch 387
Epoch 387, Loss: 0.00000049066842, Improvement: 0.00000000875770, Best Loss: 0.00000033102808 in Epoch 385
Epoch 388
Epoch 388, Loss: 0.00000044814988, Improvement: -0.00000004251853, Best Loss: 0.00000033102808 in Epoch 385
Epoch 389
Epoch 389, Loss: 0.00000044980787, Improvement: 0.00000000165799, Best Loss: 0.00000033102808 in Epoch 385
Epoch 390
Epoch 390, Loss: 0.00000058581848, Improvement: 0.00000013601061, Best Loss: 0.00000033102808 in Epoch 385
Epoch 391
Epoch 391, Loss: 0.00000128146627, Improvement: 0.00000069564779, Best Loss: 0.00000033102808 in Epoch 385
Epoch 392
Epoch 392, Loss: 0.00000278635236, Improvement: 0.00000150488609, Best Loss: 0.00000033102808 in Epoch 385
Epoch 393
Epoch 393, Loss: 0.00000392030748, Improvement: 0.00000113395512, Best Loss: 0.00000033102808 in Epoch 385
Epoch 394
Epoch 394, Loss: 0.00000300990518, Improvement: -0.00000091040230, Best Loss: 0.00000033102808 in Epoch 385
Epoch 395
Epoch 395, Loss: 0.00000137347797, Improvement: -0.00000163642721, Best Loss: 0.00000033102808 in Epoch 385
Epoch 396
Epoch 396, Loss: 0.00000070932344, Improvement: -0.00000066415452, Best Loss: 0.00000033102808 in Epoch 385
Epoch 397
Epoch 397, Loss: 0.00000059358455, Improvement: -0.00000011573890, Best Loss: 0.00000033102808 in Epoch 385
Epoch 398
Epoch 398, Loss: 0.00000053414537, Improvement: -0.00000005943918, Best Loss: 0.00000033102808 in Epoch 385
Epoch 399
Epoch 399, Loss: 0.00000048712267, Improvement: -0.00000004702269, Best Loss: 0.00000033102808 in Epoch 385
Epoch 400
Model saving checkpoint: the model trained after epoch 400 has been saved with the training errors.
Epoch 400, Loss: 0.00000047973553, Improvement: -0.00000000738714, Best Loss: 0.00000033102808 in Epoch 385
Epoch 401
Epoch 401, Loss: 0.00000046676394, Improvement: -0.00000001297159, Best Loss: 0.00000033102808 in Epoch 385
Epoch 402
Epoch 402, Loss: 0.00000045433513, Improvement: -0.00000001242881, Best Loss: 0.00000033102808 in Epoch 385
Epoch 403
Epoch 403, Loss: 0.00000045017048, Improvement: -0.00000000416465, Best Loss: 0.00000033102808 in Epoch 385
Epoch 404
Epoch 404, Loss: 0.00000044254485, Improvement: -0.00000000762562, Best Loss: 0.00000033102808 in Epoch 385
Epoch 405
A best model at epoch 405 has been saved with training error 0.00000031431421.
Epoch 405, Loss: 0.00000044525050, Improvement: 0.00000000270565, Best Loss: 0.00000031431421 in Epoch 405
Epoch 406
Epoch 406, Loss: 0.00000044069629, Improvement: -0.00000000455421, Best Loss: 0.00000031431421 in Epoch 405
Epoch 407
Epoch 407, Loss: 0.00000044503479, Improvement: 0.00000000433850, Best Loss: 0.00000031431421 in Epoch 405
Epoch 408
Epoch 408, Loss: 0.00000047090395, Improvement: 0.00000002586916, Best Loss: 0.00000031431421 in Epoch 405
Epoch 409
Epoch 409, Loss: 0.00000048055214, Improvement: 0.00000000964819, Best Loss: 0.00000031431421 in Epoch 405
Epoch 410
Epoch 410, Loss: 0.00000045786775, Improvement: -0.00000002268439, Best Loss: 0.00000031431421 in Epoch 405
Epoch 411
A best model at epoch 411 has been saved with training error 0.00000028596273.
Epoch 411, Loss: 0.00000044194991, Improvement: -0.00000001591784, Best Loss: 0.00000028596273 in Epoch 411
Epoch 412
Epoch 412, Loss: 0.00000043464437, Improvement: -0.00000000730554, Best Loss: 0.00000028596273 in Epoch 411
Epoch 413
Epoch 413, Loss: 0.00000043264260, Improvement: -0.00000000200177, Best Loss: 0.00000028596273 in Epoch 411
Epoch 414
Epoch 414, Loss: 0.00000043492369, Improvement: 0.00000000228110, Best Loss: 0.00000028596273 in Epoch 411
Epoch 415
Epoch 415, Loss: 0.00000047069882, Improvement: 0.00000003577512, Best Loss: 0.00000028596273 in Epoch 411
Epoch 416
Epoch 416, Loss: 0.00000087731698, Improvement: 0.00000040661816, Best Loss: 0.00000028596273 in Epoch 411
Epoch 417
Epoch 417, Loss: 0.00000061551445, Improvement: -0.00000026180252, Best Loss: 0.00000028596273 in Epoch 411
Epoch 418
Epoch 418, Loss: 0.00000061735724, Improvement: 0.00000000184279, Best Loss: 0.00000028596273 in Epoch 411
Epoch 419
Epoch 419, Loss: 0.00000070934984, Improvement: 0.00000009199260, Best Loss: 0.00000028596273 in Epoch 411
Epoch 420
Epoch 420, Loss: 0.00000088852383, Improvement: 0.00000017917400, Best Loss: 0.00000028596273 in Epoch 411
Epoch 421
Epoch 421, Loss: 0.00000088805230, Improvement: -0.00000000047153, Best Loss: 0.00000028596273 in Epoch 411
Epoch 422
Epoch 422, Loss: 0.00000068997118, Improvement: -0.00000019808112, Best Loss: 0.00000028596273 in Epoch 411
Epoch 423
Epoch 423, Loss: 0.00000135258218, Improvement: 0.00000066261100, Best Loss: 0.00000028596273 in Epoch 411
Epoch 424
Epoch 424, Loss: 0.00000222282200, Improvement: 0.00000087023982, Best Loss: 0.00000028596273 in Epoch 411
Epoch 425
Epoch 425, Loss: 0.00000084075928, Improvement: -0.00000138206272, Best Loss: 0.00000028596273 in Epoch 411
Epoch 426
Epoch 426, Loss: 0.00000064973512, Improvement: -0.00000019102416, Best Loss: 0.00000028596273 in Epoch 411
Epoch 427
Epoch 427, Loss: 0.00000050133368, Improvement: -0.00000014840144, Best Loss: 0.00000028596273 in Epoch 411
Epoch 428
Epoch 428, Loss: 0.00000046878805, Improvement: -0.00000003254563, Best Loss: 0.00000028596273 in Epoch 411
Epoch 429
Epoch 429, Loss: 0.00000044908747, Improvement: -0.00000001970058, Best Loss: 0.00000028596273 in Epoch 411
Epoch 430
Epoch 430, Loss: 0.00000052896945, Improvement: 0.00000007988198, Best Loss: 0.00000028596273 in Epoch 411
Epoch 431
Epoch 431, Loss: 0.00000053589220, Improvement: 0.00000000692275, Best Loss: 0.00000028596273 in Epoch 411
Epoch 432
Epoch 432, Loss: 0.00000048805584, Improvement: -0.00000004783636, Best Loss: 0.00000028596273 in Epoch 411
Epoch 433
Epoch 433, Loss: 0.00000062079584, Improvement: 0.00000013273999, Best Loss: 0.00000028596273 in Epoch 411
Epoch 434
Epoch 434, Loss: 0.00000113331887, Improvement: 0.00000051252303, Best Loss: 0.00000028596273 in Epoch 411
Epoch 435
Epoch 435, Loss: 0.00000062879612, Improvement: -0.00000050452274, Best Loss: 0.00000028596273 in Epoch 411
Epoch 436
Epoch 436, Loss: 0.00000048198900, Improvement: -0.00000014680712, Best Loss: 0.00000028596273 in Epoch 411
Epoch 437
Epoch 437, Loss: 0.00000045558962, Improvement: -0.00000002639939, Best Loss: 0.00000028596273 in Epoch 411
Epoch 438
Epoch 438, Loss: 0.00000047928204, Improvement: 0.00000002369242, Best Loss: 0.00000028596273 in Epoch 411
Epoch 439
Epoch 439, Loss: 0.00000094444082, Improvement: 0.00000046515879, Best Loss: 0.00000028596273 in Epoch 411
Epoch 440
Epoch 440, Loss: 0.00000128095927, Improvement: 0.00000033651845, Best Loss: 0.00000028596273 in Epoch 411
Epoch 441
Epoch 441, Loss: 0.00000063130470, Improvement: -0.00000064965458, Best Loss: 0.00000028596273 in Epoch 411
Epoch 442
Epoch 442, Loss: 0.00000074253523, Improvement: 0.00000011123054, Best Loss: 0.00000028596273 in Epoch 411
Epoch 443
Epoch 443, Loss: 0.00000058133895, Improvement: -0.00000016119628, Best Loss: 0.00000028596273 in Epoch 411
Epoch 444
Epoch 444, Loss: 0.00000065151634, Improvement: 0.00000007017739, Best Loss: 0.00000028596273 in Epoch 411
Epoch 445
Epoch 445, Loss: 0.00000069342925, Improvement: 0.00000004191291, Best Loss: 0.00000028596273 in Epoch 411
Epoch 446
Epoch 446, Loss: 0.00000058509969, Improvement: -0.00000010832956, Best Loss: 0.00000028596273 in Epoch 411
Epoch 447
Epoch 447, Loss: 0.00000059710415, Improvement: 0.00000001200446, Best Loss: 0.00000028596273 in Epoch 411
Epoch 448
Epoch 448, Loss: 0.00000060796543, Improvement: 0.00000001086128, Best Loss: 0.00000028596273 in Epoch 411
Epoch 449
Epoch 449, Loss: 0.00000082317854, Improvement: 0.00000021521311, Best Loss: 0.00000028596273 in Epoch 411
Epoch 450
Model saving checkpoint: the model trained after epoch 450 has been saved with the training errors.
Epoch 450, Loss: 0.00000111164795, Improvement: 0.00000028846941, Best Loss: 0.00000028596273 in Epoch 411
Epoch 451
Epoch 451, Loss: 0.00000080266762, Improvement: -0.00000030898034, Best Loss: 0.00000028596273 in Epoch 411
Epoch 452
Epoch 452, Loss: 0.00000054059268, Improvement: -0.00000026207494, Best Loss: 0.00000028596273 in Epoch 411
Epoch 453
Epoch 453, Loss: 0.00000045568231, Improvement: -0.00000008491037, Best Loss: 0.00000028596273 in Epoch 411
Epoch 454
Epoch 454, Loss: 0.00000048702942, Improvement: 0.00000003134712, Best Loss: 0.00000028596273 in Epoch 411
Epoch 455
Epoch 455, Loss: 0.00000048611904, Improvement: -0.00000000091038, Best Loss: 0.00000028596273 in Epoch 411
Epoch 456
Epoch 456, Loss: 0.00000062397305, Improvement: 0.00000013785401, Best Loss: 0.00000028596273 in Epoch 411
Epoch 457
Epoch 457, Loss: 0.00000070685120, Improvement: 0.00000008287814, Best Loss: 0.00000028596273 in Epoch 411
Epoch 458
Epoch 458, Loss: 0.00000141635296, Improvement: 0.00000070950176, Best Loss: 0.00000028596273 in Epoch 411
Epoch 459
Epoch 459, Loss: 0.00000110483769, Improvement: -0.00000031151527, Best Loss: 0.00000028596273 in Epoch 411
Epoch 460
Epoch 460, Loss: 0.00000081644566, Improvement: -0.00000028839203, Best Loss: 0.00000028596273 in Epoch 411
Epoch 461
Epoch 461, Loss: 0.00000051259803, Improvement: -0.00000030384763, Best Loss: 0.00000028596273 in Epoch 411
Epoch 462
A best model at epoch 462 has been saved with training error 0.00000025672318.
Epoch 462, Loss: 0.00000045119564, Improvement: -0.00000006140239, Best Loss: 0.00000025672318 in Epoch 462
Epoch 463
Epoch 463, Loss: 0.00000048733427, Improvement: 0.00000003613863, Best Loss: 0.00000025672318 in Epoch 462
Epoch 464
Epoch 464, Loss: 0.00000046197033, Improvement: -0.00000002536394, Best Loss: 0.00000025672318 in Epoch 462
Epoch 465
Epoch 465, Loss: 0.00000044165053, Improvement: -0.00000002031980, Best Loss: 0.00000025672318 in Epoch 462
Epoch 466
Epoch 466, Loss: 0.00000043623775, Improvement: -0.00000000541278, Best Loss: 0.00000025672318 in Epoch 462
Epoch 467
Epoch 467, Loss: 0.00000045931381, Improvement: 0.00000002307606, Best Loss: 0.00000025672318 in Epoch 462
Epoch 468
Epoch 468, Loss: 0.00000070867932, Improvement: 0.00000024936551, Best Loss: 0.00000025672318 in Epoch 462
Epoch 469
Epoch 469, Loss: 0.00000067620552, Improvement: -0.00000003247381, Best Loss: 0.00000025672318 in Epoch 462
Epoch 470
Epoch 470, Loss: 0.00000059181838, Improvement: -0.00000008438713, Best Loss: 0.00000025672318 in Epoch 462
Epoch 471
Epoch 471, Loss: 0.00000048479083, Improvement: -0.00000010702755, Best Loss: 0.00000025672318 in Epoch 462
Epoch 472
Epoch 472, Loss: 0.00000052540567, Improvement: 0.00000004061484, Best Loss: 0.00000025672318 in Epoch 462
Epoch 473
Epoch 473, Loss: 0.00000098958561, Improvement: 0.00000046417994, Best Loss: 0.00000025672318 in Epoch 462
Epoch 474
Epoch 474, Loss: 0.00000094497009, Improvement: -0.00000004461552, Best Loss: 0.00000025672318 in Epoch 462
Epoch 475
Epoch 475, Loss: 0.00000082103220, Improvement: -0.00000012393788, Best Loss: 0.00000025672318 in Epoch 462
Epoch 476
Epoch 476, Loss: 0.00000082525735, Improvement: 0.00000000422515, Best Loss: 0.00000025672318 in Epoch 462
Epoch 477
Epoch 477, Loss: 0.00000055934780, Improvement: -0.00000026590956, Best Loss: 0.00000025672318 in Epoch 462
Epoch 478
Epoch 478, Loss: 0.00000059528094, Improvement: 0.00000003593314, Best Loss: 0.00000025672318 in Epoch 462
Epoch 479
Epoch 479, Loss: 0.00000059367057, Improvement: -0.00000000161036, Best Loss: 0.00000025672318 in Epoch 462
Epoch 480
Epoch 480, Loss: 0.00000074471031, Improvement: 0.00000015103973, Best Loss: 0.00000025672318 in Epoch 462
Epoch 481
Epoch 481, Loss: 0.00000052799433, Improvement: -0.00000021671597, Best Loss: 0.00000025672318 in Epoch 462
Epoch 482
Epoch 482, Loss: 0.00000085602765, Improvement: 0.00000032803331, Best Loss: 0.00000025672318 in Epoch 462
Epoch 483
Epoch 483, Loss: 0.00000077620693, Improvement: -0.00000007982072, Best Loss: 0.00000025672318 in Epoch 462
Epoch 484
Epoch 484, Loss: 0.00000155650390, Improvement: 0.00000078029698, Best Loss: 0.00000025672318 in Epoch 462
Epoch 485
Epoch 485, Loss: 0.00000102277671, Improvement: -0.00000053372719, Best Loss: 0.00000025672318 in Epoch 462
Epoch 486
Epoch 486, Loss: 0.00000055983445, Improvement: -0.00000046294226, Best Loss: 0.00000025672318 in Epoch 462
Epoch 487
Epoch 487, Loss: 0.00000046979783, Improvement: -0.00000009003663, Best Loss: 0.00000025672318 in Epoch 462
Epoch 488
Epoch 488, Loss: 0.00000044205129, Improvement: -0.00000002774653, Best Loss: 0.00000025672318 in Epoch 462
Epoch 489
A best model at epoch 489 has been saved with training error 0.00000025264475.
Epoch 489, Loss: 0.00000042213738, Improvement: -0.00000001991391, Best Loss: 0.00000025264475 in Epoch 489
Epoch 490
Epoch 490, Loss: 0.00000041281127, Improvement: -0.00000000932611, Best Loss: 0.00000025264475 in Epoch 489
Epoch 491
Epoch 491, Loss: 0.00000040869935, Improvement: -0.00000000411192, Best Loss: 0.00000025264475 in Epoch 489
Epoch 492
Epoch 492, Loss: 0.00000039813120, Improvement: -0.00000001056815, Best Loss: 0.00000025264475 in Epoch 489
Epoch 493
Epoch 493, Loss: 0.00000038245796, Improvement: -0.00000001567325, Best Loss: 0.00000025264475 in Epoch 489
Epoch 494
Epoch 494, Loss: 0.00000039624326, Improvement: 0.00000001378530, Best Loss: 0.00000025264475 in Epoch 489
Epoch 495
Epoch 495, Loss: 0.00000039390223, Improvement: -0.00000000234103, Best Loss: 0.00000025264475 in Epoch 489
Epoch 496
Epoch 496, Loss: 0.00000040149121, Improvement: 0.00000000758898, Best Loss: 0.00000025264475 in Epoch 489
Epoch 497
Epoch 497, Loss: 0.00000049184453, Improvement: 0.00000009035331, Best Loss: 0.00000025264475 in Epoch 489
Epoch 498
Epoch 498, Loss: 0.00000084926764, Improvement: 0.00000035742312, Best Loss: 0.00000025264475 in Epoch 489
Epoch 499
Epoch 499, Loss: 0.00000086110446, Improvement: 0.00000001183682, Best Loss: 0.00000025264475 in Epoch 489
Epoch 500
Model saving checkpoint: the model trained after epoch 500 has been saved with the training errors.
Epoch 500, Loss: 0.00000068743079, Improvement: -0.00000017367367, Best Loss: 0.00000025264475 in Epoch 489
Epoch 501
Epoch 501, Loss: 0.00000058124587, Improvement: -0.00000010618492, Best Loss: 0.00000025264475 in Epoch 489
Epoch 502
Epoch 502, Loss: 0.00000051418297, Improvement: -0.00000006706289, Best Loss: 0.00000025264475 in Epoch 489
Epoch 503
Epoch 503, Loss: 0.00000049800189, Improvement: -0.00000001618108, Best Loss: 0.00000025264475 in Epoch 489
Epoch 504
Epoch 504, Loss: 0.00000052160114, Improvement: 0.00000002359924, Best Loss: 0.00000025264475 in Epoch 489
Epoch 505
Epoch 505, Loss: 0.00000048779787, Improvement: -0.00000003380326, Best Loss: 0.00000025264475 in Epoch 489
Epoch 506
Epoch 506, Loss: 0.00000051252127, Improvement: 0.00000002472339, Best Loss: 0.00000025264475 in Epoch 489
Epoch 507
Epoch 507, Loss: 0.00000044278030, Improvement: -0.00000006974097, Best Loss: 0.00000025264475 in Epoch 489
Epoch 508
Epoch 508, Loss: 0.00000049837812, Improvement: 0.00000005559782, Best Loss: 0.00000025264475 in Epoch 489
Epoch 509
Epoch 509, Loss: 0.00000067156699, Improvement: 0.00000017318887, Best Loss: 0.00000025264475 in Epoch 489
Epoch 510
Epoch 510, Loss: 0.00000094185725, Improvement: 0.00000027029025, Best Loss: 0.00000025264475 in Epoch 489
Epoch 511
Epoch 511, Loss: 0.00000125995929, Improvement: 0.00000031810205, Best Loss: 0.00000025264475 in Epoch 489
Epoch 512
Epoch 512, Loss: 0.00000073648721, Improvement: -0.00000052347208, Best Loss: 0.00000025264475 in Epoch 489
Epoch 513
Epoch 513, Loss: 0.00000054586192, Improvement: -0.00000019062529, Best Loss: 0.00000025264475 in Epoch 489
Epoch 514
Epoch 514, Loss: 0.00000039508427, Improvement: -0.00000015077765, Best Loss: 0.00000025264475 in Epoch 489
Epoch 515
Epoch 515, Loss: 0.00000038589305, Improvement: -0.00000000919123, Best Loss: 0.00000025264475 in Epoch 489
Epoch 516
Epoch 516, Loss: 0.00000038300459, Improvement: -0.00000000288846, Best Loss: 0.00000025264475 in Epoch 489
Epoch 517
Epoch 517, Loss: 0.00000037558566, Improvement: -0.00000000741893, Best Loss: 0.00000025264475 in Epoch 489
Epoch 518
Epoch 518, Loss: 0.00000041367358, Improvement: 0.00000003808792, Best Loss: 0.00000025264475 in Epoch 489
Epoch 519
Epoch 519, Loss: 0.00000042797527, Improvement: 0.00000001430169, Best Loss: 0.00000025264475 in Epoch 489
Epoch 520
Epoch 520, Loss: 0.00000043557700, Improvement: 0.00000000760173, Best Loss: 0.00000025264475 in Epoch 489
Epoch 521
Epoch 521, Loss: 0.00000058931916, Improvement: 0.00000015374216, Best Loss: 0.00000025264475 in Epoch 489
Epoch 522
Epoch 522, Loss: 0.00000063002926, Improvement: 0.00000004071011, Best Loss: 0.00000025264475 in Epoch 489
Epoch 523
Epoch 523, Loss: 0.00000057609537, Improvement: -0.00000005393389, Best Loss: 0.00000025264475 in Epoch 489
Epoch 524
Epoch 524, Loss: 0.00000049115439, Improvement: -0.00000008494099, Best Loss: 0.00000025264475 in Epoch 489
Epoch 525
Epoch 525, Loss: 0.00000049073179, Improvement: -0.00000000042260, Best Loss: 0.00000025264475 in Epoch 489
Epoch 526
Epoch 526, Loss: 0.00000055729950, Improvement: 0.00000006656771, Best Loss: 0.00000025264475 in Epoch 489
Epoch 527
Epoch 527, Loss: 0.00000055927243, Improvement: 0.00000000197294, Best Loss: 0.00000025264475 in Epoch 489
Epoch 528
Epoch 528, Loss: 0.00000047130808, Improvement: -0.00000008796436, Best Loss: 0.00000025264475 in Epoch 489
Epoch 529
Epoch 529, Loss: 0.00000039552225, Improvement: -0.00000007578583, Best Loss: 0.00000025264475 in Epoch 489
Epoch 530
Epoch 530, Loss: 0.00000041347454, Improvement: 0.00000001795229, Best Loss: 0.00000025264475 in Epoch 489
Epoch 531
Epoch 531, Loss: 0.00000039276268, Improvement: -0.00000002071186, Best Loss: 0.00000025264475 in Epoch 489
Epoch 532
Epoch 532, Loss: 0.00000061726503, Improvement: 0.00000022450234, Best Loss: 0.00000025264475 in Epoch 489
Epoch 533
Epoch 533, Loss: 0.00000121184146, Improvement: 0.00000059457643, Best Loss: 0.00000025264475 in Epoch 489
Epoch 534
Epoch 534, Loss: 0.00000111154589, Improvement: -0.00000010029557, Best Loss: 0.00000025264475 in Epoch 489
Epoch 535
Epoch 535, Loss: 0.00000099996430, Improvement: -0.00000011158159, Best Loss: 0.00000025264475 in Epoch 489
Epoch 536
Epoch 536, Loss: 0.00000077820146, Improvement: -0.00000022176283, Best Loss: 0.00000025264475 in Epoch 489
Epoch 537
Epoch 537, Loss: 0.00000047346254, Improvement: -0.00000030473892, Best Loss: 0.00000025264475 in Epoch 489
Epoch 538
Epoch 538, Loss: 0.00000041504697, Improvement: -0.00000005841557, Best Loss: 0.00000025264475 in Epoch 489
Epoch 539
Epoch 539, Loss: 0.00000045758230, Improvement: 0.00000004253533, Best Loss: 0.00000025264475 in Epoch 489
Epoch 540
Epoch 540, Loss: 0.00000039937937, Improvement: -0.00000005820292, Best Loss: 0.00000025264475 in Epoch 489
Epoch 541
Epoch 541, Loss: 0.00000040621058, Improvement: 0.00000000683121, Best Loss: 0.00000025264475 in Epoch 489
Epoch 542
Epoch 542, Loss: 0.00000070371933, Improvement: 0.00000029750875, Best Loss: 0.00000025264475 in Epoch 489
Epoch 543
Epoch 543, Loss: 0.00000088274474, Improvement: 0.00000017902541, Best Loss: 0.00000025264475 in Epoch 489
Epoch 544
Epoch 544, Loss: 0.00000075498296, Improvement: -0.00000012776178, Best Loss: 0.00000025264475 in Epoch 489
Epoch 545
Epoch 545, Loss: 0.00000047163533, Improvement: -0.00000028334763, Best Loss: 0.00000025264475 in Epoch 489
Epoch 546
Epoch 546, Loss: 0.00000037634630, Improvement: -0.00000009528903, Best Loss: 0.00000025264475 in Epoch 489
Epoch 547
Epoch 547, Loss: 0.00000035407787, Improvement: -0.00000002226842, Best Loss: 0.00000025264475 in Epoch 489
Epoch 548
Epoch 548, Loss: 0.00000039534770, Improvement: 0.00000004126982, Best Loss: 0.00000025264475 in Epoch 489
Epoch 549
Epoch 549, Loss: 0.00000037697954, Improvement: -0.00000001836815, Best Loss: 0.00000025264475 in Epoch 489
Epoch 550
Model saving checkpoint: the model trained after epoch 550 has been saved with the training errors.
Epoch 550, Loss: 0.00000036194716, Improvement: -0.00000001503238, Best Loss: 0.00000025264475 in Epoch 489
Epoch 551
Epoch 551, Loss: 0.00000037186518, Improvement: 0.00000000991801, Best Loss: 0.00000025264475 in Epoch 489
Epoch 552
Epoch 552, Loss: 0.00000038287392, Improvement: 0.00000001100874, Best Loss: 0.00000025264475 in Epoch 489
Epoch 553
Epoch 553, Loss: 0.00000035240142, Improvement: -0.00000003047250, Best Loss: 0.00000025264475 in Epoch 489
Epoch 554
Epoch 554, Loss: 0.00000040716448, Improvement: 0.00000005476306, Best Loss: 0.00000025264475 in Epoch 489
Epoch 555
Epoch 555, Loss: 0.00000036176159, Improvement: -0.00000004540289, Best Loss: 0.00000025264475 in Epoch 489
Epoch 556
A best model at epoch 556 has been saved with training error 0.00000022521152.
Epoch 556, Loss: 0.00000032466664, Improvement: -0.00000003709495, Best Loss: 0.00000022521152 in Epoch 556
Epoch 557
Epoch 557, Loss: 0.00000053287677, Improvement: 0.00000020821013, Best Loss: 0.00000022521152 in Epoch 556
Epoch 558
Epoch 558, Loss: 0.00000114055532, Improvement: 0.00000060767855, Best Loss: 0.00000022521152 in Epoch 556
Epoch 559
Epoch 559, Loss: 0.00000195520793, Improvement: 0.00000081465261, Best Loss: 0.00000022521152 in Epoch 556
Epoch 560
Epoch 560, Loss: 0.00000080787539, Improvement: -0.00000114733254, Best Loss: 0.00000022521152 in Epoch 556
Epoch 561
Epoch 561, Loss: 0.00000045278793, Improvement: -0.00000035508746, Best Loss: 0.00000022521152 in Epoch 556
Epoch 562
Epoch 562, Loss: 0.00000036275322, Improvement: -0.00000009003471, Best Loss: 0.00000022521152 in Epoch 556
Epoch 563
Epoch 563, Loss: 0.00000033321664, Improvement: -0.00000002953658, Best Loss: 0.00000022521152 in Epoch 556
Epoch 564
Epoch 564, Loss: 0.00000032540571, Improvement: -0.00000000781093, Best Loss: 0.00000022521152 in Epoch 556
Epoch 565
Epoch 565, Loss: 0.00000032148306, Improvement: -0.00000000392265, Best Loss: 0.00000022521152 in Epoch 556
Epoch 566
A best model at epoch 566 has been saved with training error 0.00000020158939.
Epoch 566, Loss: 0.00000035329775, Improvement: 0.00000003181469, Best Loss: 0.00000020158939 in Epoch 566
Epoch 567
Epoch 567, Loss: 0.00000036499860, Improvement: 0.00000001170085, Best Loss: 0.00000020158939 in Epoch 566
Epoch 568
Epoch 568, Loss: 0.00000032147327, Improvement: -0.00000004352532, Best Loss: 0.00000020158939 in Epoch 566
Epoch 569
A best model at epoch 569 has been saved with training error 0.00000019660392.
Epoch 569, Loss: 0.00000030940403, Improvement: -0.00000001206925, Best Loss: 0.00000019660392 in Epoch 569
Epoch 570
Epoch 570, Loss: 0.00000030132035, Improvement: -0.00000000808368, Best Loss: 0.00000019660392 in Epoch 569
Epoch 571
Epoch 571, Loss: 0.00000030587116, Improvement: 0.00000000455081, Best Loss: 0.00000019660392 in Epoch 569
Epoch 572
Epoch 572, Loss: 0.00000030521173, Improvement: -0.00000000065943, Best Loss: 0.00000019660392 in Epoch 569
Epoch 573
Epoch 573, Loss: 0.00000032873558, Improvement: 0.00000002352386, Best Loss: 0.00000019660392 in Epoch 569
Epoch 574
A best model at epoch 574 has been saved with training error 0.00000019193041.
Epoch 574, Loss: 0.00000032993729, Improvement: 0.00000000120171, Best Loss: 0.00000019193041 in Epoch 574
Epoch 575
Epoch 575, Loss: 0.00000045639487, Improvement: 0.00000012645757, Best Loss: 0.00000019193041 in Epoch 574
Epoch 576
Epoch 576, Loss: 0.00000044244430, Improvement: -0.00000001395057, Best Loss: 0.00000019193041 in Epoch 574
Epoch 577
Epoch 577, Loss: 0.00000040107474, Improvement: -0.00000004136956, Best Loss: 0.00000019193041 in Epoch 574
Epoch 578
Epoch 578, Loss: 0.00000042533398, Improvement: 0.00000002425924, Best Loss: 0.00000019193041 in Epoch 574
Epoch 579
Epoch 579, Loss: 0.00000031705043, Improvement: -0.00000010828355, Best Loss: 0.00000019193041 in Epoch 574
Epoch 580
Epoch 580, Loss: 0.00000028841503, Improvement: -0.00000002863540, Best Loss: 0.00000019193041 in Epoch 574
Epoch 581
Epoch 581, Loss: 0.00000028502924, Improvement: -0.00000000338579, Best Loss: 0.00000019193041 in Epoch 574
Epoch 582
Epoch 582, Loss: 0.00000042512418, Improvement: 0.00000014009494, Best Loss: 0.00000019193041 in Epoch 574
Epoch 583
Epoch 583, Loss: 0.00000034312777, Improvement: -0.00000008199641, Best Loss: 0.00000019193041 in Epoch 574
Epoch 584
A best model at epoch 584 has been saved with training error 0.00000018907305.
Epoch 584, Loss: 0.00000029691392, Improvement: -0.00000004621385, Best Loss: 0.00000018907305 in Epoch 584
Epoch 585
Epoch 585, Loss: 0.00000043725897, Improvement: 0.00000014034505, Best Loss: 0.00000018907305 in Epoch 584
Epoch 586
Epoch 586, Loss: 0.00000046310677, Improvement: 0.00000002584781, Best Loss: 0.00000018907305 in Epoch 584
Epoch 587
Epoch 587, Loss: 0.00000043857303, Improvement: -0.00000002453374, Best Loss: 0.00000018907305 in Epoch 584
Epoch 588
Epoch 588, Loss: 0.00000044767045, Improvement: 0.00000000909742, Best Loss: 0.00000018907305 in Epoch 584
Epoch 589
Epoch 589, Loss: 0.00000053232525, Improvement: 0.00000008465480, Best Loss: 0.00000018907305 in Epoch 584
Epoch 590
Epoch 590, Loss: 0.00000049282917, Improvement: -0.00000003949609, Best Loss: 0.00000018907305 in Epoch 584
Epoch 591
Epoch 591, Loss: 0.00000052216523, Improvement: 0.00000002933606, Best Loss: 0.00000018907305 in Epoch 584
Epoch 592
Epoch 592, Loss: 0.00000039692425, Improvement: -0.00000012524098, Best Loss: 0.00000018907305 in Epoch 584
Epoch 593
Epoch 593, Loss: 0.00000032932805, Improvement: -0.00000006759619, Best Loss: 0.00000018907305 in Epoch 584
Epoch 594
Epoch 594, Loss: 0.00000044303669, Improvement: 0.00000011370864, Best Loss: 0.00000018907305 in Epoch 584
Epoch 595
Epoch 595, Loss: 0.00000042164698, Improvement: -0.00000002138971, Best Loss: 0.00000018907305 in Epoch 584
Epoch 596
Epoch 596, Loss: 0.00000037706458, Improvement: -0.00000004458240, Best Loss: 0.00000018907305 in Epoch 584
Epoch 597
Epoch 597, Loss: 0.00000037133083, Improvement: -0.00000000573375, Best Loss: 0.00000018907305 in Epoch 584
Epoch 598
Epoch 598, Loss: 0.00000067915683, Improvement: 0.00000030782600, Best Loss: 0.00000018907305 in Epoch 584
Epoch 599
Epoch 599, Loss: 0.00000055515556, Improvement: -0.00000012400127, Best Loss: 0.00000018907305 in Epoch 584
Epoch 600
Model saving checkpoint: the model trained after epoch 600 has been saved with the training errors.
Epoch 600, Loss: 0.00000060856606, Improvement: 0.00000005341050, Best Loss: 0.00000018907305 in Epoch 584
Epoch 601
Epoch 601, Loss: 0.00000047260271, Improvement: -0.00000013596335, Best Loss: 0.00000018907305 in Epoch 584
Epoch 602
Epoch 602, Loss: 0.00000050995590, Improvement: 0.00000003735318, Best Loss: 0.00000018907305 in Epoch 584
Epoch 603
Epoch 603, Loss: 0.00000070091486, Improvement: 0.00000019095897, Best Loss: 0.00000018907305 in Epoch 584
Epoch 604
Epoch 604, Loss: 0.00000063857842, Improvement: -0.00000006233645, Best Loss: 0.00000018907305 in Epoch 584
Epoch 605
Epoch 605, Loss: 0.00000114093802, Improvement: 0.00000050235961, Best Loss: 0.00000018907305 in Epoch 584
Epoch 606
Epoch 606, Loss: 0.00000069550234, Improvement: -0.00000044543569, Best Loss: 0.00000018907305 in Epoch 584
Epoch 607
Epoch 607, Loss: 0.00000071441498, Improvement: 0.00000001891264, Best Loss: 0.00000018907305 in Epoch 584
Epoch 608
Epoch 608, Loss: 0.00000058303090, Improvement: -0.00000013138407, Best Loss: 0.00000018907305 in Epoch 584
Epoch 609
Epoch 609, Loss: 0.00000038788686, Improvement: -0.00000019514404, Best Loss: 0.00000018907305 in Epoch 584
Epoch 610
Epoch 610, Loss: 0.00000029929815, Improvement: -0.00000008858871, Best Loss: 0.00000018907305 in Epoch 584
Epoch 611
A best model at epoch 611 has been saved with training error 0.00000017788125.
Epoch 611, Loss: 0.00000025098432, Improvement: -0.00000004831383, Best Loss: 0.00000017788125 in Epoch 611
Epoch 612
A best model at epoch 612 has been saved with training error 0.00000017666700.
A best model at epoch 612 has been saved with training error 0.00000016938411.
Epoch 612, Loss: 0.00000024080216, Improvement: -0.00000001018215, Best Loss: 0.00000016938411 in Epoch 612
Epoch 613
Epoch 613, Loss: 0.00000024400263, Improvement: 0.00000000320046, Best Loss: 0.00000016938411 in Epoch 612
Epoch 614
A best model at epoch 614 has been saved with training error 0.00000015222389.
Epoch 614, Loss: 0.00000023520841, Improvement: -0.00000000879422, Best Loss: 0.00000015222389 in Epoch 614
Epoch 615
Epoch 615, Loss: 0.00000023485827, Improvement: -0.00000000035014, Best Loss: 0.00000015222389 in Epoch 614
Epoch 616
A best model at epoch 616 has been saved with training error 0.00000015020534.
Epoch 616, Loss: 0.00000021184002, Improvement: -0.00000002301824, Best Loss: 0.00000015020534 in Epoch 616
Epoch 617
Epoch 617, Loss: 0.00000021089290, Improvement: -0.00000000094713, Best Loss: 0.00000015020534 in Epoch 616
Epoch 618
A best model at epoch 618 has been saved with training error 0.00000013000091.
Epoch 618, Loss: 0.00000021759246, Improvement: 0.00000000669956, Best Loss: 0.00000013000091 in Epoch 618
Epoch 619
Epoch 619, Loss: 0.00000021293248, Improvement: -0.00000000465997, Best Loss: 0.00000013000091 in Epoch 618
Epoch 620
Epoch 620, Loss: 0.00000023339958, Improvement: 0.00000002046710, Best Loss: 0.00000013000091 in Epoch 618
Epoch 621
Epoch 621, Loss: 0.00000021048051, Improvement: -0.00000002291907, Best Loss: 0.00000013000091 in Epoch 618
Epoch 622
Epoch 622, Loss: 0.00000019765624, Improvement: -0.00000001282427, Best Loss: 0.00000013000091 in Epoch 618
Epoch 623
Epoch 623, Loss: 0.00000018782515, Improvement: -0.00000000983109, Best Loss: 0.00000013000091 in Epoch 618
Epoch 624
Epoch 624, Loss: 0.00000018746548, Improvement: -0.00000000035967, Best Loss: 0.00000013000091 in Epoch 618
Epoch 625
A best model at epoch 625 has been saved with training error 0.00000012033692.
Epoch 625, Loss: 0.00000020689174, Improvement: 0.00000001942625, Best Loss: 0.00000012033692 in Epoch 625
Epoch 626
Epoch 626, Loss: 0.00000025061463, Improvement: 0.00000004372289, Best Loss: 0.00000012033692 in Epoch 625
Epoch 627
Epoch 627, Loss: 0.00000023061470, Improvement: -0.00000001999993, Best Loss: 0.00000012033692 in Epoch 625
Epoch 628
Epoch 628, Loss: 0.00000025093097, Improvement: 0.00000002031626, Best Loss: 0.00000012033692 in Epoch 625
Epoch 629
Epoch 629, Loss: 0.00000022266990, Improvement: -0.00000002826106, Best Loss: 0.00000012033692 in Epoch 625
Epoch 630
Epoch 630, Loss: 0.00000019667300, Improvement: -0.00000002599691, Best Loss: 0.00000012033692 in Epoch 625
Epoch 631
Epoch 631, Loss: 0.00000020599968, Improvement: 0.00000000932668, Best Loss: 0.00000012033692 in Epoch 625
Epoch 632
Epoch 632, Loss: 0.00000027678156, Improvement: 0.00000007078188, Best Loss: 0.00000012033692 in Epoch 625
Epoch 633
Epoch 633, Loss: 0.00000023607448, Improvement: -0.00000004070708, Best Loss: 0.00000012033692 in Epoch 625
Epoch 634
Epoch 634, Loss: 0.00000035279251, Improvement: 0.00000011671803, Best Loss: 0.00000012033692 in Epoch 625
Epoch 635
Epoch 635, Loss: 0.00000063468037, Improvement: 0.00000028188785, Best Loss: 0.00000012033692 in Epoch 625
Epoch 636
Epoch 636, Loss: 0.00000042159307, Improvement: -0.00000021308729, Best Loss: 0.00000012033692 in Epoch 625
Epoch 637
Epoch 637, Loss: 0.00000024186884, Improvement: -0.00000017972423, Best Loss: 0.00000012033692 in Epoch 625
Epoch 638
Epoch 638, Loss: 0.00000022328629, Improvement: -0.00000001858255, Best Loss: 0.00000012033692 in Epoch 625
Epoch 639
Epoch 639, Loss: 0.00000020175836, Improvement: -0.00000002152793, Best Loss: 0.00000012033692 in Epoch 625
Epoch 640
Epoch 640, Loss: 0.00000027982430, Improvement: 0.00000007806594, Best Loss: 0.00000012033692 in Epoch 625
Epoch 641
Epoch 641, Loss: 0.00000051071076, Improvement: 0.00000023088646, Best Loss: 0.00000012033692 in Epoch 625
Epoch 642
Epoch 642, Loss: 0.00000039136440, Improvement: -0.00000011934637, Best Loss: 0.00000012033692 in Epoch 625
Epoch 643
Epoch 643, Loss: 0.00000032460429, Improvement: -0.00000006676010, Best Loss: 0.00000012033692 in Epoch 625
Epoch 644
Epoch 644, Loss: 0.00000048393179, Improvement: 0.00000015932749, Best Loss: 0.00000012033692 in Epoch 625
Epoch 645
Epoch 645, Loss: 0.00000086450758, Improvement: 0.00000038057579, Best Loss: 0.00000012033692 in Epoch 625
Epoch 646
Epoch 646, Loss: 0.00000037600277, Improvement: -0.00000048850481, Best Loss: 0.00000012033692 in Epoch 625
Epoch 647
Epoch 647, Loss: 0.00000022676742, Improvement: -0.00000014923535, Best Loss: 0.00000012033692 in Epoch 625
Epoch 648
Epoch 648, Loss: 0.00000023571017, Improvement: 0.00000000894274, Best Loss: 0.00000012033692 in Epoch 625
Epoch 649
Epoch 649, Loss: 0.00000022269622, Improvement: -0.00000001301395, Best Loss: 0.00000012033692 in Epoch 625
Epoch 650
Model saving checkpoint: the model trained after epoch 650 has been saved with the training errors.
Epoch 650, Loss: 0.00000019608921, Improvement: -0.00000002660701, Best Loss: 0.00000012033692 in Epoch 625
Epoch 651
Epoch 651, Loss: 0.00000017323130, Improvement: -0.00000002285791, Best Loss: 0.00000012033692 in Epoch 625
Epoch 652
Epoch 652, Loss: 0.00000016146832, Improvement: -0.00000001176299, Best Loss: 0.00000012033692 in Epoch 625
Epoch 653
A best model at epoch 653 has been saved with training error 0.00000009750763.
Epoch 653, Loss: 0.00000015877629, Improvement: -0.00000000269203, Best Loss: 0.00000009750763 in Epoch 653
Epoch 654
Epoch 654, Loss: 0.00000015759401, Improvement: -0.00000000118227, Best Loss: 0.00000009750763 in Epoch 653
Epoch 655
Epoch 655, Loss: 0.00000015589490, Improvement: -0.00000000169911, Best Loss: 0.00000009750763 in Epoch 653
Epoch 656
Epoch 656, Loss: 0.00000018284001, Improvement: 0.00000002694511, Best Loss: 0.00000009750763 in Epoch 653
Epoch 657
Epoch 657, Loss: 0.00000031651964, Improvement: 0.00000013367963, Best Loss: 0.00000009750763 in Epoch 653
Epoch 658
Epoch 658, Loss: 0.00000038399856, Improvement: 0.00000006747892, Best Loss: 0.00000009750763 in Epoch 653
Epoch 659
Epoch 659, Loss: 0.00000044218833, Improvement: 0.00000005818977, Best Loss: 0.00000009750763 in Epoch 653
Epoch 660
Epoch 660, Loss: 0.00000023606781, Improvement: -0.00000020612053, Best Loss: 0.00000009750763 in Epoch 653
Epoch 661
Epoch 661, Loss: 0.00000024866159, Improvement: 0.00000001259378, Best Loss: 0.00000009750763 in Epoch 653
Epoch 662
Epoch 662, Loss: 0.00000023677586, Improvement: -0.00000001188573, Best Loss: 0.00000009750763 in Epoch 653
Epoch 663
Epoch 663, Loss: 0.00000021235972, Improvement: -0.00000002441614, Best Loss: 0.00000009750763 in Epoch 653
Epoch 664
Epoch 664, Loss: 0.00000026444540, Improvement: 0.00000005208569, Best Loss: 0.00000009750763 in Epoch 653
Epoch 665
Epoch 665, Loss: 0.00000050244080, Improvement: 0.00000023799539, Best Loss: 0.00000009750763 in Epoch 653
Epoch 666
Epoch 666, Loss: 0.00000034020878, Improvement: -0.00000016223202, Best Loss: 0.00000009750763 in Epoch 653
Epoch 667
Epoch 667, Loss: 0.00000036679945, Improvement: 0.00000002659067, Best Loss: 0.00000009750763 in Epoch 653
Epoch 668
Epoch 668, Loss: 0.00000027756600, Improvement: -0.00000008923346, Best Loss: 0.00000009750763 in Epoch 653
Epoch 669
Epoch 669, Loss: 0.00000019362705, Improvement: -0.00000008393894, Best Loss: 0.00000009750763 in Epoch 653
Epoch 670
Epoch 670, Loss: 0.00000017379019, Improvement: -0.00000001983686, Best Loss: 0.00000009750763 in Epoch 653
Epoch 671
Epoch 671, Loss: 0.00000017831384, Improvement: 0.00000000452365, Best Loss: 0.00000009750763 in Epoch 653
Epoch 672
Epoch 672, Loss: 0.00000017773317, Improvement: -0.00000000058067, Best Loss: 0.00000009750763 in Epoch 653
Epoch 673
Epoch 673, Loss: 0.00000018205972, Improvement: 0.00000000432655, Best Loss: 0.00000009750763 in Epoch 653
Epoch 674
Epoch 674, Loss: 0.00000018085416, Improvement: -0.00000000120556, Best Loss: 0.00000009750763 in Epoch 653
Epoch 675
Epoch 675, Loss: 0.00000022124913, Improvement: 0.00000004039497, Best Loss: 0.00000009750763 in Epoch 653
Epoch 676
Epoch 676, Loss: 0.00000036684599, Improvement: 0.00000014559686, Best Loss: 0.00000009750763 in Epoch 653
Epoch 677
Epoch 677, Loss: 0.00000106444494, Improvement: 0.00000069759895, Best Loss: 0.00000009750763 in Epoch 653
Epoch 678
Epoch 678, Loss: 0.00000063179271, Improvement: -0.00000043265223, Best Loss: 0.00000009750763 in Epoch 653
Epoch 679
Epoch 679, Loss: 0.00000026334650, Improvement: -0.00000036844621, Best Loss: 0.00000009750763 in Epoch 653
Epoch 680
Epoch 680, Loss: 0.00000018637482, Improvement: -0.00000007697167, Best Loss: 0.00000009750763 in Epoch 653
Epoch 681
Epoch 681, Loss: 0.00000016896482, Improvement: -0.00000001741000, Best Loss: 0.00000009750763 in Epoch 653
Epoch 682
Epoch 682, Loss: 0.00000016686725, Improvement: -0.00000000209757, Best Loss: 0.00000009750763 in Epoch 653
Epoch 683
Epoch 683, Loss: 0.00000015964646, Improvement: -0.00000000722080, Best Loss: 0.00000009750763 in Epoch 653
Epoch 684
Epoch 684, Loss: 0.00000018150804, Improvement: 0.00000002186158, Best Loss: 0.00000009750763 in Epoch 653
Epoch 685
Epoch 685, Loss: 0.00000015638541, Improvement: -0.00000002512263, Best Loss: 0.00000009750763 in Epoch 653
Epoch 686
A best model at epoch 686 has been saved with training error 0.00000009278960.
Epoch 686, Loss: 0.00000014061190, Improvement: -0.00000001577350, Best Loss: 0.00000009278960 in Epoch 686
Epoch 687
A best model at epoch 687 has been saved with training error 0.00000008796749.
Epoch 687, Loss: 0.00000013083963, Improvement: -0.00000000977228, Best Loss: 0.00000008796749 in Epoch 687
Epoch 688
Epoch 688, Loss: 0.00000012619358, Improvement: -0.00000000464605, Best Loss: 0.00000008796749 in Epoch 687
Epoch 689
Epoch 689, Loss: 0.00000013641456, Improvement: 0.00000001022098, Best Loss: 0.00000008796749 in Epoch 687
Epoch 690
Epoch 690, Loss: 0.00000013609541, Improvement: -0.00000000031915, Best Loss: 0.00000008796749 in Epoch 687
Epoch 691
Epoch 691, Loss: 0.00000014400176, Improvement: 0.00000000790635, Best Loss: 0.00000008796749 in Epoch 687
Epoch 692
A best model at epoch 692 has been saved with training error 0.00000008251671.
Epoch 692, Loss: 0.00000013588765, Improvement: -0.00000000811411, Best Loss: 0.00000008251671 in Epoch 692
Epoch 693
Epoch 693, Loss: 0.00000012085268, Improvement: -0.00000001503497, Best Loss: 0.00000008251671 in Epoch 692
Epoch 694
A best model at epoch 694 has been saved with training error 0.00000007873300.
Epoch 694, Loss: 0.00000012308840, Improvement: 0.00000000223573, Best Loss: 0.00000007873300 in Epoch 694
Epoch 695
Epoch 695, Loss: 0.00000012495265, Improvement: 0.00000000186425, Best Loss: 0.00000007873300 in Epoch 694
Epoch 696
Epoch 696, Loss: 0.00000012974800, Improvement: 0.00000000479536, Best Loss: 0.00000007873300 in Epoch 694
Epoch 697
Epoch 697, Loss: 0.00000014665333, Improvement: 0.00000001690532, Best Loss: 0.00000007873300 in Epoch 694
Epoch 698
Epoch 698, Loss: 0.00000016830613, Improvement: 0.00000002165280, Best Loss: 0.00000007873300 in Epoch 694
Epoch 699
Epoch 699, Loss: 0.00000016773885, Improvement: -0.00000000056728, Best Loss: 0.00000007873300 in Epoch 694
Epoch 700
Model saving checkpoint: the model trained after epoch 700 has been saved with the training errors.
Epoch 700, Loss: 0.00000018160421, Improvement: 0.00000001386537, Best Loss: 0.00000007873300 in Epoch 694
Epoch 701
Epoch 701, Loss: 0.00000025720820, Improvement: 0.00000007560399, Best Loss: 0.00000007873300 in Epoch 694
Epoch 702
Epoch 702, Loss: 0.00000030585301, Improvement: 0.00000004864481, Best Loss: 0.00000007873300 in Epoch 694
Epoch 703
Epoch 703, Loss: 0.00000032014137, Improvement: 0.00000001428835, Best Loss: 0.00000007873300 in Epoch 694
Epoch 704
Epoch 704, Loss: 0.00000033388993, Improvement: 0.00000001374856, Best Loss: 0.00000007873300 in Epoch 694
Epoch 705
Epoch 705, Loss: 0.00000026858136, Improvement: -0.00000006530856, Best Loss: 0.00000007873300 in Epoch 694
Epoch 706
Epoch 706, Loss: 0.00000035114892, Improvement: 0.00000008256755, Best Loss: 0.00000007873300 in Epoch 694
Epoch 707
Epoch 707, Loss: 0.00000038911399, Improvement: 0.00000003796507, Best Loss: 0.00000007873300 in Epoch 694
Epoch 708
Epoch 708, Loss: 0.00000036851751, Improvement: -0.00000002059648, Best Loss: 0.00000007873300 in Epoch 694
Epoch 709
Epoch 709, Loss: 0.00000030629671, Improvement: -0.00000006222080, Best Loss: 0.00000007873300 in Epoch 694
Epoch 710
Epoch 710, Loss: 0.00000017651067, Improvement: -0.00000012978604, Best Loss: 0.00000007873300 in Epoch 694
Epoch 711
Epoch 711, Loss: 0.00000018479507, Improvement: 0.00000000828440, Best Loss: 0.00000007873300 in Epoch 694
Epoch 712
Epoch 712, Loss: 0.00000016354160, Improvement: -0.00000002125347, Best Loss: 0.00000007873300 in Epoch 694
Epoch 713
Epoch 713, Loss: 0.00000014772276, Improvement: -0.00000001581884, Best Loss: 0.00000007873300 in Epoch 694
Epoch 714
Epoch 714, Loss: 0.00000013467658, Improvement: -0.00000001304618, Best Loss: 0.00000007873300 in Epoch 694
Epoch 715
Epoch 715, Loss: 0.00000016751594, Improvement: 0.00000003283936, Best Loss: 0.00000007873300 in Epoch 694
Epoch 716
Epoch 716, Loss: 0.00000025926196, Improvement: 0.00000009174602, Best Loss: 0.00000007873300 in Epoch 694
Epoch 717
Epoch 717, Loss: 0.00000041845125, Improvement: 0.00000015918929, Best Loss: 0.00000007873300 in Epoch 694
Epoch 718
Epoch 718, Loss: 0.00000039378393, Improvement: -0.00000002466732, Best Loss: 0.00000007873300 in Epoch 694
Epoch 719
Epoch 719, Loss: 0.00000035259672, Improvement: -0.00000004118722, Best Loss: 0.00000007873300 in Epoch 694
Epoch 720
Epoch 720, Loss: 0.00000030647672, Improvement: -0.00000004611999, Best Loss: 0.00000007873300 in Epoch 694
Epoch 721
Epoch 721, Loss: 0.00000019717425, Improvement: -0.00000010930248, Best Loss: 0.00000007873300 in Epoch 694
Epoch 722
Epoch 722, Loss: 0.00000018127351, Improvement: -0.00000001590073, Best Loss: 0.00000007873300 in Epoch 694
Epoch 723
Epoch 723, Loss: 0.00000016741438, Improvement: -0.00000001385914, Best Loss: 0.00000007873300 in Epoch 694
Epoch 724
Epoch 724, Loss: 0.00000017921170, Improvement: 0.00000001179733, Best Loss: 0.00000007873300 in Epoch 694
Epoch 725
Epoch 725, Loss: 0.00000017648041, Improvement: -0.00000000273129, Best Loss: 0.00000007873300 in Epoch 694
Epoch 726
Epoch 726, Loss: 0.00000013364385, Improvement: -0.00000004283655, Best Loss: 0.00000007873300 in Epoch 694
Epoch 727
Epoch 727, Loss: 0.00000019141494, Improvement: 0.00000005777109, Best Loss: 0.00000007873300 in Epoch 694
Epoch 728
Epoch 728, Loss: 0.00000044241665, Improvement: 0.00000025100171, Best Loss: 0.00000007873300 in Epoch 694
Epoch 729
Epoch 729, Loss: 0.00000040229991, Improvement: -0.00000004011674, Best Loss: 0.00000007873300 in Epoch 694
Epoch 730
Epoch 730, Loss: 0.00000031214356, Improvement: -0.00000009015634, Best Loss: 0.00000007873300 in Epoch 694
Epoch 731
Epoch 731, Loss: 0.00000020032576, Improvement: -0.00000011181780, Best Loss: 0.00000007873300 in Epoch 694
Epoch 732
Epoch 732, Loss: 0.00000015152850, Improvement: -0.00000004879726, Best Loss: 0.00000007873300 in Epoch 694
Epoch 733
Epoch 733, Loss: 0.00000011680824, Improvement: -0.00000003472026, Best Loss: 0.00000007873300 in Epoch 694
Epoch 734
Epoch 734, Loss: 0.00000011029926, Improvement: -0.00000000650898, Best Loss: 0.00000007873300 in Epoch 694
Epoch 735
Epoch 735, Loss: 0.00000012995690, Improvement: 0.00000001965764, Best Loss: 0.00000007873300 in Epoch 694
Epoch 736
Epoch 736, Loss: 0.00000018457018, Improvement: 0.00000005461328, Best Loss: 0.00000007873300 in Epoch 694
Epoch 737
Epoch 737, Loss: 0.00000021749138, Improvement: 0.00000003292120, Best Loss: 0.00000007873300 in Epoch 694
Epoch 738
Epoch 738, Loss: 0.00000018517982, Improvement: -0.00000003231156, Best Loss: 0.00000007873300 in Epoch 694
Epoch 739
Epoch 739, Loss: 0.00000017941134, Improvement: -0.00000000576848, Best Loss: 0.00000007873300 in Epoch 694
Epoch 740
Epoch 740, Loss: 0.00000012212668, Improvement: -0.00000005728465, Best Loss: 0.00000007873300 in Epoch 694
Epoch 741
A best model at epoch 741 has been saved with training error 0.00000007860341.
Epoch 741, Loss: 0.00000011207700, Improvement: -0.00000001004968, Best Loss: 0.00000007860341 in Epoch 741
Epoch 742
A best model at epoch 742 has been saved with training error 0.00000007777659.
Epoch 742, Loss: 0.00000010237384, Improvement: -0.00000000970316, Best Loss: 0.00000007777659 in Epoch 742
Epoch 743
Epoch 743, Loss: 0.00000013199213, Improvement: 0.00000002961830, Best Loss: 0.00000007777659 in Epoch 742
Epoch 744
Epoch 744, Loss: 0.00000029812684, Improvement: 0.00000016613470, Best Loss: 0.00000007777659 in Epoch 742
Epoch 745
Epoch 745, Loss: 0.00000094207148, Improvement: 0.00000064394465, Best Loss: 0.00000007777659 in Epoch 742
Epoch 746
Epoch 746, Loss: 0.00000050471678, Improvement: -0.00000043735471, Best Loss: 0.00000007777659 in Epoch 742
Epoch 747
Epoch 747, Loss: 0.00000030415970, Improvement: -0.00000020055708, Best Loss: 0.00000007777659 in Epoch 742
Epoch 748
Epoch 748, Loss: 0.00000030092254, Improvement: -0.00000000323715, Best Loss: 0.00000007777659 in Epoch 742
Epoch 749
Epoch 749, Loss: 0.00000019517629, Improvement: -0.00000010574625, Best Loss: 0.00000007777659 in Epoch 742
Epoch 750
Model saving checkpoint: the model trained after epoch 750 has been saved with the training errors.
Epoch 750, Loss: 0.00000014472789, Improvement: -0.00000005044841, Best Loss: 0.00000007777659 in Epoch 742
Epoch 751
Epoch 751, Loss: 0.00000011967332, Improvement: -0.00000002505457, Best Loss: 0.00000007777659 in Epoch 742
Epoch 752
Epoch 752, Loss: 0.00000011173873, Improvement: -0.00000000793459, Best Loss: 0.00000007777659 in Epoch 742
Epoch 753
Epoch 753, Loss: 0.00000011652900, Improvement: 0.00000000479027, Best Loss: 0.00000007777659 in Epoch 742
Epoch 754
Epoch 754, Loss: 0.00000011639602, Improvement: -0.00000000013298, Best Loss: 0.00000007777659 in Epoch 742
Epoch 755
Epoch 755, Loss: 0.00000011160328, Improvement: -0.00000000479273, Best Loss: 0.00000007777659 in Epoch 742
Epoch 756
A best model at epoch 756 has been saved with training error 0.00000007468064.
Epoch 756, Loss: 0.00000010692885, Improvement: -0.00000000467444, Best Loss: 0.00000007468064 in Epoch 756
Epoch 757
Epoch 757, Loss: 0.00000011116421, Improvement: 0.00000000423536, Best Loss: 0.00000007468064 in Epoch 756
Epoch 758
Epoch 758, Loss: 0.00000009972967, Improvement: -0.00000001143453, Best Loss: 0.00000007468064 in Epoch 756
Epoch 759
Epoch 759, Loss: 0.00000009946604, Improvement: -0.00000000026364, Best Loss: 0.00000007468064 in Epoch 756
Epoch 760
A best model at epoch 760 has been saved with training error 0.00000006607493.
Epoch 760, Loss: 0.00000010225419, Improvement: 0.00000000278815, Best Loss: 0.00000006607493 in Epoch 760
Epoch 761
Epoch 761, Loss: 0.00000013769991, Improvement: 0.00000003544573, Best Loss: 0.00000006607493 in Epoch 760
Epoch 762
Epoch 762, Loss: 0.00000011817635, Improvement: -0.00000001952357, Best Loss: 0.00000006607493 in Epoch 760
Epoch 763
Epoch 763, Loss: 0.00000010339637, Improvement: -0.00000001477998, Best Loss: 0.00000006607493 in Epoch 760
Epoch 764
Epoch 764, Loss: 0.00000009750688, Improvement: -0.00000000588949, Best Loss: 0.00000006607493 in Epoch 760
Epoch 765
Epoch 765, Loss: 0.00000011180236, Improvement: 0.00000001429548, Best Loss: 0.00000006607493 in Epoch 760
Epoch 766
Epoch 766, Loss: 0.00000016496904, Improvement: 0.00000005316668, Best Loss: 0.00000006607493 in Epoch 760
Epoch 767
Epoch 767, Loss: 0.00000016071256, Improvement: -0.00000000425648, Best Loss: 0.00000006607493 in Epoch 760
Epoch 768
Epoch 768, Loss: 0.00000022171946, Improvement: 0.00000006100690, Best Loss: 0.00000006607493 in Epoch 760
Epoch 769
Epoch 769, Loss: 0.00000020692225, Improvement: -0.00000001479720, Best Loss: 0.00000006607493 in Epoch 760
Epoch 770
Epoch 770, Loss: 0.00000019347008, Improvement: -0.00000001345217, Best Loss: 0.00000006607493 in Epoch 760
Epoch 771
Epoch 771, Loss: 0.00000015906526, Improvement: -0.00000003440482, Best Loss: 0.00000006607493 in Epoch 760
Epoch 772
Epoch 772, Loss: 0.00000024574179, Improvement: 0.00000008667652, Best Loss: 0.00000006607493 in Epoch 760
Epoch 773
Epoch 773, Loss: 0.00000026841480, Improvement: 0.00000002267301, Best Loss: 0.00000006607493 in Epoch 760
Epoch 774
Epoch 774, Loss: 0.00000027086489, Improvement: 0.00000000245009, Best Loss: 0.00000006607493 in Epoch 760
Epoch 775
Epoch 775, Loss: 0.00000037123808, Improvement: 0.00000010037319, Best Loss: 0.00000006607493 in Epoch 760
Epoch 776
Epoch 776, Loss: 0.00000082399846, Improvement: 0.00000045276038, Best Loss: 0.00000006607493 in Epoch 760
Epoch 777
Epoch 777, Loss: 0.00000063954696, Improvement: -0.00000018445150, Best Loss: 0.00000006607493 in Epoch 760
Epoch 778
Epoch 778, Loss: 0.00000031488466, Improvement: -0.00000032466230, Best Loss: 0.00000006607493 in Epoch 760
Epoch 779
Epoch 779, Loss: 0.00000019366840, Improvement: -0.00000012121626, Best Loss: 0.00000006607493 in Epoch 760
Epoch 780
Epoch 780, Loss: 0.00000013027011, Improvement: -0.00000006339829, Best Loss: 0.00000006607493 in Epoch 760
Epoch 781
Epoch 781, Loss: 0.00000010611453, Improvement: -0.00000002415558, Best Loss: 0.00000006607493 in Epoch 760
Epoch 782
Epoch 782, Loss: 0.00000010147765, Improvement: -0.00000000463688, Best Loss: 0.00000006607493 in Epoch 760
Epoch 783
Epoch 783, Loss: 0.00000013880439, Improvement: 0.00000003732674, Best Loss: 0.00000006607493 in Epoch 760
Epoch 784
Epoch 784, Loss: 0.00000028231917, Improvement: 0.00000014351478, Best Loss: 0.00000006607493 in Epoch 760
Epoch 785
Epoch 785, Loss: 0.00000019450540, Improvement: -0.00000008781377, Best Loss: 0.00000006607493 in Epoch 760
Epoch 786
Epoch 786, Loss: 0.00000016690752, Improvement: -0.00000002759788, Best Loss: 0.00000006607493 in Epoch 760
Epoch 787
Epoch 787, Loss: 0.00000014320074, Improvement: -0.00000002370678, Best Loss: 0.00000006607493 in Epoch 760
Epoch 788
Epoch 788, Loss: 0.00000017472173, Improvement: 0.00000003152099, Best Loss: 0.00000006607493 in Epoch 760
Epoch 789
Epoch 789, Loss: 0.00000013989894, Improvement: -0.00000003482279, Best Loss: 0.00000006607493 in Epoch 760
Epoch 790
Epoch 790, Loss: 0.00000012846306, Improvement: -0.00000001143589, Best Loss: 0.00000006607493 in Epoch 760
Epoch 791
Epoch 791, Loss: 0.00000012161785, Improvement: -0.00000000684521, Best Loss: 0.00000006607493 in Epoch 760
Epoch 792
Epoch 792, Loss: 0.00000013878254, Improvement: 0.00000001716469, Best Loss: 0.00000006607493 in Epoch 760
Epoch 793
Epoch 793, Loss: 0.00000012363949, Improvement: -0.00000001514305, Best Loss: 0.00000006607493 in Epoch 760
Epoch 794
Epoch 794, Loss: 0.00000011700313, Improvement: -0.00000000663636, Best Loss: 0.00000006607493 in Epoch 760
Epoch 795
Epoch 795, Loss: 0.00000023416284, Improvement: 0.00000011715972, Best Loss: 0.00000006607493 in Epoch 760
Epoch 796
Epoch 796, Loss: 0.00000015357326, Improvement: -0.00000008058958, Best Loss: 0.00000006607493 in Epoch 760
Epoch 797
Epoch 797, Loss: 0.00000012249202, Improvement: -0.00000003108124, Best Loss: 0.00000006607493 in Epoch 760
Epoch 798
Epoch 798, Loss: 0.00000011992399, Improvement: -0.00000000256803, Best Loss: 0.00000006607493 in Epoch 760
Epoch 799
Epoch 799, Loss: 0.00000010261192, Improvement: -0.00000001731208, Best Loss: 0.00000006607493 in Epoch 760
Epoch 800
Model saving checkpoint: the model trained after epoch 800 has been saved with the training errors.
Epoch 800, Loss: 0.00000013039377, Improvement: 0.00000002778185, Best Loss: 0.00000006607493 in Epoch 760
Epoch 801
Epoch 801, Loss: 0.00000010315266, Improvement: -0.00000002724111, Best Loss: 0.00000006607493 in Epoch 760
Epoch 802
Epoch 802, Loss: 0.00000008960450, Improvement: -0.00000001354816, Best Loss: 0.00000006607493 in Epoch 760
Epoch 803
A best model at epoch 803 has been saved with training error 0.00000006520126.
A best model at epoch 803 has been saved with training error 0.00000006076286.
Epoch 803, Loss: 0.00000008879311, Improvement: -0.00000000081139, Best Loss: 0.00000006076286 in Epoch 803
Epoch 804
Epoch 804, Loss: 0.00000009310487, Improvement: 0.00000000431176, Best Loss: 0.00000006076286 in Epoch 803
Epoch 805
A best model at epoch 805 has been saved with training error 0.00000005911017.
A best model at epoch 805 has been saved with training error 0.00000005248481.
Epoch 805, Loss: 0.00000009002810, Improvement: -0.00000000307676, Best Loss: 0.00000005248481 in Epoch 805
Epoch 806
Epoch 806, Loss: 0.00000008665001, Improvement: -0.00000000337809, Best Loss: 0.00000005248481 in Epoch 805
Epoch 807
Epoch 807, Loss: 0.00000011047069, Improvement: 0.00000002382068, Best Loss: 0.00000005248481 in Epoch 805
Epoch 808
Epoch 808, Loss: 0.00000012824199, Improvement: 0.00000001777130, Best Loss: 0.00000005248481 in Epoch 805
Epoch 809
Epoch 809, Loss: 0.00000013816733, Improvement: 0.00000000992534, Best Loss: 0.00000005248481 in Epoch 805
Epoch 810
Epoch 810, Loss: 0.00000019631754, Improvement: 0.00000005815021, Best Loss: 0.00000005248481 in Epoch 805
Epoch 811
Epoch 811, Loss: 0.00000014720103, Improvement: -0.00000004911650, Best Loss: 0.00000005248481 in Epoch 805
Epoch 812
Epoch 812, Loss: 0.00000012016129, Improvement: -0.00000002703975, Best Loss: 0.00000005248481 in Epoch 805
Epoch 813
Epoch 813, Loss: 0.00000011622825, Improvement: -0.00000000393304, Best Loss: 0.00000005248481 in Epoch 805
Epoch 814
Epoch 814, Loss: 0.00000012529851, Improvement: 0.00000000907026, Best Loss: 0.00000005248481 in Epoch 805
Epoch 815
Epoch 815, Loss: 0.00000015536348, Improvement: 0.00000003006497, Best Loss: 0.00000005248481 in Epoch 805
Epoch 816
Epoch 816, Loss: 0.00000048241757, Improvement: 0.00000032705409, Best Loss: 0.00000005248481 in Epoch 805
Epoch 817
Epoch 817, Loss: 0.00000061525180, Improvement: 0.00000013283423, Best Loss: 0.00000005248481 in Epoch 805
Epoch 818
Epoch 818, Loss: 0.00000038136587, Improvement: -0.00000023388593, Best Loss: 0.00000005248481 in Epoch 805
Epoch 819
Epoch 819, Loss: 0.00000016891753, Improvement: -0.00000021244834, Best Loss: 0.00000005248481 in Epoch 805
Epoch 820
Epoch 820, Loss: 0.00000011773965, Improvement: -0.00000005117788, Best Loss: 0.00000005248481 in Epoch 805
Epoch 821
Epoch 821, Loss: 0.00000010548810, Improvement: -0.00000001225155, Best Loss: 0.00000005248481 in Epoch 805
Epoch 822
Epoch 822, Loss: 0.00000010016535, Improvement: -0.00000000532275, Best Loss: 0.00000005248481 in Epoch 805
Epoch 823
Epoch 823, Loss: 0.00000011218840, Improvement: 0.00000001202305, Best Loss: 0.00000005248481 in Epoch 805
Epoch 824
Epoch 824, Loss: 0.00000010820488, Improvement: -0.00000000398352, Best Loss: 0.00000005248481 in Epoch 805
Epoch 825
Epoch 825, Loss: 0.00000009411842, Improvement: -0.00000001408646, Best Loss: 0.00000005248481 in Epoch 805
Epoch 826
Epoch 826, Loss: 0.00000010741348, Improvement: 0.00000001329506, Best Loss: 0.00000005248481 in Epoch 805
Epoch 827
Epoch 827, Loss: 0.00000011772534, Improvement: 0.00000001031186, Best Loss: 0.00000005248481 in Epoch 805
Epoch 828
Epoch 828, Loss: 0.00000010810584, Improvement: -0.00000000961950, Best Loss: 0.00000005248481 in Epoch 805
Epoch 829
Epoch 829, Loss: 0.00000012527946, Improvement: 0.00000001717362, Best Loss: 0.00000005248481 in Epoch 805
Epoch 830
Epoch 830, Loss: 0.00000008953953, Improvement: -0.00000003573993, Best Loss: 0.00000005248481 in Epoch 805
Epoch 831
Epoch 831, Loss: 0.00000008890972, Improvement: -0.00000000062982, Best Loss: 0.00000005248481 in Epoch 805
Epoch 832
Epoch 832, Loss: 0.00000009785905, Improvement: 0.00000000894933, Best Loss: 0.00000005248481 in Epoch 805
Epoch 833
Epoch 833, Loss: 0.00000012395212, Improvement: 0.00000002609307, Best Loss: 0.00000005248481 in Epoch 805
Epoch 834
Epoch 834, Loss: 0.00000014596930, Improvement: 0.00000002201717, Best Loss: 0.00000005248481 in Epoch 805
Epoch 835
Epoch 835, Loss: 0.00000013940823, Improvement: -0.00000000656107, Best Loss: 0.00000005248481 in Epoch 805
Epoch 836
Epoch 836, Loss: 0.00000015304791, Improvement: 0.00000001363968, Best Loss: 0.00000005248481 in Epoch 805
Epoch 837
Epoch 837, Loss: 0.00000024575740, Improvement: 0.00000009270949, Best Loss: 0.00000005248481 in Epoch 805
Epoch 838
Epoch 838, Loss: 0.00000028891860, Improvement: 0.00000004316120, Best Loss: 0.00000005248481 in Epoch 805
Epoch 839
Epoch 839, Loss: 0.00000021424845, Improvement: -0.00000007467015, Best Loss: 0.00000005248481 in Epoch 805
Epoch 840
Epoch 840, Loss: 0.00000041048760, Improvement: 0.00000019623916, Best Loss: 0.00000005248481 in Epoch 805
Epoch 841
Epoch 841, Loss: 0.00000044740861, Improvement: 0.00000003692101, Best Loss: 0.00000005248481 in Epoch 805
Epoch 842
Epoch 842, Loss: 0.00000028778547, Improvement: -0.00000015962314, Best Loss: 0.00000005248481 in Epoch 805
Epoch 843
Epoch 843, Loss: 0.00000016467118, Improvement: -0.00000012311429, Best Loss: 0.00000005248481 in Epoch 805
Epoch 844
Epoch 844, Loss: 0.00000011415497, Improvement: -0.00000005051621, Best Loss: 0.00000005248481 in Epoch 805
Epoch 845
Epoch 845, Loss: 0.00000010076126, Improvement: -0.00000001339371, Best Loss: 0.00000005248481 in Epoch 805
Epoch 846
Epoch 846, Loss: 0.00000009470325, Improvement: -0.00000000605801, Best Loss: 0.00000005248481 in Epoch 805
Epoch 847
Epoch 847, Loss: 0.00000009014979, Improvement: -0.00000000455346, Best Loss: 0.00000005248481 in Epoch 805
Epoch 848
Epoch 848, Loss: 0.00000009240127, Improvement: 0.00000000225149, Best Loss: 0.00000005248481 in Epoch 805
Epoch 849
Epoch 849, Loss: 0.00000010763686, Improvement: 0.00000001523558, Best Loss: 0.00000005248481 in Epoch 805
Epoch 850
Model saving checkpoint: the model trained after epoch 850 has been saved with the training errors.
Epoch 850, Loss: 0.00000010197455, Improvement: -0.00000000566231, Best Loss: 0.00000005248481 in Epoch 805
Epoch 851
Epoch 851, Loss: 0.00000011054151, Improvement: 0.00000000856696, Best Loss: 0.00000005248481 in Epoch 805
Epoch 852
Epoch 852, Loss: 0.00000011448452, Improvement: 0.00000000394301, Best Loss: 0.00000005248481 in Epoch 805
Epoch 853
Epoch 853, Loss: 0.00000010940804, Improvement: -0.00000000507649, Best Loss: 0.00000005248481 in Epoch 805
Epoch 854
Epoch 854, Loss: 0.00000010564609, Improvement: -0.00000000376195, Best Loss: 0.00000005248481 in Epoch 805
Epoch 855
Epoch 855, Loss: 0.00000010492594, Improvement: -0.00000000072015, Best Loss: 0.00000005248481 in Epoch 805
Epoch 856
Epoch 856, Loss: 0.00000009412150, Improvement: -0.00000001080443, Best Loss: 0.00000005248481 in Epoch 805
Epoch 857
Epoch 857, Loss: 0.00000010022704, Improvement: 0.00000000610554, Best Loss: 0.00000005248481 in Epoch 805
Epoch 858
Epoch 858, Loss: 0.00000010667586, Improvement: 0.00000000644882, Best Loss: 0.00000005248481 in Epoch 805
Epoch 859
Epoch 859, Loss: 0.00000008561865, Improvement: -0.00000002105721, Best Loss: 0.00000005248481 in Epoch 805
Epoch 860
Epoch 860, Loss: 0.00000008787925, Improvement: 0.00000000226060, Best Loss: 0.00000005248481 in Epoch 805
Epoch 861
Epoch 861, Loss: 0.00000013571007, Improvement: 0.00000004783082, Best Loss: 0.00000005248481 in Epoch 805
Epoch 862
Epoch 862, Loss: 0.00000016557148, Improvement: 0.00000002986141, Best Loss: 0.00000005248481 in Epoch 805
Epoch 863
Epoch 863, Loss: 0.00000036454623, Improvement: 0.00000019897475, Best Loss: 0.00000005248481 in Epoch 805
Epoch 864
Epoch 864, Loss: 0.00000029964394, Improvement: -0.00000006490229, Best Loss: 0.00000005248481 in Epoch 805
Epoch 865
Epoch 865, Loss: 0.00000021565992, Improvement: -0.00000008398402, Best Loss: 0.00000005248481 in Epoch 805
Epoch 866
Epoch 866, Loss: 0.00000031053440, Improvement: 0.00000009487448, Best Loss: 0.00000005248481 in Epoch 805
Epoch 867
Epoch 867, Loss: 0.00000028661758, Improvement: -0.00000002391681, Best Loss: 0.00000005248481 in Epoch 805
Epoch 868
Epoch 868, Loss: 0.00000019934126, Improvement: -0.00000008727632, Best Loss: 0.00000005248481 in Epoch 805
Epoch 869
Epoch 869, Loss: 0.00000016816579, Improvement: -0.00000003117547, Best Loss: 0.00000005248481 in Epoch 805
Epoch 870
Epoch 870, Loss: 0.00000025190163, Improvement: 0.00000008373584, Best Loss: 0.00000005248481 in Epoch 805
Epoch 871
Epoch 871, Loss: 0.00000017620197, Improvement: -0.00000007569966, Best Loss: 0.00000005248481 in Epoch 805
Epoch 872
Epoch 872, Loss: 0.00000017900628, Improvement: 0.00000000280431, Best Loss: 0.00000005248481 in Epoch 805
Epoch 873
Epoch 873, Loss: 0.00000010671896, Improvement: -0.00000007228732, Best Loss: 0.00000005248481 in Epoch 805
Epoch 874
Epoch 874, Loss: 0.00000010041749, Improvement: -0.00000000630147, Best Loss: 0.00000005248481 in Epoch 805
Epoch 875
Epoch 875, Loss: 0.00000016848019, Improvement: 0.00000006806270, Best Loss: 0.00000005248481 in Epoch 805
Epoch 876
Epoch 876, Loss: 0.00000014687541, Improvement: -0.00000002160479, Best Loss: 0.00000005248481 in Epoch 805
Epoch 877
Epoch 877, Loss: 0.00000014332215, Improvement: -0.00000000355326, Best Loss: 0.00000005248481 in Epoch 805
Epoch 878
Epoch 878, Loss: 0.00000042394918, Improvement: 0.00000028062704, Best Loss: 0.00000005248481 in Epoch 805
Epoch 879
Epoch 879, Loss: 0.00000047434141, Improvement: 0.00000005039223, Best Loss: 0.00000005248481 in Epoch 805
Epoch 880
Epoch 880, Loss: 0.00000027071258, Improvement: -0.00000020362883, Best Loss: 0.00000005248481 in Epoch 805
Epoch 881
Epoch 881, Loss: 0.00000020661793, Improvement: -0.00000006409465, Best Loss: 0.00000005248481 in Epoch 805
Epoch 882
Epoch 882, Loss: 0.00000014675997, Improvement: -0.00000005985796, Best Loss: 0.00000005248481 in Epoch 805
Epoch 883
Epoch 883, Loss: 0.00000013692425, Improvement: -0.00000000983572, Best Loss: 0.00000005248481 in Epoch 805
Epoch 884
Epoch 884, Loss: 0.00000014270942, Improvement: 0.00000000578516, Best Loss: 0.00000005248481 in Epoch 805
Epoch 885
Epoch 885, Loss: 0.00000010336559, Improvement: -0.00000003934382, Best Loss: 0.00000005248481 in Epoch 805
Epoch 886
Epoch 886, Loss: 0.00000010651092, Improvement: 0.00000000314533, Best Loss: 0.00000005248481 in Epoch 805
Epoch 887
Epoch 887, Loss: 0.00000009671356, Improvement: -0.00000000979736, Best Loss: 0.00000005248481 in Epoch 805
Epoch 888
Epoch 888, Loss: 0.00000010061901, Improvement: 0.00000000390545, Best Loss: 0.00000005248481 in Epoch 805
Epoch 889
Epoch 889, Loss: 0.00000015059148, Improvement: 0.00000004997247, Best Loss: 0.00000005248481 in Epoch 805
Epoch 890
Epoch 890, Loss: 0.00000030005177, Improvement: 0.00000014946029, Best Loss: 0.00000005248481 in Epoch 805
Epoch 891
Epoch 891, Loss: 0.00000043089339, Improvement: 0.00000013084162, Best Loss: 0.00000005248481 in Epoch 805
Epoch 892
Epoch 892, Loss: 0.00000032326023, Improvement: -0.00000010763316, Best Loss: 0.00000005248481 in Epoch 805
Epoch 893
Epoch 893, Loss: 0.00000015221452, Improvement: -0.00000017104571, Best Loss: 0.00000005248481 in Epoch 805
Epoch 894
Epoch 894, Loss: 0.00000009565616, Improvement: -0.00000005655836, Best Loss: 0.00000005248481 in Epoch 805
Epoch 895
Epoch 895, Loss: 0.00000012549899, Improvement: 0.00000002984283, Best Loss: 0.00000005248481 in Epoch 805
Epoch 896
Epoch 896, Loss: 0.00000012721224, Improvement: 0.00000000171325, Best Loss: 0.00000005248481 in Epoch 805
Epoch 897
Epoch 897, Loss: 0.00000016109050, Improvement: 0.00000003387826, Best Loss: 0.00000005248481 in Epoch 805
Epoch 898
Epoch 898, Loss: 0.00000019458702, Improvement: 0.00000003349653, Best Loss: 0.00000005248481 in Epoch 805
Epoch 899
Epoch 899, Loss: 0.00000024227473, Improvement: 0.00000004768771, Best Loss: 0.00000005248481 in Epoch 805
Epoch 900
Model saving checkpoint: the model trained after epoch 900 has been saved with the training errors.
Epoch 900, Loss: 0.00000015244028, Improvement: -0.00000008983445, Best Loss: 0.00000005248481 in Epoch 805
Epoch 901
Epoch 901, Loss: 0.00000012775107, Improvement: -0.00000002468922, Best Loss: 0.00000005248481 in Epoch 805
Epoch 902
Epoch 902, Loss: 0.00000010643219, Improvement: -0.00000002131887, Best Loss: 0.00000005248481 in Epoch 805
Epoch 903
Epoch 903, Loss: 0.00000010781825, Improvement: 0.00000000138606, Best Loss: 0.00000005248481 in Epoch 805
Epoch 904
Epoch 904, Loss: 0.00000016305333, Improvement: 0.00000005523508, Best Loss: 0.00000005248481 in Epoch 805
Epoch 905
Epoch 905, Loss: 0.00000029832465, Improvement: 0.00000013527132, Best Loss: 0.00000005248481 in Epoch 805
Epoch 906
Epoch 906, Loss: 0.00000024285730, Improvement: -0.00000005546735, Best Loss: 0.00000005248481 in Epoch 805
Epoch 907
Epoch 907, Loss: 0.00000014916435, Improvement: -0.00000009369295, Best Loss: 0.00000005248481 in Epoch 805
Epoch 908
Epoch 908, Loss: 0.00000010816386, Improvement: -0.00000004100050, Best Loss: 0.00000005248481 in Epoch 805
Epoch 909
Epoch 909, Loss: 0.00000008810020, Improvement: -0.00000002006365, Best Loss: 0.00000005248481 in Epoch 805
Epoch 910
Epoch 910, Loss: 0.00000008043351, Improvement: -0.00000000766670, Best Loss: 0.00000005248481 in Epoch 805
Epoch 911
Epoch 911, Loss: 0.00000008803024, Improvement: 0.00000000759673, Best Loss: 0.00000005248481 in Epoch 805
Epoch 912
Epoch 912, Loss: 0.00000010883420, Improvement: 0.00000002080397, Best Loss: 0.00000005248481 in Epoch 805
Epoch 913
Epoch 913, Loss: 0.00000010928117, Improvement: 0.00000000044697, Best Loss: 0.00000005248481 in Epoch 805
Epoch 914
Epoch 914, Loss: 0.00000010974114, Improvement: 0.00000000045996, Best Loss: 0.00000005248481 in Epoch 805
Epoch 915
Epoch 915, Loss: 0.00000010779794, Improvement: -0.00000000194320, Best Loss: 0.00000005248481 in Epoch 805
Epoch 916
Epoch 916, Loss: 0.00000014389237, Improvement: 0.00000003609443, Best Loss: 0.00000005248481 in Epoch 805
Epoch 917
Epoch 917, Loss: 0.00000014700193, Improvement: 0.00000000310956, Best Loss: 0.00000005248481 in Epoch 805
Epoch 918
Epoch 918, Loss: 0.00000012492734, Improvement: -0.00000002207459, Best Loss: 0.00000005248481 in Epoch 805
Epoch 919
Epoch 919, Loss: 0.00000015017238, Improvement: 0.00000002524504, Best Loss: 0.00000005248481 in Epoch 805
Epoch 920
Epoch 920, Loss: 0.00000020099074, Improvement: 0.00000005081836, Best Loss: 0.00000005248481 in Epoch 805
Epoch 921
Epoch 921, Loss: 0.00000027189675, Improvement: 0.00000007090601, Best Loss: 0.00000005248481 in Epoch 805
Epoch 922
Epoch 922, Loss: 0.00000021374568, Improvement: -0.00000005815106, Best Loss: 0.00000005248481 in Epoch 805
Epoch 923
Epoch 923, Loss: 0.00000019373153, Improvement: -0.00000002001415, Best Loss: 0.00000005248481 in Epoch 805
Epoch 924
Epoch 924, Loss: 0.00000018286021, Improvement: -0.00000001087132, Best Loss: 0.00000005248481 in Epoch 805
Epoch 925
Epoch 925, Loss: 0.00000012368323, Improvement: -0.00000005917698, Best Loss: 0.00000005248481 in Epoch 805
Epoch 926
Epoch 926, Loss: 0.00000009504556, Improvement: -0.00000002863767, Best Loss: 0.00000005248481 in Epoch 805
Epoch 927
Epoch 927, Loss: 0.00000009004510, Improvement: -0.00000000500046, Best Loss: 0.00000005248481 in Epoch 805
Epoch 928
Epoch 928, Loss: 0.00000009874441, Improvement: 0.00000000869931, Best Loss: 0.00000005248481 in Epoch 805
Epoch 929
Epoch 929, Loss: 0.00000012689732, Improvement: 0.00000002815291, Best Loss: 0.00000005248481 in Epoch 805
Epoch 930
Epoch 930, Loss: 0.00000012331785, Improvement: -0.00000000357947, Best Loss: 0.00000005248481 in Epoch 805
Epoch 931
Epoch 931, Loss: 0.00000013873775, Improvement: 0.00000001541990, Best Loss: 0.00000005248481 in Epoch 805
Epoch 932
Epoch 932, Loss: 0.00000013657809, Improvement: -0.00000000215966, Best Loss: 0.00000005248481 in Epoch 805
Epoch 933
Epoch 933, Loss: 0.00000015021833, Improvement: 0.00000001364024, Best Loss: 0.00000005248481 in Epoch 805
Epoch 934
Epoch 934, Loss: 0.00000045601418, Improvement: 0.00000030579585, Best Loss: 0.00000005248481 in Epoch 805
Epoch 935
Epoch 935, Loss: 0.00000034019848, Improvement: -0.00000011581570, Best Loss: 0.00000005248481 in Epoch 805
Epoch 936
Epoch 936, Loss: 0.00000032726357, Improvement: -0.00000001293491, Best Loss: 0.00000005248481 in Epoch 805
Epoch 937
Epoch 937, Loss: 0.00000016196464, Improvement: -0.00000016529893, Best Loss: 0.00000005248481 in Epoch 805
Epoch 938
Epoch 938, Loss: 0.00000013401217, Improvement: -0.00000002795248, Best Loss: 0.00000005248481 in Epoch 805
Epoch 939
Epoch 939, Loss: 0.00000010715145, Improvement: -0.00000002686072, Best Loss: 0.00000005248481 in Epoch 805
Epoch 940
Epoch 940, Loss: 0.00000009878229, Improvement: -0.00000000836916, Best Loss: 0.00000005248481 in Epoch 805
Epoch 941
Epoch 941, Loss: 0.00000009550377, Improvement: -0.00000000327852, Best Loss: 0.00000005248481 in Epoch 805
Epoch 942
Epoch 942, Loss: 0.00000009087737, Improvement: -0.00000000462639, Best Loss: 0.00000005248481 in Epoch 805
Epoch 943
A best model at epoch 943 has been saved with training error 0.00000004672538.
Epoch 943, Loss: 0.00000007329805, Improvement: -0.00000001757932, Best Loss: 0.00000004672538 in Epoch 943
Epoch 944
Epoch 944, Loss: 0.00000007565161, Improvement: 0.00000000235356, Best Loss: 0.00000004672538 in Epoch 943
Epoch 945
Epoch 945, Loss: 0.00000008580637, Improvement: 0.00000001015476, Best Loss: 0.00000004672538 in Epoch 943
Epoch 946
Epoch 946, Loss: 0.00000007883321, Improvement: -0.00000000697317, Best Loss: 0.00000004672538 in Epoch 943
Epoch 947
Epoch 947, Loss: 0.00000016155077, Improvement: 0.00000008271756, Best Loss: 0.00000004672538 in Epoch 943
Epoch 948
Epoch 948, Loss: 0.00000013647616, Improvement: -0.00000002507461, Best Loss: 0.00000004672538 in Epoch 943
Epoch 949
Epoch 949, Loss: 0.00000009949141, Improvement: -0.00000003698475, Best Loss: 0.00000004672538 in Epoch 943
Epoch 950
Model saving checkpoint: the model trained after epoch 950 has been saved with the training errors.
Epoch 950, Loss: 0.00000013285170, Improvement: 0.00000003336029, Best Loss: 0.00000004672538 in Epoch 943
Epoch 951
Epoch 951, Loss: 0.00000015420635, Improvement: 0.00000002135465, Best Loss: 0.00000004672538 in Epoch 943
Epoch 952
Epoch 952, Loss: 0.00000014722763, Improvement: -0.00000000697871, Best Loss: 0.00000004672538 in Epoch 943
Epoch 953
Epoch 953, Loss: 0.00000009554609, Improvement: -0.00000005168154, Best Loss: 0.00000004672538 in Epoch 943
Epoch 954
Epoch 954, Loss: 0.00000009128529, Improvement: -0.00000000426080, Best Loss: 0.00000004672538 in Epoch 943
Epoch 955
Epoch 955, Loss: 0.00000010912556, Improvement: 0.00000001784026, Best Loss: 0.00000004672538 in Epoch 943
Epoch 956
Epoch 956, Loss: 0.00000033344060, Improvement: 0.00000022431505, Best Loss: 0.00000004672538 in Epoch 943
Epoch 957
Epoch 957, Loss: 0.00000073486758, Improvement: 0.00000040142697, Best Loss: 0.00000004672538 in Epoch 943
Epoch 958
Epoch 958, Loss: 0.00000041911996, Improvement: -0.00000031574762, Best Loss: 0.00000004672538 in Epoch 943
Epoch 959
Epoch 959, Loss: 0.00000024283346, Improvement: -0.00000017628649, Best Loss: 0.00000004672538 in Epoch 943
Epoch 960
Epoch 960, Loss: 0.00000016400978, Improvement: -0.00000007882368, Best Loss: 0.00000004672538 in Epoch 943
Epoch 961
Epoch 961, Loss: 0.00000009164199, Improvement: -0.00000007236780, Best Loss: 0.00000004672538 in Epoch 943
Epoch 962
A best model at epoch 962 has been saved with training error 0.00000004410630.
Epoch 962, Loss: 0.00000007267374, Improvement: -0.00000001896825, Best Loss: 0.00000004410630 in Epoch 962
Epoch 963
Epoch 963, Loss: 0.00000007478504, Improvement: 0.00000000211130, Best Loss: 0.00000004410630 in Epoch 962
Epoch 964
Epoch 964, Loss: 0.00000006725699, Improvement: -0.00000000752805, Best Loss: 0.00000004410630 in Epoch 962
Epoch 965
Epoch 965, Loss: 0.00000006529943, Improvement: -0.00000000195755, Best Loss: 0.00000004410630 in Epoch 962
Epoch 966
Epoch 966, Loss: 0.00000007017591, Improvement: 0.00000000487648, Best Loss: 0.00000004410630 in Epoch 962
Epoch 967
Epoch 967, Loss: 0.00000007540440, Improvement: 0.00000000522849, Best Loss: 0.00000004410630 in Epoch 962
Epoch 968
Epoch 968, Loss: 0.00000007832759, Improvement: 0.00000000292318, Best Loss: 0.00000004410630 in Epoch 962
Epoch 969
Epoch 969, Loss: 0.00000006966046, Improvement: -0.00000000866713, Best Loss: 0.00000004410630 in Epoch 962
Epoch 970
Epoch 970, Loss: 0.00000007299466, Improvement: 0.00000000333420, Best Loss: 0.00000004410630 in Epoch 962
Epoch 971
Epoch 971, Loss: 0.00000006319754, Improvement: -0.00000000979712, Best Loss: 0.00000004410630 in Epoch 962
Epoch 972
Epoch 972, Loss: 0.00000006297090, Improvement: -0.00000000022664, Best Loss: 0.00000004410630 in Epoch 962
Epoch 973
Epoch 973, Loss: 0.00000009926922, Improvement: 0.00000003629832, Best Loss: 0.00000004410630 in Epoch 962
Epoch 974
Epoch 974, Loss: 0.00000009352603, Improvement: -0.00000000574320, Best Loss: 0.00000004410630 in Epoch 962
Epoch 975
Epoch 975, Loss: 0.00000007792951, Improvement: -0.00000001559651, Best Loss: 0.00000004410630 in Epoch 962
Epoch 976
Epoch 976, Loss: 0.00000007772516, Improvement: -0.00000000020436, Best Loss: 0.00000004410630 in Epoch 962
Epoch 977
A best model at epoch 977 has been saved with training error 0.00000003943535.
Epoch 977, Loss: 0.00000006429660, Improvement: -0.00000001342856, Best Loss: 0.00000003943535 in Epoch 977
Epoch 978
Epoch 978, Loss: 0.00000007373314, Improvement: 0.00000000943654, Best Loss: 0.00000003943535 in Epoch 977
Epoch 979
Epoch 979, Loss: 0.00000008764809, Improvement: 0.00000001391496, Best Loss: 0.00000003943535 in Epoch 977
Epoch 980
Epoch 980, Loss: 0.00000007075050, Improvement: -0.00000001689759, Best Loss: 0.00000003943535 in Epoch 977
Epoch 981
Epoch 981, Loss: 0.00000008570858, Improvement: 0.00000001495807, Best Loss: 0.00000003943535 in Epoch 977
Epoch 982
Epoch 982, Loss: 0.00000007240578, Improvement: -0.00000001330279, Best Loss: 0.00000003943535 in Epoch 977
Epoch 983
Epoch 983, Loss: 0.00000007572074, Improvement: 0.00000000331495, Best Loss: 0.00000003943535 in Epoch 977
Epoch 984
Epoch 984, Loss: 0.00000007036064, Improvement: -0.00000000536010, Best Loss: 0.00000003943535 in Epoch 977
Epoch 985
Epoch 985, Loss: 0.00000012611833, Improvement: 0.00000005575769, Best Loss: 0.00000003943535 in Epoch 977
Epoch 986
Epoch 986, Loss: 0.00000011525229, Improvement: -0.00000001086604, Best Loss: 0.00000003943535 in Epoch 977
Epoch 987
Epoch 987, Loss: 0.00000020179369, Improvement: 0.00000008654140, Best Loss: 0.00000003943535 in Epoch 977
Epoch 988
Epoch 988, Loss: 0.00000018966459, Improvement: -0.00000001212910, Best Loss: 0.00000003943535 in Epoch 977
Epoch 989
Epoch 989, Loss: 0.00000013308257, Improvement: -0.00000005658202, Best Loss: 0.00000003943535 in Epoch 977
Epoch 990
Epoch 990, Loss: 0.00000017599206, Improvement: 0.00000004290948, Best Loss: 0.00000003943535 in Epoch 977
Epoch 991
Epoch 991, Loss: 0.00000019198162, Improvement: 0.00000001598956, Best Loss: 0.00000003943535 in Epoch 977
Epoch 992
Epoch 992, Loss: 0.00000025442430, Improvement: 0.00000006244268, Best Loss: 0.00000003943535 in Epoch 977
Epoch 993
Epoch 993, Loss: 0.00000014185964, Improvement: -0.00000011256465, Best Loss: 0.00000003943535 in Epoch 977
Epoch 994
Epoch 994, Loss: 0.00000009647995, Improvement: -0.00000004537970, Best Loss: 0.00000003943535 in Epoch 977
Epoch 995
Epoch 995, Loss: 0.00000013830703, Improvement: 0.00000004182709, Best Loss: 0.00000003943535 in Epoch 977
Epoch 996
Epoch 996, Loss: 0.00000010211478, Improvement: -0.00000003619226, Best Loss: 0.00000003943535 in Epoch 977
Epoch 997
Epoch 997, Loss: 0.00000012483113, Improvement: 0.00000002271635, Best Loss: 0.00000003943535 in Epoch 977
Epoch 998
Epoch 998, Loss: 0.00000009502607, Improvement: -0.00000002980506, Best Loss: 0.00000003943535 in Epoch 977
Epoch 999
Epoch 999, Loss: 0.00000012565368, Improvement: 0.00000003062761, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1000
Model saving checkpoint: the model trained after epoch 1000 has been saved with the training errors.
Epoch 1000, Loss: 0.00000022451744, Improvement: 0.00000009886376, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1001
Epoch 1001, Loss: 0.00000031261814, Improvement: 0.00000008810070, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1002
Epoch 1002, Loss: 0.00000013841958, Improvement: -0.00000017419856, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1003
Epoch 1003, Loss: 0.00000011549426, Improvement: -0.00000002292532, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1004
Epoch 1004, Loss: 0.00000008133572, Improvement: -0.00000003415853, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1005
Epoch 1005, Loss: 0.00000007605173, Improvement: -0.00000000528399, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1006
Epoch 1006, Loss: 0.00000007151573, Improvement: -0.00000000453600, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1007
Epoch 1007, Loss: 0.00000007051388, Improvement: -0.00000000100185, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1008
Epoch 1008, Loss: 0.00000007430272, Improvement: 0.00000000378883, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1009
Epoch 1009, Loss: 0.00000007198579, Improvement: -0.00000000231692, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1010
Epoch 1010, Loss: 0.00000009365626, Improvement: 0.00000002167047, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1011
Epoch 1011, Loss: 0.00000016233108, Improvement: 0.00000006867482, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1012
Epoch 1012, Loss: 0.00000013851220, Improvement: -0.00000002381888, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1013
Epoch 1013, Loss: 0.00000008351583, Improvement: -0.00000005499637, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1014
Epoch 1014, Loss: 0.00000007767901, Improvement: -0.00000000583682, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1015
Epoch 1015, Loss: 0.00000009399785, Improvement: 0.00000001631885, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1016
Epoch 1016, Loss: 0.00000010368562, Improvement: 0.00000000968777, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1017
Epoch 1017, Loss: 0.00000014202926, Improvement: 0.00000003834364, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1018
Epoch 1018, Loss: 0.00000011239346, Improvement: -0.00000002963580, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1019
Epoch 1019, Loss: 0.00000010763668, Improvement: -0.00000000475678, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1020
Epoch 1020, Loss: 0.00000013409984, Improvement: 0.00000002646316, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1021
Epoch 1021, Loss: 0.00000010763907, Improvement: -0.00000002646077, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1022
Epoch 1022, Loss: 0.00000042948940, Improvement: 0.00000032185033, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1023
Epoch 1023, Loss: 0.00000073022943, Improvement: 0.00000030074002, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1024
Epoch 1024, Loss: 0.00000033146370, Improvement: -0.00000039876573, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1025
Epoch 1025, Loss: 0.00000018556902, Improvement: -0.00000014589468, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1026
Epoch 1026, Loss: 0.00000009984307, Improvement: -0.00000008572595, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1027
Epoch 1027, Loss: 0.00000006797649, Improvement: -0.00000003186658, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1028
Epoch 1028, Loss: 0.00000006159695, Improvement: -0.00000000637953, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1029
Epoch 1029, Loss: 0.00000006239040, Improvement: 0.00000000079345, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1030
Epoch 1030, Loss: 0.00000005766239, Improvement: -0.00000000472801, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1031
Epoch 1031, Loss: 0.00000005613437, Improvement: -0.00000000152802, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1032
Epoch 1032, Loss: 0.00000005625377, Improvement: 0.00000000011940, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1033
Epoch 1033, Loss: 0.00000005677821, Improvement: 0.00000000052444, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1034
Epoch 1034, Loss: 0.00000005597865, Improvement: -0.00000000079956, Best Loss: 0.00000003943535 in Epoch 977
Epoch 1035
A best model at epoch 1035 has been saved with training error 0.00000003693520.
A best model at epoch 1035 has been saved with training error 0.00000003674539.
Epoch 1035, Loss: 0.00000005361582, Improvement: -0.00000000236283, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1036
Epoch 1036, Loss: 0.00000005584326, Improvement: 0.00000000222744, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1037
Epoch 1037, Loss: 0.00000006050644, Improvement: 0.00000000466318, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1038
Epoch 1038, Loss: 0.00000005769691, Improvement: -0.00000000280954, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1039
Epoch 1039, Loss: 0.00000005922902, Improvement: 0.00000000153211, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1040
Epoch 1040, Loss: 0.00000005509428, Improvement: -0.00000000413474, Best Loss: 0.00000003674539 in Epoch 1035
Epoch 1041
A best model at epoch 1041 has been saved with training error 0.00000003584745.
Epoch 1041, Loss: 0.00000004886538, Improvement: -0.00000000622890, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1042
Epoch 1042, Loss: 0.00000005859452, Improvement: 0.00000000972913, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1043
Epoch 1043, Loss: 0.00000011928523, Improvement: 0.00000006069071, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1044
Epoch 1044, Loss: 0.00000015865023, Improvement: 0.00000003936500, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1045
Epoch 1045, Loss: 0.00000008566124, Improvement: -0.00000007298898, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1046
Epoch 1046, Loss: 0.00000005911641, Improvement: -0.00000002654484, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1047
Epoch 1047, Loss: 0.00000005739140, Improvement: -0.00000000172501, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1048
Epoch 1048, Loss: 0.00000007944636, Improvement: 0.00000002205496, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1049
Epoch 1049, Loss: 0.00000014412177, Improvement: 0.00000006467540, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1050
Model saving checkpoint: the model trained after epoch 1050 has been saved with the training errors.
Epoch 1050, Loss: 0.00000020301042, Improvement: 0.00000005888865, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1051
Epoch 1051, Loss: 0.00000014187977, Improvement: -0.00000006113065, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1052
Epoch 1052, Loss: 0.00000009671158, Improvement: -0.00000004516819, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1053
Epoch 1053, Loss: 0.00000009380862, Improvement: -0.00000000290296, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1054
Epoch 1054, Loss: 0.00000007012062, Improvement: -0.00000002368799, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1055
Epoch 1055, Loss: 0.00000005492274, Improvement: -0.00000001519788, Best Loss: 0.00000003584745 in Epoch 1041
Epoch 1056
A best model at epoch 1056 has been saved with training error 0.00000003567187.
Epoch 1056, Loss: 0.00000005183009, Improvement: -0.00000000309265, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1057
Epoch 1057, Loss: 0.00000006858889, Improvement: 0.00000001675880, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1058
Epoch 1058, Loss: 0.00000008184758, Improvement: 0.00000001325869, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1059
Epoch 1059, Loss: 0.00000010543983, Improvement: 0.00000002359225, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1060
Epoch 1060, Loss: 0.00000020240187, Improvement: 0.00000009696204, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1061
Epoch 1061, Loss: 0.00000021870374, Improvement: 0.00000001630187, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1062
Epoch 1062, Loss: 0.00000018629089, Improvement: -0.00000003241285, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1063
Epoch 1063, Loss: 0.00000012460814, Improvement: -0.00000006168275, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1064
Epoch 1064, Loss: 0.00000016868797, Improvement: 0.00000004407983, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1065
Epoch 1065, Loss: 0.00000023866189, Improvement: 0.00000006997392, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1066
Epoch 1066, Loss: 0.00000023962189, Improvement: 0.00000000096000, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1067
Epoch 1067, Loss: 0.00000016269126, Improvement: -0.00000007693063, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1068
Epoch 1068, Loss: 0.00000016044580, Improvement: -0.00000000224546, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1069
Epoch 1069, Loss: 0.00000039810190, Improvement: 0.00000023765610, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1070
Epoch 1070, Loss: 0.00000020848929, Improvement: -0.00000018961261, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1071
Epoch 1071, Loss: 0.00000020100357, Improvement: -0.00000000748572, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1072
Epoch 1072, Loss: 0.00000009985137, Improvement: -0.00000010115220, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1073
Epoch 1073, Loss: 0.00000007264302, Improvement: -0.00000002720835, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1074
Epoch 1074, Loss: 0.00000006533963, Improvement: -0.00000000730339, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1075
Epoch 1075, Loss: 0.00000006568224, Improvement: 0.00000000034261, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1076
Epoch 1076, Loss: 0.00000007055080, Improvement: 0.00000000486856, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1077
Epoch 1077, Loss: 0.00000008257790, Improvement: 0.00000001202710, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1078
Epoch 1078, Loss: 0.00000005994880, Improvement: -0.00000002262910, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1079
Epoch 1079, Loss: 0.00000005771709, Improvement: -0.00000000223171, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1080
Epoch 1080, Loss: 0.00000009255621, Improvement: 0.00000003483912, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1081
Epoch 1081, Loss: 0.00000007775686, Improvement: -0.00000001479936, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1082
Epoch 1082, Loss: 0.00000007112868, Improvement: -0.00000000662817, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1083
Epoch 1083, Loss: 0.00000007519951, Improvement: 0.00000000407083, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1084
Epoch 1084, Loss: 0.00000012773834, Improvement: 0.00000005253883, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1085
Epoch 1085, Loss: 0.00000010283312, Improvement: -0.00000002490522, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1086
Epoch 1086, Loss: 0.00000009644731, Improvement: -0.00000000638581, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1087
Epoch 1087, Loss: 0.00000007492779, Improvement: -0.00000002151951, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1088
Epoch 1088, Loss: 0.00000010566438, Improvement: 0.00000003073658, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1089
Epoch 1089, Loss: 0.00000020006428, Improvement: 0.00000009439990, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1090
Epoch 1090, Loss: 0.00000019553617, Improvement: -0.00000000452811, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1091
Epoch 1091, Loss: 0.00000019467063, Improvement: -0.00000000086554, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1092
Epoch 1092, Loss: 0.00000016410098, Improvement: -0.00000003056966, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1093
Epoch 1093, Loss: 0.00000021095571, Improvement: 0.00000004685473, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1094
Epoch 1094, Loss: 0.00000023635350, Improvement: 0.00000002539779, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1095
Epoch 1095, Loss: 0.00000022136563, Improvement: -0.00000001498787, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1096
Epoch 1096, Loss: 0.00000020258852, Improvement: -0.00000001877711, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1097
Epoch 1097, Loss: 0.00000014721260, Improvement: -0.00000005537592, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1098
Epoch 1098, Loss: 0.00000016625294, Improvement: 0.00000001904034, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1099
Epoch 1099, Loss: 0.00000021231692, Improvement: 0.00000004606398, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1100
Model saving checkpoint: the model trained after epoch 1100 has been saved with the training errors.
Epoch 1100, Loss: 0.00000016808828, Improvement: -0.00000004422865, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1101
Epoch 1101, Loss: 0.00000009739409, Improvement: -0.00000007069419, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1102
Epoch 1102, Loss: 0.00000008756868, Improvement: -0.00000000982540, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1103
Epoch 1103, Loss: 0.00000008073932, Improvement: -0.00000000682937, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1104
Epoch 1104, Loss: 0.00000007120852, Improvement: -0.00000000953080, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1105
Epoch 1105, Loss: 0.00000006499918, Improvement: -0.00000000620934, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1106
Epoch 1106, Loss: 0.00000009880674, Improvement: 0.00000003380755, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1107
Epoch 1107, Loss: 0.00000007042724, Improvement: -0.00000002837949, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1108
Epoch 1108, Loss: 0.00000006910716, Improvement: -0.00000000132008, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1109
Epoch 1109, Loss: 0.00000006982832, Improvement: 0.00000000072115, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1110
Epoch 1110, Loss: 0.00000010677857, Improvement: 0.00000003695026, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1111
Epoch 1111, Loss: 0.00000011024785, Improvement: 0.00000000346927, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1112
Epoch 1112, Loss: 0.00000008462025, Improvement: -0.00000002562760, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1113
Epoch 1113, Loss: 0.00000007131830, Improvement: -0.00000001330194, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1114
Epoch 1114, Loss: 0.00000007927526, Improvement: 0.00000000795695, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1115
Epoch 1115, Loss: 0.00000011050236, Improvement: 0.00000003122711, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1116
Epoch 1116, Loss: 0.00000013099094, Improvement: 0.00000002048858, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1117
Epoch 1117, Loss: 0.00000019978229, Improvement: 0.00000006879134, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1118
Epoch 1118, Loss: 0.00000014378551, Improvement: -0.00000005599678, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1119
Epoch 1119, Loss: 0.00000014049779, Improvement: -0.00000000328773, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1120
Epoch 1120, Loss: 0.00000010282126, Improvement: -0.00000003767652, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1121
Epoch 1121, Loss: 0.00000012499861, Improvement: 0.00000002217734, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1122
Epoch 1122, Loss: 0.00000048190253, Improvement: 0.00000035690393, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1123
Epoch 1123, Loss: 0.00000024717160, Improvement: -0.00000023473093, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1124
Epoch 1124, Loss: 0.00000013127830, Improvement: -0.00000011589330, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1125
Epoch 1125, Loss: 0.00000013619452, Improvement: 0.00000000491622, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1126
Epoch 1126, Loss: 0.00000010957197, Improvement: -0.00000002662254, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1127
Epoch 1127, Loss: 0.00000014001815, Improvement: 0.00000003044617, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1128
Epoch 1128, Loss: 0.00000007980087, Improvement: -0.00000006021727, Best Loss: 0.00000003567187 in Epoch 1056
Epoch 1129
A best model at epoch 1129 has been saved with training error 0.00000003184845.
Epoch 1129, Loss: 0.00000005881075, Improvement: -0.00000002099013, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1130
Epoch 1130, Loss: 0.00000007101819, Improvement: 0.00000001220744, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1131
Epoch 1131, Loss: 0.00000006750399, Improvement: -0.00000000351420, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1132
Epoch 1132, Loss: 0.00000006825350, Improvement: 0.00000000074951, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1133
Epoch 1133, Loss: 0.00000007447895, Improvement: 0.00000000622545, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1134
Epoch 1134, Loss: 0.00000012683038, Improvement: 0.00000005235142, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1135
Epoch 1135, Loss: 0.00000011555196, Improvement: -0.00000001127842, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1136
Epoch 1136, Loss: 0.00000009106631, Improvement: -0.00000002448565, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1137
Epoch 1137, Loss: 0.00000007794959, Improvement: -0.00000001311672, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1138
Epoch 1138, Loss: 0.00000008192241, Improvement: 0.00000000397281, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1139
Epoch 1139, Loss: 0.00000008160296, Improvement: -0.00000000031945, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1140
Epoch 1140, Loss: 0.00000007911951, Improvement: -0.00000000248344, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1141
Epoch 1141, Loss: 0.00000005935680, Improvement: -0.00000001976271, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1142
Epoch 1142, Loss: 0.00000006466189, Improvement: 0.00000000530508, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1143
Epoch 1143, Loss: 0.00000006277715, Improvement: -0.00000000188474, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1144
Epoch 1144, Loss: 0.00000008208382, Improvement: 0.00000001930667, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1145
Epoch 1145, Loss: 0.00000008028107, Improvement: -0.00000000180275, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1146
Epoch 1146, Loss: 0.00000009954759, Improvement: 0.00000001926652, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1147
Epoch 1147, Loss: 0.00000015127496, Improvement: 0.00000005172737, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1148
Epoch 1148, Loss: 0.00000015630060, Improvement: 0.00000000502565, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1149
Epoch 1149, Loss: 0.00000023355080, Improvement: 0.00000007725020, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1150
Model saving checkpoint: the model trained after epoch 1150 has been saved with the training errors.
Epoch 1150, Loss: 0.00000013377395, Improvement: -0.00000009977685, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1151
Epoch 1151, Loss: 0.00000007614584, Improvement: -0.00000005762811, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1152
Epoch 1152, Loss: 0.00000005254254, Improvement: -0.00000002360330, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1153
Epoch 1153, Loss: 0.00000005003692, Improvement: -0.00000000250562, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1154
Epoch 1154, Loss: 0.00000005170256, Improvement: 0.00000000166565, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1155
Epoch 1155, Loss: 0.00000004554792, Improvement: -0.00000000615464, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1156
Epoch 1156, Loss: 0.00000004674992, Improvement: 0.00000000120200, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1157
Epoch 1157, Loss: 0.00000005120097, Improvement: 0.00000000445105, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1158
Epoch 1158, Loss: 0.00000006281689, Improvement: 0.00000001161591, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1159
Epoch 1159, Loss: 0.00000013259801, Improvement: 0.00000006978113, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1160
Epoch 1160, Loss: 0.00000010852266, Improvement: -0.00000002407535, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1161
Epoch 1161, Loss: 0.00000009323169, Improvement: -0.00000001529097, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1162
Epoch 1162, Loss: 0.00000013133884, Improvement: 0.00000003810716, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1163
Epoch 1163, Loss: 0.00000017385780, Improvement: 0.00000004251896, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1164
Epoch 1164, Loss: 0.00000014428175, Improvement: -0.00000002957605, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1165
Epoch 1165, Loss: 0.00000015430558, Improvement: 0.00000001002383, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1166
Epoch 1166, Loss: 0.00000012390066, Improvement: -0.00000003040492, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1167
Epoch 1167, Loss: 0.00000011933829, Improvement: -0.00000000456237, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1168
Epoch 1168, Loss: 0.00000007401299, Improvement: -0.00000004532530, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1169
Epoch 1169, Loss: 0.00000006513758, Improvement: -0.00000000887540, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1170
Epoch 1170, Loss: 0.00000007216662, Improvement: 0.00000000702904, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1171
Epoch 1171, Loss: 0.00000006245417, Improvement: -0.00000000971245, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1172
Epoch 1172, Loss: 0.00000006911415, Improvement: 0.00000000665999, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1173
Epoch 1173, Loss: 0.00000008201146, Improvement: 0.00000001289730, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1174
Epoch 1174, Loss: 0.00000017831808, Improvement: 0.00000009630663, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1175
Epoch 1175, Loss: 0.00000024977055, Improvement: 0.00000007145246, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1176
Epoch 1176, Loss: 0.00000024116925, Improvement: -0.00000000860130, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1177
Epoch 1177, Loss: 0.00000022169613, Improvement: -0.00000001947312, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1178
Epoch 1178, Loss: 0.00000011702244, Improvement: -0.00000010467369, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1179
Epoch 1179, Loss: 0.00000007017652, Improvement: -0.00000004684592, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1180
Epoch 1180, Loss: 0.00000005777997, Improvement: -0.00000001239655, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1181
Epoch 1181, Loss: 0.00000005558652, Improvement: -0.00000000219345, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1182
Epoch 1182, Loss: 0.00000005986959, Improvement: 0.00000000428306, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1183
Epoch 1183, Loss: 0.00000005814099, Improvement: -0.00000000172860, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1184
Epoch 1184, Loss: 0.00000006394062, Improvement: 0.00000000579963, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1185
Epoch 1185, Loss: 0.00000005717414, Improvement: -0.00000000676649, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1186
Epoch 1186, Loss: 0.00000005082497, Improvement: -0.00000000634916, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1187
Epoch 1187, Loss: 0.00000005016367, Improvement: -0.00000000066131, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1188
Epoch 1188, Loss: 0.00000010120843, Improvement: 0.00000005104477, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1189
Epoch 1189, Loss: 0.00000020701503, Improvement: 0.00000010580659, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1190
Epoch 1190, Loss: 0.00000020801963, Improvement: 0.00000000100461, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1191
Epoch 1191, Loss: 0.00000014430262, Improvement: -0.00000006371701, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1192
Epoch 1192, Loss: 0.00000007797901, Improvement: -0.00000006632361, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1193
Epoch 1193, Loss: 0.00000005824530, Improvement: -0.00000001973372, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1194
Epoch 1194, Loss: 0.00000006887959, Improvement: 0.00000001063429, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1195
Epoch 1195, Loss: 0.00000007209903, Improvement: 0.00000000321944, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1196
Epoch 1196, Loss: 0.00000008728865, Improvement: 0.00000001518961, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1197
Epoch 1197, Loss: 0.00000013037567, Improvement: 0.00000004308703, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1198
Epoch 1198, Loss: 0.00000014399450, Improvement: 0.00000001361882, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1199
Epoch 1199, Loss: 0.00000011499630, Improvement: -0.00000002899820, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1200
Model saving checkpoint: the model trained after epoch 1200 has been saved with the training errors.
Epoch 1200, Loss: 0.00000007356909, Improvement: -0.00000004142721, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1201
Epoch 1201, Loss: 0.00000005975434, Improvement: -0.00000001381475, Best Loss: 0.00000003184845 in Epoch 1129
Epoch 1202
A best model at epoch 1202 has been saved with training error 0.00000003075582.
Epoch 1202, Loss: 0.00000005369852, Improvement: -0.00000000605582, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1203
Epoch 1203, Loss: 0.00000007890618, Improvement: 0.00000002520766, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1204
Epoch 1204, Loss: 0.00000009642876, Improvement: 0.00000001752258, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1205
Epoch 1205, Loss: 0.00000013209792, Improvement: 0.00000003566916, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1206
Epoch 1206, Loss: 0.00000016355528, Improvement: 0.00000003145735, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1207
Epoch 1207, Loss: 0.00000011210144, Improvement: -0.00000005145384, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1208
Epoch 1208, Loss: 0.00000008836728, Improvement: -0.00000002373416, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1209
Epoch 1209, Loss: 0.00000008290821, Improvement: -0.00000000545907, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1210
Epoch 1210, Loss: 0.00000008421952, Improvement: 0.00000000131131, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1211
Epoch 1211, Loss: 0.00000006939302, Improvement: -0.00000001482650, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1212
Epoch 1212, Loss: 0.00000005623969, Improvement: -0.00000001315334, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1213
Epoch 1213, Loss: 0.00000005229703, Improvement: -0.00000000394266, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1214
Epoch 1214, Loss: 0.00000007485350, Improvement: 0.00000002255647, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1215
Epoch 1215, Loss: 0.00000005675818, Improvement: -0.00000001809532, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1216
Epoch 1216, Loss: 0.00000004825839, Improvement: -0.00000000849979, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1217
Epoch 1217, Loss: 0.00000004888966, Improvement: 0.00000000063127, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1218
Epoch 1218, Loss: 0.00000006458297, Improvement: 0.00000001569331, Best Loss: 0.00000003075582 in Epoch 1202
Epoch 1219
A best model at epoch 1219 has been saved with training error 0.00000003030027.
Epoch 1219, Loss: 0.00000005774034, Improvement: -0.00000000684263, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1220
Epoch 1220, Loss: 0.00000011472136, Improvement: 0.00000005698102, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1221
Epoch 1221, Loss: 0.00000009827821, Improvement: -0.00000001644315, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1222
Epoch 1222, Loss: 0.00000006677668, Improvement: -0.00000003150153, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1223
Epoch 1223, Loss: 0.00000007218460, Improvement: 0.00000000540792, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1224
Epoch 1224, Loss: 0.00000010804673, Improvement: 0.00000003586213, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1225
Epoch 1225, Loss: 0.00000014055185, Improvement: 0.00000003250512, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1226
Epoch 1226, Loss: 0.00000020954461, Improvement: 0.00000006899276, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1227
Epoch 1227, Loss: 0.00000024825547, Improvement: 0.00000003871086, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1228
Epoch 1228, Loss: 0.00000011416324, Improvement: -0.00000013409224, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1229
Epoch 1229, Loss: 0.00000036953023, Improvement: 0.00000025536699, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1230
Epoch 1230, Loss: 0.00000020962536, Improvement: -0.00000015990487, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1231
Epoch 1231, Loss: 0.00000009877702, Improvement: -0.00000011084833, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1232
Epoch 1232, Loss: 0.00000007742588, Improvement: -0.00000002135114, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1233
Epoch 1233, Loss: 0.00000007709149, Improvement: -0.00000000033439, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1234
Epoch 1234, Loss: 0.00000006445056, Improvement: -0.00000001264093, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1235
Epoch 1235, Loss: 0.00000005799723, Improvement: -0.00000000645334, Best Loss: 0.00000003030027 in Epoch 1219
Epoch 1236
A best model at epoch 1236 has been saved with training error 0.00000002934684.
Epoch 1236, Loss: 0.00000004290077, Improvement: -0.00000001509646, Best Loss: 0.00000002934684 in Epoch 1236
Epoch 1237
Epoch 1237, Loss: 0.00000003870151, Improvement: -0.00000000419926, Best Loss: 0.00000002934684 in Epoch 1236
Epoch 1238
Epoch 1238, Loss: 0.00000003972551, Improvement: 0.00000000102400, Best Loss: 0.00000002934684 in Epoch 1236
Epoch 1239
A best model at epoch 1239 has been saved with training error 0.00000002926653.
Epoch 1239, Loss: 0.00000003900582, Improvement: -0.00000000071969, Best Loss: 0.00000002926653 in Epoch 1239
Epoch 1240
Epoch 1240, Loss: 0.00000004724213, Improvement: 0.00000000823631, Best Loss: 0.00000002926653 in Epoch 1239
Epoch 1241
Epoch 1241, Loss: 0.00000005507636, Improvement: 0.00000000783423, Best Loss: 0.00000002926653 in Epoch 1239
Epoch 1242
Epoch 1242, Loss: 0.00000004576319, Improvement: -0.00000000931316, Best Loss: 0.00000002926653 in Epoch 1239
Epoch 1243
A best model at epoch 1243 has been saved with training error 0.00000002857005.
Epoch 1243, Loss: 0.00000004125170, Improvement: -0.00000000451149, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1244
Epoch 1244, Loss: 0.00000007084405, Improvement: 0.00000002959234, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1245
Epoch 1245, Loss: 0.00000012746061, Improvement: 0.00000005661656, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1246
Epoch 1246, Loss: 0.00000014411403, Improvement: 0.00000001665342, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1247
Epoch 1247, Loss: 0.00000016917176, Improvement: 0.00000002505774, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1248
Epoch 1248, Loss: 0.00000017427193, Improvement: 0.00000000510017, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1249
Epoch 1249, Loss: 0.00000015254915, Improvement: -0.00000002172278, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1250
Model saving checkpoint: the model trained after epoch 1250 has been saved with the training errors.
Epoch 1250, Loss: 0.00000010677641, Improvement: -0.00000004577274, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1251
Epoch 1251, Loss: 0.00000006754646, Improvement: -0.00000003922995, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1252
Epoch 1252, Loss: 0.00000005905674, Improvement: -0.00000000848972, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1253
Epoch 1253, Loss: 0.00000008269494, Improvement: 0.00000002363820, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1254
Epoch 1254, Loss: 0.00000011679916, Improvement: 0.00000003410422, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1255
Epoch 1255, Loss: 0.00000006879716, Improvement: -0.00000004800200, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1256
Epoch 1256, Loss: 0.00000006333475, Improvement: -0.00000000546240, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1257
Epoch 1257, Loss: 0.00000010841783, Improvement: 0.00000004508308, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1258
Epoch 1258, Loss: 0.00000007235606, Improvement: -0.00000003606177, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1259
Epoch 1259, Loss: 0.00000008377716, Improvement: 0.00000001142110, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1260
Epoch 1260, Loss: 0.00000008043237, Improvement: -0.00000000334479, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1261
Epoch 1261, Loss: 0.00000008576162, Improvement: 0.00000000532924, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1262
Epoch 1262, Loss: 0.00000008296715, Improvement: -0.00000000279446, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1263
Epoch 1263, Loss: 0.00000007647970, Improvement: -0.00000000648746, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1264
Epoch 1264, Loss: 0.00000005856169, Improvement: -0.00000001791800, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1265
Epoch 1265, Loss: 0.00000005672407, Improvement: -0.00000000183762, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1266
Epoch 1266, Loss: 0.00000007525355, Improvement: 0.00000001852948, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1267
Epoch 1267, Loss: 0.00000011499902, Improvement: 0.00000003974546, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1268
Epoch 1268, Loss: 0.00000012942800, Improvement: 0.00000001442899, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1269
Epoch 1269, Loss: 0.00000013329755, Improvement: 0.00000000386955, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1270
Epoch 1270, Loss: 0.00000015823647, Improvement: 0.00000002493892, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1271
Epoch 1271, Loss: 0.00000010162421, Improvement: -0.00000005661226, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1272
Epoch 1272, Loss: 0.00000010200197, Improvement: 0.00000000037776, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1273
Epoch 1273, Loss: 0.00000018800732, Improvement: 0.00000008600534, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1274
Epoch 1274, Loss: 0.00000026777268, Improvement: 0.00000007976537, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1275
Epoch 1275, Loss: 0.00000026224060, Improvement: -0.00000000553209, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1276
Epoch 1276, Loss: 0.00000011204562, Improvement: -0.00000015019498, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1277
Epoch 1277, Loss: 0.00000008462199, Improvement: -0.00000002742363, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1278
Epoch 1278, Loss: 0.00000005676350, Improvement: -0.00000002785850, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1279
Epoch 1279, Loss: 0.00000005268880, Improvement: -0.00000000407469, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1280
Epoch 1280, Loss: 0.00000004861585, Improvement: -0.00000000407296, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1281
Epoch 1281, Loss: 0.00000005274068, Improvement: 0.00000000412483, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1282
Epoch 1282, Loss: 0.00000005760970, Improvement: 0.00000000486903, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1283
Epoch 1283, Loss: 0.00000006094837, Improvement: 0.00000000333867, Best Loss: 0.00000002857005 in Epoch 1243
Epoch 1284
A best model at epoch 1284 has been saved with training error 0.00000002742865.
Epoch 1284, Loss: 0.00000004958796, Improvement: -0.00000001136041, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1285
Epoch 1285, Loss: 0.00000004885523, Improvement: -0.00000000073272, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1286
Epoch 1286, Loss: 0.00000005407780, Improvement: 0.00000000522256, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1287
Epoch 1287, Loss: 0.00000008211332, Improvement: 0.00000002803552, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1288
Epoch 1288, Loss: 0.00000017236561, Improvement: 0.00000009025230, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1289
Epoch 1289, Loss: 0.00000014071659, Improvement: -0.00000003164902, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1290
Epoch 1290, Loss: 0.00000010019680, Improvement: -0.00000004051979, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1291
Epoch 1291, Loss: 0.00000008187925, Improvement: -0.00000001831754, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1292
Epoch 1292, Loss: 0.00000008610859, Improvement: 0.00000000422934, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1293
Epoch 1293, Loss: 0.00000006597768, Improvement: -0.00000002013091, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1294
Epoch 1294, Loss: 0.00000008539064, Improvement: 0.00000001941296, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1295
Epoch 1295, Loss: 0.00000007310679, Improvement: -0.00000001228385, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1296
Epoch 1296, Loss: 0.00000004567107, Improvement: -0.00000002743572, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1297
Epoch 1297, Loss: 0.00000004191423, Improvement: -0.00000000375684, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1298
Epoch 1298, Loss: 0.00000005977375, Improvement: 0.00000001785952, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1299
Epoch 1299, Loss: 0.00000005730109, Improvement: -0.00000000247266, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1300
Model saving checkpoint: the model trained after epoch 1300 has been saved with the training errors.
Epoch 1300, Loss: 0.00000008037060, Improvement: 0.00000002306950, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1301
Epoch 1301, Loss: 0.00000007516737, Improvement: -0.00000000520322, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1302
Epoch 1302, Loss: 0.00000004906587, Improvement: -0.00000002610151, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1303
Epoch 1303, Loss: 0.00000005808886, Improvement: 0.00000000902299, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1304
Epoch 1304, Loss: 0.00000008253234, Improvement: 0.00000002444348, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1305
Epoch 1305, Loss: 0.00000005186629, Improvement: -0.00000003066605, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1306
Epoch 1306, Loss: 0.00000005575082, Improvement: 0.00000000388453, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1307
Epoch 1307, Loss: 0.00000006948061, Improvement: 0.00000001372979, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1308
Epoch 1308, Loss: 0.00000005436600, Improvement: -0.00000001511461, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1309
Epoch 1309, Loss: 0.00000010457001, Improvement: 0.00000005020401, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1310
Epoch 1310, Loss: 0.00000011397152, Improvement: 0.00000000940151, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1311
Epoch 1311, Loss: 0.00000025119215, Improvement: 0.00000013722063, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1312
Epoch 1312, Loss: 0.00000018357851, Improvement: -0.00000006761364, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1313
Epoch 1313, Loss: 0.00000012300061, Improvement: -0.00000006057790, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1314
Epoch 1314, Loss: 0.00000010056645, Improvement: -0.00000002243416, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1315
Epoch 1315, Loss: 0.00000006988059, Improvement: -0.00000003068586, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1316
Epoch 1316, Loss: 0.00000005939494, Improvement: -0.00000001048565, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1317
Epoch 1317, Loss: 0.00000005956048, Improvement: 0.00000000016555, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1318
Epoch 1318, Loss: 0.00000005111266, Improvement: -0.00000000844782, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1319
Epoch 1319, Loss: 0.00000004061063, Improvement: -0.00000001050204, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1320
Epoch 1320, Loss: 0.00000003919915, Improvement: -0.00000000141147, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1321
Epoch 1321, Loss: 0.00000004386803, Improvement: 0.00000000466888, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1322
Epoch 1322, Loss: 0.00000009762348, Improvement: 0.00000005375545, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1323
Epoch 1323, Loss: 0.00000008316308, Improvement: -0.00000001446040, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1324
Epoch 1324, Loss: 0.00000007676681, Improvement: -0.00000000639627, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1325
Epoch 1325, Loss: 0.00000006584354, Improvement: -0.00000001092327, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1326
Epoch 1326, Loss: 0.00000004346604, Improvement: -0.00000002237750, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1327
Epoch 1327, Loss: 0.00000004348923, Improvement: 0.00000000002319, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1328
Epoch 1328, Loss: 0.00000004919586, Improvement: 0.00000000570662, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1329
Epoch 1329, Loss: 0.00000005257600, Improvement: 0.00000000338015, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1330
Epoch 1330, Loss: 0.00000016246913, Improvement: 0.00000010989313, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1331
Epoch 1331, Loss: 0.00000031276780, Improvement: 0.00000015029867, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1332
Epoch 1332, Loss: 0.00000009712434, Improvement: -0.00000021564346, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1333
Epoch 1333, Loss: 0.00000006257648, Improvement: -0.00000003454786, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1334
Epoch 1334, Loss: 0.00000006112807, Improvement: -0.00000000144841, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1335
Epoch 1335, Loss: 0.00000004639156, Improvement: -0.00000001473652, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1336
Epoch 1336, Loss: 0.00000004765120, Improvement: 0.00000000125964, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1337
Epoch 1337, Loss: 0.00000005224370, Improvement: 0.00000000459250, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1338
Epoch 1338, Loss: 0.00000004242995, Improvement: -0.00000000981376, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1339
Epoch 1339, Loss: 0.00000004502279, Improvement: 0.00000000259285, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1340
Epoch 1340, Loss: 0.00000004814956, Improvement: 0.00000000312677, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1341
Epoch 1341, Loss: 0.00000005872027, Improvement: 0.00000001057071, Best Loss: 0.00000002742865 in Epoch 1284
Epoch 1342
A best model at epoch 1342 has been saved with training error 0.00000002464124.
Epoch 1342, Loss: 0.00000003955272, Improvement: -0.00000001916755, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1343
Epoch 1343, Loss: 0.00000006458486, Improvement: 0.00000002503213, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1344
Epoch 1344, Loss: 0.00000005961772, Improvement: -0.00000000496713, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1345
Epoch 1345, Loss: 0.00000004975098, Improvement: -0.00000000986674, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1346
Epoch 1346, Loss: 0.00000006040868, Improvement: 0.00000001065770, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1347
Epoch 1347, Loss: 0.00000009852680, Improvement: 0.00000003811813, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1348
Epoch 1348, Loss: 0.00000006855976, Improvement: -0.00000002996704, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1349
Epoch 1349, Loss: 0.00000005951287, Improvement: -0.00000000904690, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1350
Model saving checkpoint: the model trained after epoch 1350 has been saved with the training errors.
Epoch 1350, Loss: 0.00000006466046, Improvement: 0.00000000514760, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1351
Epoch 1351, Loss: 0.00000005375734, Improvement: -0.00000001090313, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1352
Epoch 1352, Loss: 0.00000006388573, Improvement: 0.00000001012839, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1353
Epoch 1353, Loss: 0.00000010491596, Improvement: 0.00000004103023, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1354
Epoch 1354, Loss: 0.00000007457913, Improvement: -0.00000003033683, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1355
Epoch 1355, Loss: 0.00000009187069, Improvement: 0.00000001729156, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1356
Epoch 1356, Loss: 0.00000007618244, Improvement: -0.00000001568825, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1357
Epoch 1357, Loss: 0.00000006452003, Improvement: -0.00000001166242, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1358
Epoch 1358, Loss: 0.00000008265261, Improvement: 0.00000001813258, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1359
Epoch 1359, Loss: 0.00000005921046, Improvement: -0.00000002344214, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1360
Epoch 1360, Loss: 0.00000010488296, Improvement: 0.00000004567249, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1361
Epoch 1361, Loss: 0.00000007616834, Improvement: -0.00000002871462, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1362
Epoch 1362, Loss: 0.00000007227391, Improvement: -0.00000000389443, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1363
Epoch 1363, Loss: 0.00000008949669, Improvement: 0.00000001722278, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1364
Epoch 1364, Loss: 0.00000011056154, Improvement: 0.00000002106484, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1365
Epoch 1365, Loss: 0.00000011988952, Improvement: 0.00000000932798, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1366
Epoch 1366, Loss: 0.00000009747776, Improvement: -0.00000002241176, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1367
Epoch 1367, Loss: 0.00000007769326, Improvement: -0.00000001978450, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1368
Epoch 1368, Loss: 0.00000006056736, Improvement: -0.00000001712590, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1369
Epoch 1369, Loss: 0.00000009574655, Improvement: 0.00000003517919, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1370
Epoch 1370, Loss: 0.00000012568175, Improvement: 0.00000002993520, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1371
Epoch 1371, Loss: 0.00000013929585, Improvement: 0.00000001361409, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1372
Epoch 1372, Loss: 0.00000010780947, Improvement: -0.00000003148638, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1373
Epoch 1373, Loss: 0.00000009273993, Improvement: -0.00000001506954, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1374
Epoch 1374, Loss: 0.00000008033830, Improvement: -0.00000001240164, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1375
Epoch 1375, Loss: 0.00000007956281, Improvement: -0.00000000077549, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1376
Epoch 1376, Loss: 0.00000008100052, Improvement: 0.00000000143771, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1377
Epoch 1377, Loss: 0.00000007506167, Improvement: -0.00000000593885, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1378
Epoch 1378, Loss: 0.00000006005439, Improvement: -0.00000001500728, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1379
Epoch 1379, Loss: 0.00000006159582, Improvement: 0.00000000154144, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1380
Epoch 1380, Loss: 0.00000007095043, Improvement: 0.00000000935460, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1381
Epoch 1381, Loss: 0.00000004525781, Improvement: -0.00000002569262, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1382
Epoch 1382, Loss: 0.00000004383030, Improvement: -0.00000000142750, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1383
Epoch 1383, Loss: 0.00000005340578, Improvement: 0.00000000957548, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1384
Epoch 1384, Loss: 0.00000006879064, Improvement: 0.00000001538486, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1385
Epoch 1385, Loss: 0.00000005447547, Improvement: -0.00000001431517, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1386
Epoch 1386, Loss: 0.00000005370200, Improvement: -0.00000000077347, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1387
Epoch 1387, Loss: 0.00000006741842, Improvement: 0.00000001371642, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1388
Epoch 1388, Loss: 0.00000006957050, Improvement: 0.00000000215208, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1389
Epoch 1389, Loss: 0.00000008769248, Improvement: 0.00000001812197, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1390
Epoch 1390, Loss: 0.00000015804356, Improvement: 0.00000007035109, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1391
Epoch 1391, Loss: 0.00000018292890, Improvement: 0.00000002488534, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1392
Epoch 1392, Loss: 0.00000018885007, Improvement: 0.00000000592117, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1393
Epoch 1393, Loss: 0.00000012829232, Improvement: -0.00000006055775, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1394
Epoch 1394, Loss: 0.00000014728266, Improvement: 0.00000001899035, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1395
Epoch 1395, Loss: 0.00000010612621, Improvement: -0.00000004115646, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1396
Epoch 1396, Loss: 0.00000006553615, Improvement: -0.00000004059006, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1397
Epoch 1397, Loss: 0.00000004933755, Improvement: -0.00000001619860, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1398
Epoch 1398, Loss: 0.00000004298350, Improvement: -0.00000000635404, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1399
Epoch 1399, Loss: 0.00000004377489, Improvement: 0.00000000079139, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1400
Model saving checkpoint: the model trained after epoch 1400 has been saved with the training errors.
Epoch 1400, Loss: 0.00000004441331, Improvement: 0.00000000063841, Best Loss: 0.00000002464124 in Epoch 1342
Epoch 1401
A best model at epoch 1401 has been saved with training error 0.00000002374874.
Epoch 1401, Loss: 0.00000003760110, Improvement: -0.00000000681221, Best Loss: 0.00000002374874 in Epoch 1401
Epoch 1402
Epoch 1402, Loss: 0.00000004320237, Improvement: 0.00000000560127, Best Loss: 0.00000002374874 in Epoch 1401
Epoch 1403
Epoch 1403, Loss: 0.00000003740359, Improvement: -0.00000000579879, Best Loss: 0.00000002374874 in Epoch 1401
Epoch 1404
A best model at epoch 1404 has been saved with training error 0.00000002314765.
Epoch 1404, Loss: 0.00000003292826, Improvement: -0.00000000447533, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1405
Epoch 1405, Loss: 0.00000006841609, Improvement: 0.00000003548783, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1406
Epoch 1406, Loss: 0.00000007796069, Improvement: 0.00000000954460, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1407
Epoch 1407, Loss: 0.00000005776855, Improvement: -0.00000002019213, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1408
Epoch 1408, Loss: 0.00000004880333, Improvement: -0.00000000896522, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1409
Epoch 1409, Loss: 0.00000006647515, Improvement: 0.00000001767182, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1410
Epoch 1410, Loss: 0.00000006785234, Improvement: 0.00000000137719, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1411
Epoch 1411, Loss: 0.00000005427442, Improvement: -0.00000001357792, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1412
Epoch 1412, Loss: 0.00000019886562, Improvement: 0.00000014459120, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1413
Epoch 1413, Loss: 0.00000027458766, Improvement: 0.00000007572204, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1414
Epoch 1414, Loss: 0.00000018590255, Improvement: -0.00000008868511, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1415
Epoch 1415, Loss: 0.00000010549316, Improvement: -0.00000008040940, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1416
Epoch 1416, Loss: 0.00000005579177, Improvement: -0.00000004970139, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1417
Epoch 1417, Loss: 0.00000003701852, Improvement: -0.00000001877325, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1418
Epoch 1418, Loss: 0.00000004038473, Improvement: 0.00000000336621, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1419
Epoch 1419, Loss: 0.00000003481538, Improvement: -0.00000000556935, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1420
Epoch 1420, Loss: 0.00000003243183, Improvement: -0.00000000238356, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1421
Epoch 1421, Loss: 0.00000003214123, Improvement: -0.00000000029060, Best Loss: 0.00000002314765 in Epoch 1404
Epoch 1422
A best model at epoch 1422 has been saved with training error 0.00000002299793.
Epoch 1422, Loss: 0.00000003150895, Improvement: -0.00000000063227, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1423
Epoch 1423, Loss: 0.00000003895172, Improvement: 0.00000000744276, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1424
Epoch 1424, Loss: 0.00000004822663, Improvement: 0.00000000927491, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1425
Epoch 1425, Loss: 0.00000004724197, Improvement: -0.00000000098466, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1426
Epoch 1426, Loss: 0.00000004627044, Improvement: -0.00000000097153, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1427
Epoch 1427, Loss: 0.00000003796512, Improvement: -0.00000000830532, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1428
Epoch 1428, Loss: 0.00000003346933, Improvement: -0.00000000449579, Best Loss: 0.00000002299793 in Epoch 1422
Epoch 1429
A best model at epoch 1429 has been saved with training error 0.00000002272992.
A best model at epoch 1429 has been saved with training error 0.00000002140214.
Epoch 1429, Loss: 0.00000003243183, Improvement: -0.00000000103750, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1430
Epoch 1430, Loss: 0.00000003894747, Improvement: 0.00000000651565, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1431
Epoch 1431, Loss: 0.00000007831145, Improvement: 0.00000003936397, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1432
Epoch 1432, Loss: 0.00000020347245, Improvement: 0.00000012516100, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1433
Epoch 1433, Loss: 0.00000016726483, Improvement: -0.00000003620763, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1434
Epoch 1434, Loss: 0.00000010449865, Improvement: -0.00000006276618, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1435
Epoch 1435, Loss: 0.00000007154344, Improvement: -0.00000003295521, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1436
Epoch 1436, Loss: 0.00000007680291, Improvement: 0.00000000525947, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1437
Epoch 1437, Loss: 0.00000011557821, Improvement: 0.00000003877531, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1438
Epoch 1438, Loss: 0.00000010768950, Improvement: -0.00000000788872, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1439
Epoch 1439, Loss: 0.00000008437757, Improvement: -0.00000002331193, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1440
Epoch 1440, Loss: 0.00000006410493, Improvement: -0.00000002027263, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1441
Epoch 1441, Loss: 0.00000005675908, Improvement: -0.00000000734585, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1442
Epoch 1442, Loss: 0.00000009408014, Improvement: 0.00000003732106, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1443
Epoch 1443, Loss: 0.00000007356874, Improvement: -0.00000002051139, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1444
Epoch 1444, Loss: 0.00000007026914, Improvement: -0.00000000329960, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1445
Epoch 1445, Loss: 0.00000006437889, Improvement: -0.00000000589025, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1446
Epoch 1446, Loss: 0.00000007182662, Improvement: 0.00000000744773, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1447
Epoch 1447, Loss: 0.00000006719974, Improvement: -0.00000000462688, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1448
Epoch 1448, Loss: 0.00000006377830, Improvement: -0.00000000342144, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1449
Epoch 1449, Loss: 0.00000005412072, Improvement: -0.00000000965759, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1450
Model saving checkpoint: the model trained after epoch 1450 has been saved with the training errors.
Epoch 1450, Loss: 0.00000005340605, Improvement: -0.00000000071466, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1451
Epoch 1451, Loss: 0.00000004543443, Improvement: -0.00000000797162, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1452
Epoch 1452, Loss: 0.00000005337665, Improvement: 0.00000000794222, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1453
Epoch 1453, Loss: 0.00000004237409, Improvement: -0.00000001100256, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1454
Epoch 1454, Loss: 0.00000003856656, Improvement: -0.00000000380754, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1455
Epoch 1455, Loss: 0.00000004819768, Improvement: 0.00000000963112, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1456
Epoch 1456, Loss: 0.00000006846205, Improvement: 0.00000002026437, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1457
Epoch 1457, Loss: 0.00000009490701, Improvement: 0.00000002644496, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1458
Epoch 1458, Loss: 0.00000012340274, Improvement: 0.00000002849573, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1459
Epoch 1459, Loss: 0.00000018103599, Improvement: 0.00000005763325, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1460
Epoch 1460, Loss: 0.00000014873124, Improvement: -0.00000003230475, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1461
Epoch 1461, Loss: 0.00000011698577, Improvement: -0.00000003174547, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1462
Epoch 1462, Loss: 0.00000006054250, Improvement: -0.00000005644327, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1463
Epoch 1463, Loss: 0.00000004486599, Improvement: -0.00000001567651, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1464
Epoch 1464, Loss: 0.00000004931488, Improvement: 0.00000000444889, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1465
Epoch 1465, Loss: 0.00000004222421, Improvement: -0.00000000709067, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1466
Epoch 1466, Loss: 0.00000004673395, Improvement: 0.00000000450975, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1467
Epoch 1467, Loss: 0.00000005179709, Improvement: 0.00000000506314, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1468
Epoch 1468, Loss: 0.00000005600254, Improvement: 0.00000000420545, Best Loss: 0.00000002140214 in Epoch 1429
Epoch 1469
A best model at epoch 1469 has been saved with training error 0.00000001980125.
Epoch 1469, Loss: 0.00000004570638, Improvement: -0.00000001029615, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1470
Epoch 1470, Loss: 0.00000005155516, Improvement: 0.00000000584878, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1471
Epoch 1471, Loss: 0.00000008089215, Improvement: 0.00000002933699, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1472
Epoch 1472, Loss: 0.00000012634919, Improvement: 0.00000004545704, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1473
Epoch 1473, Loss: 0.00000014979768, Improvement: 0.00000002344848, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1474
Epoch 1474, Loss: 0.00000015065816, Improvement: 0.00000000086049, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1475
Epoch 1475, Loss: 0.00000011432900, Improvement: -0.00000003632917, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1476
Epoch 1476, Loss: 0.00000008207078, Improvement: -0.00000003225822, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1477
Epoch 1477, Loss: 0.00000016878167, Improvement: 0.00000008671089, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1478
Epoch 1478, Loss: 0.00000012294052, Improvement: -0.00000004584115, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1479
Epoch 1479, Loss: 0.00000015825064, Improvement: 0.00000003531012, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1480
Epoch 1480, Loss: 0.00000014541814, Improvement: -0.00000001283250, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1481
Epoch 1481, Loss: 0.00000008470694, Improvement: -0.00000006071120, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1482
Epoch 1482, Loss: 0.00000010396625, Improvement: 0.00000001925930, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1483
Epoch 1483, Loss: 0.00000008405182, Improvement: -0.00000001991443, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1484
Epoch 1484, Loss: 0.00000004399261, Improvement: -0.00000004005921, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1485
Epoch 1485, Loss: 0.00000005047096, Improvement: 0.00000000647835, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1486
Epoch 1486, Loss: 0.00000005724891, Improvement: 0.00000000677796, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1487
Epoch 1487, Loss: 0.00000004713057, Improvement: -0.00000001011834, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1488
Epoch 1488, Loss: 0.00000004487655, Improvement: -0.00000000225402, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1489
Epoch 1489, Loss: 0.00000003471598, Improvement: -0.00000001016057, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1490
Epoch 1490, Loss: 0.00000003363748, Improvement: -0.00000000107850, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1491
Epoch 1491, Loss: 0.00000004062970, Improvement: 0.00000000699222, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1492
Epoch 1492, Loss: 0.00000005541773, Improvement: 0.00000001478803, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1493
Epoch 1493, Loss: 0.00000004265284, Improvement: -0.00000001276489, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1494
Epoch 1494, Loss: 0.00000005465220, Improvement: 0.00000001199935, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1495
Epoch 1495, Loss: 0.00000009767432, Improvement: 0.00000004302213, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1496
Epoch 1496, Loss: 0.00000013914933, Improvement: 0.00000004147501, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1497
Epoch 1497, Loss: 0.00000010772746, Improvement: -0.00000003142187, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1498
Epoch 1498, Loss: 0.00000008139016, Improvement: -0.00000002633731, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1499
Epoch 1499, Loss: 0.00000019931850, Improvement: 0.00000011792835, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1500
Model saving checkpoint: the model trained after epoch 1500 has been saved with the training errors.
Epoch 1500, Loss: 0.00000017025147, Improvement: -0.00000002906703, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1501
Epoch 1501, Loss: 0.00000009717797, Improvement: -0.00000007307350, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1502
Epoch 1502, Loss: 0.00000006964975, Improvement: -0.00000002752821, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1503
Epoch 1503, Loss: 0.00000005577290, Improvement: -0.00000001387685, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1504
Epoch 1504, Loss: 0.00000003858163, Improvement: -0.00000001719127, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1505
Epoch 1505, Loss: 0.00000003592025, Improvement: -0.00000000266138, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1506
Epoch 1506, Loss: 0.00000003993651, Improvement: 0.00000000401626, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1507
Epoch 1507, Loss: 0.00000004694733, Improvement: 0.00000000701082, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1508
Epoch 1508, Loss: 0.00000005506508, Improvement: 0.00000000811775, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1509
Epoch 1509, Loss: 0.00000005256836, Improvement: -0.00000000249672, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1510
Epoch 1510, Loss: 0.00000005272683, Improvement: 0.00000000015847, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1511
Epoch 1511, Loss: 0.00000006546583, Improvement: 0.00000001273900, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1512
Epoch 1512, Loss: 0.00000005479678, Improvement: -0.00000001066905, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1513
Epoch 1513, Loss: 0.00000006408469, Improvement: 0.00000000928791, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1514
Epoch 1514, Loss: 0.00000005758084, Improvement: -0.00000000650385, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1515
Epoch 1515, Loss: 0.00000006324196, Improvement: 0.00000000566112, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1516
Epoch 1516, Loss: 0.00000006108002, Improvement: -0.00000000216194, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1517
Epoch 1517, Loss: 0.00000010881400, Improvement: 0.00000004773398, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1518
Epoch 1518, Loss: 0.00000014947311, Improvement: 0.00000004065911, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1519
Epoch 1519, Loss: 0.00000017355895, Improvement: 0.00000002408584, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1520
Epoch 1520, Loss: 0.00000020371229, Improvement: 0.00000003015334, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1521
Epoch 1521, Loss: 0.00000011904190, Improvement: -0.00000008467039, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1522
Epoch 1522, Loss: 0.00000007355023, Improvement: -0.00000004549167, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1523
Epoch 1523, Loss: 0.00000005547946, Improvement: -0.00000001807077, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1524
Epoch 1524, Loss: 0.00000005462855, Improvement: -0.00000000085091, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1525
Epoch 1525, Loss: 0.00000005401063, Improvement: -0.00000000061792, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1526
Epoch 1526, Loss: 0.00000004274585, Improvement: -0.00000001126478, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1527
Epoch 1527, Loss: 0.00000003779051, Improvement: -0.00000000495534, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1528
Epoch 1528, Loss: 0.00000004203417, Improvement: 0.00000000424366, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1529
Epoch 1529, Loss: 0.00000004153973, Improvement: -0.00000000049443, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1530
Epoch 1530, Loss: 0.00000003999668, Improvement: -0.00000000154306, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1531
Epoch 1531, Loss: 0.00000003611465, Improvement: -0.00000000388203, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1532
Epoch 1532, Loss: 0.00000004767078, Improvement: 0.00000001155614, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1533
Epoch 1533, Loss: 0.00000006413959, Improvement: 0.00000001646880, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1534
Epoch 1534, Loss: 0.00000005955505, Improvement: -0.00000000458454, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1535
Epoch 1535, Loss: 0.00000009534118, Improvement: 0.00000003578613, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1536
Epoch 1536, Loss: 0.00000008100695, Improvement: -0.00000001433423, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1537
Epoch 1537, Loss: 0.00000009036160, Improvement: 0.00000000935464, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1538
Epoch 1538, Loss: 0.00000008409645, Improvement: -0.00000000626515, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1539
Epoch 1539, Loss: 0.00000006611197, Improvement: -0.00000001798449, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1540
Epoch 1540, Loss: 0.00000008154775, Improvement: 0.00000001543579, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1541
Epoch 1541, Loss: 0.00000006936241, Improvement: -0.00000001218535, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1542
Epoch 1542, Loss: 0.00000007218270, Improvement: 0.00000000282029, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1543
Epoch 1543, Loss: 0.00000005945511, Improvement: -0.00000001272759, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1544
Epoch 1544, Loss: 0.00000006342640, Improvement: 0.00000000397129, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1545
Epoch 1545, Loss: 0.00000009496218, Improvement: 0.00000003153578, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1546
Epoch 1546, Loss: 0.00000012087269, Improvement: 0.00000002591051, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1547
Epoch 1547, Loss: 0.00000015023644, Improvement: 0.00000002936375, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1548
Epoch 1548, Loss: 0.00000007147177, Improvement: -0.00000007876467, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1549
Epoch 1549, Loss: 0.00000010085117, Improvement: 0.00000002937940, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1550
Model saving checkpoint: the model trained after epoch 1550 has been saved with the training errors.
Epoch 1550, Loss: 0.00000006584737, Improvement: -0.00000003500380, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1551
Epoch 1551, Loss: 0.00000006290771, Improvement: -0.00000000293966, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1552
Epoch 1552, Loss: 0.00000003851519, Improvement: -0.00000002439252, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1553
Epoch 1553, Loss: 0.00000003835435, Improvement: -0.00000000016085, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1554
Epoch 1554, Loss: 0.00000003553902, Improvement: -0.00000000281532, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1555
Epoch 1555, Loss: 0.00000003507751, Improvement: -0.00000000046151, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1556
Epoch 1556, Loss: 0.00000003827049, Improvement: 0.00000000319298, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1557
Epoch 1557, Loss: 0.00000004990272, Improvement: 0.00000001163223, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1558
Epoch 1558, Loss: 0.00000006351296, Improvement: 0.00000001361024, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1559
Epoch 1559, Loss: 0.00000006520527, Improvement: 0.00000000169230, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1560
Epoch 1560, Loss: 0.00000006068023, Improvement: -0.00000000452504, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1561
Epoch 1561, Loss: 0.00000006859109, Improvement: 0.00000000791086, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1562
Epoch 1562, Loss: 0.00000009021706, Improvement: 0.00000002162597, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1563
Epoch 1563, Loss: 0.00000008996817, Improvement: -0.00000000024889, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1564
Epoch 1564, Loss: 0.00000010863694, Improvement: 0.00000001866878, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1565
Epoch 1565, Loss: 0.00000014129584, Improvement: 0.00000003265890, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1566
Epoch 1566, Loss: 0.00000015284671, Improvement: 0.00000001155087, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1567
Epoch 1567, Loss: 0.00000011875659, Improvement: -0.00000003409013, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1568
Epoch 1568, Loss: 0.00000011009145, Improvement: -0.00000000866514, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1569
Epoch 1569, Loss: 0.00000008236166, Improvement: -0.00000002772979, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1570
Epoch 1570, Loss: 0.00000006228890, Improvement: -0.00000002007276, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1571
Epoch 1571, Loss: 0.00000007893247, Improvement: 0.00000001664356, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1572
Epoch 1572, Loss: 0.00000006867499, Improvement: -0.00000001025748, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1573
Epoch 1573, Loss: 0.00000013907118, Improvement: 0.00000007039620, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1574
Epoch 1574, Loss: 0.00000011126077, Improvement: -0.00000002781041, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1575
Epoch 1575, Loss: 0.00000005975457, Improvement: -0.00000005150620, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1576
Epoch 1576, Loss: 0.00000005052114, Improvement: -0.00000000923344, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1577
Epoch 1577, Loss: 0.00000004778415, Improvement: -0.00000000273699, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1578
Epoch 1578, Loss: 0.00000007187743, Improvement: 0.00000002409328, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1579
Epoch 1579, Loss: 0.00000004642693, Improvement: -0.00000002545050, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1580
Epoch 1580, Loss: 0.00000004069876, Improvement: -0.00000000572817, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1581
Epoch 1581, Loss: 0.00000003883431, Improvement: -0.00000000186445, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1582
Epoch 1582, Loss: 0.00000003440069, Improvement: -0.00000000443362, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1583
Epoch 1583, Loss: 0.00000003183748, Improvement: -0.00000000256321, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1584
Epoch 1584, Loss: 0.00000004678754, Improvement: 0.00000001495007, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1585
Epoch 1585, Loss: 0.00000008025516, Improvement: 0.00000003346762, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1586
Epoch 1586, Loss: 0.00000006407016, Improvement: -0.00000001618500, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1587
Epoch 1587, Loss: 0.00000004648319, Improvement: -0.00000001758698, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1588
Epoch 1588, Loss: 0.00000004318546, Improvement: -0.00000000329773, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1589
Epoch 1589, Loss: 0.00000004875964, Improvement: 0.00000000557418, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1590
Epoch 1590, Loss: 0.00000006043620, Improvement: 0.00000001167656, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1591
Epoch 1591, Loss: 0.00000009933046, Improvement: 0.00000003889426, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1592
Epoch 1592, Loss: 0.00000012022396, Improvement: 0.00000002089350, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1593
Epoch 1593, Loss: 0.00000011890608, Improvement: -0.00000000131788, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1594
Epoch 1594, Loss: 0.00000004992098, Improvement: -0.00000006898510, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1595
Epoch 1595, Loss: 0.00000004272789, Improvement: -0.00000000719309, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1596
Epoch 1596, Loss: 0.00000004941162, Improvement: 0.00000000668373, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1597
Epoch 1597, Loss: 0.00000003687830, Improvement: -0.00000001253332, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1598
Epoch 1598, Loss: 0.00000004429875, Improvement: 0.00000000742045, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1599
Epoch 1599, Loss: 0.00000008118737, Improvement: 0.00000003688862, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1600
Model saving checkpoint: the model trained after epoch 1600 has been saved with the training errors.
Epoch 1600, Loss: 0.00000010750212, Improvement: 0.00000002631476, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1601
Epoch 1601, Loss: 0.00000006281214, Improvement: -0.00000004468999, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1602
Epoch 1602, Loss: 0.00000005374927, Improvement: -0.00000000906287, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1603
Epoch 1603, Loss: 0.00000006760410, Improvement: 0.00000001385483, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1604
Epoch 1604, Loss: 0.00000005596761, Improvement: -0.00000001163649, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1605
Epoch 1605, Loss: 0.00000004108318, Improvement: -0.00000001488443, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1606
Epoch 1606, Loss: 0.00000004127767, Improvement: 0.00000000019449, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1607
Epoch 1607, Loss: 0.00000005678645, Improvement: 0.00000001550878, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1608
Epoch 1608, Loss: 0.00000004209081, Improvement: -0.00000001469564, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1609
Epoch 1609, Loss: 0.00000005451142, Improvement: 0.00000001242061, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1610
Epoch 1610, Loss: 0.00000005121458, Improvement: -0.00000000329684, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1611
Epoch 1611, Loss: 0.00000006145171, Improvement: 0.00000001023714, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1612
Epoch 1612, Loss: 0.00000003974821, Improvement: -0.00000002170351, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1613
Epoch 1613, Loss: 0.00000005932169, Improvement: 0.00000001957348, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1614
Epoch 1614, Loss: 0.00000008714669, Improvement: 0.00000002782500, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1615
Epoch 1615, Loss: 0.00000008214518, Improvement: -0.00000000500151, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1616
Epoch 1616, Loss: 0.00000006082503, Improvement: -0.00000002132016, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1617
Epoch 1617, Loss: 0.00000004256912, Improvement: -0.00000001825591, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1618
Epoch 1618, Loss: 0.00000004088084, Improvement: -0.00000000168828, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1619
Epoch 1619, Loss: 0.00000005731926, Improvement: 0.00000001643842, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1620
Epoch 1620, Loss: 0.00000008745831, Improvement: 0.00000003013905, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1621
Epoch 1621, Loss: 0.00000011440770, Improvement: 0.00000002694939, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1622
Epoch 1622, Loss: 0.00000013175685, Improvement: 0.00000001734915, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1623
Epoch 1623, Loss: 0.00000007479795, Improvement: -0.00000005695890, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1624
Epoch 1624, Loss: 0.00000004590868, Improvement: -0.00000002888927, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1625
Epoch 1625, Loss: 0.00000004215356, Improvement: -0.00000000375512, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1626
Epoch 1626, Loss: 0.00000004215257, Improvement: -0.00000000000099, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1627
Epoch 1627, Loss: 0.00000003662921, Improvement: -0.00000000552336, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1628
Epoch 1628, Loss: 0.00000003952059, Improvement: 0.00000000289138, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1629
Epoch 1629, Loss: 0.00000003810086, Improvement: -0.00000000141972, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1630
Epoch 1630, Loss: 0.00000004028272, Improvement: 0.00000000218186, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1631
Epoch 1631, Loss: 0.00000005396173, Improvement: 0.00000001367901, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1632
Epoch 1632, Loss: 0.00000011535104, Improvement: 0.00000006138931, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1633
Epoch 1633, Loss: 0.00000013524778, Improvement: 0.00000001989674, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1634
Epoch 1634, Loss: 0.00000012627939, Improvement: -0.00000000896839, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1635
Epoch 1635, Loss: 0.00000010921503, Improvement: -0.00000001706437, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1636
Epoch 1636, Loss: 0.00000018682947, Improvement: 0.00000007761445, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1637
Epoch 1637, Loss: 0.00000008631749, Improvement: -0.00000010051198, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1638
Epoch 1638, Loss: 0.00000007251174, Improvement: -0.00000001380575, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1639
Epoch 1639, Loss: 0.00000005959249, Improvement: -0.00000001291925, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1640
Epoch 1640, Loss: 0.00000005195528, Improvement: -0.00000000763721, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1641
Epoch 1641, Loss: 0.00000004817077, Improvement: -0.00000000378452, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1642
Epoch 1642, Loss: 0.00000004713430, Improvement: -0.00000000103647, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1643
Epoch 1643, Loss: 0.00000004623074, Improvement: -0.00000000090356, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1644
Epoch 1644, Loss: 0.00000004607377, Improvement: -0.00000000015697, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1645
Epoch 1645, Loss: 0.00000005067598, Improvement: 0.00000000460221, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1646
Epoch 1646, Loss: 0.00000004493197, Improvement: -0.00000000574401, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1647
Epoch 1647, Loss: 0.00000004825614, Improvement: 0.00000000332418, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1648
Epoch 1648, Loss: 0.00000005426834, Improvement: 0.00000000601219, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1649
Epoch 1649, Loss: 0.00000004994981, Improvement: -0.00000000431853, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1650
Model saving checkpoint: the model trained after epoch 1650 has been saved with the training errors.
Epoch 1650, Loss: 0.00000005533465, Improvement: 0.00000000538483, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1651
Epoch 1651, Loss: 0.00000005899665, Improvement: 0.00000000366201, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1652
Epoch 1652, Loss: 0.00000009617526, Improvement: 0.00000003717861, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1653
Epoch 1653, Loss: 0.00000006633557, Improvement: -0.00000002983970, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1654
Epoch 1654, Loss: 0.00000005074956, Improvement: -0.00000001558601, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1655
Epoch 1655, Loss: 0.00000005546065, Improvement: 0.00000000471109, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1656
Epoch 1656, Loss: 0.00000004350142, Improvement: -0.00000001195924, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1657
Epoch 1657, Loss: 0.00000003558459, Improvement: -0.00000000791682, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1658
Epoch 1658, Loss: 0.00000005137633, Improvement: 0.00000001579174, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1659
Epoch 1659, Loss: 0.00000004688056, Improvement: -0.00000000449577, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1660
Epoch 1660, Loss: 0.00000005558321, Improvement: 0.00000000870265, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1661
Epoch 1661, Loss: 0.00000005736750, Improvement: 0.00000000178428, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1662
Epoch 1662, Loss: 0.00000004361537, Improvement: -0.00000001375213, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1663
Epoch 1663, Loss: 0.00000004500000, Improvement: 0.00000000138463, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1664
Epoch 1664, Loss: 0.00000006783626, Improvement: 0.00000002283626, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1665
Epoch 1665, Loss: 0.00000022504043, Improvement: 0.00000015720418, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1666
Epoch 1666, Loss: 0.00000015474199, Improvement: -0.00000007029844, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1667
Epoch 1667, Loss: 0.00000005840209, Improvement: -0.00000009633989, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1668
Epoch 1668, Loss: 0.00000003704417, Improvement: -0.00000002135793, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1669
Epoch 1669, Loss: 0.00000003465607, Improvement: -0.00000000238810, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1670
Epoch 1670, Loss: 0.00000003826253, Improvement: 0.00000000360646, Best Loss: 0.00000001980125 in Epoch 1469
Epoch 1671
A best model at epoch 1671 has been saved with training error 0.00000001970570.
Epoch 1671, Loss: 0.00000003298956, Improvement: -0.00000000527297, Best Loss: 0.00000001970570 in Epoch 1671
Epoch 1672
Epoch 1672, Loss: 0.00000003403797, Improvement: 0.00000000104842, Best Loss: 0.00000001970570 in Epoch 1671
Epoch 1673
Epoch 1673, Loss: 0.00000003268401, Improvement: -0.00000000135397, Best Loss: 0.00000001970570 in Epoch 1671
Epoch 1674
A best model at epoch 1674 has been saved with training error 0.00000001901682.
Epoch 1674, Loss: 0.00000002724517, Improvement: -0.00000000543883, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1675
Epoch 1675, Loss: 0.00000002933523, Improvement: 0.00000000209006, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1676
Epoch 1676, Loss: 0.00000004172239, Improvement: 0.00000001238716, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1677
Epoch 1677, Loss: 0.00000004702529, Improvement: 0.00000000530290, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1678
Epoch 1678, Loss: 0.00000005057701, Improvement: 0.00000000355172, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1679
Epoch 1679, Loss: 0.00000004656529, Improvement: -0.00000000401171, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1680
Epoch 1680, Loss: 0.00000006820331, Improvement: 0.00000002163802, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1681
Epoch 1681, Loss: 0.00000008303699, Improvement: 0.00000001483368, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1682
Epoch 1682, Loss: 0.00000007299130, Improvement: -0.00000001004569, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1683
Epoch 1683, Loss: 0.00000005294678, Improvement: -0.00000002004452, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1684
Epoch 1684, Loss: 0.00000005102172, Improvement: -0.00000000192507, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1685
Epoch 1685, Loss: 0.00000006279118, Improvement: 0.00000001176946, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1686
Epoch 1686, Loss: 0.00000008140185, Improvement: 0.00000001861067, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1687
Epoch 1687, Loss: 0.00000008644486, Improvement: 0.00000000504301, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1688
Epoch 1688, Loss: 0.00000005799417, Improvement: -0.00000002845069, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1689
Epoch 1689, Loss: 0.00000006112427, Improvement: 0.00000000313010, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1690
Epoch 1690, Loss: 0.00000008070044, Improvement: 0.00000001957617, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1691
Epoch 1691, Loss: 0.00000008677054, Improvement: 0.00000000607011, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1692
Epoch 1692, Loss: 0.00000012184446, Improvement: 0.00000003507391, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1693
Epoch 1693, Loss: 0.00000007731826, Improvement: -0.00000004452620, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1694
Epoch 1694, Loss: 0.00000008437518, Improvement: 0.00000000705692, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1695
Epoch 1695, Loss: 0.00000007152056, Improvement: -0.00000001285462, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1696
Epoch 1696, Loss: 0.00000004193591, Improvement: -0.00000002958466, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1697
Epoch 1697, Loss: 0.00000003517577, Improvement: -0.00000000676013, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1698
Epoch 1698, Loss: 0.00000004223359, Improvement: 0.00000000705781, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1699
Epoch 1699, Loss: 0.00000006399671, Improvement: 0.00000002176312, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1700
Model saving checkpoint: the model trained after epoch 1700 has been saved with the training errors.
Epoch 1700, Loss: 0.00000006936486, Improvement: 0.00000000536815, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1701
Epoch 1701, Loss: 0.00000007091237, Improvement: 0.00000000154751, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1702
Epoch 1702, Loss: 0.00000012445532, Improvement: 0.00000005354295, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1703
Epoch 1703, Loss: 0.00000008550376, Improvement: -0.00000003895156, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1704
Epoch 1704, Loss: 0.00000007639249, Improvement: -0.00000000911127, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1705
Epoch 1705, Loss: 0.00000006116823, Improvement: -0.00000001522426, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1706
Epoch 1706, Loss: 0.00000003842376, Improvement: -0.00000002274447, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1707
Epoch 1707, Loss: 0.00000003987570, Improvement: 0.00000000145194, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1708
Epoch 1708, Loss: 0.00000007511725, Improvement: 0.00000003524155, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1709
Epoch 1709, Loss: 0.00000010949546, Improvement: 0.00000003437821, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1710
Epoch 1710, Loss: 0.00000005917669, Improvement: -0.00000005031877, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1711
Epoch 1711, Loss: 0.00000004879990, Improvement: -0.00000001037679, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1712
Epoch 1712, Loss: 0.00000004000720, Improvement: -0.00000000879270, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1713
Epoch 1713, Loss: 0.00000003515995, Improvement: -0.00000000484726, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1714
Epoch 1714, Loss: 0.00000004442950, Improvement: 0.00000000926956, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1715
Epoch 1715, Loss: 0.00000006380190, Improvement: 0.00000001937239, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1716
Epoch 1716, Loss: 0.00000004936160, Improvement: -0.00000001444030, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1717
Epoch 1717, Loss: 0.00000004590214, Improvement: -0.00000000345946, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1718
Epoch 1718, Loss: 0.00000003469735, Improvement: -0.00000001120479, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1719
Epoch 1719, Loss: 0.00000004127399, Improvement: 0.00000000657664, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1720
Epoch 1720, Loss: 0.00000005304162, Improvement: 0.00000001176762, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1721
Epoch 1721, Loss: 0.00000007366956, Improvement: 0.00000002062794, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1722
Epoch 1722, Loss: 0.00000020977170, Improvement: 0.00000013610214, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1723
Epoch 1723, Loss: 0.00000016602461, Improvement: -0.00000004374709, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1724
Epoch 1724, Loss: 0.00000008066460, Improvement: -0.00000008536001, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1725
Epoch 1725, Loss: 0.00000004939933, Improvement: -0.00000003126527, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1726
Epoch 1726, Loss: 0.00000003508546, Improvement: -0.00000001431387, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1727
Epoch 1727, Loss: 0.00000002772721, Improvement: -0.00000000735825, Best Loss: 0.00000001901682 in Epoch 1674
Epoch 1728
A best model at epoch 1728 has been saved with training error 0.00000001827474.
Epoch 1728, Loss: 0.00000002632838, Improvement: -0.00000000139883, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1729
Epoch 1729, Loss: 0.00000002680402, Improvement: 0.00000000047564, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1730
Epoch 1730, Loss: 0.00000002974849, Improvement: 0.00000000294447, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1731
Epoch 1731, Loss: 0.00000003751442, Improvement: 0.00000000776594, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1732
Epoch 1732, Loss: 0.00000003047380, Improvement: -0.00000000704062, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1733
Epoch 1733, Loss: 0.00000003063877, Improvement: 0.00000000016497, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1734
Epoch 1734, Loss: 0.00000002773071, Improvement: -0.00000000290806, Best Loss: 0.00000001827474 in Epoch 1728
Epoch 1735
A best model at epoch 1735 has been saved with training error 0.00000001673714.
Epoch 1735, Loss: 0.00000002643865, Improvement: -0.00000000129206, Best Loss: 0.00000001673714 in Epoch 1735
Epoch 1736
Epoch 1736, Loss: 0.00000002619103, Improvement: -0.00000000024762, Best Loss: 0.00000001673714 in Epoch 1735
Epoch 1737
Epoch 1737, Loss: 0.00000002938710, Improvement: 0.00000000319607, Best Loss: 0.00000001673714 in Epoch 1735
Epoch 1738
Epoch 1738, Loss: 0.00000003335255, Improvement: 0.00000000396545, Best Loss: 0.00000001673714 in Epoch 1735
Epoch 1739
A best model at epoch 1739 has been saved with training error 0.00000001620755.
Epoch 1739, Loss: 0.00000003237377, Improvement: -0.00000000097879, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1740
Epoch 1740, Loss: 0.00000003308487, Improvement: 0.00000000071110, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1741
Epoch 1741, Loss: 0.00000004605940, Improvement: 0.00000001297453, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1742
Epoch 1742, Loss: 0.00000004425621, Improvement: -0.00000000180319, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1743
Epoch 1743, Loss: 0.00000005914934, Improvement: 0.00000001489313, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1744
Epoch 1744, Loss: 0.00000016293913, Improvement: 0.00000010378979, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1745
Epoch 1745, Loss: 0.00000015558020, Improvement: -0.00000000735893, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1746
Epoch 1746, Loss: 0.00000008228258, Improvement: -0.00000007329762, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1747
Epoch 1747, Loss: 0.00000004822255, Improvement: -0.00000003406003, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1748
Epoch 1748, Loss: 0.00000003611514, Improvement: -0.00000001210741, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1749
Epoch 1749, Loss: 0.00000003032941, Improvement: -0.00000000578573, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1750
Model saving checkpoint: the model trained after epoch 1750 has been saved with the training errors.
Epoch 1750, Loss: 0.00000003163204, Improvement: 0.00000000130264, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1751
Epoch 1751, Loss: 0.00000003448874, Improvement: 0.00000000285670, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1752
Epoch 1752, Loss: 0.00000003520470, Improvement: 0.00000000071596, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1753
Epoch 1753, Loss: 0.00000003523079, Improvement: 0.00000000002609, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1754
Epoch 1754, Loss: 0.00000003765741, Improvement: 0.00000000242662, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1755
Epoch 1755, Loss: 0.00000005803836, Improvement: 0.00000002038095, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1756
Epoch 1756, Loss: 0.00000007675230, Improvement: 0.00000001871395, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1757
Epoch 1757, Loss: 0.00000006902828, Improvement: -0.00000000772402, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1758
Epoch 1758, Loss: 0.00000005891927, Improvement: -0.00000001010901, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1759
Epoch 1759, Loss: 0.00000006139535, Improvement: 0.00000000247609, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1760
Epoch 1760, Loss: 0.00000008474165, Improvement: 0.00000002334629, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1761
Epoch 1761, Loss: 0.00000005816653, Improvement: -0.00000002657511, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1762
Epoch 1762, Loss: 0.00000004516768, Improvement: -0.00000001299885, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1763
Epoch 1763, Loss: 0.00000005161135, Improvement: 0.00000000644366, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1764
Epoch 1764, Loss: 0.00000003354178, Improvement: -0.00000001806957, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1765
Epoch 1765, Loss: 0.00000005133268, Improvement: 0.00000001779091, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1766
Epoch 1766, Loss: 0.00000004891406, Improvement: -0.00000000241862, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1767
Epoch 1767, Loss: 0.00000003454814, Improvement: -0.00000001436592, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1768
Epoch 1768, Loss: 0.00000002587469, Improvement: -0.00000000867345, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1769
Epoch 1769, Loss: 0.00000003190133, Improvement: 0.00000000602664, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1770
Epoch 1770, Loss: 0.00000006215254, Improvement: 0.00000003025121, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1771
Epoch 1771, Loss: 0.00000012465157, Improvement: 0.00000006249903, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1772
Epoch 1772, Loss: 0.00000016865378, Improvement: 0.00000004400221, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1773
Epoch 1773, Loss: 0.00000007195076, Improvement: -0.00000009670302, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1774
Epoch 1774, Loss: 0.00000005433326, Improvement: -0.00000001761749, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1775
Epoch 1775, Loss: 0.00000004508998, Improvement: -0.00000000924328, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1776
Epoch 1776, Loss: 0.00000004133452, Improvement: -0.00000000375546, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1777
Epoch 1777, Loss: 0.00000005431776, Improvement: 0.00000001298324, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1778
Epoch 1778, Loss: 0.00000006464461, Improvement: 0.00000001032685, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1779
Epoch 1779, Loss: 0.00000010888082, Improvement: 0.00000004423621, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1780
Epoch 1780, Loss: 0.00000007844461, Improvement: -0.00000003043622, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1781
Epoch 1781, Loss: 0.00000005008021, Improvement: -0.00000002836440, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1782
Epoch 1782, Loss: 0.00000004115185, Improvement: -0.00000000892836, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1783
Epoch 1783, Loss: 0.00000002952464, Improvement: -0.00000001162721, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1784
Epoch 1784, Loss: 0.00000002766221, Improvement: -0.00000000186243, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1785
Epoch 1785, Loss: 0.00000003707320, Improvement: 0.00000000941099, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1786
Epoch 1786, Loss: 0.00000003704542, Improvement: -0.00000000002778, Best Loss: 0.00000001620755 in Epoch 1739
Epoch 1787
A best model at epoch 1787 has been saved with training error 0.00000001549641.
Epoch 1787, Loss: 0.00000002927115, Improvement: -0.00000000777427, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1788
Epoch 1788, Loss: 0.00000003483952, Improvement: 0.00000000556837, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1789
Epoch 1789, Loss: 0.00000006511898, Improvement: 0.00000003027946, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1790
Epoch 1790, Loss: 0.00000009032452, Improvement: 0.00000002520554, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1791
Epoch 1791, Loss: 0.00000005426776, Improvement: -0.00000003605676, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1792
Epoch 1792, Loss: 0.00000004964491, Improvement: -0.00000000462286, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1793
Epoch 1793, Loss: 0.00000004404212, Improvement: -0.00000000560279, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1794
Epoch 1794, Loss: 0.00000003475506, Improvement: -0.00000000928706, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1795
Epoch 1795, Loss: 0.00000002975444, Improvement: -0.00000000500062, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1796
Epoch 1796, Loss: 0.00000004327825, Improvement: 0.00000001352381, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1797
Epoch 1797, Loss: 0.00000006794660, Improvement: 0.00000002466835, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1798
Epoch 1798, Loss: 0.00000009269179, Improvement: 0.00000002474519, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1799
Epoch 1799, Loss: 0.00000008863789, Improvement: -0.00000000405390, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1800
Model saving checkpoint: the model trained after epoch 1800 has been saved with the training errors.
Epoch 1800, Loss: 0.00000009000989, Improvement: 0.00000000137201, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1801
Epoch 1801, Loss: 0.00000007803735, Improvement: -0.00000001197254, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1802
Epoch 1802, Loss: 0.00000007930369, Improvement: 0.00000000126633, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1803
Epoch 1803, Loss: 0.00000005677095, Improvement: -0.00000002253274, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1804
Epoch 1804, Loss: 0.00000004659961, Improvement: -0.00000001017134, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1805
Epoch 1805, Loss: 0.00000003762998, Improvement: -0.00000000896963, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1806
Epoch 1806, Loss: 0.00000003761066, Improvement: -0.00000000001932, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1807
Epoch 1807, Loss: 0.00000003975157, Improvement: 0.00000000214091, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1808
Epoch 1808, Loss: 0.00000004277159, Improvement: 0.00000000302001, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1809
Epoch 1809, Loss: 0.00000004731997, Improvement: 0.00000000454838, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1810
Epoch 1810, Loss: 0.00000006561755, Improvement: 0.00000001829758, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1811
Epoch 1811, Loss: 0.00000007223336, Improvement: 0.00000000661581, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1812
Epoch 1812, Loss: 0.00000008622245, Improvement: 0.00000001398909, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1813
Epoch 1813, Loss: 0.00000016227333, Improvement: 0.00000007605088, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1814
Epoch 1814, Loss: 0.00000022677557, Improvement: 0.00000006450224, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1815
Epoch 1815, Loss: 0.00000013325654, Improvement: -0.00000009351903, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1816
Epoch 1816, Loss: 0.00000010321814, Improvement: -0.00000003003840, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1817
Epoch 1817, Loss: 0.00000006320944, Improvement: -0.00000004000870, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1818
Epoch 1818, Loss: 0.00000003400694, Improvement: -0.00000002920250, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1819
Epoch 1819, Loss: 0.00000002623404, Improvement: -0.00000000777290, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1820
Epoch 1820, Loss: 0.00000002999667, Improvement: 0.00000000376263, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1821
Epoch 1821, Loss: 0.00000002731049, Improvement: -0.00000000268618, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1822
Epoch 1822, Loss: 0.00000002918720, Improvement: 0.00000000187671, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1823
Epoch 1823, Loss: 0.00000003194482, Improvement: 0.00000000275762, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1824
Epoch 1824, Loss: 0.00000003036041, Improvement: -0.00000000158441, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1825
Epoch 1825, Loss: 0.00000002812417, Improvement: -0.00000000223624, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1826
Epoch 1826, Loss: 0.00000002405904, Improvement: -0.00000000406514, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1827
Epoch 1827, Loss: 0.00000002609654, Improvement: 0.00000000203751, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1828
Epoch 1828, Loss: 0.00000003034742, Improvement: 0.00000000425087, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1829
Epoch 1829, Loss: 0.00000003458009, Improvement: 0.00000000423268, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1830
Epoch 1830, Loss: 0.00000002909288, Improvement: -0.00000000548722, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1831
Epoch 1831, Loss: 0.00000002921254, Improvement: 0.00000000011967, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1832
Epoch 1832, Loss: 0.00000002636266, Improvement: -0.00000000284988, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1833
Epoch 1833, Loss: 0.00000002523746, Improvement: -0.00000000112521, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1834
Epoch 1834, Loss: 0.00000002976274, Improvement: 0.00000000452529, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1835
Epoch 1835, Loss: 0.00000003493667, Improvement: 0.00000000517393, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1836
Epoch 1836, Loss: 0.00000005010097, Improvement: 0.00000001516429, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1837
Epoch 1837, Loss: 0.00000003718902, Improvement: -0.00000001291195, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1838
Epoch 1838, Loss: 0.00000003776949, Improvement: 0.00000000058047, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1839
Epoch 1839, Loss: 0.00000005087379, Improvement: 0.00000001310430, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1840
Epoch 1840, Loss: 0.00000004196653, Improvement: -0.00000000890726, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1841
Epoch 1841, Loss: 0.00000008072998, Improvement: 0.00000003876345, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1842
Epoch 1842, Loss: 0.00000008954464, Improvement: 0.00000000881466, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1843
Epoch 1843, Loss: 0.00000006614658, Improvement: -0.00000002339805, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1844
Epoch 1844, Loss: 0.00000006187841, Improvement: -0.00000000426818, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1845
Epoch 1845, Loss: 0.00000007216540, Improvement: 0.00000001028700, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1846
Epoch 1846, Loss: 0.00000004381890, Improvement: -0.00000002834651, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1847
Epoch 1847, Loss: 0.00000002942141, Improvement: -0.00000001439748, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1848
Epoch 1848, Loss: 0.00000002674138, Improvement: -0.00000000268003, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1849
Epoch 1849, Loss: 0.00000002685960, Improvement: 0.00000000011822, Best Loss: 0.00000001549641 in Epoch 1787
Epoch 1850
A best model at epoch 1850 has been saved with training error 0.00000001392809.
Model saving checkpoint: the model trained after epoch 1850 has been saved with the training errors.
Epoch 1850, Loss: 0.00000002845996, Improvement: 0.00000000160037, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1851
Epoch 1851, Loss: 0.00000003133765, Improvement: 0.00000000287768, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1852
Epoch 1852, Loss: 0.00000002646816, Improvement: -0.00000000486948, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1853
Epoch 1853, Loss: 0.00000002558962, Improvement: -0.00000000087855, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1854
Epoch 1854, Loss: 0.00000003668309, Improvement: 0.00000001109347, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1855
Epoch 1855, Loss: 0.00000004633375, Improvement: 0.00000000965065, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1856
Epoch 1856, Loss: 0.00000006058731, Improvement: 0.00000001425357, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1857
Epoch 1857, Loss: 0.00000014071624, Improvement: 0.00000008012893, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1858
Epoch 1858, Loss: 0.00000009847246, Improvement: -0.00000004224378, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1859
Epoch 1859, Loss: 0.00000008507291, Improvement: -0.00000001339955, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1860
Epoch 1860, Loss: 0.00000009640909, Improvement: 0.00000001133618, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1861
Epoch 1861, Loss: 0.00000007859655, Improvement: -0.00000001781254, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1862
Epoch 1862, Loss: 0.00000005013650, Improvement: -0.00000002846005, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1863
Epoch 1863, Loss: 0.00000005289048, Improvement: 0.00000000275398, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1864
Epoch 1864, Loss: 0.00000003525384, Improvement: -0.00000001763664, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1865
Epoch 1865, Loss: 0.00000002694785, Improvement: -0.00000000830599, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1866
Epoch 1866, Loss: 0.00000003582933, Improvement: 0.00000000888148, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1867
Epoch 1867, Loss: 0.00000003771548, Improvement: 0.00000000188616, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1868
Epoch 1868, Loss: 0.00000004044572, Improvement: 0.00000000273024, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1869
Epoch 1869, Loss: 0.00000005080326, Improvement: 0.00000001035754, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1870
Epoch 1870, Loss: 0.00000005744159, Improvement: 0.00000000663832, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1871
Epoch 1871, Loss: 0.00000006308824, Improvement: 0.00000000564665, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1872
Epoch 1872, Loss: 0.00000008464882, Improvement: 0.00000002156058, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1873
Epoch 1873, Loss: 0.00000015353296, Improvement: 0.00000006888414, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1874
Epoch 1874, Loss: 0.00000010380418, Improvement: -0.00000004972879, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1875
Epoch 1875, Loss: 0.00000005799919, Improvement: -0.00000004580498, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1876
Epoch 1876, Loss: 0.00000004642717, Improvement: -0.00000001157202, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1877
Epoch 1877, Loss: 0.00000003877808, Improvement: -0.00000000764910, Best Loss: 0.00000001392809 in Epoch 1850
Epoch 1878
A best model at epoch 1878 has been saved with training error 0.00000001299362.
Epoch 1878, Loss: 0.00000002856929, Improvement: -0.00000001020878, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1879
Epoch 1879, Loss: 0.00000002728863, Improvement: -0.00000000128066, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1880
Epoch 1880, Loss: 0.00000002793211, Improvement: 0.00000000064348, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1881
Epoch 1881, Loss: 0.00000003224310, Improvement: 0.00000000431099, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1882
Epoch 1882, Loss: 0.00000004099670, Improvement: 0.00000000875360, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1883
Epoch 1883, Loss: 0.00000003455813, Improvement: -0.00000000643857, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1884
Epoch 1884, Loss: 0.00000003862266, Improvement: 0.00000000406452, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1885
Epoch 1885, Loss: 0.00000005073150, Improvement: 0.00000001210885, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1886
Epoch 1886, Loss: 0.00000003499407, Improvement: -0.00000001573744, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1887
Epoch 1887, Loss: 0.00000003238174, Improvement: -0.00000000261233, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1888
Epoch 1888, Loss: 0.00000003894984, Improvement: 0.00000000656810, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1889
Epoch 1889, Loss: 0.00000004726386, Improvement: 0.00000000831402, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1890
Epoch 1890, Loss: 0.00000004481454, Improvement: -0.00000000244932, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1891
Epoch 1891, Loss: 0.00000003984054, Improvement: -0.00000000497400, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1892
Epoch 1892, Loss: 0.00000005911607, Improvement: 0.00000001927553, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1893
Epoch 1893, Loss: 0.00000005737819, Improvement: -0.00000000173788, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1894
Epoch 1894, Loss: 0.00000004460203, Improvement: -0.00000001277616, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1895
Epoch 1895, Loss: 0.00000005967177, Improvement: 0.00000001506974, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1896
Epoch 1896, Loss: 0.00000009248630, Improvement: 0.00000003281453, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1897
Epoch 1897, Loss: 0.00000010625101, Improvement: 0.00000001376471, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1898
Epoch 1898, Loss: 0.00000011328926, Improvement: 0.00000000703825, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1899
Epoch 1899, Loss: 0.00000006696036, Improvement: -0.00000004632890, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1900
Model saving checkpoint: the model trained after epoch 1900 has been saved with the training errors.
Epoch 1900, Loss: 0.00000004875918, Improvement: -0.00000001820118, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1901
Epoch 1901, Loss: 0.00000003203125, Improvement: -0.00000001672793, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1902
Epoch 1902, Loss: 0.00000003708001, Improvement: 0.00000000504875, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1903
Epoch 1903, Loss: 0.00000003489916, Improvement: -0.00000000218085, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1904
Epoch 1904, Loss: 0.00000007564158, Improvement: 0.00000004074243, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1905
Epoch 1905, Loss: 0.00000004959729, Improvement: -0.00000002604430, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1906
Epoch 1906, Loss: 0.00000003323437, Improvement: -0.00000001636291, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1907
Epoch 1907, Loss: 0.00000002608246, Improvement: -0.00000000715191, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1908
Epoch 1908, Loss: 0.00000002288340, Improvement: -0.00000000319907, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1909
Epoch 1909, Loss: 0.00000002582044, Improvement: 0.00000000293705, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1910
Epoch 1910, Loss: 0.00000002804890, Improvement: 0.00000000222846, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1911
Epoch 1911, Loss: 0.00000003641567, Improvement: 0.00000000836677, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1912
Epoch 1912, Loss: 0.00000003217154, Improvement: -0.00000000424413, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1913
Epoch 1913, Loss: 0.00000004661340, Improvement: 0.00000001444186, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1914
Epoch 1914, Loss: 0.00000004573802, Improvement: -0.00000000087537, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1915
Epoch 1915, Loss: 0.00000007113568, Improvement: 0.00000002539766, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1916
Epoch 1916, Loss: 0.00000003929443, Improvement: -0.00000003184125, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1917
Epoch 1917, Loss: 0.00000003715186, Improvement: -0.00000000214256, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1918
Epoch 1918, Loss: 0.00000008789592, Improvement: 0.00000005074405, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1919
Epoch 1919, Loss: 0.00000007825800, Improvement: -0.00000000963792, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1920
Epoch 1920, Loss: 0.00000011784104, Improvement: 0.00000003958304, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1921
Epoch 1921, Loss: 0.00000012230980, Improvement: 0.00000000446875, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1922
Epoch 1922, Loss: 0.00000010545044, Improvement: -0.00000001685936, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1923
Epoch 1923, Loss: 0.00000009707426, Improvement: -0.00000000837618, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1924
Epoch 1924, Loss: 0.00000006203145, Improvement: -0.00000003504281, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1925
Epoch 1925, Loss: 0.00000003947726, Improvement: -0.00000002255419, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1926
Epoch 1926, Loss: 0.00000004009985, Improvement: 0.00000000062259, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1927
Epoch 1927, Loss: 0.00000003100184, Improvement: -0.00000000909800, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1928
Epoch 1928, Loss: 0.00000003780601, Improvement: 0.00000000680417, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1929
Epoch 1929, Loss: 0.00000002998260, Improvement: -0.00000000782341, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1930
Epoch 1930, Loss: 0.00000003008394, Improvement: 0.00000000010134, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1931
Epoch 1931, Loss: 0.00000003230720, Improvement: 0.00000000222326, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1932
Epoch 1932, Loss: 0.00000002938797, Improvement: -0.00000000291923, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1933
Epoch 1933, Loss: 0.00000003046782, Improvement: 0.00000000107985, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1934
Epoch 1934, Loss: 0.00000003644820, Improvement: 0.00000000598039, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1935
Epoch 1935, Loss: 0.00000004487854, Improvement: 0.00000000843033, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1936
Epoch 1936, Loss: 0.00000005117408, Improvement: 0.00000000629554, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1937
Epoch 1937, Loss: 0.00000008244158, Improvement: 0.00000003126750, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1938
Epoch 1938, Loss: 0.00000009769646, Improvement: 0.00000001525488, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1939
Epoch 1939, Loss: 0.00000006780465, Improvement: -0.00000002989181, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1940
Epoch 1940, Loss: 0.00000004668613, Improvement: -0.00000002111852, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1941
Epoch 1941, Loss: 0.00000005007118, Improvement: 0.00000000338505, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1942
Epoch 1942, Loss: 0.00000005244444, Improvement: 0.00000000237326, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1943
Epoch 1943, Loss: 0.00000006756077, Improvement: 0.00000001511633, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1944
Epoch 1944, Loss: 0.00000005322649, Improvement: -0.00000001433428, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1945
Epoch 1945, Loss: 0.00000003346071, Improvement: -0.00000001976577, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1946
Epoch 1946, Loss: 0.00000004457413, Improvement: 0.00000001111342, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1947
Epoch 1947, Loss: 0.00000008073435, Improvement: 0.00000003616021, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1948
Epoch 1948, Loss: 0.00000010534925, Improvement: 0.00000002461491, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1949
Epoch 1949, Loss: 0.00000012765156, Improvement: 0.00000002230231, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1950
Model saving checkpoint: the model trained after epoch 1950 has been saved with the training errors.
Epoch 1950, Loss: 0.00000013794188, Improvement: 0.00000001029031, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1951
Epoch 1951, Loss: 0.00000011641486, Improvement: -0.00000002152702, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1952
Epoch 1952, Loss: 0.00000007107761, Improvement: -0.00000004533725, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1953
Epoch 1953, Loss: 0.00000004606654, Improvement: -0.00000002501108, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1954
Epoch 1954, Loss: 0.00000002930377, Improvement: -0.00000001676277, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1955
Epoch 1955, Loss: 0.00000002744814, Improvement: -0.00000000185563, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1956
Epoch 1956, Loss: 0.00000002679715, Improvement: -0.00000000065099, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1957
Epoch 1957, Loss: 0.00000003421153, Improvement: 0.00000000741437, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1958
Epoch 1958, Loss: 0.00000015707371, Improvement: 0.00000012286218, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1959
Epoch 1959, Loss: 0.00000010126091, Improvement: -0.00000005581280, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1960
Epoch 1960, Loss: 0.00000006183942, Improvement: -0.00000003942149, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1961
Epoch 1961, Loss: 0.00000004933887, Improvement: -0.00000001250055, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1962
Epoch 1962, Loss: 0.00000004401488, Improvement: -0.00000000532398, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1963
Epoch 1963, Loss: 0.00000003302532, Improvement: -0.00000001098956, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1964
Epoch 1964, Loss: 0.00000003055592, Improvement: -0.00000000246940, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1965
Epoch 1965, Loss: 0.00000002795075, Improvement: -0.00000000260517, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1966
Epoch 1966, Loss: 0.00000003119432, Improvement: 0.00000000324358, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1967
Epoch 1967, Loss: 0.00000003111758, Improvement: -0.00000000007674, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1968
Epoch 1968, Loss: 0.00000002499331, Improvement: -0.00000000612428, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1969
Epoch 1969, Loss: 0.00000002838437, Improvement: 0.00000000339106, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1970
Epoch 1970, Loss: 0.00000003367703, Improvement: 0.00000000529266, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1971
Epoch 1971, Loss: 0.00000003629533, Improvement: 0.00000000261830, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1972
Epoch 1972, Loss: 0.00000006752430, Improvement: 0.00000003122897, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1973
Epoch 1973, Loss: 0.00000005684788, Improvement: -0.00000001067642, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1974
Epoch 1974, Loss: 0.00000004791006, Improvement: -0.00000000893782, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1975
Epoch 1975, Loss: 0.00000004146325, Improvement: -0.00000000644682, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1976
Epoch 1976, Loss: 0.00000003595258, Improvement: -0.00000000551067, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1977
Epoch 1977, Loss: 0.00000003425081, Improvement: -0.00000000170177, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1978
Epoch 1978, Loss: 0.00000007211876, Improvement: 0.00000003786794, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1979
Epoch 1979, Loss: 0.00000006075376, Improvement: -0.00000001136499, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1980
Epoch 1980, Loss: 0.00000003823935, Improvement: -0.00000002251441, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1981
Epoch 1981, Loss: 0.00000003116968, Improvement: -0.00000000706967, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1982
Epoch 1982, Loss: 0.00000003486324, Improvement: 0.00000000369356, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1983
Epoch 1983, Loss: 0.00000002569204, Improvement: -0.00000000917120, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1984
Epoch 1984, Loss: 0.00000002826335, Improvement: 0.00000000257131, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1985
Epoch 1985, Loss: 0.00000003104268, Improvement: 0.00000000277933, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1986
Epoch 1986, Loss: 0.00000003662809, Improvement: 0.00000000558541, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1987
Epoch 1987, Loss: 0.00000004469379, Improvement: 0.00000000806570, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1988
Epoch 1988, Loss: 0.00000004177631, Improvement: -0.00000000291748, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1989
Epoch 1989, Loss: 0.00000011460521, Improvement: 0.00000007282890, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1990
Epoch 1990, Loss: 0.00000007035396, Improvement: -0.00000004425125, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1991
Epoch 1991, Loss: 0.00000003172826, Improvement: -0.00000003862570, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1992
Epoch 1992, Loss: 0.00000002617177, Improvement: -0.00000000555648, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1993
Epoch 1993, Loss: 0.00000002313003, Improvement: -0.00000000304175, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1994
Epoch 1994, Loss: 0.00000003631366, Improvement: 0.00000001318364, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1995
Epoch 1995, Loss: 0.00000003330438, Improvement: -0.00000000300928, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1996
Epoch 1996, Loss: 0.00000002641932, Improvement: -0.00000000688506, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1997
Epoch 1997, Loss: 0.00000002223381, Improvement: -0.00000000418551, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1998
Epoch 1998, Loss: 0.00000002604015, Improvement: 0.00000000380634, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 1999
Epoch 1999, Loss: 0.00000003183501, Improvement: 0.00000000579486, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2000
Model saving checkpoint: the model trained after epoch 2000 has been saved with the training errors.
Epoch 2000, Loss: 0.00000003431560, Improvement: 0.00000000248059, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2001
Epoch 2001, Loss: 0.00000014700918, Improvement: 0.00000011269358, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2002
Epoch 2002, Loss: 0.00000010438028, Improvement: -0.00000004262889, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2003
Epoch 2003, Loss: 0.00000005105050, Improvement: -0.00000005332978, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2004
Epoch 2004, Loss: 0.00000003150458, Improvement: -0.00000001954593, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2005
Epoch 2005, Loss: 0.00000002726591, Improvement: -0.00000000423867, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2006
Epoch 2006, Loss: 0.00000003244494, Improvement: 0.00000000517903, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2007
Epoch 2007, Loss: 0.00000002951876, Improvement: -0.00000000292618, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2008
Epoch 2008, Loss: 0.00000002403185, Improvement: -0.00000000548691, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2009
Epoch 2009, Loss: 0.00000002331031, Improvement: -0.00000000072154, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2010
Epoch 2010, Loss: 0.00000002557215, Improvement: 0.00000000226185, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2011
Epoch 2011, Loss: 0.00000002571046, Improvement: 0.00000000013831, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2012
Epoch 2012, Loss: 0.00000003205755, Improvement: 0.00000000634709, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2013
Epoch 2013, Loss: 0.00000003191131, Improvement: -0.00000000014624, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2014
Epoch 2014, Loss: 0.00000003255666, Improvement: 0.00000000064535, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2015
Epoch 2015, Loss: 0.00000003707572, Improvement: 0.00000000451906, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2016
Epoch 2016, Loss: 0.00000002793409, Improvement: -0.00000000914163, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2017
Epoch 2017, Loss: 0.00000002330392, Improvement: -0.00000000463017, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2018
Epoch 2018, Loss: 0.00000002927970, Improvement: 0.00000000597578, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2019
Epoch 2019, Loss: 0.00000003509820, Improvement: 0.00000000581850, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2020
Epoch 2020, Loss: 0.00000007296214, Improvement: 0.00000003786394, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2021
Epoch 2021, Loss: 0.00000011293540, Improvement: 0.00000003997326, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2022
Epoch 2022, Loss: 0.00000005445079, Improvement: -0.00000005848461, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2023
Epoch 2023, Loss: 0.00000004622967, Improvement: -0.00000000822112, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2024
Epoch 2024, Loss: 0.00000006632401, Improvement: 0.00000002009434, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2025
Epoch 2025, Loss: 0.00000006831737, Improvement: 0.00000000199336, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2026
Epoch 2026, Loss: 0.00000004309058, Improvement: -0.00000002522679, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2027
Epoch 2027, Loss: 0.00000005466714, Improvement: 0.00000001157656, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2028
Epoch 2028, Loss: 0.00000005906409, Improvement: 0.00000000439695, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2029
Epoch 2029, Loss: 0.00000006832551, Improvement: 0.00000000926142, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2030
Epoch 2030, Loss: 0.00000008942552, Improvement: 0.00000002110001, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2031
Epoch 2031, Loss: 0.00000010015249, Improvement: 0.00000001072697, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2032
Epoch 2032, Loss: 0.00000008432474, Improvement: -0.00000001582775, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2033
Epoch 2033, Loss: 0.00000007798956, Improvement: -0.00000000633518, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2034
Epoch 2034, Loss: 0.00000005917409, Improvement: -0.00000001881547, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2035
Epoch 2035, Loss: 0.00000003191057, Improvement: -0.00000002726352, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2036
Epoch 2036, Loss: 0.00000003393046, Improvement: 0.00000000201989, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2037
Epoch 2037, Loss: 0.00000005759149, Improvement: 0.00000002366103, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2038
Epoch 2038, Loss: 0.00000003377707, Improvement: -0.00000002381441, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2039
Epoch 2039, Loss: 0.00000003802703, Improvement: 0.00000000424996, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2040
Epoch 2040, Loss: 0.00000004762329, Improvement: 0.00000000959625, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2041
Epoch 2041, Loss: 0.00000002852543, Improvement: -0.00000001909786, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2042
Epoch 2042, Loss: 0.00000004556084, Improvement: 0.00000001703541, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2043
Epoch 2043, Loss: 0.00000005769656, Improvement: 0.00000001213572, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2044
Epoch 2044, Loss: 0.00000003418731, Improvement: -0.00000002350925, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2045
Epoch 2045, Loss: 0.00000003267523, Improvement: -0.00000000151208, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2046
Epoch 2046, Loss: 0.00000003388041, Improvement: 0.00000000120518, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2047
Epoch 2047, Loss: 0.00000003842978, Improvement: 0.00000000454937, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2048
Epoch 2048, Loss: 0.00000004683546, Improvement: 0.00000000840568, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2049
Epoch 2049, Loss: 0.00000003798114, Improvement: -0.00000000885432, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2050
Model saving checkpoint: the model trained after epoch 2050 has been saved with the training errors.
Epoch 2050, Loss: 0.00000004048893, Improvement: 0.00000000250780, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2051
Epoch 2051, Loss: 0.00000005366735, Improvement: 0.00000001317841, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2052
Epoch 2052, Loss: 0.00000006331704, Improvement: 0.00000000964969, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2053
Epoch 2053, Loss: 0.00000004475041, Improvement: -0.00000001856663, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2054
Epoch 2054, Loss: 0.00000003080852, Improvement: -0.00000001394189, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2055
Epoch 2055, Loss: 0.00000004249638, Improvement: 0.00000001168787, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2056
Epoch 2056, Loss: 0.00000008558359, Improvement: 0.00000004308720, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2057
Epoch 2057, Loss: 0.00000007356723, Improvement: -0.00000001201636, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2058
Epoch 2058, Loss: 0.00000006321819, Improvement: -0.00000001034904, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2059
Epoch 2059, Loss: 0.00000005408640, Improvement: -0.00000000913179, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2060
Epoch 2060, Loss: 0.00000007129139, Improvement: 0.00000001720499, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2061
Epoch 2061, Loss: 0.00000003678757, Improvement: -0.00000003450382, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2062
Epoch 2062, Loss: 0.00000004158996, Improvement: 0.00000000480239, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2063
Epoch 2063, Loss: 0.00000007731241, Improvement: 0.00000003572246, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2064
Epoch 2064, Loss: 0.00000006248213, Improvement: -0.00000001483029, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2065
Epoch 2065, Loss: 0.00000005146907, Improvement: -0.00000001101306, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2066
Epoch 2066, Loss: 0.00000005711136, Improvement: 0.00000000564229, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2067
Epoch 2067, Loss: 0.00000005733390, Improvement: 0.00000000022253, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2068
Epoch 2068, Loss: 0.00000004214788, Improvement: -0.00000001518602, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2069
Epoch 2069, Loss: 0.00000003441324, Improvement: -0.00000000773463, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2070
Epoch 2070, Loss: 0.00000003566809, Improvement: 0.00000000125484, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2071
Epoch 2071, Loss: 0.00000005208189, Improvement: 0.00000001641380, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2072
Epoch 2072, Loss: 0.00000005981489, Improvement: 0.00000000773300, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2073
Epoch 2073, Loss: 0.00000005838350, Improvement: -0.00000000143139, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2074
Epoch 2074, Loss: 0.00000004870223, Improvement: -0.00000000968127, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2075
Epoch 2075, Loss: 0.00000005247662, Improvement: 0.00000000377439, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2076
Epoch 2076, Loss: 0.00000005115137, Improvement: -0.00000000132525, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2077
Epoch 2077, Loss: 0.00000005547122, Improvement: 0.00000000431985, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2078
Epoch 2078, Loss: 0.00000009628185, Improvement: 0.00000004081063, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2079
Epoch 2079, Loss: 0.00000006544498, Improvement: -0.00000003083687, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2080
Epoch 2080, Loss: 0.00000005939157, Improvement: -0.00000000605341, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2081
Epoch 2081, Loss: 0.00000005129674, Improvement: -0.00000000809483, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2082
Epoch 2082, Loss: 0.00000007799358, Improvement: 0.00000002669684, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2083
Epoch 2083, Loss: 0.00000010058445, Improvement: 0.00000002259087, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2084
Epoch 2084, Loss: 0.00000005560297, Improvement: -0.00000004498148, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2085
Epoch 2085, Loss: 0.00000004736449, Improvement: -0.00000000823848, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2086
Epoch 2086, Loss: 0.00000006308763, Improvement: 0.00000001572314, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2087
Epoch 2087, Loss: 0.00000010228860, Improvement: 0.00000003920097, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2088
Epoch 2088, Loss: 0.00000008726018, Improvement: -0.00000001502842, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2089
Epoch 2089, Loss: 0.00000005285399, Improvement: -0.00000003440619, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2090
Epoch 2090, Loss: 0.00000004585346, Improvement: -0.00000000700053, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2091
Epoch 2091, Loss: 0.00000004036318, Improvement: -0.00000000549027, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2092
Epoch 2092, Loss: 0.00000003490176, Improvement: -0.00000000546142, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2093
Epoch 2093, Loss: 0.00000003137474, Improvement: -0.00000000352702, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2094
Epoch 2094, Loss: 0.00000002821889, Improvement: -0.00000000315585, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2095
Epoch 2095, Loss: 0.00000002435879, Improvement: -0.00000000386010, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2096
Epoch 2096, Loss: 0.00000002803368, Improvement: 0.00000000367489, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2097
Epoch 2097, Loss: 0.00000003833797, Improvement: 0.00000001030429, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2098
Epoch 2098, Loss: 0.00000005228060, Improvement: 0.00000001394263, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2099
Epoch 2099, Loss: 0.00000008725209, Improvement: 0.00000003497149, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2100
Model saving checkpoint: the model trained after epoch 2100 has been saved with the training errors.
Epoch 2100, Loss: 0.00000007884149, Improvement: -0.00000000841060, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2101
Epoch 2101, Loss: 0.00000004573005, Improvement: -0.00000003311144, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2102
Epoch 2102, Loss: 0.00000003796325, Improvement: -0.00000000776679, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2103
Epoch 2103, Loss: 0.00000004641035, Improvement: 0.00000000844710, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2104
Epoch 2104, Loss: 0.00000005610173, Improvement: 0.00000000969138, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2105
Epoch 2105, Loss: 0.00000003957702, Improvement: -0.00000001652470, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2106
Epoch 2106, Loss: 0.00000004075469, Improvement: 0.00000000117767, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2107
Epoch 2107, Loss: 0.00000004522114, Improvement: 0.00000000446645, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2108
Epoch 2108, Loss: 0.00000006094868, Improvement: 0.00000001572753, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2109
Epoch 2109, Loss: 0.00000003482312, Improvement: -0.00000002612555, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2110
Epoch 2110, Loss: 0.00000002986536, Improvement: -0.00000000495776, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2111
Epoch 2111, Loss: 0.00000004282493, Improvement: 0.00000001295957, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2112
Epoch 2112, Loss: 0.00000005254167, Improvement: 0.00000000971674, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2113
Epoch 2113, Loss: 0.00000005456267, Improvement: 0.00000000202100, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2114
Epoch 2114, Loss: 0.00000009234824, Improvement: 0.00000003778557, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2115
Epoch 2115, Loss: 0.00000006828875, Improvement: -0.00000002405949, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2116
Epoch 2116, Loss: 0.00000005074864, Improvement: -0.00000001754011, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2117
Epoch 2117, Loss: 0.00000004228824, Improvement: -0.00000000846040, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2118
Epoch 2118, Loss: 0.00000004636006, Improvement: 0.00000000407182, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2119
Epoch 2119, Loss: 0.00000003439715, Improvement: -0.00000001196291, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2120
Epoch 2120, Loss: 0.00000002444566, Improvement: -0.00000000995149, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2121
Epoch 2121, Loss: 0.00000003803249, Improvement: 0.00000001358683, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2122
Epoch 2122, Loss: 0.00000005108063, Improvement: 0.00000001304814, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2123
Epoch 2123, Loss: 0.00000004058325, Improvement: -0.00000001049738, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2124
Epoch 2124, Loss: 0.00000003881064, Improvement: -0.00000000177261, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2125
Epoch 2125, Loss: 0.00000003398045, Improvement: -0.00000000483020, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2126
Epoch 2126, Loss: 0.00000004143679, Improvement: 0.00000000745635, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2127
Epoch 2127, Loss: 0.00000003302041, Improvement: -0.00000000841638, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2128
Epoch 2128, Loss: 0.00000005058722, Improvement: 0.00000001756681, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2129
Epoch 2129, Loss: 0.00000014735896, Improvement: 0.00000009677174, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2130
Epoch 2130, Loss: 0.00000009901591, Improvement: -0.00000004834305, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2131
Epoch 2131, Loss: 0.00000005764008, Improvement: -0.00000004137583, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2132
Epoch 2132, Loss: 0.00000002755153, Improvement: -0.00000003008855, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2133
Epoch 2133, Loss: 0.00000002884704, Improvement: 0.00000000129550, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2134
Epoch 2134, Loss: 0.00000003418361, Improvement: 0.00000000533658, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2135
Epoch 2135, Loss: 0.00000003108738, Improvement: -0.00000000309623, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2136
Epoch 2136, Loss: 0.00000002670609, Improvement: -0.00000000438129, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2137
Epoch 2137, Loss: 0.00000002370230, Improvement: -0.00000000300380, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2138
Epoch 2138, Loss: 0.00000002399141, Improvement: 0.00000000028911, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2139
Epoch 2139, Loss: 0.00000002186845, Improvement: -0.00000000212295, Best Loss: 0.00000001299362 in Epoch 1878
Epoch 2140
A best model at epoch 2140 has been saved with training error 0.00000001297335.
Epoch 2140, Loss: 0.00000002136263, Improvement: -0.00000000050582, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2141
Epoch 2141, Loss: 0.00000003276751, Improvement: 0.00000001140488, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2142
Epoch 2142, Loss: 0.00000003394892, Improvement: 0.00000000118141, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2143
Epoch 2143, Loss: 0.00000003041657, Improvement: -0.00000000353235, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2144
Epoch 2144, Loss: 0.00000002894569, Improvement: -0.00000000147088, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2145
Epoch 2145, Loss: 0.00000002463690, Improvement: -0.00000000430879, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2146
Epoch 2146, Loss: 0.00000003240039, Improvement: 0.00000000776349, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2147
Epoch 2147, Loss: 0.00000002851452, Improvement: -0.00000000388586, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2148
Epoch 2148, Loss: 0.00000003367213, Improvement: 0.00000000515761, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2149
Epoch 2149, Loss: 0.00000003818206, Improvement: 0.00000000450992, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2150
Model saving checkpoint: the model trained after epoch 2150 has been saved with the training errors.
Epoch 2150, Loss: 0.00000010844356, Improvement: 0.00000007026150, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2151
Epoch 2151, Loss: 0.00000006958847, Improvement: -0.00000003885509, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2152
Epoch 2152, Loss: 0.00000009602246, Improvement: 0.00000002643399, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2153
Epoch 2153, Loss: 0.00000010760528, Improvement: 0.00000001158281, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2154
Epoch 2154, Loss: 0.00000009226346, Improvement: -0.00000001534182, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2155
Epoch 2155, Loss: 0.00000005275331, Improvement: -0.00000003951015, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2156
Epoch 2156, Loss: 0.00000004144245, Improvement: -0.00000001131086, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2157
Epoch 2157, Loss: 0.00000003646421, Improvement: -0.00000000497824, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2158
Epoch 2158, Loss: 0.00000002547244, Improvement: -0.00000001099177, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2159
Epoch 2159, Loss: 0.00000002028870, Improvement: -0.00000000518374, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2160
Epoch 2160, Loss: 0.00000002009863, Improvement: -0.00000000019007, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2161
Epoch 2161, Loss: 0.00000002919983, Improvement: 0.00000000910119, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2162
Epoch 2162, Loss: 0.00000003209870, Improvement: 0.00000000289888, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2163
Epoch 2163, Loss: 0.00000003401556, Improvement: 0.00000000191686, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2164
Epoch 2164, Loss: 0.00000004473876, Improvement: 0.00000001072319, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2165
Epoch 2165, Loss: 0.00000004752633, Improvement: 0.00000000278757, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2166
Epoch 2166, Loss: 0.00000005968789, Improvement: 0.00000001216156, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2167
Epoch 2167, Loss: 0.00000007408096, Improvement: 0.00000001439307, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2168
Epoch 2168, Loss: 0.00000009803321, Improvement: 0.00000002395226, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2169
Epoch 2169, Loss: 0.00000007458102, Improvement: -0.00000002345219, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2170
Epoch 2170, Loss: 0.00000004938302, Improvement: -0.00000002519800, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2171
Epoch 2171, Loss: 0.00000002995578, Improvement: -0.00000001942723, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2172
Epoch 2172, Loss: 0.00000003013953, Improvement: 0.00000000018375, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2173
Epoch 2173, Loss: 0.00000005298673, Improvement: 0.00000002284720, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2174
Epoch 2174, Loss: 0.00000004763705, Improvement: -0.00000000534968, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2175
Epoch 2175, Loss: 0.00000004066050, Improvement: -0.00000000697655, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2176
Epoch 2176, Loss: 0.00000006517236, Improvement: 0.00000002451186, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2177
Epoch 2177, Loss: 0.00000005753575, Improvement: -0.00000000763661, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2178
Epoch 2178, Loss: 0.00000004171849, Improvement: -0.00000001581727, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2179
Epoch 2179, Loss: 0.00000003165998, Improvement: -0.00000001005851, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2180
Epoch 2180, Loss: 0.00000005323290, Improvement: 0.00000002157292, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2181
Epoch 2181, Loss: 0.00000006633333, Improvement: 0.00000001310043, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2182
Epoch 2182, Loss: 0.00000006490725, Improvement: -0.00000000142607, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2183
Epoch 2183, Loss: 0.00000005533185, Improvement: -0.00000000957541, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2184
Epoch 2184, Loss: 0.00000006163273, Improvement: 0.00000000630088, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2185
Epoch 2185, Loss: 0.00000006842769, Improvement: 0.00000000679496, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2186
Epoch 2186, Loss: 0.00000007147442, Improvement: 0.00000000304674, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2187
Epoch 2187, Loss: 0.00000006222120, Improvement: -0.00000000925322, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2188
Epoch 2188, Loss: 0.00000004215377, Improvement: -0.00000002006743, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2189
Epoch 2189, Loss: 0.00000003637677, Improvement: -0.00000000577700, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2190
Epoch 2190, Loss: 0.00000002918668, Improvement: -0.00000000719010, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2191
Epoch 2191, Loss: 0.00000003428163, Improvement: 0.00000000509495, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2192
Epoch 2192, Loss: 0.00000003569164, Improvement: 0.00000000141002, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2193
Epoch 2193, Loss: 0.00000003390289, Improvement: -0.00000000178876, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2194
Epoch 2194, Loss: 0.00000005239942, Improvement: 0.00000001849653, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2195
Epoch 2195, Loss: 0.00000005056685, Improvement: -0.00000000183257, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2196
Epoch 2196, Loss: 0.00000006998176, Improvement: 0.00000001941492, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2197
Epoch 2197, Loss: 0.00000005799583, Improvement: -0.00000001198594, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2198
Epoch 2198, Loss: 0.00000003424328, Improvement: -0.00000002375255, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2199
Epoch 2199, Loss: 0.00000003234884, Improvement: -0.00000000189444, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2200
Model saving checkpoint: the model trained after epoch 2200 has been saved with the training errors.
Epoch 2200, Loss: 0.00000002941022, Improvement: -0.00000000293862, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2201
Epoch 2201, Loss: 0.00000004123656, Improvement: 0.00000001182634, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2202
Epoch 2202, Loss: 0.00000007016257, Improvement: 0.00000002892600, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2203
Epoch 2203, Loss: 0.00000006602201, Improvement: -0.00000000414055, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2204
Epoch 2204, Loss: 0.00000003813725, Improvement: -0.00000002788477, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2205
Epoch 2205, Loss: 0.00000007290839, Improvement: 0.00000003477114, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2206
Epoch 2206, Loss: 0.00000012725654, Improvement: 0.00000005434815, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2207
Epoch 2207, Loss: 0.00000006219758, Improvement: -0.00000006505896, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2208
Epoch 2208, Loss: 0.00000003236534, Improvement: -0.00000002983225, Best Loss: 0.00000001297335 in Epoch 2140
Epoch 2209
A best model at epoch 2209 has been saved with training error 0.00000001131318.
Epoch 2209, Loss: 0.00000002357419, Improvement: -0.00000000879115, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2210
Epoch 2210, Loss: 0.00000002213285, Improvement: -0.00000000144134, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2211
Epoch 2211, Loss: 0.00000002104769, Improvement: -0.00000000108516, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2212
Epoch 2212, Loss: 0.00000002253013, Improvement: 0.00000000148244, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2213
Epoch 2213, Loss: 0.00000002263714, Improvement: 0.00000000010701, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2214
Epoch 2214, Loss: 0.00000001913929, Improvement: -0.00000000349786, Best Loss: 0.00000001131318 in Epoch 2209
Epoch 2215
A best model at epoch 2215 has been saved with training error 0.00000000993591.
Epoch 2215, Loss: 0.00000001906772, Improvement: -0.00000000007157, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2216
Epoch 2216, Loss: 0.00000001955465, Improvement: 0.00000000048693, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2217
Epoch 2217, Loss: 0.00000002367663, Improvement: 0.00000000412199, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2218
Epoch 2218, Loss: 0.00000002610355, Improvement: 0.00000000242692, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2219
Epoch 2219, Loss: 0.00000003853403, Improvement: 0.00000001243048, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2220
Epoch 2220, Loss: 0.00000004042232, Improvement: 0.00000000188829, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2221
Epoch 2221, Loss: 0.00000005844110, Improvement: 0.00000001801878, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2222
Epoch 2222, Loss: 0.00000004076114, Improvement: -0.00000001767996, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2223
Epoch 2223, Loss: 0.00000004118841, Improvement: 0.00000000042727, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2224
Epoch 2224, Loss: 0.00000006636359, Improvement: 0.00000002517518, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2225
Epoch 2225, Loss: 0.00000006317239, Improvement: -0.00000000319120, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2226
Epoch 2226, Loss: 0.00000006834046, Improvement: 0.00000000516808, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2227
Epoch 2227, Loss: 0.00000007424768, Improvement: 0.00000000590722, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2228
Epoch 2228, Loss: 0.00000009506254, Improvement: 0.00000002081486, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2229
Epoch 2229, Loss: 0.00000004582490, Improvement: -0.00000004923764, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2230
Epoch 2230, Loss: 0.00000002687304, Improvement: -0.00000001895186, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2231
Epoch 2231, Loss: 0.00000002171656, Improvement: -0.00000000515648, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2232
Epoch 2232, Loss: 0.00000002533295, Improvement: 0.00000000361639, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2233
Epoch 2233, Loss: 0.00000002416154, Improvement: -0.00000000117140, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2234
Epoch 2234, Loss: 0.00000003579357, Improvement: 0.00000001163203, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2235
Epoch 2235, Loss: 0.00000003485857, Improvement: -0.00000000093500, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2236
Epoch 2236, Loss: 0.00000002981548, Improvement: -0.00000000504309, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2237
Epoch 2237, Loss: 0.00000002683605, Improvement: -0.00000000297943, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2238
Epoch 2238, Loss: 0.00000002672125, Improvement: -0.00000000011480, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2239
Epoch 2239, Loss: 0.00000003665923, Improvement: 0.00000000993798, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2240
Epoch 2240, Loss: 0.00000003234651, Improvement: -0.00000000431271, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2241
Epoch 2241, Loss: 0.00000004495980, Improvement: 0.00000001261329, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2242
Epoch 2242, Loss: 0.00000005315914, Improvement: 0.00000000819934, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2243
Epoch 2243, Loss: 0.00000003760747, Improvement: -0.00000001555168, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2244
Epoch 2244, Loss: 0.00000006351531, Improvement: 0.00000002590784, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2245
Epoch 2245, Loss: 0.00000009533176, Improvement: 0.00000003181646, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2246
Epoch 2246, Loss: 0.00000006482002, Improvement: -0.00000003051174, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2247
Epoch 2247, Loss: 0.00000004530966, Improvement: -0.00000001951036, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2248
Epoch 2248, Loss: 0.00000004207732, Improvement: -0.00000000323234, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2249
Epoch 2249, Loss: 0.00000003759060, Improvement: -0.00000000448673, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2250
Model saving checkpoint: the model trained after epoch 2250 has been saved with the training errors.
Epoch 2250, Loss: 0.00000003929980, Improvement: 0.00000000170921, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2251
Epoch 2251, Loss: 0.00000003172769, Improvement: -0.00000000757211, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2252
Epoch 2252, Loss: 0.00000003392896, Improvement: 0.00000000220127, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2253
Epoch 2253, Loss: 0.00000003115520, Improvement: -0.00000000277375, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2254
Epoch 2254, Loss: 0.00000003547677, Improvement: 0.00000000432157, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2255
Epoch 2255, Loss: 0.00000004180573, Improvement: 0.00000000632896, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2256
Epoch 2256, Loss: 0.00000005568083, Improvement: 0.00000001387510, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2257
Epoch 2257, Loss: 0.00000007032801, Improvement: 0.00000001464717, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2258
Epoch 2258, Loss: 0.00000007941404, Improvement: 0.00000000908604, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2259
Epoch 2259, Loss: 0.00000008974481, Improvement: 0.00000001033077, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2260
Epoch 2260, Loss: 0.00000012679777, Improvement: 0.00000003705296, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2261
Epoch 2261, Loss: 0.00000008320728, Improvement: -0.00000004359048, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2262
Epoch 2262, Loss: 0.00000004005589, Improvement: -0.00000004315139, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2263
Epoch 2263, Loss: 0.00000002759575, Improvement: -0.00000001246014, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2264
Epoch 2264, Loss: 0.00000002782454, Improvement: 0.00000000022879, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2265
Epoch 2265, Loss: 0.00000002092710, Improvement: -0.00000000689745, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2266
Epoch 2266, Loss: 0.00000002072205, Improvement: -0.00000000020505, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2267
Epoch 2267, Loss: 0.00000002145795, Improvement: 0.00000000073590, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2268
Epoch 2268, Loss: 0.00000002000751, Improvement: -0.00000000145045, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2269
Epoch 2269, Loss: 0.00000002066871, Improvement: 0.00000000066120, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2270
Epoch 2270, Loss: 0.00000002013988, Improvement: -0.00000000052883, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2271
Epoch 2271, Loss: 0.00000003121140, Improvement: 0.00000001107153, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2272
Epoch 2272, Loss: 0.00000002996403, Improvement: -0.00000000124737, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2273
Epoch 2273, Loss: 0.00000002727325, Improvement: -0.00000000269078, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2274
Epoch 2274, Loss: 0.00000002335372, Improvement: -0.00000000391953, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2275
Epoch 2275, Loss: 0.00000002826274, Improvement: 0.00000000490902, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2276
Epoch 2276, Loss: 0.00000002947423, Improvement: 0.00000000121148, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2277
Epoch 2277, Loss: 0.00000004902737, Improvement: 0.00000001955315, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2278
Epoch 2278, Loss: 0.00000005079991, Improvement: 0.00000000177254, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2279
Epoch 2279, Loss: 0.00000004743910, Improvement: -0.00000000336081, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2280
Epoch 2280, Loss: 0.00000004054675, Improvement: -0.00000000689235, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2281
Epoch 2281, Loss: 0.00000005280002, Improvement: 0.00000001225326, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2282
Epoch 2282, Loss: 0.00000005425487, Improvement: 0.00000000145486, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2283
Epoch 2283, Loss: 0.00000007093498, Improvement: 0.00000001668011, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2284
Epoch 2284, Loss: 0.00000004000458, Improvement: -0.00000003093040, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2285
Epoch 2285, Loss: 0.00000004997711, Improvement: 0.00000000997253, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2286
Epoch 2286, Loss: 0.00000004196472, Improvement: -0.00000000801239, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2287
Epoch 2287, Loss: 0.00000002783642, Improvement: -0.00000001412830, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2288
Epoch 2288, Loss: 0.00000002267206, Improvement: -0.00000000516436, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2289
Epoch 2289, Loss: 0.00000001810399, Improvement: -0.00000000456807, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2290
Epoch 2290, Loss: 0.00000001857352, Improvement: 0.00000000046954, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2291
Epoch 2291, Loss: 0.00000002700901, Improvement: 0.00000000843549, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2292
Epoch 2292, Loss: 0.00000005659989, Improvement: 0.00000002959088, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2293
Epoch 2293, Loss: 0.00000009248242, Improvement: 0.00000003588253, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2294
Epoch 2294, Loss: 0.00000007246810, Improvement: -0.00000002001432, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2295
Epoch 2295, Loss: 0.00000006468810, Improvement: -0.00000000778000, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2296
Epoch 2296, Loss: 0.00000003828854, Improvement: -0.00000002639956, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2297
Epoch 2297, Loss: 0.00000002948831, Improvement: -0.00000000880022, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2298
Epoch 2298, Loss: 0.00000002260025, Improvement: -0.00000000688807, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2299
Epoch 2299, Loss: 0.00000002611245, Improvement: 0.00000000351220, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2300
Model saving checkpoint: the model trained after epoch 2300 has been saved with the training errors.
Epoch 2300, Loss: 0.00000002259429, Improvement: -0.00000000351815, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2301
Epoch 2301, Loss: 0.00000002215587, Improvement: -0.00000000043842, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2302
Epoch 2302, Loss: 0.00000003667278, Improvement: 0.00000001451691, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2303
Epoch 2303, Loss: 0.00000005734607, Improvement: 0.00000002067329, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2304
Epoch 2304, Loss: 0.00000006104417, Improvement: 0.00000000369810, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2305
Epoch 2305, Loss: 0.00000004622421, Improvement: -0.00000001481996, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2306
Epoch 2306, Loss: 0.00000004073779, Improvement: -0.00000000548643, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2307
Epoch 2307, Loss: 0.00000002663077, Improvement: -0.00000001410702, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2308
Epoch 2308, Loss: 0.00000004068325, Improvement: 0.00000001405248, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2309
Epoch 2309, Loss: 0.00000002991094, Improvement: -0.00000001077231, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2310
Epoch 2310, Loss: 0.00000003542696, Improvement: 0.00000000551602, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2311
Epoch 2311, Loss: 0.00000003489625, Improvement: -0.00000000053071, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2312
Epoch 2312, Loss: 0.00000008078928, Improvement: 0.00000004589303, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2313
Epoch 2313, Loss: 0.00000007077391, Improvement: -0.00000001001537, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2314
Epoch 2314, Loss: 0.00000003656672, Improvement: -0.00000003420719, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2315
Epoch 2315, Loss: 0.00000003852509, Improvement: 0.00000000195837, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2316
Epoch 2316, Loss: 0.00000003556948, Improvement: -0.00000000295561, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2317
Epoch 2317, Loss: 0.00000002449714, Improvement: -0.00000001107234, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2318
Epoch 2318, Loss: 0.00000002003145, Improvement: -0.00000000446569, Best Loss: 0.00000000993591 in Epoch 2215
Epoch 2319
A best model at epoch 2319 has been saved with training error 0.00000000979805.
Epoch 2319, Loss: 0.00000001844068, Improvement: -0.00000000159076, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2320
Epoch 2320, Loss: 0.00000001893363, Improvement: 0.00000000049295, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2321
Epoch 2321, Loss: 0.00000002341350, Improvement: 0.00000000447987, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2322
Epoch 2322, Loss: 0.00000002158602, Improvement: -0.00000000182749, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2323
Epoch 2323, Loss: 0.00000002429736, Improvement: 0.00000000271134, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2324
Epoch 2324, Loss: 0.00000002994734, Improvement: 0.00000000564998, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2325
Epoch 2325, Loss: 0.00000008710199, Improvement: 0.00000005715465, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2326
Epoch 2326, Loss: 0.00000004488731, Improvement: -0.00000004221468, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2327
Epoch 2327, Loss: 0.00000002994594, Improvement: -0.00000001494138, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2328
Epoch 2328, Loss: 0.00000002150179, Improvement: -0.00000000844415, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2329
Epoch 2329, Loss: 0.00000002098982, Improvement: -0.00000000051196, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2330
Epoch 2330, Loss: 0.00000002422330, Improvement: 0.00000000323347, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2331
Epoch 2331, Loss: 0.00000002572788, Improvement: 0.00000000150458, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2332
Epoch 2332, Loss: 0.00000002167343, Improvement: -0.00000000405445, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2333
Epoch 2333, Loss: 0.00000002426487, Improvement: 0.00000000259144, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2334
Epoch 2334, Loss: 0.00000003603978, Improvement: 0.00000001177490, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2335
Epoch 2335, Loss: 0.00000005758336, Improvement: 0.00000002154358, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2336
Epoch 2336, Loss: 0.00000003694708, Improvement: -0.00000002063627, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2337
Epoch 2337, Loss: 0.00000003689712, Improvement: -0.00000000004996, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2338
Epoch 2338, Loss: 0.00000006888401, Improvement: 0.00000003198689, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2339
Epoch 2339, Loss: 0.00000005627799, Improvement: -0.00000001260602, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2340
Epoch 2340, Loss: 0.00000005119255, Improvement: -0.00000000508544, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2341
Epoch 2341, Loss: 0.00000004046839, Improvement: -0.00000001072417, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2342
Epoch 2342, Loss: 0.00000009113236, Improvement: 0.00000005066398, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2343
Epoch 2343, Loss: 0.00000011262547, Improvement: 0.00000002149310, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2344
Epoch 2344, Loss: 0.00000007059041, Improvement: -0.00000004203506, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2345
Epoch 2345, Loss: 0.00000004700482, Improvement: -0.00000002358559, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2346
Epoch 2346, Loss: 0.00000003682665, Improvement: -0.00000001017817, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2347
Epoch 2347, Loss: 0.00000002889359, Improvement: -0.00000000793306, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2348
Epoch 2348, Loss: 0.00000001991707, Improvement: -0.00000000897652, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2349
Epoch 2349, Loss: 0.00000002036409, Improvement: 0.00000000044702, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2350
Model saving checkpoint: the model trained after epoch 2350 has been saved with the training errors.
Epoch 2350, Loss: 0.00000002287474, Improvement: 0.00000000251065, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2351
Epoch 2351, Loss: 0.00000003314424, Improvement: 0.00000001026950, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2352
Epoch 2352, Loss: 0.00000003046952, Improvement: -0.00000000267472, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2353
Epoch 2353, Loss: 0.00000002952261, Improvement: -0.00000000094691, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2354
Epoch 2354, Loss: 0.00000002953840, Improvement: 0.00000000001579, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2355
Epoch 2355, Loss: 0.00000001972539, Improvement: -0.00000000981301, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2356
Epoch 2356, Loss: 0.00000002154193, Improvement: 0.00000000181653, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2357
Epoch 2357, Loss: 0.00000002133286, Improvement: -0.00000000020907, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2358
Epoch 2358, Loss: 0.00000002082712, Improvement: -0.00000000050574, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2359
Epoch 2359, Loss: 0.00000002689330, Improvement: 0.00000000606618, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2360
Epoch 2360, Loss: 0.00000003126582, Improvement: 0.00000000437252, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2361
Epoch 2361, Loss: 0.00000003934393, Improvement: 0.00000000807811, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2362
Epoch 2362, Loss: 0.00000003143694, Improvement: -0.00000000790699, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2363
Epoch 2363, Loss: 0.00000003135169, Improvement: -0.00000000008526, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2364
Epoch 2364, Loss: 0.00000004172038, Improvement: 0.00000001036869, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2365
Epoch 2365, Loss: 0.00000006035720, Improvement: 0.00000001863682, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2366
Epoch 2366, Loss: 0.00000005195518, Improvement: -0.00000000840202, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2367
Epoch 2367, Loss: 0.00000004048814, Improvement: -0.00000001146703, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2368
Epoch 2368, Loss: 0.00000005347673, Improvement: 0.00000001298859, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2369
Epoch 2369, Loss: 0.00000008094706, Improvement: 0.00000002747033, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2370
Epoch 2370, Loss: 0.00000007582387, Improvement: -0.00000000512319, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2371
Epoch 2371, Loss: 0.00000004782124, Improvement: -0.00000002800263, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2372
Epoch 2372, Loss: 0.00000003614541, Improvement: -0.00000001167583, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2373
Epoch 2373, Loss: 0.00000003933986, Improvement: 0.00000000319445, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2374
Epoch 2374, Loss: 0.00000003942540, Improvement: 0.00000000008554, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2375
Epoch 2375, Loss: 0.00000003574550, Improvement: -0.00000000367990, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2376
Epoch 2376, Loss: 0.00000004693584, Improvement: 0.00000001119034, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2377
Epoch 2377, Loss: 0.00000003192914, Improvement: -0.00000001500671, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2378
Epoch 2378, Loss: 0.00000002230998, Improvement: -0.00000000961915, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2379
Epoch 2379, Loss: 0.00000002409122, Improvement: 0.00000000178124, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2380
Epoch 2380, Loss: 0.00000002584448, Improvement: 0.00000000175326, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2381
Epoch 2381, Loss: 0.00000002590453, Improvement: 0.00000000006005, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2382
Epoch 2382, Loss: 0.00000003398772, Improvement: 0.00000000808319, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2383
Epoch 2383, Loss: 0.00000003503568, Improvement: 0.00000000104796, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2384
Epoch 2384, Loss: 0.00000006342416, Improvement: 0.00000002838848, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2385
Epoch 2385, Loss: 0.00000006498593, Improvement: 0.00000000156178, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2386
Epoch 2386, Loss: 0.00000005020885, Improvement: -0.00000001477709, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2387
Epoch 2387, Loss: 0.00000002733410, Improvement: -0.00000002287475, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2388
Epoch 2388, Loss: 0.00000002520278, Improvement: -0.00000000213132, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2389
Epoch 2389, Loss: 0.00000002836385, Improvement: 0.00000000316107, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2390
Epoch 2390, Loss: 0.00000003691328, Improvement: 0.00000000854943, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2391
Epoch 2391, Loss: 0.00000003258994, Improvement: -0.00000000432334, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2392
Epoch 2392, Loss: 0.00000004739586, Improvement: 0.00000001480592, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2393
Epoch 2393, Loss: 0.00000005255235, Improvement: 0.00000000515649, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2394
Epoch 2394, Loss: 0.00000003364791, Improvement: -0.00000001890444, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2395
Epoch 2395, Loss: 0.00000006490365, Improvement: 0.00000003125574, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2396
Epoch 2396, Loss: 0.00000010209358, Improvement: 0.00000003718993, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2397
Epoch 2397, Loss: 0.00000003922406, Improvement: -0.00000006286952, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2398
Epoch 2398, Loss: 0.00000002856187, Improvement: -0.00000001066219, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2399
Epoch 2399, Loss: 0.00000002209638, Improvement: -0.00000000646549, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2400
Model saving checkpoint: the model trained after epoch 2400 has been saved with the training errors.
Epoch 2400, Loss: 0.00000002429210, Improvement: 0.00000000219571, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2401
Epoch 2401, Loss: 0.00000004530660, Improvement: 0.00000002101450, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2402
Epoch 2402, Loss: 0.00000005000083, Improvement: 0.00000000469423, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2403
Epoch 2403, Loss: 0.00000003777673, Improvement: -0.00000001222410, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2404
Epoch 2404, Loss: 0.00000005512273, Improvement: 0.00000001734600, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2405
Epoch 2405, Loss: 0.00000005796172, Improvement: 0.00000000283898, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2406
Epoch 2406, Loss: 0.00000003342264, Improvement: -0.00000002453908, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2407
Epoch 2407, Loss: 0.00000003604215, Improvement: 0.00000000261951, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2408
Epoch 2408, Loss: 0.00000003564305, Improvement: -0.00000000039909, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2409
Epoch 2409, Loss: 0.00000008288933, Improvement: 0.00000004724627, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2410
Epoch 2410, Loss: 0.00000010278253, Improvement: 0.00000001989321, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2411
Epoch 2411, Loss: 0.00000008874263, Improvement: -0.00000001403991, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2412
Epoch 2412, Loss: 0.00000003542940, Improvement: -0.00000005331322, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2413
Epoch 2413, Loss: 0.00000002223616, Improvement: -0.00000001319324, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2414
Epoch 2414, Loss: 0.00000001809244, Improvement: -0.00000000414373, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2415
Epoch 2415, Loss: 0.00000001991127, Improvement: 0.00000000181884, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2416
Epoch 2416, Loss: 0.00000001812608, Improvement: -0.00000000178520, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2417
Epoch 2417, Loss: 0.00000002344663, Improvement: 0.00000000532055, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2418
Epoch 2418, Loss: 0.00000002985574, Improvement: 0.00000000640911, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2419
Epoch 2419, Loss: 0.00000003756112, Improvement: 0.00000000770539, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2420
Epoch 2420, Loss: 0.00000004757249, Improvement: 0.00000001001137, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2421
Epoch 2421, Loss: 0.00000005919505, Improvement: 0.00000001162255, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2422
Epoch 2422, Loss: 0.00000003838680, Improvement: -0.00000002080824, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2423
Epoch 2423, Loss: 0.00000002993923, Improvement: -0.00000000844757, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2424
Epoch 2424, Loss: 0.00000002317362, Improvement: -0.00000000676561, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2425
Epoch 2425, Loss: 0.00000002178716, Improvement: -0.00000000138647, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2426
Epoch 2426, Loss: 0.00000002159059, Improvement: -0.00000000019656, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2427
Epoch 2427, Loss: 0.00000001933517, Improvement: -0.00000000225543, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2428
Epoch 2428, Loss: 0.00000002561593, Improvement: 0.00000000628077, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2429
Epoch 2429, Loss: 0.00000003140158, Improvement: 0.00000000578564, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2430
Epoch 2430, Loss: 0.00000005341337, Improvement: 0.00000002201179, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2431
Epoch 2431, Loss: 0.00000003571568, Improvement: -0.00000001769769, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2432
Epoch 2432, Loss: 0.00000003263529, Improvement: -0.00000000308038, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2433
Epoch 2433, Loss: 0.00000003883602, Improvement: 0.00000000620072, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2434
Epoch 2434, Loss: 0.00000014393575, Improvement: 0.00000010509973, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2435
Epoch 2435, Loss: 0.00000008794825, Improvement: -0.00000005598750, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2436
Epoch 2436, Loss: 0.00000004822032, Improvement: -0.00000003972792, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2437
Epoch 2437, Loss: 0.00000004075558, Improvement: -0.00000000746474, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2438
Epoch 2438, Loss: 0.00000003518008, Improvement: -0.00000000557551, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2439
Epoch 2439, Loss: 0.00000003491144, Improvement: -0.00000000026864, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2440
Epoch 2440, Loss: 0.00000002496044, Improvement: -0.00000000995100, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2441
Epoch 2441, Loss: 0.00000002249840, Improvement: -0.00000000246204, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2442
Epoch 2442, Loss: 0.00000002059482, Improvement: -0.00000000190358, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2443
Epoch 2443, Loss: 0.00000002115265, Improvement: 0.00000000055783, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2444
Epoch 2444, Loss: 0.00000001933139, Improvement: -0.00000000182126, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2445
Epoch 2445, Loss: 0.00000003627276, Improvement: 0.00000001694137, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2446
Epoch 2446, Loss: 0.00000003804237, Improvement: 0.00000000176962, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2447
Epoch 2447, Loss: 0.00000004109515, Improvement: 0.00000000305277, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2448
Epoch 2448, Loss: 0.00000002355810, Improvement: -0.00000001753705, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2449
Epoch 2449, Loss: 0.00000002331624, Improvement: -0.00000000024185, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2450
Model saving checkpoint: the model trained after epoch 2450 has been saved with the training errors.
Epoch 2450, Loss: 0.00000001987824, Improvement: -0.00000000343800, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2451
Epoch 2451, Loss: 0.00000002195520, Improvement: 0.00000000207695, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2452
Epoch 2452, Loss: 0.00000003407068, Improvement: 0.00000001211548, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2453
Epoch 2453, Loss: 0.00000003296403, Improvement: -0.00000000110664, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2454
Epoch 2454, Loss: 0.00000002870231, Improvement: -0.00000000426173, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2455
Epoch 2455, Loss: 0.00000003448591, Improvement: 0.00000000578360, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2456
Epoch 2456, Loss: 0.00000003136342, Improvement: -0.00000000312249, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2457
Epoch 2457, Loss: 0.00000003693295, Improvement: 0.00000000556953, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2458
Epoch 2458, Loss: 0.00000006167209, Improvement: 0.00000002473914, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2459
Epoch 2459, Loss: 0.00000004667660, Improvement: -0.00000001499549, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2460
Epoch 2460, Loss: 0.00000002841821, Improvement: -0.00000001825838, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2461
Epoch 2461, Loss: 0.00000003359679, Improvement: 0.00000000517858, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2462
Epoch 2462, Loss: 0.00000005404578, Improvement: 0.00000002044899, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2463
Epoch 2463, Loss: 0.00000003981827, Improvement: -0.00000001422751, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2464
Epoch 2464, Loss: 0.00000005612702, Improvement: 0.00000001630875, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2465
Epoch 2465, Loss: 0.00000005968483, Improvement: 0.00000000355781, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2466
Epoch 2466, Loss: 0.00000006171739, Improvement: 0.00000000203256, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2467
Epoch 2467, Loss: 0.00000005922339, Improvement: -0.00000000249399, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2468
Epoch 2468, Loss: 0.00000004098678, Improvement: -0.00000001823661, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2469
Epoch 2469, Loss: 0.00000002975147, Improvement: -0.00000001123531, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2470
Epoch 2470, Loss: 0.00000002850989, Improvement: -0.00000000124158, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2471
Epoch 2471, Loss: 0.00000002781236, Improvement: -0.00000000069753, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2472
Epoch 2472, Loss: 0.00000004301786, Improvement: 0.00000001520549, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2473
Epoch 2473, Loss: 0.00000006750447, Improvement: 0.00000002448661, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2474
Epoch 2474, Loss: 0.00000009490753, Improvement: 0.00000002740306, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2475
Epoch 2475, Loss: 0.00000013290341, Improvement: 0.00000003799588, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2476
Epoch 2476, Loss: 0.00000007582282, Improvement: -0.00000005708059, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2477
Epoch 2477, Loss: 0.00000003126024, Improvement: -0.00000004456258, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2478
Epoch 2478, Loss: 0.00000002924020, Improvement: -0.00000000202004, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2479
Epoch 2479, Loss: 0.00000003016281, Improvement: 0.00000000092261, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2480
Epoch 2480, Loss: 0.00000002506742, Improvement: -0.00000000509539, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2481
Epoch 2481, Loss: 0.00000001967397, Improvement: -0.00000000539345, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2482
Epoch 2482, Loss: 0.00000001845036, Improvement: -0.00000000122361, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2483
Epoch 2483, Loss: 0.00000001698168, Improvement: -0.00000000146868, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2484
Epoch 2484, Loss: 0.00000001639676, Improvement: -0.00000000058492, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2485
Epoch 2485, Loss: 0.00000001848582, Improvement: 0.00000000208907, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2486
Epoch 2486, Loss: 0.00000001761251, Improvement: -0.00000000087331, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2487
Epoch 2487, Loss: 0.00000001913106, Improvement: 0.00000000151855, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2488
Epoch 2488, Loss: 0.00000001772737, Improvement: -0.00000000140369, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2489
Epoch 2489, Loss: 0.00000002004088, Improvement: 0.00000000231351, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2490
Epoch 2490, Loss: 0.00000002073756, Improvement: 0.00000000069669, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2491
Epoch 2491, Loss: 0.00000002057144, Improvement: -0.00000000016612, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2492
Epoch 2492, Loss: 0.00000003199024, Improvement: 0.00000001141880, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2493
Epoch 2493, Loss: 0.00000002906692, Improvement: -0.00000000292332, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2494
Epoch 2494, Loss: 0.00000002157978, Improvement: -0.00000000748714, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2495
Epoch 2495, Loss: 0.00000002118169, Improvement: -0.00000000039809, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2496
Epoch 2496, Loss: 0.00000002510823, Improvement: 0.00000000392654, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2497
Epoch 2497, Loss: 0.00000002825077, Improvement: 0.00000000314254, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2498
Epoch 2498, Loss: 0.00000003266056, Improvement: 0.00000000440978, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2499
Epoch 2499, Loss: 0.00000005145981, Improvement: 0.00000001879925, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2500
Model saving checkpoint: the model trained after epoch 2500 has been saved with the training errors.
Epoch 2500, Loss: 0.00000008667712, Improvement: 0.00000003521731, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2501
Epoch 2501, Loss: 0.00000008287533, Improvement: -0.00000000380178, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2502
Epoch 2502, Loss: 0.00000005079296, Improvement: -0.00000003208237, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2503
Epoch 2503, Loss: 0.00000002458327, Improvement: -0.00000002620969, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2504
Epoch 2504, Loss: 0.00000001812910, Improvement: -0.00000000645417, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2505
Epoch 2505, Loss: 0.00000001617814, Improvement: -0.00000000195096, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2506
Epoch 2506, Loss: 0.00000001690740, Improvement: 0.00000000072926, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2507
Epoch 2507, Loss: 0.00000001772270, Improvement: 0.00000000081530, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2508
Epoch 2508, Loss: 0.00000001839839, Improvement: 0.00000000067570, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2509
Epoch 2509, Loss: 0.00000001691880, Improvement: -0.00000000147959, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2510
Epoch 2510, Loss: 0.00000001954484, Improvement: 0.00000000262604, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2511
Epoch 2511, Loss: 0.00000002493850, Improvement: 0.00000000539365, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2512
Epoch 2512, Loss: 0.00000001964495, Improvement: -0.00000000529355, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2513
Epoch 2513, Loss: 0.00000001624154, Improvement: -0.00000000340341, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2514
Epoch 2514, Loss: 0.00000001744431, Improvement: 0.00000000120277, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2515
Epoch 2515, Loss: 0.00000001956047, Improvement: 0.00000000211615, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2516
Epoch 2516, Loss: 0.00000003600612, Improvement: 0.00000001644566, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2517
Epoch 2517, Loss: 0.00000007701539, Improvement: 0.00000004100927, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2518
Epoch 2518, Loss: 0.00000009951442, Improvement: 0.00000002249903, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2519
Epoch 2519, Loss: 0.00000005732773, Improvement: -0.00000004218669, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2520
Epoch 2520, Loss: 0.00000003754369, Improvement: -0.00000001978404, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2521
Epoch 2521, Loss: 0.00000003068633, Improvement: -0.00000000685736, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2522
Epoch 2522, Loss: 0.00000001634586, Improvement: -0.00000001434047, Best Loss: 0.00000000979805 in Epoch 2319
Epoch 2523
A best model at epoch 2523 has been saved with training error 0.00000000909830.
Epoch 2523, Loss: 0.00000001526505, Improvement: -0.00000000108081, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2524
Epoch 2524, Loss: 0.00000001566336, Improvement: 0.00000000039831, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2525
Epoch 2525, Loss: 0.00000001540484, Improvement: -0.00000000025851, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2526
Epoch 2526, Loss: 0.00000001458093, Improvement: -0.00000000082391, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2527
Epoch 2527, Loss: 0.00000001432984, Improvement: -0.00000000025109, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2528
Epoch 2528, Loss: 0.00000001541668, Improvement: 0.00000000108684, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2529
Epoch 2529, Loss: 0.00000001895953, Improvement: 0.00000000354285, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2530
Epoch 2530, Loss: 0.00000001837264, Improvement: -0.00000000058688, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2531
Epoch 2531, Loss: 0.00000002734908, Improvement: 0.00000000897644, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2532
Epoch 2532, Loss: 0.00000002683343, Improvement: -0.00000000051565, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2533
Epoch 2533, Loss: 0.00000003130363, Improvement: 0.00000000447020, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2534
Epoch 2534, Loss: 0.00000002327767, Improvement: -0.00000000802596, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2535
Epoch 2535, Loss: 0.00000001651135, Improvement: -0.00000000676632, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2536
Epoch 2536, Loss: 0.00000001531053, Improvement: -0.00000000120082, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2537
Epoch 2537, Loss: 0.00000001891627, Improvement: 0.00000000360574, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2538
Epoch 2538, Loss: 0.00000001910212, Improvement: 0.00000000018585, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2539
Epoch 2539, Loss: 0.00000002552919, Improvement: 0.00000000642707, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2540
Epoch 2540, Loss: 0.00000002113779, Improvement: -0.00000000439140, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2541
Epoch 2541, Loss: 0.00000002387660, Improvement: 0.00000000273881, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2542
Epoch 2542, Loss: 0.00000002979629, Improvement: 0.00000000591969, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2543
Epoch 2543, Loss: 0.00000005040050, Improvement: 0.00000002060420, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2544
Epoch 2544, Loss: 0.00000009358359, Improvement: 0.00000004318309, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2545
Epoch 2545, Loss: 0.00000005956520, Improvement: -0.00000003401839, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2546
Epoch 2546, Loss: 0.00000002366559, Improvement: -0.00000003589960, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2547
Epoch 2547, Loss: 0.00000002304343, Improvement: -0.00000000062216, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2548
Epoch 2548, Loss: 0.00000002990264, Improvement: 0.00000000685921, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2549
Epoch 2549, Loss: 0.00000001945390, Improvement: -0.00000001044874, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2550
Model saving checkpoint: the model trained after epoch 2550 has been saved with the training errors.
Epoch 2550, Loss: 0.00000002073016, Improvement: 0.00000000127625, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2551
Epoch 2551, Loss: 0.00000002440958, Improvement: 0.00000000367942, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2552
Epoch 2552, Loss: 0.00000002757738, Improvement: 0.00000000316780, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2553
Epoch 2553, Loss: 0.00000004914974, Improvement: 0.00000002157236, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2554
Epoch 2554, Loss: 0.00000004489172, Improvement: -0.00000000425802, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2555
Epoch 2555, Loss: 0.00000002877005, Improvement: -0.00000001612167, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2556
Epoch 2556, Loss: 0.00000002328191, Improvement: -0.00000000548814, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2557
Epoch 2557, Loss: 0.00000002844882, Improvement: 0.00000000516691, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2558
Epoch 2558, Loss: 0.00000003438399, Improvement: 0.00000000593517, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2559
Epoch 2559, Loss: 0.00000003471551, Improvement: 0.00000000033152, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2560
Epoch 2560, Loss: 0.00000002232301, Improvement: -0.00000001239250, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2561
Epoch 2561, Loss: 0.00000001710239, Improvement: -0.00000000522062, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2562
Epoch 2562, Loss: 0.00000002150108, Improvement: 0.00000000439870, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2563
Epoch 2563, Loss: 0.00000002337268, Improvement: 0.00000000187160, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2564
Epoch 2564, Loss: 0.00000002200175, Improvement: -0.00000000137093, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2565
Epoch 2565, Loss: 0.00000003828977, Improvement: 0.00000001628802, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2566
Epoch 2566, Loss: 0.00000004135309, Improvement: 0.00000000306332, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2567
Epoch 2567, Loss: 0.00000004630407, Improvement: 0.00000000495099, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2568
Epoch 2568, Loss: 0.00000003261832, Improvement: -0.00000001368575, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2569
Epoch 2569, Loss: 0.00000002256884, Improvement: -0.00000001004948, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2570
Epoch 2570, Loss: 0.00000001776298, Improvement: -0.00000000480586, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2571
Epoch 2571, Loss: 0.00000001900164, Improvement: 0.00000000123866, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2572
Epoch 2572, Loss: 0.00000002343668, Improvement: 0.00000000443504, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2573
Epoch 2573, Loss: 0.00000003734371, Improvement: 0.00000001390704, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2574
Epoch 2574, Loss: 0.00000008057057, Improvement: 0.00000004322686, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2575
Epoch 2575, Loss: 0.00000005109825, Improvement: -0.00000002947232, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2576
Epoch 2576, Loss: 0.00000004417543, Improvement: -0.00000000692282, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2577
Epoch 2577, Loss: 0.00000004081952, Improvement: -0.00000000335590, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2578
Epoch 2578, Loss: 0.00000003680725, Improvement: -0.00000000401228, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2579
Epoch 2579, Loss: 0.00000003355446, Improvement: -0.00000000325279, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2580
Epoch 2580, Loss: 0.00000002939412, Improvement: -0.00000000416034, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2581
Epoch 2581, Loss: 0.00000002559051, Improvement: -0.00000000380361, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2582
Epoch 2582, Loss: 0.00000004156459, Improvement: 0.00000001597408, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2583
Epoch 2583, Loss: 0.00000005305099, Improvement: 0.00000001148640, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2584
Epoch 2584, Loss: 0.00000009664574, Improvement: 0.00000004359475, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2585
Epoch 2585, Loss: 0.00000004737139, Improvement: -0.00000004927436, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2586
Epoch 2586, Loss: 0.00000002458865, Improvement: -0.00000002278274, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2587
Epoch 2587, Loss: 0.00000003173708, Improvement: 0.00000000714843, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2588
Epoch 2588, Loss: 0.00000004041542, Improvement: 0.00000000867834, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2589
Epoch 2589, Loss: 0.00000005555616, Improvement: 0.00000001514073, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2590
Epoch 2590, Loss: 0.00000005410376, Improvement: -0.00000000145239, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2591
Epoch 2591, Loss: 0.00000004056950, Improvement: -0.00000001353427, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2592
Epoch 2592, Loss: 0.00000002771005, Improvement: -0.00000001285945, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2593
Epoch 2593, Loss: 0.00000002612208, Improvement: -0.00000000158796, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2594
Epoch 2594, Loss: 0.00000002570044, Improvement: -0.00000000042164, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2595
Epoch 2595, Loss: 0.00000002276092, Improvement: -0.00000000293952, Best Loss: 0.00000000909830 in Epoch 2523
Epoch 2596
A best model at epoch 2596 has been saved with training error 0.00000000770543.
Epoch 2596, Loss: 0.00000001931115, Improvement: -0.00000000344977, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2597
Epoch 2597, Loss: 0.00000002730948, Improvement: 0.00000000799833, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2598
Epoch 2598, Loss: 0.00000002049824, Improvement: -0.00000000681125, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2599
Epoch 2599, Loss: 0.00000002112766, Improvement: 0.00000000062943, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2600
Model saving checkpoint: the model trained after epoch 2600 has been saved with the training errors.
Epoch 2600, Loss: 0.00000002420616, Improvement: 0.00000000307850, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2601
Epoch 2601, Loss: 0.00000007553652, Improvement: 0.00000005133036, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2602
Epoch 2602, Loss: 0.00000005975484, Improvement: -0.00000001578169, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2603
Epoch 2603, Loss: 0.00000003245212, Improvement: -0.00000002730272, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2604
Epoch 2604, Loss: 0.00000002534596, Improvement: -0.00000000710616, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2605
Epoch 2605, Loss: 0.00000001771170, Improvement: -0.00000000763425, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2606
Epoch 2606, Loss: 0.00000001795004, Improvement: 0.00000000023833, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2607
Epoch 2607, Loss: 0.00000001843093, Improvement: 0.00000000048089, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2608
Epoch 2608, Loss: 0.00000002816218, Improvement: 0.00000000973125, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2609
Epoch 2609, Loss: 0.00000006070964, Improvement: 0.00000003254746, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2610
Epoch 2610, Loss: 0.00000004898464, Improvement: -0.00000001172500, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2611
Epoch 2611, Loss: 0.00000002876368, Improvement: -0.00000002022097, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2612
Epoch 2612, Loss: 0.00000002277711, Improvement: -0.00000000598657, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2613
Epoch 2613, Loss: 0.00000002169857, Improvement: -0.00000000107854, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2614
Epoch 2614, Loss: 0.00000001778697, Improvement: -0.00000000391160, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2615
Epoch 2615, Loss: 0.00000001583301, Improvement: -0.00000000195396, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2616
Epoch 2616, Loss: 0.00000001795037, Improvement: 0.00000000211736, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2617
Epoch 2617, Loss: 0.00000002172923, Improvement: 0.00000000377886, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2618
Epoch 2618, Loss: 0.00000002933161, Improvement: 0.00000000760239, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2619
Epoch 2619, Loss: 0.00000003505836, Improvement: 0.00000000572675, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2620
Epoch 2620, Loss: 0.00000009660535, Improvement: 0.00000006154699, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2621
Epoch 2621, Loss: 0.00000019805770, Improvement: 0.00000010145236, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2622
Epoch 2622, Loss: 0.00000016498079, Improvement: -0.00000003307691, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2623
Epoch 2623, Loss: 0.00000004747759, Improvement: -0.00000011750319, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2624
Epoch 2624, Loss: 0.00000002142198, Improvement: -0.00000002605561, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2625
Epoch 2625, Loss: 0.00000001858095, Improvement: -0.00000000284103, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2626
Epoch 2626, Loss: 0.00000001571458, Improvement: -0.00000000286637, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2627
Epoch 2627, Loss: 0.00000001397654, Improvement: -0.00000000173804, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2628
Epoch 2628, Loss: 0.00000001269453, Improvement: -0.00000000128201, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2629
Epoch 2629, Loss: 0.00000001219532, Improvement: -0.00000000049921, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2630
Epoch 2630, Loss: 0.00000001308705, Improvement: 0.00000000089173, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2631
Epoch 2631, Loss: 0.00000001239423, Improvement: -0.00000000069283, Best Loss: 0.00000000770543 in Epoch 2596
Epoch 2632
A best model at epoch 2632 has been saved with training error 0.00000000704325.
Epoch 2632, Loss: 0.00000001224033, Improvement: -0.00000000015390, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2633
Epoch 2633, Loss: 0.00000001302445, Improvement: 0.00000000078412, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2634
Epoch 2634, Loss: 0.00000001350512, Improvement: 0.00000000048067, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2635
Epoch 2635, Loss: 0.00000001452968, Improvement: 0.00000000102457, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2636
Epoch 2636, Loss: 0.00000001436672, Improvement: -0.00000000016296, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2637
Epoch 2637, Loss: 0.00000001433405, Improvement: -0.00000000003267, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2638
Epoch 2638, Loss: 0.00000001396015, Improvement: -0.00000000037390, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2639
Epoch 2639, Loss: 0.00000001588249, Improvement: 0.00000000192233, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2640
Epoch 2640, Loss: 0.00000001338615, Improvement: -0.00000000249634, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2641
Epoch 2641, Loss: 0.00000001311030, Improvement: -0.00000000027585, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2642
Epoch 2642, Loss: 0.00000001285638, Improvement: -0.00000000025393, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2643
Epoch 2643, Loss: 0.00000001421558, Improvement: 0.00000000135920, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2644
Epoch 2644, Loss: 0.00000001242579, Improvement: -0.00000000178979, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2645
Epoch 2645, Loss: 0.00000001251787, Improvement: 0.00000000009208, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2646
Epoch 2646, Loss: 0.00000001484386, Improvement: 0.00000000232598, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2647
Epoch 2647, Loss: 0.00000001369747, Improvement: -0.00000000114638, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2648
Epoch 2648, Loss: 0.00000001343277, Improvement: -0.00000000026470, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2649
Epoch 2649, Loss: 0.00000001620697, Improvement: 0.00000000277420, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2650
Model saving checkpoint: the model trained after epoch 2650 has been saved with the training errors.
Epoch 2650, Loss: 0.00000001784512, Improvement: 0.00000000163816, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2651
Epoch 2651, Loss: 0.00000001722623, Improvement: -0.00000000061889, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2652
Epoch 2652, Loss: 0.00000001668655, Improvement: -0.00000000053968, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2653
Epoch 2653, Loss: 0.00000001374537, Improvement: -0.00000000294118, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2654
Epoch 2654, Loss: 0.00000001289019, Improvement: -0.00000000085518, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2655
Epoch 2655, Loss: 0.00000001393384, Improvement: 0.00000000104366, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2656
Epoch 2656, Loss: 0.00000001621709, Improvement: 0.00000000228324, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2657
Epoch 2657, Loss: 0.00000001835612, Improvement: 0.00000000213903, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2658
Epoch 2658, Loss: 0.00000002042817, Improvement: 0.00000000207205, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2659
Epoch 2659, Loss: 0.00000001843545, Improvement: -0.00000000199271, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2660
Epoch 2660, Loss: 0.00000002316548, Improvement: 0.00000000473002, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2661
Epoch 2661, Loss: 0.00000002819971, Improvement: 0.00000000503424, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2662
Epoch 2662, Loss: 0.00000003654834, Improvement: 0.00000000834862, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2663
Epoch 2663, Loss: 0.00000004729735, Improvement: 0.00000001074901, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2664
Epoch 2664, Loss: 0.00000002371069, Improvement: -0.00000002358666, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2665
Epoch 2665, Loss: 0.00000002411162, Improvement: 0.00000000040093, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2666
Epoch 2666, Loss: 0.00000002225226, Improvement: -0.00000000185936, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2667
Epoch 2667, Loss: 0.00000003642454, Improvement: 0.00000001417228, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2668
Epoch 2668, Loss: 0.00000003403560, Improvement: -0.00000000238894, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2669
Epoch 2669, Loss: 0.00000002378769, Improvement: -0.00000001024792, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2670
Epoch 2670, Loss: 0.00000002501738, Improvement: 0.00000000122970, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2671
Epoch 2671, Loss: 0.00000003342218, Improvement: 0.00000000840480, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2672
Epoch 2672, Loss: 0.00000002224789, Improvement: -0.00000001117429, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2673
Epoch 2673, Loss: 0.00000002681141, Improvement: 0.00000000456352, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2674
Epoch 2674, Loss: 0.00000002871321, Improvement: 0.00000000190180, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2675
Epoch 2675, Loss: 0.00000002232428, Improvement: -0.00000000638892, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2676
Epoch 2676, Loss: 0.00000001740079, Improvement: -0.00000000492349, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2677
Epoch 2677, Loss: 0.00000002002969, Improvement: 0.00000000262890, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2678
Epoch 2678, Loss: 0.00000003124440, Improvement: 0.00000001121472, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2679
Epoch 2679, Loss: 0.00000006596121, Improvement: 0.00000003471680, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2680
Epoch 2680, Loss: 0.00000006532270, Improvement: -0.00000000063850, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2681
Epoch 2681, Loss: 0.00000005694344, Improvement: -0.00000000837926, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2682
Epoch 2682, Loss: 0.00000003424413, Improvement: -0.00000002269931, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2683
Epoch 2683, Loss: 0.00000001786648, Improvement: -0.00000001637765, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2684
Epoch 2684, Loss: 0.00000002418308, Improvement: 0.00000000631660, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2685
Epoch 2685, Loss: 0.00000003073659, Improvement: 0.00000000655351, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2686
Epoch 2686, Loss: 0.00000002266218, Improvement: -0.00000000807441, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2687
Epoch 2687, Loss: 0.00000002342844, Improvement: 0.00000000076627, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2688
Epoch 2688, Loss: 0.00000002024595, Improvement: -0.00000000318250, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2689
Epoch 2689, Loss: 0.00000004627823, Improvement: 0.00000002603228, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2690
Epoch 2690, Loss: 0.00000003989032, Improvement: -0.00000000638790, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2691
Epoch 2691, Loss: 0.00000002639262, Improvement: -0.00000001349770, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2692
Epoch 2692, Loss: 0.00000001740184, Improvement: -0.00000000899078, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2693
Epoch 2693, Loss: 0.00000001437451, Improvement: -0.00000000302733, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2694
Epoch 2694, Loss: 0.00000001519814, Improvement: 0.00000000082363, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2695
Epoch 2695, Loss: 0.00000001793200, Improvement: 0.00000000273386, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2696
Epoch 2696, Loss: 0.00000001452681, Improvement: -0.00000000340520, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2697
Epoch 2697, Loss: 0.00000001680201, Improvement: 0.00000000227520, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2698
Epoch 2698, Loss: 0.00000002377099, Improvement: 0.00000000696898, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2699
Epoch 2699, Loss: 0.00000002580138, Improvement: 0.00000000203039, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2700
Model saving checkpoint: the model trained after epoch 2700 has been saved with the training errors.
Epoch 2700, Loss: 0.00000002932422, Improvement: 0.00000000352284, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2701
Epoch 2701, Loss: 0.00000002759046, Improvement: -0.00000000173376, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2702
Epoch 2702, Loss: 0.00000002802550, Improvement: 0.00000000043504, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2703
Epoch 2703, Loss: 0.00000001975786, Improvement: -0.00000000826764, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2704
Epoch 2704, Loss: 0.00000002326269, Improvement: 0.00000000350484, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2705
Epoch 2705, Loss: 0.00000003019768, Improvement: 0.00000000693499, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2706
Epoch 2706, Loss: 0.00000003098049, Improvement: 0.00000000078280, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2707
Epoch 2707, Loss: 0.00000004055744, Improvement: 0.00000000957695, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2708
Epoch 2708, Loss: 0.00000005737079, Improvement: 0.00000001681335, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2709
Epoch 2709, Loss: 0.00000005415130, Improvement: -0.00000000321949, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2710
Epoch 2710, Loss: 0.00000006150075, Improvement: 0.00000000734945, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2711
Epoch 2711, Loss: 0.00000003858839, Improvement: -0.00000002291236, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2712
Epoch 2712, Loss: 0.00000004089027, Improvement: 0.00000000230188, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2713
Epoch 2713, Loss: 0.00000004080955, Improvement: -0.00000000008072, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2714
Epoch 2714, Loss: 0.00000005804020, Improvement: 0.00000001723065, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2715
Epoch 2715, Loss: 0.00000003605199, Improvement: -0.00000002198821, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2716
Epoch 2716, Loss: 0.00000002057250, Improvement: -0.00000001547949, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2717
Epoch 2717, Loss: 0.00000002061543, Improvement: 0.00000000004293, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2718
Epoch 2718, Loss: 0.00000002283825, Improvement: 0.00000000222281, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2719
Epoch 2719, Loss: 0.00000002739941, Improvement: 0.00000000456116, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2720
Epoch 2720, Loss: 0.00000002865536, Improvement: 0.00000000125595, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2721
Epoch 2721, Loss: 0.00000003968581, Improvement: 0.00000001103046, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2722
Epoch 2722, Loss: 0.00000006943033, Improvement: 0.00000002974452, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2723
Epoch 2723, Loss: 0.00000006790150, Improvement: -0.00000000152883, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2724
Epoch 2724, Loss: 0.00000003939604, Improvement: -0.00000002850546, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2725
Epoch 2725, Loss: 0.00000006999988, Improvement: 0.00000003060384, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2726
Epoch 2726, Loss: 0.00000004768845, Improvement: -0.00000002231143, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2727
Epoch 2727, Loss: 0.00000002946577, Improvement: -0.00000001822269, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2728
Epoch 2728, Loss: 0.00000002149491, Improvement: -0.00000000797086, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2729
Epoch 2729, Loss: 0.00000002664463, Improvement: 0.00000000514973, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2730
Epoch 2730, Loss: 0.00000003213007, Improvement: 0.00000000548543, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2731
Epoch 2731, Loss: 0.00000002281811, Improvement: -0.00000000931195, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2732
Epoch 2732, Loss: 0.00000003455841, Improvement: 0.00000001174030, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2733
Epoch 2733, Loss: 0.00000003284840, Improvement: -0.00000000171001, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2734
Epoch 2734, Loss: 0.00000001893774, Improvement: -0.00000001391066, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2735
Epoch 2735, Loss: 0.00000001418904, Improvement: -0.00000000474870, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2736
Epoch 2736, Loss: 0.00000001964839, Improvement: 0.00000000545935, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2737
Epoch 2737, Loss: 0.00000001736713, Improvement: -0.00000000228126, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2738
Epoch 2738, Loss: 0.00000002095207, Improvement: 0.00000000358494, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2739
Epoch 2739, Loss: 0.00000002369027, Improvement: 0.00000000273820, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2740
Epoch 2740, Loss: 0.00000002578393, Improvement: 0.00000000209366, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2741
Epoch 2741, Loss: 0.00000003374737, Improvement: 0.00000000796344, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2742
Epoch 2742, Loss: 0.00000004482158, Improvement: 0.00000001107421, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2743
Epoch 2743, Loss: 0.00000004706150, Improvement: 0.00000000223992, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2744
Epoch 2744, Loss: 0.00000008874948, Improvement: 0.00000004168799, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2745
Epoch 2745, Loss: 0.00000007425015, Improvement: -0.00000001449933, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2746
Epoch 2746, Loss: 0.00000003545013, Improvement: -0.00000003880001, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2747
Epoch 2747, Loss: 0.00000002112316, Improvement: -0.00000001432697, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2748
Epoch 2748, Loss: 0.00000001812786, Improvement: -0.00000000299530, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2749
Epoch 2749, Loss: 0.00000001509105, Improvement: -0.00000000303681, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2750
Model saving checkpoint: the model trained after epoch 2750 has been saved with the training errors.
Epoch 2750, Loss: 0.00000001489469, Improvement: -0.00000000019636, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2751
Epoch 2751, Loss: 0.00000001793862, Improvement: 0.00000000304393, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2752
Epoch 2752, Loss: 0.00000001756448, Improvement: -0.00000000037414, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2753
Epoch 2753, Loss: 0.00000002784548, Improvement: 0.00000001028100, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2754
Epoch 2754, Loss: 0.00000008074434, Improvement: 0.00000005289885, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2755
Epoch 2755, Loss: 0.00000006586628, Improvement: -0.00000001487805, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2756
Epoch 2756, Loss: 0.00000009103540, Improvement: 0.00000002516911, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2757
Epoch 2757, Loss: 0.00000006507370, Improvement: -0.00000002596169, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2758
Epoch 2758, Loss: 0.00000002957513, Improvement: -0.00000003549858, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2759
Epoch 2759, Loss: 0.00000001764991, Improvement: -0.00000001192521, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2760
Epoch 2760, Loss: 0.00000001447506, Improvement: -0.00000000317485, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2761
Epoch 2761, Loss: 0.00000001365374, Improvement: -0.00000000082133, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2762
Epoch 2762, Loss: 0.00000001244705, Improvement: -0.00000000120669, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2763
Epoch 2763, Loss: 0.00000001325167, Improvement: 0.00000000080462, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2764
Epoch 2764, Loss: 0.00000001453423, Improvement: 0.00000000128256, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2765
Epoch 2765, Loss: 0.00000001537861, Improvement: 0.00000000084438, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2766
Epoch 2766, Loss: 0.00000001371202, Improvement: -0.00000000166659, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2767
Epoch 2767, Loss: 0.00000001260585, Improvement: -0.00000000110617, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2768
Epoch 2768, Loss: 0.00000001302785, Improvement: 0.00000000042200, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2769
Epoch 2769, Loss: 0.00000001278670, Improvement: -0.00000000024115, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2770
Epoch 2770, Loss: 0.00000001333548, Improvement: 0.00000000054878, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2771
Epoch 2771, Loss: 0.00000001807920, Improvement: 0.00000000474372, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2772
Epoch 2772, Loss: 0.00000001480312, Improvement: -0.00000000327608, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2773
Epoch 2773, Loss: 0.00000001459505, Improvement: -0.00000000020807, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2774
Epoch 2774, Loss: 0.00000001455095, Improvement: -0.00000000004410, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2775
Epoch 2775, Loss: 0.00000001861071, Improvement: 0.00000000405976, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2776
Epoch 2776, Loss: 0.00000002152490, Improvement: 0.00000000291419, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2777
Epoch 2777, Loss: 0.00000003183205, Improvement: 0.00000001030715, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2778
Epoch 2778, Loss: 0.00000007259601, Improvement: 0.00000004076396, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2779
Epoch 2779, Loss: 0.00000012025017, Improvement: 0.00000004765415, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2780
Epoch 2780, Loss: 0.00000007993164, Improvement: -0.00000004031853, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2781
Epoch 2781, Loss: 0.00000003971258, Improvement: -0.00000004021906, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2782
Epoch 2782, Loss: 0.00000002375939, Improvement: -0.00000001595319, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2783
Epoch 2783, Loss: 0.00000001940048, Improvement: -0.00000000435891, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2784
Epoch 2784, Loss: 0.00000001423562, Improvement: -0.00000000516486, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2785
Epoch 2785, Loss: 0.00000001495446, Improvement: 0.00000000071884, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2786
Epoch 2786, Loss: 0.00000001442284, Improvement: -0.00000000053162, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2787
Epoch 2787, Loss: 0.00000001385714, Improvement: -0.00000000056570, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2788
Epoch 2788, Loss: 0.00000001662898, Improvement: 0.00000000277184, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2789
Epoch 2789, Loss: 0.00000001831883, Improvement: 0.00000000168985, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2790
Epoch 2790, Loss: 0.00000002828636, Improvement: 0.00000000996753, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2791
Epoch 2791, Loss: 0.00000001883979, Improvement: -0.00000000944658, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2792
Epoch 2792, Loss: 0.00000002412528, Improvement: 0.00000000528550, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2793
Epoch 2793, Loss: 0.00000002040917, Improvement: -0.00000000371611, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2794
Epoch 2794, Loss: 0.00000002326112, Improvement: 0.00000000285195, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2795
Epoch 2795, Loss: 0.00000003467255, Improvement: 0.00000001141143, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2796
Epoch 2796, Loss: 0.00000003509278, Improvement: 0.00000000042023, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2797
Epoch 2797, Loss: 0.00000002998442, Improvement: -0.00000000510836, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2798
Epoch 2798, Loss: 0.00000002215927, Improvement: -0.00000000782516, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2799
Epoch 2799, Loss: 0.00000002224792, Improvement: 0.00000000008866, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2800
Model saving checkpoint: the model trained after epoch 2800 has been saved with the training errors.
Epoch 2800, Loss: 0.00000003510528, Improvement: 0.00000001285735, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2801
Epoch 2801, Loss: 0.00000003615327, Improvement: 0.00000000104799, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2802
Epoch 2802, Loss: 0.00000002298877, Improvement: -0.00000001316450, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2803
Epoch 2803, Loss: 0.00000002387340, Improvement: 0.00000000088463, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2804
Epoch 2804, Loss: 0.00000002547483, Improvement: 0.00000000160143, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2805
Epoch 2805, Loss: 0.00000004298952, Improvement: 0.00000001751469, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2806
Epoch 2806, Loss: 0.00000004203248, Improvement: -0.00000000095704, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2807
Epoch 2807, Loss: 0.00000003046454, Improvement: -0.00000001156794, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2808
Epoch 2808, Loss: 0.00000003270288, Improvement: 0.00000000223834, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2809
Epoch 2809, Loss: 0.00000003755356, Improvement: 0.00000000485068, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2810
Epoch 2810, Loss: 0.00000002374172, Improvement: -0.00000001381184, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2811
Epoch 2811, Loss: 0.00000002725923, Improvement: 0.00000000351752, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2812
Epoch 2812, Loss: 0.00000002463079, Improvement: -0.00000000262844, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2813
Epoch 2813, Loss: 0.00000002486782, Improvement: 0.00000000023703, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2814
Epoch 2814, Loss: 0.00000002608053, Improvement: 0.00000000121272, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2815
Epoch 2815, Loss: 0.00000002154546, Improvement: -0.00000000453507, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2816
Epoch 2816, Loss: 0.00000002093419, Improvement: -0.00000000061127, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2817
Epoch 2817, Loss: 0.00000004428122, Improvement: 0.00000002334702, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2818
Epoch 2818, Loss: 0.00000005274881, Improvement: 0.00000000846759, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2819
Epoch 2819, Loss: 0.00000004188981, Improvement: -0.00000001085900, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2820
Epoch 2820, Loss: 0.00000003058706, Improvement: -0.00000001130275, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2821
Epoch 2821, Loss: 0.00000004363024, Improvement: 0.00000001304319, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2822
Epoch 2822, Loss: 0.00000003636563, Improvement: -0.00000000726461, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2823
Epoch 2823, Loss: 0.00000002703151, Improvement: -0.00000000933412, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2824
Epoch 2824, Loss: 0.00000003580855, Improvement: 0.00000000877705, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2825
Epoch 2825, Loss: 0.00000004038715, Improvement: 0.00000000457860, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2826
Epoch 2826, Loss: 0.00000003726624, Improvement: -0.00000000312091, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2827
Epoch 2827, Loss: 0.00000002491837, Improvement: -0.00000001234787, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2828
Epoch 2828, Loss: 0.00000001698108, Improvement: -0.00000000793729, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2829
Epoch 2829, Loss: 0.00000001511396, Improvement: -0.00000000186713, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2830
Epoch 2830, Loss: 0.00000001937262, Improvement: 0.00000000425867, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2831
Epoch 2831, Loss: 0.00000002922088, Improvement: 0.00000000984825, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2832
Epoch 2832, Loss: 0.00000002147097, Improvement: -0.00000000774990, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2833
Epoch 2833, Loss: 0.00000002894975, Improvement: 0.00000000747878, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2834
Epoch 2834, Loss: 0.00000003542424, Improvement: 0.00000000647449, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2835
Epoch 2835, Loss: 0.00000002282019, Improvement: -0.00000001260405, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2836
Epoch 2836, Loss: 0.00000002270820, Improvement: -0.00000000011199, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2837
Epoch 2837, Loss: 0.00000002102166, Improvement: -0.00000000168654, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2838
Epoch 2838, Loss: 0.00000003169079, Improvement: 0.00000001066914, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2839
Epoch 2839, Loss: 0.00000002592807, Improvement: -0.00000000576272, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2840
Epoch 2840, Loss: 0.00000003621577, Improvement: 0.00000001028770, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2841
Epoch 2841, Loss: 0.00000002473153, Improvement: -0.00000001148424, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2842
Epoch 2842, Loss: 0.00000002030865, Improvement: -0.00000000442288, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2843
Epoch 2843, Loss: 0.00000002308362, Improvement: 0.00000000277497, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2844
Epoch 2844, Loss: 0.00000004378358, Improvement: 0.00000002069996, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2845
Epoch 2845, Loss: 0.00000003020749, Improvement: -0.00000001357609, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2846
Epoch 2846, Loss: 0.00000005522024, Improvement: 0.00000002501275, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2847
Epoch 2847, Loss: 0.00000004647392, Improvement: -0.00000000874632, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2848
Epoch 2848, Loss: 0.00000003699948, Improvement: -0.00000000947444, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2849
Epoch 2849, Loss: 0.00000003709147, Improvement: 0.00000000009198, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2850
Model saving checkpoint: the model trained after epoch 2850 has been saved with the training errors.
Epoch 2850, Loss: 0.00000003718555, Improvement: 0.00000000009408, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2851
Epoch 2851, Loss: 0.00000006557467, Improvement: 0.00000002838912, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2852
Epoch 2852, Loss: 0.00000002976159, Improvement: -0.00000003581308, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2853
Epoch 2853, Loss: 0.00000001667470, Improvement: -0.00000001308689, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2854
Epoch 2854, Loss: 0.00000001586803, Improvement: -0.00000000080667, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2855
Epoch 2855, Loss: 0.00000004906909, Improvement: 0.00000003320106, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2856
Epoch 2856, Loss: 0.00000007522443, Improvement: 0.00000002615534, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2857
Epoch 2857, Loss: 0.00000008377737, Improvement: 0.00000000855294, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2858
Epoch 2858, Loss: 0.00000002865890, Improvement: -0.00000005511847, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2859
Epoch 2859, Loss: 0.00000001690339, Improvement: -0.00000001175551, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2860
Epoch 2860, Loss: 0.00000001274009, Improvement: -0.00000000416330, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2861
Epoch 2861, Loss: 0.00000001303960, Improvement: 0.00000000029951, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2862
Epoch 2862, Loss: 0.00000001571774, Improvement: 0.00000000267814, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2863
Epoch 2863, Loss: 0.00000001445181, Improvement: -0.00000000126593, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2864
Epoch 2864, Loss: 0.00000001357215, Improvement: -0.00000000087966, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2865
Epoch 2865, Loss: 0.00000001367798, Improvement: 0.00000000010583, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2866
Epoch 2866, Loss: 0.00000001199055, Improvement: -0.00000000168742, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2867
Epoch 2867, Loss: 0.00000001749262, Improvement: 0.00000000550207, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2868
Epoch 2868, Loss: 0.00000002098822, Improvement: 0.00000000349560, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2869
Epoch 2869, Loss: 0.00000002786403, Improvement: 0.00000000687581, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2870
Epoch 2870, Loss: 0.00000001651895, Improvement: -0.00000001134507, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2871
Epoch 2871, Loss: 0.00000002778233, Improvement: 0.00000001126337, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2872
Epoch 2872, Loss: 0.00000003098063, Improvement: 0.00000000319830, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2873
Epoch 2873, Loss: 0.00000003331306, Improvement: 0.00000000233243, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2874
Epoch 2874, Loss: 0.00000003439388, Improvement: 0.00000000108082, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2875
Epoch 2875, Loss: 0.00000003021723, Improvement: -0.00000000417665, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2876
Epoch 2876, Loss: 0.00000003433024, Improvement: 0.00000000411301, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2877
Epoch 2877, Loss: 0.00000004485434, Improvement: 0.00000001052410, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2878
Epoch 2878, Loss: 0.00000004897793, Improvement: 0.00000000412360, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2879
Epoch 2879, Loss: 0.00000002067714, Improvement: -0.00000002830079, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2880
Epoch 2880, Loss: 0.00000001616283, Improvement: -0.00000000451431, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2881
Epoch 2881, Loss: 0.00000001752418, Improvement: 0.00000000136135, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2882
Epoch 2882, Loss: 0.00000004287252, Improvement: 0.00000002534834, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2883
Epoch 2883, Loss: 0.00000006809310, Improvement: 0.00000002522058, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2884
Epoch 2884, Loss: 0.00000004668615, Improvement: -0.00000002140695, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2885
Epoch 2885, Loss: 0.00000003536533, Improvement: -0.00000001132082, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2886
Epoch 2886, Loss: 0.00000002038801, Improvement: -0.00000001497732, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2887
Epoch 2887, Loss: 0.00000001718108, Improvement: -0.00000000320693, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2888
Epoch 2888, Loss: 0.00000001468002, Improvement: -0.00000000250105, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2889
Epoch 2889, Loss: 0.00000001648632, Improvement: 0.00000000180629, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2890
Epoch 2890, Loss: 0.00000002819515, Improvement: 0.00000001170883, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2891
Epoch 2891, Loss: 0.00000002758628, Improvement: -0.00000000060887, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2892
Epoch 2892, Loss: 0.00000002332220, Improvement: -0.00000000426408, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2893
Epoch 2893, Loss: 0.00000002034685, Improvement: -0.00000000297535, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2894
Epoch 2894, Loss: 0.00000002373881, Improvement: 0.00000000339196, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2895
Epoch 2895, Loss: 0.00000002356793, Improvement: -0.00000000017088, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2896
Epoch 2896, Loss: 0.00000003433700, Improvement: 0.00000001076908, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2897
Epoch 2897, Loss: 0.00000003312726, Improvement: -0.00000000120975, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2898
Epoch 2898, Loss: 0.00000003464822, Improvement: 0.00000000152096, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2899
Epoch 2899, Loss: 0.00000005261177, Improvement: 0.00000001796355, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2900
Model saving checkpoint: the model trained after epoch 2900 has been saved with the training errors.
Epoch 2900, Loss: 0.00000003139076, Improvement: -0.00000002122101, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2901
Epoch 2901, Loss: 0.00000005315032, Improvement: 0.00000002175956, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2902
Epoch 2902, Loss: 0.00000005461671, Improvement: 0.00000000146639, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2903
Epoch 2903, Loss: 0.00000004872457, Improvement: -0.00000000589214, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2904
Epoch 2904, Loss: 0.00000002638584, Improvement: -0.00000002233874, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2905
Epoch 2905, Loss: 0.00000001844086, Improvement: -0.00000000794497, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2906
Epoch 2906, Loss: 0.00000001287140, Improvement: -0.00000000556946, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2907
Epoch 2907, Loss: 0.00000001278969, Improvement: -0.00000000008171, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2908
Epoch 2908, Loss: 0.00000001555726, Improvement: 0.00000000276757, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2909
Epoch 2909, Loss: 0.00000001549858, Improvement: -0.00000000005869, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2910
Epoch 2910, Loss: 0.00000002025980, Improvement: 0.00000000476123, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2911
Epoch 2911, Loss: 0.00000001711145, Improvement: -0.00000000314835, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2912
Epoch 2912, Loss: 0.00000002232695, Improvement: 0.00000000521550, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2913
Epoch 2913, Loss: 0.00000002387855, Improvement: 0.00000000155160, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2914
Epoch 2914, Loss: 0.00000001894692, Improvement: -0.00000000493163, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2915
Epoch 2915, Loss: 0.00000001363853, Improvement: -0.00000000530839, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2916
Epoch 2916, Loss: 0.00000001555466, Improvement: 0.00000000191613, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2917
Epoch 2917, Loss: 0.00000001449470, Improvement: -0.00000000105995, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2918
Epoch 2918, Loss: 0.00000001290842, Improvement: -0.00000000158628, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2919
Epoch 2919, Loss: 0.00000002277963, Improvement: 0.00000000987121, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2920
Epoch 2920, Loss: 0.00000003675432, Improvement: 0.00000001397469, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2921
Epoch 2921, Loss: 0.00000003594722, Improvement: -0.00000000080710, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2922
Epoch 2922, Loss: 0.00000004929702, Improvement: 0.00000001334980, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2923
Epoch 2923, Loss: 0.00000006458894, Improvement: 0.00000001529192, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2924
Epoch 2924, Loss: 0.00000004216954, Improvement: -0.00000002241940, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2925
Epoch 2925, Loss: 0.00000003156953, Improvement: -0.00000001060001, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2926
Epoch 2926, Loss: 0.00000002003300, Improvement: -0.00000001153653, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2927
Epoch 2927, Loss: 0.00000002156212, Improvement: 0.00000000152912, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2928
Epoch 2928, Loss: 0.00000002132279, Improvement: -0.00000000023933, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2929
Epoch 2929, Loss: 0.00000003736504, Improvement: 0.00000001604225, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2930
Epoch 2930, Loss: 0.00000002491736, Improvement: -0.00000001244768, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2931
Epoch 2931, Loss: 0.00000002846059, Improvement: 0.00000000354323, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2932
Epoch 2932, Loss: 0.00000004347446, Improvement: 0.00000001501387, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2933
Epoch 2933, Loss: 0.00000007068284, Improvement: 0.00000002720838, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2934
Epoch 2934, Loss: 0.00000008614934, Improvement: 0.00000001546650, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2935
Epoch 2935, Loss: 0.00000004595425, Improvement: -0.00000004019510, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2936
Epoch 2936, Loss: 0.00000002713443, Improvement: -0.00000001881981, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2937
Epoch 2937, Loss: 0.00000001647331, Improvement: -0.00000001066113, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2938
Epoch 2938, Loss: 0.00000001343093, Improvement: -0.00000000304237, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2939
Epoch 2939, Loss: 0.00000001365103, Improvement: 0.00000000022009, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2940
Epoch 2940, Loss: 0.00000001384133, Improvement: 0.00000000019030, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2941
Epoch 2941, Loss: 0.00000001344411, Improvement: -0.00000000039722, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2942
Epoch 2942, Loss: 0.00000001216615, Improvement: -0.00000000127796, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2943
Epoch 2943, Loss: 0.00000001214351, Improvement: -0.00000000002263, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2944
Epoch 2944, Loss: 0.00000001541864, Improvement: 0.00000000327513, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2945
Epoch 2945, Loss: 0.00000001669619, Improvement: 0.00000000127755, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2946
Epoch 2946, Loss: 0.00000001531299, Improvement: -0.00000000138320, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2947
Epoch 2947, Loss: 0.00000001642080, Improvement: 0.00000000110781, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2948
Epoch 2948, Loss: 0.00000003367797, Improvement: 0.00000001725717, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2949
Epoch 2949, Loss: 0.00000006408763, Improvement: 0.00000003040966, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2950
Model saving checkpoint: the model trained after epoch 2950 has been saved with the training errors.
Epoch 2950, Loss: 0.00000006126612, Improvement: -0.00000000282151, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2951
Epoch 2951, Loss: 0.00000005169727, Improvement: -0.00000000956884, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2952
Epoch 2952, Loss: 0.00000010114769, Improvement: 0.00000004945042, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2953
Epoch 2953, Loss: 0.00000007501235, Improvement: -0.00000002613534, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2954
Epoch 2954, Loss: 0.00000004455875, Improvement: -0.00000003045360, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2955
Epoch 2955, Loss: 0.00000002544379, Improvement: -0.00000001911497, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2956
Epoch 2956, Loss: 0.00000001416913, Improvement: -0.00000001127466, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2957
Epoch 2957, Loss: 0.00000001318515, Improvement: -0.00000000098399, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2958
Epoch 2958, Loss: 0.00000001199578, Improvement: -0.00000000118937, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2959
Epoch 2959, Loss: 0.00000001216123, Improvement: 0.00000000016544, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2960
Epoch 2960, Loss: 0.00000001222867, Improvement: 0.00000000006744, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2961
Epoch 2961, Loss: 0.00000001423273, Improvement: 0.00000000200407, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2962
Epoch 2962, Loss: 0.00000001387028, Improvement: -0.00000000036245, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2963
Epoch 2963, Loss: 0.00000001295046, Improvement: -0.00000000091982, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2964
Epoch 2964, Loss: 0.00000001114647, Improvement: -0.00000000180399, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2965
Epoch 2965, Loss: 0.00000001650694, Improvement: 0.00000000536047, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2966
Epoch 2966, Loss: 0.00000001568290, Improvement: -0.00000000082404, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2967
Epoch 2967, Loss: 0.00000001294434, Improvement: -0.00000000273856, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2968
Epoch 2968, Loss: 0.00000001163001, Improvement: -0.00000000131433, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2969
Epoch 2969, Loss: 0.00000001231500, Improvement: 0.00000000068499, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2970
Epoch 2970, Loss: 0.00000001603168, Improvement: 0.00000000371667, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2971
Epoch 2971, Loss: 0.00000001164286, Improvement: -0.00000000438882, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2972
Epoch 2972, Loss: 0.00000001309310, Improvement: 0.00000000145025, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2973
Epoch 2973, Loss: 0.00000002371094, Improvement: 0.00000001061784, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2974
Epoch 2974, Loss: 0.00000003650794, Improvement: 0.00000001279701, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2975
Epoch 2975, Loss: 0.00000007954370, Improvement: 0.00000004303576, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2976
Epoch 2976, Loss: 0.00000006267180, Improvement: -0.00000001687189, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2977
Epoch 2977, Loss: 0.00000004885650, Improvement: -0.00000001381531, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2978
Epoch 2978, Loss: 0.00000002667172, Improvement: -0.00000002218478, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2979
Epoch 2979, Loss: 0.00000002666249, Improvement: -0.00000000000923, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2980
Epoch 2980, Loss: 0.00000002155199, Improvement: -0.00000000511050, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2981
Epoch 2981, Loss: 0.00000001783107, Improvement: -0.00000000372092, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2982
Epoch 2982, Loss: 0.00000001732986, Improvement: -0.00000000050121, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2983
Epoch 2983, Loss: 0.00000001392501, Improvement: -0.00000000340485, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2984
Epoch 2984, Loss: 0.00000001478006, Improvement: 0.00000000085505, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2985
Epoch 2985, Loss: 0.00000001229108, Improvement: -0.00000000248898, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2986
Epoch 2986, Loss: 0.00000001220030, Improvement: -0.00000000009077, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2987
Epoch 2987, Loss: 0.00000001582585, Improvement: 0.00000000362555, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2988
Epoch 2988, Loss: 0.00000002794091, Improvement: 0.00000001211506, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2989
Epoch 2989, Loss: 0.00000004193199, Improvement: 0.00000001399108, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2990
Epoch 2990, Loss: 0.00000005012794, Improvement: 0.00000000819594, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2991
Epoch 2991, Loss: 0.00000003954483, Improvement: -0.00000001058311, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2992
Epoch 2992, Loss: 0.00000002779324, Improvement: -0.00000001175159, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2993
Epoch 2993, Loss: 0.00000002209760, Improvement: -0.00000000569565, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2994
Epoch 2994, Loss: 0.00000001817517, Improvement: -0.00000000392243, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2995
Epoch 2995, Loss: 0.00000002457124, Improvement: 0.00000000639608, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2996
Epoch 2996, Loss: 0.00000002944225, Improvement: 0.00000000487100, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2997
Epoch 2997, Loss: 0.00000002583612, Improvement: -0.00000000360613, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2998
Epoch 2998, Loss: 0.00000001969949, Improvement: -0.00000000613663, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 2999
Epoch 2999, Loss: 0.00000001329357, Improvement: -0.00000000640593, Best Loss: 0.00000000704325 in Epoch 2632
Epoch 3000
Model saving checkpoint: the model trained after epoch 3000 has been saved with the training errors.
Epoch 3000, Loss: 0.00000001191545, Improvement: -0.00000000137811, Best Loss: 0.00000000704325 in Epoch 2632
