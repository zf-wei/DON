{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T01:24:09.026384Z",
     "start_time": "2024-11-19T01:23:58.031411Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import fipy as fp\n",
    "\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "'''\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"DeepONet with configurable parameters.\")\n",
    "parser.add_argument('--var', type=int, default=0, help='Variant of DeepONet')\n",
    "parser.add_argument('--struct', type=int, default=2, help='Structure of DeepONet')\n",
    "parser.add_argument('--sensor', type=int, default=50, help='Number of sensors')\n",
    "parser.add_argument('--boundary_parameter', type=float, default=0, help='Weight parameter for boundary conditions')\n",
    "# 解析命令行参数\n",
    "args = parser.parse_args()\n",
    "var = args.var\n",
    "struct = args.struct\n",
    "n_points = args.sensor\n",
    "boundary_parameter = args.boundary_parameter\n",
    "'''\n",
    "var = 0\n",
    "struct = 2\n",
    "n_points = 50\n",
    "boundary_parameter = 0\n",
    "\n",
    "\n",
    "epochs = 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "43c301c190545560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:24:38.375254Z",
     "start_time": "2024-11-19T01:24:38.362240Z"
    }
   },
   "source": [
    "time_limit = 1\n",
    "time_step = 0.01\n",
    "total_time_steps = int(time_limit/time_step)\n",
    "total_sample = 500\n",
    "boundary = int(total_sample*4/5) # 设置训练集和测试集的边界\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "branch_input_dim = n_points  # Number of points to represent the original function\n",
    "trunk_input_dim = 2     # Coordinate where we evaluate the transformed function\n",
    "\n",
    "# Define the dictionary mapping struct values to neural network structures\n",
    "if var!=6:\n",
    "    structures = {\n",
    "        1: {'hidden_dims': [100, 100, 100, 100], 'output_dim': 50},\n",
    "        2: {'hidden_dims': [200, 200, 200, 200], 'output_dim': 50}\n",
    "    }\n",
    "\n",
    "    # Get the configuration based on the struct value\n",
    "    config = structures.get(struct, {'hidden_dims': [], 'output_dim': 0})\n",
    "\n",
    "    hidden_dims = config['hidden_dims']\n",
    "    output_dim = config['output_dim']\n",
    "elif var==6:\n",
    "    structure_params = {\n",
    "        1: (3, 3, 100, 50),\n",
    "        2: (3, 3, 200, 50),\n",
    "    }\n",
    "    if struct in structure_params:\n",
    "        branch_depth, trunk_depth, hidden_dim, output_dim = structure_params[struct]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid structure type\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:24:41.824750Z",
     "start_time": "2024-11-19T01:24:41.809871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys.path.append(os.path.abspath(os.path.expanduser(\"~/DON/utilities\")))\n",
    "\n",
    "from DON_Variants import DeepONets\n",
    "from loss_fns import loss_fn"
   ],
   "id": "7bc1106181255a38",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "494baac2c1b96444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:03.071306Z",
     "start_time": "2024-11-19T01:25:03.047038Z"
    }
   },
   "source": [
    "# In this cell, we define the function to get the cell centers of a 1D mesh. \n",
    "# Also, we set up the spatial and temporal grid points for the training and testing datasets.\n",
    "# This is the so-called y_expanded tensor. \n",
    "def get_cell_centers(time_limit = 1, n_points = 50):\n",
    "    \"\"\"\n",
    "    Get the cell center positions for a 1D mesh with the specified number of grid points.\n",
    "\n",
    "    Parameters:\n",
    "    - n_points: Number of grid points in the spatial domain.\n",
    "\n",
    "    Returns:\n",
    "    - cell_centers: The x-positions of the cell centers.\n",
    "    \"\"\"\n",
    "    L = time_limit  # Length of the domain\n",
    "    dx = L / n_points\n",
    "\n",
    "    # Create a 1D mesh\n",
    "    mesh = fp.Grid1D(nx=n_points, dx=dx)\n",
    "\n",
    "    # Get the cell center positions\n",
    "    cell_centers = mesh.cellCenters[0]  # These are the x-positions of the cell centers\n",
    "    cell_centers = np.array(cell_centers)\n",
    "\n",
    "    return cell_centers\n",
    "\n",
    "# Example usage:\n",
    "cell_centers = get_cell_centers(n_points=n_points)\n",
    "cell_centers = np.around(cell_centers, decimals=2)\n",
    "\n",
    "time_steps = np.arange(time_step, time_limit+time_step, time_step)\n",
    "time_steps = np.around(time_steps, decimals=2)\n",
    "\n",
    "Y1, Y2 = np.meshgrid(cell_centers, time_steps)  # 第一个变量进行行展开，第二个变量进行列展开\n",
    "\n",
    "y = np.column_stack([Y2.ravel(),Y1.ravel()]) \n",
    "# 先将 Y2 和 Y1 进行展开，然后将展开后的两个向量进行列合并\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "print(f\"The dimension of y_tensor is {y_tensor.shape}.\")\n",
    "y_expanded = y_tensor.unsqueeze(0).expand(total_sample, -1, -1)\n",
    "print(f\"The dimension of y_expanded is {y_expanded.shape} after expanding.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of y_tensor is torch.Size([5000, 2]).\n",
      "The dimension of y_expanded is torch.Size([500, 5000, 2]) after expanding.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "97c2618d2af6012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:07.247555Z",
     "start_time": "2024-11-19T01:25:07.195756Z"
    }
   },
   "source": [
    "# In this cell, we load the initial conditions and solutions from the saved files.\n",
    "\n",
    "# Define the directory where you want to save the file\n",
    "from pathlib import Path\n",
    "# Get the current directory\n",
    "current_dir = Path.cwd()\n",
    "#data_directory = os.path.join(current_dir.parent, 'data')\n",
    "data_directory = os.path.join(current_dir, 'data')\n",
    "initials_name = f'heat_initials_{len(cell_centers)}.npy'\n",
    "solutions_name = f'heat_solutions_{len(cell_centers)}.npy'\n",
    "\n",
    "# Define the file paths\n",
    "initials_path = os.path.join(data_directory, initials_name)\n",
    "solutions_path = os.path.join(data_directory, solutions_name)\n",
    "\n",
    "# Load the data\n",
    "initials = np.load(initials_path)\n",
    "solutions = np.load(solutions_path)\n",
    "\n",
    "print(f\"The dimensions of the initial conditions are: {initials.shape}\")\n",
    "print(f\"The dimensions of the solutions are: {solutions.shape}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the initial conditions are: (500, 50)\n",
      "The dimensions of the solutions are: (500, 100, 50)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "4bd2ad7a59d3c19d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:21.248580Z",
     "start_time": "2024-11-19T01:25:21.240939Z"
    }
   },
   "source": [
    "# In this cell, we arrange the initial conditions into the desired format for training the DeepONet.\n",
    "# This is the so-called u_expanded tensor.\n",
    "u_tensor = torch.tensor(initials, dtype=torch.float)\n",
    "print(f\"The dimension of u_tensor is {u_tensor.shape}.\")\n",
    "\n",
    "u_expanded = u_tensor.unsqueeze(1) # u_expanded: tensor[total_sample, 1, n_points]\n",
    "u_expanded = u_expanded.expand(-1, total_time_steps*n_points, -1) # u_expanded: tensor[total_sample, total_time_steps*n_points, n_points]\n",
    "print(f\"The dimension of u_expanded is {u_expanded.shape} after expanding.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of u_tensor is torch.Size([500, 50]).\n",
      "The dimension of u_expanded is torch.Size([500, 5000, 50]) after expanding.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6b86789219fee30b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:23.613830Z",
     "start_time": "2024-11-19T01:25:23.599224Z"
    }
   },
   "source": [
    "# I have a tensor of shape (total_sample, n_points) representing the initial conditions. In this cell, I wanted to expand it to (total_sample, total_time_steps*n_points) by repeating the initial conditions for each time step.\n",
    "\n",
    "# Assuming u_tensor is the tensor of shape (total_sample, n_points)\n",
    "# Expand the tensor to (total_sample, total_time_steps*n_points)\n",
    "u_corresponding = u_tensor.repeat(1, total_time_steps)\n",
    "u_corresponding = u_corresponding.unsqueeze(2)\n",
    "# print(u_corresponding.shape)\n",
    "\n",
    "if var==2 or var==3:\n",
    "    y_expanded = torch.cat((y_expanded, u_corresponding), dim=-1)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "bc20320d16f0200d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:27.016823Z",
     "start_time": "2024-11-19T01:25:26.983387Z"
    }
   },
   "source": [
    "# In this cell, we arrange the solutions into the desired format for training the DeepONet.\n",
    "# This is the so-called s_expanded tensor.\n",
    "\n",
    "solutions_linear = np.zeros((total_sample, total_time_steps*n_points))\n",
    "\n",
    "for i in range(total_sample):\n",
    "    solutions_linear[i] = solutions[i].flatten()\n",
    "\n",
    "# solutions is a 3D array of shape (total_sample, total_time_steps, n_points)\n",
    "print(f\"The loaded solution dataset has dimension {solutions.shape},\\n\\t while the arranged linearized dataset has dimension {solutions_linear.shape}.\")\n",
    "\n",
    "s_tensor  = torch.tensor(solutions_linear, dtype=torch.float) # s_tensor: tensor[total_sample, total_time_steps*n_points]\n",
    "s_expanded  = s_tensor.unsqueeze(2) # s_expanded: tensor[total_sample, total_time_steps*n_points, 1]\n",
    "\n",
    "print(f\"The dimension of s_tensor is {s_tensor.shape}.\")\n",
    "print(f\"The dimension of s_expanded is {s_expanded.shape} after expanding.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded solution dataset has dimension (500, 100, 50),\n",
      "\t while the arranged linearized dataset has dimension (500, 5000).\n",
      "The dimension of s_tensor is torch.Size([500, 5000]).\n",
      "The dimension of s_expanded is torch.Size([500, 5000, 1]) after expanding.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c7fb3d4c0f958ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:30.673086Z",
     "start_time": "2024-11-19T01:25:30.664135Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "This is the function to well organize the dataset\n",
    "\"\"\"\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input1_data, input2_data, targets):\n",
    "        self.input1_data = input1_data\n",
    "        self.input2_data = input2_data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input1_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input1 = self.input1_data[idx]\n",
    "        input2 = self.input2_data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return input1, input2, target\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "dcdc8073c92892a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:34.413712Z",
     "start_time": "2024-11-19T01:25:34.405270Z"
    }
   },
   "source": [
    "train_set = CustomDataset(u_expanded[:boundary], y_expanded[:boundary], s_expanded[:boundary])\n",
    "test_set = CustomDataset(u_expanded[boundary:], y_expanded[boundary:], s_expanded[boundary:])\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1) \n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1) \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "b6bf19a07bbf4385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:41.040576Z",
     "start_time": "2024-11-19T01:25:37.730844Z"
    }
   },
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if var!=6:\n",
    "    model = DeepONets[var](branch_input_dim, trunk_input_dim, hidden_dims, output_dim).to(device)\n",
    "elif var==6:\n",
    "    model = DeepONets[var](branch_input_dim, branch_depth, trunk_input_dim, trunk_depth, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "685c9105c4470cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:25:49.961128Z",
     "start_time": "2024-11-19T01:25:44.389844Z"
    }
   },
   "source": [
    "# 训练模型\n",
    "error_list = []\n",
    "err_best = float('inf')\n",
    "err_prev = 0\n",
    "best_epoch = 0\n",
    "model_best = model.state_dict().copy()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\") \n",
    "    err = []\n",
    "    for input1_batch, input2_batch, target_batch in train_loader:\n",
    "        input1_batch = input1_batch.to(device)\n",
    "        input2_batch = input2_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1_batch, input2_batch)\n",
    "        loss = loss_fn(outputs, target_batch, boundary_parameter)\n",
    "        err.append(loss.item())\n",
    "        if loss.item()<err_best:\n",
    "            err_best = loss.item()\n",
    "            best_epoch = epoch\n",
    "            model_best = model.state_dict().copy()\n",
    "            model_filename_best = f\"Var{var}_Struct{struct}_Sensor{n_points}_Batch{batch_size}-best.pth\"\n",
    "            torch.save(model_best, model_filename_best)\n",
    "            print(f\"A best model at epoch {epoch+1} has been saved with training error {err_best:.9f}.\", file=sys.stderr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del input1_batch, input2_batch, outputs, loss\n",
    "        torch.cuda.empty_cache()  # 释放当前批次的缓存\n",
    "    error_list.append(err)\n",
    "    err_curr = np.mean(err)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {err_curr:.9f}, Improvement: {err_curr - err_prev:.9f}, Best Loss: {err_best:.9f} in Epoch {best_epoch+1}\")\n",
    "    err_prev = err_curr\n",
    "    if epoch%50==49:\n",
    "        # 保存损失值和模型，修改文件名以包含参数信息  \n",
    "        output_filename = f\"Var{var}_Struct{struct}_Sensor{n_points}_Batch{batch_size}-final.npy\"\n",
    "        model_filename = f\"Var{var}_Struct{struct}_Sensor{n_points}_Batch{batch_size}-final.pth\"\n",
    "        np.save(output_filename, np.array(error_list))\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "        print(f\"Model saving checkpoint: the model trained after epoch {epoch+1} has been saved with the training errors.\", file=sys.stderr)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 16368) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1243\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_queue\u001B[38;5;241m.\u001B[39mget(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m   1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\multiprocessing\\queues.py:114\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[1;32m--> 114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \n\u001B[0;32m     11\u001B[0m err \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m input1_batch, input2_batch, target_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     13\u001B[0m     input1_batch \u001B[38;5;241m=\u001B[39m input1_batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     14\u001B[0m     input2_batch \u001B[38;5;241m=\u001B[39m input2_batch\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1448\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_data()\n\u001B[0;32m   1449\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1451\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1408\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1411\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1412\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_get_data()\n\u001B[0;32m   1413\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1414\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\.conda\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1255\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[1;32m-> 1256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   1257\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpids_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1258\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[0;32m   1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 16368) exited unexpectedly"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f10a0718ea561fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:49:24.147728Z",
     "start_time": "2024-11-17T04:49:24.145168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0182706  0.01090199]\n"
     ]
    }
   ],
   "source": [
    "errs = np.array(error_list)\n",
    "\n",
    "print(np.mean(errs,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81b8cd7c1dc06fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T04:49:24.242253Z",
     "start_time": "2024-11-17T04:49:24.215951Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存损失值和模型，修改文件名以包含参数信息\n",
    "output_filename = f\"Var{var}_Struct{struct}_Sensor{n_points}_Batch{batch_size}-final.npy\"\n",
    "model_filename = f\"Var{var}_Struct{struct}_Sensor{n_points}_Batch{batch_size}-final.pth\"\n",
    "\n",
    "np.save(output_filename, np.array(error_list))\n",
    "torch.save(model.state_dict(), model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
