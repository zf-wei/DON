{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7581882a095f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.014100Z",
     "start_time": "2025-01-14T04:08:51.939954Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.expanduser(\"~/DON\")))\n",
    "\n",
    "'''\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"DeepONet with configurable parameters.\")\n",
    "parser.add_argument('--problem', type=str, default=\"heat\", help='Problem to solve')\n",
    "parser.add_argument('--var', type=int, default=0, help='Variant of DeepONet')\n",
    "parser.add_argument('--visc', type=float, default=0.0001, help='Viscosity')\n",
    "parser.add_argument('--struct', type=int, default=1, help='Structure of DeepONet')\n",
    "parser.add_argument('--sensor', type=int, default=50, help='Number of sensors')\n",
    "parser.add_argument('--boundary_parameter', type=float, default=0, help='Weight parameter for boundary conditions')\n",
    "parser.add_argument('--initial_parameter', type=float, default=0, help='Weight parameter for initial conditions')\n",
    "parser.add_argument('--batch_size', type=int, default=8000, help='Batch size')\n",
    "# 解析命令行参数\n",
    "args = parser.parse_args()\n",
    "problem = args.problem\n",
    "var = args.var\n",
    "visc = args.visc\n",
    "struct = args.struct\n",
    "n_points = args.sensor\n",
    "boundary_parameter = args.boundary_parameter\n",
    "initial_parameter = args.initial_parameter\n",
    "batch_size = args.batch_size\n",
    "'''\n",
    "problem = \"burgers\"\n",
    "var = 0\n",
    "visc = 0.0001\n",
    "struct = 1\n",
    "n_points = 101\n",
    "boundary_parameter = 0\n",
    "initial_parameter = 0\n",
    "batch_size = 8000\n",
    "\n",
    "iterations = 2500\n",
    "## 需要修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e92850a88f7af6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.138941Z",
     "start_time": "2025-01-14T04:08:54.049266Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we define the configurable parameters for the DeepONet\n",
    "\n",
    "temporal_limit = 1\n",
    "temporal_step = 0.01\n",
    "\n",
    "if problem==\"heat\":\n",
    "    temporal_start = temporal_step\n",
    "    total_temporal_steps = int(temporal_limit / temporal_step)\n",
    "    from utilities.tools import get_cell_centers\n",
    "    evaluating_points = get_cell_centers(n_points=n_points)\n",
    "elif problem==\"burgers\":\n",
    "    temporal_start = 0\n",
    "    total_temporal_steps = (int(temporal_limit / temporal_step) + 1)\n",
    "    evaluating_points = np.linspace(0, 1, n_points)\n",
    "\n",
    "evaluating_points = np.around(evaluating_points, decimals=2)\n",
    "\n",
    "total_sample = 500\n",
    "train_val_split_index = int(total_sample * 4 / 5) # 设置训练集和验证集的边界\n",
    "val_test_split_index = int(total_sample * 9 / 10) # 设置验证集和测试集的边界\n",
    "\n",
    "# Hyperparameters\n",
    "branch_input_dim = n_points  # Number of points to represent the original function\n",
    "trunk_input_dim = 2     # Coordinate where we evaluate the transformed function\n",
    "\n",
    "# Define the dictionary mapping struct values to neural network structures\n",
    "if var!=6:\n",
    "    structures = {\n",
    "        1: {'hidden_dims': [100, 100, 100, 100, 100, 100], 'output_dim': 50},\n",
    "        2: {'hidden_dims': [200, 200, 200, 200], 'output_dim': 50}\n",
    "    }\n",
    "\n",
    "    # Get the configuration based on the struct value\n",
    "    config = structures.get(struct, {'hidden_dims': [], 'output_dim': 0})\n",
    "\n",
    "    hidden_dims = config['hidden_dims']\n",
    "    output_dim = config['output_dim']\n",
    "elif var==6:\n",
    "    structure_params = {\n",
    "        1: (6, 6, 100, 50),\n",
    "        2: (4, 4, 200, 50),\n",
    "    }\n",
    "    if struct in structure_params:\n",
    "        branch_depth, trunk_depth, hidden_dim, output_dim = structure_params[struct]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid structure type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b174778098c640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.225760Z",
     "start_time": "2025-01-14T04:08:54.221319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of y_tensor is torch.Size([10201, 2]).\n",
      "The dimension of y_expanded is torch.Size([500, 10201, 2]) after expanding.\n",
      "The zero coordinate of y_expanded is temporal and the first coordinate is space.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we import the function to get the cell centers of a 1D mesh.\n",
    "# Also, we set up the spatial and temporal grid points for the training and testing datasets.\n",
    "# This is the so-called y_expanded tensor.\n",
    "temporal_steps = np.arange(temporal_start, temporal_limit + temporal_step, temporal_step)\n",
    "temporal_steps = np.around(temporal_steps, decimals=2)\n",
    "\n",
    "Y1, Y2 = np.meshgrid(evaluating_points, temporal_steps)  # 第一个变量进行行展开，第二个变量进行列展开\n",
    "\n",
    "y = np.column_stack([Y2.ravel(),Y1.ravel()]) \n",
    "# 先将 Y2 和 Y1 进行展开，然后将展开后的两个向量进行列合并\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float)\n",
    "print(f\"The dimension of y_tensor is {y_tensor.shape}.\")\n",
    "y_expanded = y_tensor.unsqueeze(0).expand(total_sample, -1, -1)\n",
    "print(f\"The dimension of y_expanded is {y_expanded.shape} after expanding.\")\n",
    "print(\"The zero coordinate of y_expanded is temporal and the first coordinate is space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5207f665c8910c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.314254Z",
     "start_time": "2025-01-14T04:08:54.298594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the initial conditions are: (500, 101)\n",
      "The dimensions of the solutions are: (500, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we load the initial conditions and solutions from the saved files.\n",
    "\n",
    "# Define the directory where you want to save the file\n",
    "from pathlib import Path\n",
    "# Get the current directory\n",
    "current_dir = Path.cwd()\n",
    "#data_directory = os.path.join(current_dir.parent, 'data')\n",
    "## 需要修改\n",
    "data_directory = os.path.join(current_dir, 'data')\n",
    "initials_name = f'{problem}_initials_{visc}_{len(evaluating_points)}.npy'\n",
    "solutions_name = f'{problem}_solutions_{visc}_{len(evaluating_points)}.npy'\n",
    "\n",
    "# Define the file paths\n",
    "initials_path = os.path.join(data_directory, initials_name)\n",
    "solutions_path = os.path.join(data_directory, solutions_name)\n",
    "\n",
    "# Load the data\n",
    "initials = np.load(initials_path)\n",
    "solutions = np.load(solutions_path)\n",
    "\n",
    "print(f\"The dimensions of the initial conditions are: {initials.shape}\")\n",
    "print(f\"The dimensions of the solutions are: {solutions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263385cef0858257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.388214Z",
     "start_time": "2025-01-14T04:08:54.381490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of u_tensor is torch.Size([500, 101]).\n",
      "The dimension of u_expanded is torch.Size([500, 10201, 101]) after expanding.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we arrange the initial conditions into the desired format for training the DeepONet.\n",
    "# This is the so-called u_expanded tensor.\n",
    "u_tensor = torch.tensor(initials, dtype=torch.float)\n",
    "print(f\"The dimension of u_tensor is {u_tensor.shape}.\")\n",
    "\n",
    "u_expanded = u_tensor.unsqueeze(1) # u_expanded: tensor[total_sample, 1, n_points]\n",
    "u_expanded = u_expanded.expand(-1, total_temporal_steps * n_points, -1) # u_expanded: tensor[total_sample, total_temporal_steps*n_points, n_points]\n",
    "print(f\"The dimension of u_expanded is {u_expanded.shape} after expanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e93725c26537d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.467459Z",
     "start_time": "2025-01-14T04:08:54.458139Z"
    }
   },
   "outputs": [],
   "source": [
    "# I have a tensor of shape (total_sample, n_points) representing the initial conditions. In this cell, I wanted to expand it to (total_sample, total_temporal_steps*n_points) by repeating the initial conditions for each temporal step.\n",
    "\n",
    "# Assuming u_tensor is the tensor of shape (total_sample, n_points)\n",
    "# Expand the tensor to (total_sample, total_temporal_steps*n_points)\n",
    "u_corresponding = u_tensor.repeat(1, total_temporal_steps)\n",
    "u_corresponding = u_corresponding.unsqueeze(2) # This is the so-called corresponding initial value\n",
    "\n",
    "# Take the spatial coordinate of the y_expanded tensor\n",
    "y_space = y_expanded[:, :, 1].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addb4d7d40a31a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.542490Z",
     "start_time": "2025-01-14T04:08:54.539617Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we modify the input of the DeepONet based on the variant chosen.\n",
    "# We also update the input dimensions of the DeepONet based on the variant chosen.\n",
    "if var==2 or var==3:\n",
    "    y_expanded = torch.cat((y_expanded, u_corresponding), dim=-1)\n",
    "elif var==4:\n",
    "    y_expanded = torch.cat((y_expanded, u_expanded), dim=-1)\n",
    "\n",
    "if var== 1 or var==3 or var==4:\n",
    "    u_expanded = torch.cat((u_expanded, y_space), dim=-1)\n",
    "\n",
    "var_mapping = {\n",
    "    1: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim},\n",
    "    2: {'var_branch_input_dim': branch_input_dim, 'var_trunk_input_dim': trunk_input_dim + 1},\n",
    "    3: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim + 1},\n",
    "    4: {'var_branch_input_dim': branch_input_dim + 1, 'var_trunk_input_dim': trunk_input_dim + branch_input_dim}\n",
    "}\n",
    "\n",
    "if var in var_mapping:\n",
    "    branch_input_dim = var_mapping[var]['var_branch_input_dim']\n",
    "    trunk_input_dim = var_mapping[var]['var_trunk_input_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8da03c7d319f1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.664563Z",
     "start_time": "2025-01-14T04:08:54.622654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded solution dataset has dimension (500, 101, 101),\n",
      "\t while the arranged linearized dataset has dimension (500, 10201).\n",
      "The dimension of s_tensor is torch.Size([500, 10201]).\n",
      "The dimension of s_expanded is torch.Size([500, 10201, 1]) after expanding.\n"
     ]
    }
   ],
   "source": [
    "# In this cell, we arrange the solutions into the desired format for training the DeepONet.\n",
    "# This is the so-called s_expanded tensor.\n",
    "\n",
    "solutions_linear = np.zeros((total_sample, total_temporal_steps * n_points))\n",
    "\n",
    "for i in range(total_sample):\n",
    "    solutions_linear[i] = solutions[i].flatten()\n",
    "\n",
    "# solutions is a 3D array of shape (total_sample, total_temporal_steps, n_points)\n",
    "print(f\"The loaded solution dataset has dimension {solutions.shape},\\n\\t while the arranged linearized dataset has dimension {solutions_linear.shape}.\")\n",
    "\n",
    "s_tensor  = torch.tensor(solutions_linear, dtype=torch.float) # s_tensor: tensor[total_sample, total_temporal_steps*n_points]\n",
    "s_expanded  = s_tensor.unsqueeze(2) # s_expanded: tensor[total_sample, total_temporal_steps*n_points, 1]\n",
    "\n",
    "print(f\"The dimension of s_tensor is {s_tensor.shape}.\")\n",
    "print(f\"The dimension of s_expanded is {s_expanded.shape} after expanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93069c846cf6a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:54.902347Z",
     "start_time": "2025-01-14T04:08:54.783274Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we create the training and testing datasets and dataloader for the DeepONet.\n",
    "\n",
    "u_train = u_expanded[:train_val_split_index]\n",
    "y_train = y_expanded[:train_val_split_index]\n",
    "s_train = s_expanded[:train_val_split_index]\n",
    "\n",
    "u_val = u_expanded[train_val_split_index:val_test_split_index]\n",
    "y_val = y_expanded[train_val_split_index:val_test_split_index]\n",
    "s_val = s_expanded[train_val_split_index:val_test_split_index]\n",
    "\n",
    "u_train_combined = u_train.reshape(-1, u_train.shape[-1])\n",
    "y_train_combined = y_train.reshape(-1, y_train.shape[-1])\n",
    "s_train_combined = s_train.reshape(-1, s_train.shape[-1])\n",
    "\n",
    "u_val_combined = u_val.reshape(-1, u_val.shape[-1])\n",
    "y_val_combined = y_val.reshape(-1, y_val.shape[-1])\n",
    "s_val_combined = s_val.reshape(-1, s_val.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d86eb88beb95f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:55.476022Z",
     "start_time": "2025-01-14T04:08:54.948385Z"
    }
   },
   "outputs": [],
   "source": [
    "from utilities.tools import DataGenerator as DataGenerator\n",
    "\n",
    "# Create the Train DataGenerator\n",
    "train_loader = DataGenerator(u_train_combined, y_train_combined, s_train_combined, batch_size=8000, seed=2025)\n",
    "\n",
    "# Create an iterator\n",
    "train_batch = iter(train_loader)\n",
    "\n",
    "# Create the Validation DataGenerator\n",
    "validation_loader = DataGenerator(u_val_combined, y_val_combined, s_val_combined, batch_size=8000, seed=2025)\n",
    "\n",
    "# Create an iterator\n",
    "validation_batch = iter(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2eb4da6aa21f586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:08:56.711969Z",
     "start_time": "2025-01-14T04:08:55.508864Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, we import and set up the model and load the trained parameters\n",
    "# The loss function is also imported here.\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if var!=6:\n",
    "    from utilities.DeepONets import DeepONet\n",
    "    model = DeepONet(branch_input_dim, trunk_input_dim, hidden_dims, output_dim).to(device)\n",
    "elif var==6:\n",
    "    from utilities.DeepONets import ModifiedDeepONet\n",
    "    model = ModifiedDeepONet(branch_input_dim, branch_depth, trunk_input_dim, trunk_depth, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "from utilities.loss_fns import loss_fn_1d_combined as loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc153cac14e1864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:09:18.705933Z",
     "start_time": "2025-01-14T04:08:56.773230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 0.03644691780210, Loss Change: 0.03644691780210, Best Loss: 0.03644691780210 in iteration 1, Time: 0.19 seconds\n",
      "Iteration 2, Loss: 0.03485604748130, Loss Change: -0.00159087032080, Best Loss: 0.03485604748130 in iteration 2, Time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 1 has been saved with training loss 0.03644691780210.\n",
      "A best model at iteration 2 has been saved with training loss 0.03485604748130.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Loss: 0.03484020754695, Loss Change: -0.00001583993435, Best Loss: 0.03484020754695 in iteration 3, Time: 0.17 seconds\n",
      "Iteration 4, Loss: 0.03392666578293, Loss Change: -0.00091354176402, Best Loss: 0.03392666578293 in iteration 4, Time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 3 has been saved with training loss 0.03484020754695.\n",
      "A best model at iteration 4 has been saved with training loss 0.03392666578293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Loss: 0.03127730637789, Loss Change: -0.00264935940504, Best Loss: 0.03127730637789 in iteration 5, Time: 0.17 seconds\n",
      "Iteration 6, Loss: 0.03100288286805, Loss Change: -0.00027442350984, Best Loss: 0.03100288286805 in iteration 6, Time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 5 has been saved with training loss 0.03127730637789.\n",
      "A best model at iteration 6 has been saved with training loss 0.03100288286805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Loss: 0.02773292921484, Loss Change: -0.00326995365322, Best Loss: 0.02773292921484 in iteration 7, Time: 0.17 seconds\n",
      "Iteration 8, Loss: 0.02826429530978, Loss Change: 0.00053136609495, Best Loss: 0.02773292921484 in iteration 7, Time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 7 has been saved with training loss 0.02773292921484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Loss: 0.02709574811161, Loss Change: -0.00116854719818, Best Loss: 0.02709574811161 in iteration 9, Time: 0.19 seconds\n",
      "Iteration 10, Loss: 0.02714342437685, Loss Change: 0.00004767626524, Best Loss: 0.02709574811161 in iteration 9, Time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 9 has been saved with training loss 0.02709574811161.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, Loss: 0.02698705345392, Loss Change: -0.00015637092292, Best Loss: 0.02698705345392 in iteration 11, Time: 0.17 seconds\n",
      "Iteration 12, Loss: 0.02561341226101, Loss Change: -0.00137364119291, Best Loss: 0.02561341226101 in iteration 12, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 11 has been saved with training loss 0.02698705345392.\n",
      "A best model at iteration 12 has been saved with training loss 0.02561341226101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, Loss: 0.02502015233040, Loss Change: -0.00059325993061, Best Loss: 0.02502015233040 in iteration 13, Time: 0.17 seconds\n",
      "Iteration 14, Loss: 0.02494475804269, Loss Change: -0.00007539428771, Best Loss: 0.02494475804269 in iteration 14, Time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 13 has been saved with training loss 0.02502015233040.\n",
      "A best model at iteration 14 has been saved with training loss 0.02494475804269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, Loss: 0.02489442937076, Loss Change: -0.00005032867193, Best Loss: 0.02489442937076 in iteration 15, Time: 0.17 seconds\n",
      "Iteration 16, Loss: 0.02381210774183, Loss Change: -0.00108232162893, Best Loss: 0.02381210774183 in iteration 16, Time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 15 has been saved with training loss 0.02489442937076.\n",
      "A best model at iteration 16 has been saved with training loss 0.02381210774183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, Loss: 0.02438257634640, Loss Change: 0.00057046860456, Best Loss: 0.02381210774183 in iteration 16, Time: 0.15 seconds\n",
      "Iteration 18, Loss: 0.02136880718172, Loss Change: -0.00301376916468, Best Loss: 0.02136880718172 in iteration 18, Time: 0.16 seconds\n",
      "Iteration 19, Loss: 0.02223337069154, Loss Change: 0.00086456350982, Best Loss: 0.02136880718172 in iteration 18, Time: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 18 has been saved with training loss 0.02136880718172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, Loss: 0.02098408155143, Loss Change: -0.00124928914011, Best Loss: 0.02098408155143 in iteration 20, Time: 0.16 seconds\n",
      "Iteration 21, Loss: 0.01976518891752, Loss Change: -0.00121889263391, Best Loss: 0.01976518891752 in iteration 21, Time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 20 has been saved with training loss 0.02098408155143.\n",
      "A best model at iteration 21 has been saved with training loss 0.01976518891752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, Loss: 0.01812908612192, Loss Change: -0.00163610279560, Best Loss: 0.01812908612192 in iteration 22, Time: 0.16 seconds\n",
      "Iteration 23, Loss: 0.01625175215304, Loss Change: -0.00187733396888, Best Loss: 0.01625175215304 in iteration 23, Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 22 has been saved with training loss 0.01812908612192.\n",
      "A best model at iteration 23 has been saved with training loss 0.01625175215304.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, Loss: 0.01643700897694, Loss Change: 0.00018525682390, Best Loss: 0.01625175215304 in iteration 23, Time: 0.13 seconds\n",
      "Iteration 25, Loss: 0.01514556072652, Loss Change: -0.00129144825041, Best Loss: 0.01514556072652 in iteration 25, Time: 0.15 seconds\n",
      "Iteration 26, Loss: 0.01509169861674, Loss Change: -0.00005386210978, Best Loss: 0.01509169861674 in iteration 26, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 25 has been saved with training loss 0.01514556072652.\n",
      "A best model at iteration 26 has been saved with training loss 0.01509169861674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, Loss: 0.01452103536576, Loss Change: -0.00057066325098, Best Loss: 0.01452103536576 in iteration 27, Time: 0.16 seconds\n",
      "Iteration 28, Loss: 0.01428282726556, Loss Change: -0.00023820810020, Best Loss: 0.01428282726556 in iteration 28, Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 27 has been saved with training loss 0.01452103536576.\n",
      "A best model at iteration 28 has been saved with training loss 0.01428282726556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, Loss: 0.01317674666643, Loss Change: -0.00110608059913, Best Loss: 0.01317674666643 in iteration 29, Time: 0.18 seconds\n",
      "Iteration 30, Loss: 0.01227641571313, Loss Change: -0.00090033095330, Best Loss: 0.01227641571313 in iteration 30, Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 29 has been saved with training loss 0.01317674666643.\n",
      "A best model at iteration 30 has been saved with training loss 0.01227641571313.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, Loss: 0.01166144385934, Loss Change: -0.00061497185379, Best Loss: 0.01166144385934 in iteration 31, Time: 0.15 seconds\n",
      "Iteration 32, Loss: 0.01229294389486, Loss Change: 0.00063150003552, Best Loss: 0.01166144385934 in iteration 31, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 31 has been saved with training loss 0.01166144385934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, Loss: 0.01113838609308, Loss Change: -0.00115455780178, Best Loss: 0.01113838609308 in iteration 33, Time: 0.15 seconds\n",
      "Iteration 34, Loss: 0.01066385861486, Loss Change: -0.00047452747822, Best Loss: 0.01066385861486 in iteration 34, Time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 33 has been saved with training loss 0.01113838609308.\n",
      "A best model at iteration 34 has been saved with training loss 0.01066385861486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, Loss: 0.01111877057701, Loss Change: 0.00045491196215, Best Loss: 0.01066385861486 in iteration 34, Time: 0.15 seconds\n",
      "Iteration 36, Loss: 0.00995150953531, Loss Change: -0.00116726104170, Best Loss: 0.00995150953531 in iteration 36, Time: 0.16 seconds\n",
      "Iteration 37, Loss: 0.00947531498969, Loss Change: -0.00047619454563, Best Loss: 0.00947531498969 in iteration 37, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 36 has been saved with training loss 0.00995150953531.\n",
      "A best model at iteration 37 has been saved with training loss 0.00947531498969.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, Loss: 0.00912205781788, Loss Change: -0.00035325717181, Best Loss: 0.00912205781788 in iteration 38, Time: 0.15 seconds\n",
      "Iteration 39, Loss: 0.00936118327081, Loss Change: 0.00023912545294, Best Loss: 0.00912205781788 in iteration 38, Time: 0.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 38 has been saved with training loss 0.00912205781788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, Loss: 0.00877270568162, Loss Change: -0.00058847758919, Best Loss: 0.00877270568162 in iteration 40, Time: 0.15 seconds\n",
      "Iteration 41, Loss: 0.00837343279272, Loss Change: -0.00039927288890, Best Loss: 0.00837343279272 in iteration 41, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 40 has been saved with training loss 0.00877270568162.\n",
      "A best model at iteration 41 has been saved with training loss 0.00837343279272.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, Loss: 0.00851909630001, Loss Change: 0.00014566350728, Best Loss: 0.00837343279272 in iteration 41, Time: 0.14 seconds\n",
      "Iteration 43, Loss: 0.00873236637563, Loss Change: 0.00021327007562, Best Loss: 0.00837343279272 in iteration 41, Time: 0.15 seconds\n",
      "Iteration 44, Loss: 0.00799606181681, Loss Change: -0.00073630455881, Best Loss: 0.00799606181681 in iteration 44, Time: 0.15 seconds\n",
      "Iteration 45, Loss: 0.00839630514383, Loss Change: 0.00040024332702, Best Loss: 0.00799606181681 in iteration 44, Time: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 44 has been saved with training loss 0.00799606181681.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, Loss: 0.00806101225317, Loss Change: -0.00033529289067, Best Loss: 0.00799606181681 in iteration 44, Time: 0.14 seconds\n",
      "Iteration 47, Loss: 0.00842086691409, Loss Change: 0.00035985466093, Best Loss: 0.00799606181681 in iteration 44, Time: 0.15 seconds\n",
      "Iteration 48, Loss: 0.00816914439201, Loss Change: -0.00025172252208, Best Loss: 0.00799606181681 in iteration 44, Time: 0.14 seconds\n",
      "Iteration 49, Loss: 0.00803047325462, Loss Change: -0.00013867113739, Best Loss: 0.00799606181681 in iteration 44, Time: 0.15 seconds\n",
      "Iteration 50, Loss: 0.00799096841365, Loss Change: -0.00003950484097, Best Loss: 0.00799096841365 in iteration 50, Time: 0.15 seconds\n",
      "Iteration 51, Loss: 0.00819147191942, Loss Change: 0.00020050350577, Best Loss: 0.00799096841365 in iteration 50, Time: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 50 has been saved with training loss 0.00799096841365.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, Loss: 0.00750392070040, Loss Change: -0.00068755121902, Best Loss: 0.00750392070040 in iteration 52, Time: 0.16 seconds\n",
      "Iteration 53, Loss: 0.00778994662687, Loss Change: 0.00028602592647, Best Loss: 0.00750392070040 in iteration 52, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 52 has been saved with training loss 0.00750392070040.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, Loss: 0.00795469246805, Loss Change: 0.00016474584118, Best Loss: 0.00750392070040 in iteration 52, Time: 0.14 seconds\n",
      "Iteration 55, Loss: 0.00713961105794, Loss Change: -0.00081508141011, Best Loss: 0.00713961105794 in iteration 55, Time: 0.15 seconds\n",
      "Iteration 56, Loss: 0.00774681707844, Loss Change: 0.00060720602050, Best Loss: 0.00713961105794 in iteration 55, Time: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 55 has been saved with training loss 0.00713961105794.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, Loss: 0.00758708314970, Loss Change: -0.00015973392874, Best Loss: 0.00713961105794 in iteration 55, Time: 0.15 seconds\n",
      "Iteration 58, Loss: 0.00731426896527, Loss Change: -0.00027281418443, Best Loss: 0.00713961105794 in iteration 55, Time: 0.14 seconds\n",
      "Iteration 59, Loss: 0.00784885603935, Loss Change: 0.00053458707407, Best Loss: 0.00713961105794 in iteration 55, Time: 0.13 seconds\n",
      "Iteration 60, Loss: 0.00767648918554, Loss Change: -0.00017236685380, Best Loss: 0.00713961105794 in iteration 55, Time: 0.14 seconds\n",
      "Iteration 61, Loss: 0.00754581205547, Loss Change: -0.00013067713007, Best Loss: 0.00713961105794 in iteration 55, Time: 0.14 seconds\n",
      "Iteration 62, Loss: 0.00742733851075, Loss Change: -0.00011847354472, Best Loss: 0.00713961105794 in iteration 55, Time: 0.16 seconds\n",
      "Iteration 63, Loss: 0.00742736412212, Loss Change: 0.00000002561137, Best Loss: 0.00713961105794 in iteration 55, Time: 0.16 seconds\n",
      "Iteration 64, Loss: 0.00803094822913, Loss Change: 0.00060358410701, Best Loss: 0.00713961105794 in iteration 55, Time: 0.16 seconds\n",
      "Iteration 65, Loss: 0.00838645547628, Loss Change: 0.00035550724715, Best Loss: 0.00713961105794 in iteration 55, Time: 0.15 seconds\n",
      "Iteration 66, Loss: 0.00766206951812, Loss Change: -0.00072438595816, Best Loss: 0.00713961105794 in iteration 55, Time: 0.14 seconds\n",
      "Iteration 67, Loss: 0.00735517544672, Loss Change: -0.00030689407140, Best Loss: 0.00713961105794 in iteration 55, Time: 0.16 seconds\n",
      "Iteration 68, Loss: 0.00770931225270, Loss Change: 0.00035413680598, Best Loss: 0.00713961105794 in iteration 55, Time: 0.15 seconds\n",
      "Iteration 69, Loss: 0.00790165085346, Loss Change: 0.00019233860075, Best Loss: 0.00713961105794 in iteration 55, Time: 0.15 seconds\n",
      "Iteration 70, Loss: 0.00708403903991, Loss Change: -0.00081761181355, Best Loss: 0.00708403903991 in iteration 70, Time: 0.16 seconds\n",
      "Iteration 71, Loss: 0.00703286239877, Loss Change: -0.00005117664114, Best Loss: 0.00703286239877 in iteration 71, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 70 has been saved with training loss 0.00708403903991.\n",
      "A best model at iteration 71 has been saved with training loss 0.00703286239877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, Loss: 0.00790215469897, Loss Change: 0.00086929230019, Best Loss: 0.00703286239877 in iteration 71, Time: 0.14 seconds\n",
      "Iteration 73, Loss: 0.00760819250718, Loss Change: -0.00029396219179, Best Loss: 0.00703286239877 in iteration 71, Time: 0.13 seconds\n",
      "Iteration 74, Loss: 0.00728558795527, Loss Change: -0.00032260455191, Best Loss: 0.00703286239877 in iteration 71, Time: 0.14 seconds\n",
      "Iteration 75, Loss: 0.00721293175593, Loss Change: -0.00007265619934, Best Loss: 0.00703286239877 in iteration 71, Time: 0.13 seconds\n",
      "Iteration 76, Loss: 0.00718450639397, Loss Change: -0.00002842536196, Best Loss: 0.00703286239877 in iteration 71, Time: 0.14 seconds\n",
      "Iteration 77, Loss: 0.00675430474803, Loss Change: -0.00043020164594, Best Loss: 0.00675430474803 in iteration 77, Time: 0.15 seconds\n",
      "Iteration 78, Loss: 0.00719233788550, Loss Change: 0.00043803313747, Best Loss: 0.00675430474803 in iteration 77, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 77 has been saved with training loss 0.00675430474803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, Loss: 0.00696803489700, Loss Change: -0.00022430298850, Best Loss: 0.00675430474803 in iteration 77, Time: 0.13 seconds\n",
      "Iteration 80, Loss: 0.00662207510322, Loss Change: -0.00034595979378, Best Loss: 0.00662207510322 in iteration 80, Time: 0.15 seconds\n",
      "Iteration 81, Loss: 0.00735264364630, Loss Change: 0.00073056854308, Best Loss: 0.00662207510322 in iteration 80, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 80 has been saved with training loss 0.00662207510322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, Loss: 0.00671258056536, Loss Change: -0.00064006308094, Best Loss: 0.00662207510322 in iteration 80, Time: 0.14 seconds\n",
      "Iteration 83, Loss: 0.00695636915043, Loss Change: 0.00024378858507, Best Loss: 0.00662207510322 in iteration 80, Time: 0.14 seconds\n",
      "Iteration 84, Loss: 0.00661703106016, Loss Change: -0.00033933809027, Best Loss: 0.00661703106016 in iteration 84, Time: 0.15 seconds\n",
      "Iteration 85, Loss: 0.00651854509488, Loss Change: -0.00009848596528, Best Loss: 0.00651854509488 in iteration 85, Time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 84 has been saved with training loss 0.00661703106016.\n",
      "A best model at iteration 85 has been saved with training loss 0.00651854509488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86, Loss: 0.00650789495558, Loss Change: -0.00001065013930, Best Loss: 0.00650789495558 in iteration 86, Time: 0.16 seconds\n",
      "Iteration 87, Loss: 0.00704494584352, Loss Change: 0.00053705088794, Best Loss: 0.00650789495558 in iteration 86, Time: 0.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 86 has been saved with training loss 0.00650789495558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 88, Loss: 0.00627519330010, Loss Change: -0.00076975254342, Best Loss: 0.00627519330010 in iteration 88, Time: 0.15 seconds\n",
      "Iteration 89, Loss: 0.00672040320933, Loss Change: 0.00044520990923, Best Loss: 0.00627519330010 in iteration 88, Time: 0.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 88 has been saved with training loss 0.00627519330010.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90, Loss: 0.00635515386239, Loss Change: -0.00036524934694, Best Loss: 0.00627519330010 in iteration 88, Time: 0.15 seconds\n",
      "Iteration 91, Loss: 0.00667732208967, Loss Change: 0.00032216822729, Best Loss: 0.00627519330010 in iteration 88, Time: 0.14 seconds\n",
      "Iteration 92, Loss: 0.00648121023551, Loss Change: -0.00019611185417, Best Loss: 0.00627519330010 in iteration 88, Time: 0.18 seconds\n",
      "Iteration 93, Loss: 0.00623409263790, Loss Change: -0.00024711759761, Best Loss: 0.00623409263790 in iteration 93, Time: 0.16 seconds\n",
      "Iteration 94, Loss: 0.00581370200962, Loss Change: -0.00042039062828, Best Loss: 0.00581370200962 in iteration 94, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 93 has been saved with training loss 0.00623409263790.\n",
      "A best model at iteration 94 has been saved with training loss 0.00581370200962.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, Loss: 0.00624383753166, Loss Change: 0.00043013552204, Best Loss: 0.00581370200962 in iteration 94, Time: 0.14 seconds\n",
      "Iteration 96, Loss: 0.00626730220392, Loss Change: 0.00002346467227, Best Loss: 0.00581370200962 in iteration 94, Time: 0.14 seconds\n",
      "Iteration 97, Loss: 0.00660070125014, Loss Change: 0.00033339904621, Best Loss: 0.00581370200962 in iteration 94, Time: 0.14 seconds\n",
      "Iteration 98, Loss: 0.00606952002272, Loss Change: -0.00053118122742, Best Loss: 0.00581370200962 in iteration 94, Time: 0.14 seconds\n",
      "Iteration 99, Loss: 0.00637784367427, Loss Change: 0.00030832365155, Best Loss: 0.00581370200962 in iteration 94, Time: 0.16 seconds\n",
      "Iteration 100, Loss: 0.00634525902569, Loss Change: -0.00003258464858, Best Loss: 0.00581370200962 in iteration 94, Time: 0.21 seconds\n",
      "Iteration 101, Loss: 0.00568639906123, Loss Change: -0.00065885996446, Best Loss: 0.00568639906123 in iteration 101, Time: 0.17 seconds\n",
      "Iteration 102, Loss: 0.00679223285988, Loss Change: 0.00110583379865, Best Loss: 0.00568639906123 in iteration 101, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 101 has been saved with training loss 0.00568639906123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 103, Loss: 0.00615600170568, Loss Change: -0.00063623115420, Best Loss: 0.00568639906123 in iteration 101, Time: 0.15 seconds\n",
      "Iteration 104, Loss: 0.00604227045551, Loss Change: -0.00011373125017, Best Loss: 0.00568639906123 in iteration 101, Time: 0.15 seconds\n",
      "Iteration 105, Loss: 0.00629328843206, Loss Change: 0.00025101797655, Best Loss: 0.00568639906123 in iteration 101, Time: 0.16 seconds\n",
      "Iteration 106, Loss: 0.00557186501101, Loss Change: -0.00072142342106, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 107, Loss: 0.00652088737115, Loss Change: 0.00094902236015, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 106 has been saved with training loss 0.00557186501101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, Loss: 0.00573559431359, Loss Change: -0.00078529305756, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 109, Loss: 0.00637412024662, Loss Change: 0.00063852593303, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n",
      "Iteration 110, Loss: 0.00626013753936, Loss Change: -0.00011398270726, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n",
      "Iteration 111, Loss: 0.00612541008741, Loss Change: -0.00013472745195, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n",
      "Iteration 112, Loss: 0.00572781870142, Loss Change: -0.00039759138599, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n",
      "Iteration 113, Loss: 0.00615361519158, Loss Change: 0.00042579649016, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 114, Loss: 0.00622640922666, Loss Change: 0.00007279403508, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 115, Loss: 0.00576039636508, Loss Change: -0.00046601286158, Best Loss: 0.00557186501101 in iteration 106, Time: 0.16 seconds\n",
      "Iteration 116, Loss: 0.00623409915715, Loss Change: 0.00047370279208, Best Loss: 0.00557186501101 in iteration 106, Time: 0.14 seconds\n",
      "Iteration 117, Loss: 0.00573116680607, Loss Change: -0.00050293235108, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 118, Loss: 0.00611934950575, Loss Change: 0.00038818269968, Best Loss: 0.00557186501101 in iteration 106, Time: 0.15 seconds\n",
      "Iteration 119, Loss: 0.00489762704819, Loss Change: -0.00122172245756, Best Loss: 0.00489762704819 in iteration 119, Time: 0.17 seconds\n",
      "Iteration 120, Loss: 0.00555160269141, Loss Change: 0.00065397564322, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A best model at iteration 119 has been saved with training loss 0.00489762704819.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 121, Loss: 0.00567283853889, Loss Change: 0.00012123584747, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n",
      "Iteration 122, Loss: 0.00588582362980, Loss Change: 0.00021298509091, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n",
      "Iteration 123, Loss: 0.00538795115426, Loss Change: -0.00049787247553, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 124, Loss: 0.00521085597575, Loss Change: -0.00017709517851, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 125, Loss: 0.00542647344992, Loss Change: 0.00021561747417, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 126, Loss: 0.00553544983268, Loss Change: 0.00010897638276, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 127, Loss: 0.00541020324454, Loss Change: -0.00012524658814, Best Loss: 0.00489762704819 in iteration 119, Time: 0.14 seconds\n",
      "Iteration 128, Loss: 0.00571994762868, Loss Change: 0.00030974438414, Best Loss: 0.00489762704819 in iteration 119, Time: 0.18 seconds\n",
      "Iteration 129, Loss: 0.00556079577655, Loss Change: -0.00015915185213, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 130, Loss: 0.00562962191179, Loss Change: 0.00006882613525, Best Loss: 0.00489762704819 in iteration 119, Time: 0.17 seconds\n",
      "Iteration 131, Loss: 0.00543650425971, Loss Change: -0.00019311765209, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 132, Loss: 0.00526391575113, Loss Change: -0.00017258850858, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 133, Loss: 0.00535421026871, Loss Change: 0.00009029451758, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n",
      "Iteration 134, Loss: 0.00517424335703, Loss Change: -0.00017996691167, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n",
      "Iteration 135, Loss: 0.00550024304539, Loss Change: 0.00032599968836, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n",
      "Iteration 136, Loss: 0.00498630898073, Loss Change: -0.00051393406466, Best Loss: 0.00489762704819 in iteration 119, Time: 0.15 seconds\n",
      "Iteration 137, Loss: 0.00495288753882, Loss Change: -0.00003342144191, Best Loss: 0.00489762704819 in iteration 119, Time: 0.16 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m outputs_train \u001b[38;5;241m=\u001b[39m model(input1_batch, input2_batch)\n\u001b[1;32m     21\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss_fn(outputs_train, target_batch, boundary_parameter, initial_parameter, total_temporal_steps, n_points)\n\u001b[0;32m---> 22\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "loss_list = []\n",
    "time_list = []\n",
    "loss_val_best = float('inf')\n",
    "loss_val_prev = 0\n",
    "best_iter = 0\n",
    "# model_best = model.state_dict().copy()\n",
    "\n",
    "model_params_best = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-best.pth\"\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for iteration in range(iterations):\n",
    "    input1_batch, input2_batch, target_batch  = next(train_batch)\n",
    "    input1_batch = input1_batch.to(device)\n",
    "    input2_batch = input2_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs_train = model(input1_batch, input2_batch)\n",
    "    loss_train = loss_fn(outputs_train, target_batch, boundary_parameter, initial_parameter, total_temporal_steps, n_points)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input1_batch, input2_batch, target_batch  = next(validation_batch)\n",
    "        input1_batch = input1_batch.to(device)\n",
    "        input2_batch = input2_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        outputs_val = model(input1_batch, input2_batch)\n",
    "        loss_val = loss_fn(outputs_val, target_batch, boundary_parameter, initial_parameter, total_temporal_steps, n_points)\n",
    "        loss_list.append(loss_val.item())\n",
    "\n",
    "        if loss_val.item()<loss_val_best:\n",
    "            loss_val_best = loss_val.item()\n",
    "            best_iter = iteration\n",
    "            torch.save(model.state_dict(), model_params_best)\n",
    "            print(f\"A best model at iteration {iteration+1} has been saved with training loss {loss_val_best:.14f}.\", file=sys.stderr)\n",
    "\n",
    "    iter_time = time.time() - start_time  # Calculate the elapsed time\n",
    "    time_list.append(iter_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "    if True or iteration%10==9:\n",
    "        print(f\"Iteration {iteration+1}, Loss: {loss_val:.14f}, Loss Change: {loss_val - loss_val_prev:.14f}, Best Loss: {loss_val_best:.14f} in iteration {best_iter+1}, Time: {iter_time:.2f} seconds\")\n",
    "    loss_val_prev = loss_val\n",
    "\n",
    "    if iteration%100==99:\n",
    "        # 保存损失值和模型\n",
    "        training_loss_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-final.npy\"\n",
    "        model_params_final = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-final.pth\"\n",
    "        np.save(training_loss_list, np.array(loss_list))\n",
    "        torch.save(model.state_dict(), model_params_final)\n",
    "        # print(f\"Model saving checkpoint: the model trained after iteration {iteration+1} has been saved with the training losses.\", file=sys.stderr)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df781e2b09174ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:09:20.241011Z",
     "start_time": "2025-01-14T04:09:20.219605Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存损失值和模型\n",
    "training_loss_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-final.npy\"\n",
    "training_time_list = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-time.npy\"\n",
    "model_params_final = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-final.pth\"\n",
    "\n",
    "np.save(training_loss_list, np.array(loss_list)) # Here, we save the loss list on the validation set\n",
    "np.save(training_time_list, np.array(time_list))\n",
    "torch.save(model.state_dict(), model_params_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f52988f2a73f329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:09:22.647765Z",
     "start_time": "2025-01-14T04:09:22.632976Z"
    }
   },
   "outputs": [],
   "source": [
    "u_test = u_expanded[val_test_split_index:]\n",
    "y_test = y_expanded[val_test_split_index:]\n",
    "s_test = s_expanded[val_test_split_index:]\n",
    "\n",
    "u_test_combined = u_test.reshape(-1, u_test.shape[-1])\n",
    "y_test_combined = y_test.reshape(-1, y_test.shape[-1])\n",
    "s_test_combined = s_test.reshape(-1, s_test.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f2df8e65e652ecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:09:24.211660Z",
     "start_time": "2025-01-14T04:09:24.207944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 4080400 samples, while the train_loader has 511 batches.\n",
      "The validation dataset has 510050 samples, while the validation_loader has 64 batches.\n",
      "The testing dataset has 510050 samples, while the test_loader has 64 batches.\n"
     ]
    }
   ],
   "source": [
    "from utilities.tools import CustomDataset as CustomDataset\n",
    "\n",
    "train_set = CustomDataset(u_train_combined, y_train_combined, s_train_combined)\n",
    "validation_set = CustomDataset(u_val_combined, y_val_combined, s_val_combined)\n",
    "test_set = CustomDataset(u_test_combined, y_test_combined, s_test_combined)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "validation_data = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_data = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "print(f\"The training dataset has {len(train_set)} samples, while the train_loader has {len(train_data)} batches.\")\n",
    "print(f\"The validation dataset has {len(validation_set)} samples, while the validation_loader has {len(validation_data)} batches.\")\n",
    "print(f\"The testing dataset has {len(test_set)} samples, while the test_loader has {len(test_data)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2acf165b01f82a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:09:26.524685Z",
     "start_time": "2025-01-14T04:09:26.521786Z"
    }
   },
   "outputs": [],
   "source": [
    "# # In this cell, we compute training and testing loss for the model\n",
    "def compute_loss(model, data_loader, device, description=\"Computing loss\"):\n",
    "    \"\"\"\n",
    "    Compute the loss for a given dataset using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - data_loader: DataLoader for the dataset (train or test).\n",
    "    - device: Device to run the computations on (CPU or GPU).\n",
    "    - description: Description for the tqdm progress bar.\n",
    "\n",
    "    Returns:\n",
    "    - average_loss: The average loss over the dataset.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input1_batch, input2_batch, target_batch in data_loader:\n",
    "            input1_batch = input1_batch.to(device)\n",
    "            input2_batch = input2_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            outputs = model(input1_batch, input2_batch)\n",
    "            loss = loss_fn(outputs, target_batch, boundary_parameter, initial_parameter, total_temporal_steps, n_points)\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f70f1aeac644ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:10:41.804319Z",
     "start_time": "2025-01-14T04:09:28.565283Z"
    }
   },
   "outputs": [],
   "source": [
    "training_loss_best = f\"{problem}_Var{var}_Visc{visc}_Struct{struct}_Sensor{n_points}_Boundary{boundary_parameter}_Initial{initial_parameter}_Batch{batch_size}-best.loss\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_params_best, map_location=torch.device(device), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "train_loss = compute_loss(model, train_data, device, description=\"Computing training loss\")\n",
    "validation_loss = compute_loss(model, validation_data, device, description=\"Computing validation loss\")\n",
    "test_loss = compute_loss(model, test_data, device, description=\"Computing testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec09faf72e752ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:10:41.891931Z",
     "start_time": "2025-01-14T04:10:41.885492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the best parameters:\n",
      "the training loss is 0.00564355014891 ,\n",
      " the validation loss is 0.00556408984107,\n",
      " the testing loss is 0.00565794745216.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With the best parameters:\\n\"\n",
    "        f\"the training loss is {train_loss:.14f} ,\\n\",\n",
    "        f\"the validation loss is {validation_loss:.14f},\\n\",\n",
    "        f\"the testing loss is {test_loss:.14f}.\")\n",
    "\n",
    "with open(training_loss_best, 'w') as f:\n",
    "    f.write(\"With the best parameters, the losses for the model are as follows:\\n\")\n",
    "    f.write(f\"Training loss: {train_loss:.14f}\\n\")\n",
    "    f.write(f\"Validation loss: {validation_loss:.14f}\\n\")\n",
    "    f.write(f\"Testing loss: {test_loss:.14f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
